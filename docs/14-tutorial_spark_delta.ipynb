{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tutorial Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# %%\n",
    "# Configure Spark session with Delta Lake dependencies\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Delta Lake\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### 1. Create a Simple Dataset\n",
    "\n",
    "\n",
    "\n",
    " To start with, let's create a simple dataset consisting of letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create a simple dataset\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 4), (\"e\", 5)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"letter\", \"number\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Write Data to a Delta Lake Table\n",
    "\n",
    "\n",
    "\n",
    " Now, we'll write the data to a Delta Lake table named `letters.delta`. Delta Lake provides ACID transactions and time travel capabilities, making it suitable for reliable data storage and versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Write the data to a Delta Lake table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"data/letters.delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Read Data from the Delta Lake Table\n",
    "\n",
    "\n",
    "\n",
    " Let's read the data back from the Delta Lake table and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     d|     4|\n",
      "|     c|     3|\n",
      "|     e|     5|\n",
      "|     b|     2|\n",
      "|     a|     1|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Read the data from the Delta Lake table\n",
    "df = spark.read.format(\"delta\").load(\"data/letters.delta\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4. Append Data to the Delta Lake Table\n",
    "\n",
    "\n",
    "\n",
    " We can append new data to the existing Delta Lake table without affecting the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2024-07-03 14:01:52.167|NULL  |NULL    |WRITE    |{mode -> Append, partitionBy -> []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 1809}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Append data to the Delta Lake table\n",
    "new_data = [(\"f\", 6), (\"g\", 7)]\n",
    "\n",
    "df_new = spark.createDataFrame(new_data, [\"letter\", \"number\"])\n",
    "df_new.write.format(\"delta\").mode(\"append\").save(\"data/letters.delta\")\n",
    "\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\n",
    "history_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     d|     4|\n",
      "|     c|     3|\n",
      "|     g|     7|\n",
      "|     f|     6|\n",
      "|     e|     5|\n",
      "|     b|     2|\n",
      "|     a|     1|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Read the Delta Lake table with appended data\n",
    "df = spark.read.format(\"delta\").load(\"data/letters.delta\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5. Update Data in the Delta Lake Table\n",
    "\n",
    "\n",
    "\n",
    " Delta Lake supports updating data in place. Let's update the data to set the number to 100 where the letter is 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Update data in the Delta Lake table\n",
    "df = spark.read.format(\"delta\").load(\"data/letters.delta\")\n",
    "\n",
    "# Update the data\n",
    "df = df.withColumn(\n",
    "    \"number\", F.when(F.col(\"letter\") == \"a\", 100).otherwise(F.col(\"number\"))\n",
    ")\n",
    "\n",
    "# Overwrite the Delta Lake table with updated data\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"data/letters.delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|2      |2024-07-03 14:02:04.829|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -> 7, numOutputRows -> 7, numOutputBytes -> 4963}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "|1      |2024-07-03 14:01:52.167|NULL  |NULL    |WRITE    |{mode -> Append, partitionBy -> []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 1809}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6. Read Data from a Specific Version of the Delta Lake Table\n",
    "\n",
    "\n",
    "\n",
    " Delta Lake maintains a version history of changes. Let's retrieve data from a specific version of the Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     d|     4|\n",
      "|     c|     3|\n",
      "|     g|     7|\n",
      "|     f|     6|\n",
      "|     e|     5|\n",
      "|     b|     2|\n",
      "|     a|     1|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Read data from a specific version of the Delta Lake table\n",
    "df_version1 = (\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"data/letters.delta\")\n",
    ")\n",
    "df_version1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7. Get History of Changes Made to the Delta Lake Table\n",
    "\n",
    "\n",
    "\n",
    " Delta Lake allows us to view the history of changes made to the table, which is useful for auditing and understanding data evolution over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|2      |2024-07-03 14:02:04.829|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -> 7, numOutputRows -> 7, numOutputBytes -> 4963}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "|1      |2024-07-03 14:01:52.167|NULL  |NULL    |WRITE    |{mode -> Append, partitionBy -> []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 1809}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 6, numOutputRows -> 5, numOutputBytes -> 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Get the history of changes made to the Delta Lake table\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\n",
    "history_df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
