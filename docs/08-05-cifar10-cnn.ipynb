{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99425f64",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes utilizando redes neuronales convolucionales en PyTorch\n",
    "\n",
    "### Parte 5 de \"Aprendizaje profundo con Pytorch: de cero a GAN\"\n",
    "\n",
    "Esta serie de tutoriales es una introducción práctica y sencilla para principiantes al aprendizaje profundo utilizando [PyTorch](https://pytorch.org), una biblioteca de redes neuronales de código abierto. Estos tutoriales adoptan un enfoque práctico y centrado en la codificación. La mejor manera de aprender el material es ejecutar el código y experimentar con él usted mismo. Mira la serie completa aquí:\n",
    "\n",
    "1. [Conceptos básicos de PyTorch: tensores y degradados] (https://jovian.ai/aakashns/01-pytorch-basics)\n",
    "2. [Descenso de gradiente y regresión lineal](https://jovian.ai/aakashns/02-linear-regression)\n",
    "3. [Trabajar con imágenes y regresión logística](https://jovian.ai/aakashns/03-logistic-regression) \n",
    "4. [Entrenamiento de redes neuronales profundas en una GPU](https://jovian.ai/aakashns/04-feedforward-nn)\n",
    "5. [Clasificación de imágenes mediante redes neuronales convolucionales] (https://jovian.ai/aakashns/05-cifar10-cnn)\n",
    "6. [Aumento de datos, regularización y ResNets](https://jovian.ai/aakashns/05b-cifar10-resnet)\n",
    "7. [Generación de imágenes mediante redes generativas adversarias](https://jovian.ai/aakashns/06b-anime-dcgan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef486c",
   "metadata": {},
   "source": [
    "Este tutorial cubre los siguientes temas: \n",
    "\n",
    "- Descarga de un conjunto de datos de imágenes desde la URL web\n",
    "- Comprensión de las capas de convolución y agrupación.\n",
    "- Creación de una red neuronal convolucional (CNN) usando PyTorch\n",
    "- Entrenamiento de una CNN desde cero y seguimiento del rendimiento.\n",
    "- Underfitting, overfitting y cómo superarlos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438d16b",
   "metadata": {},
   "source": [
    "### Cómo ejecutar el código\n",
    "\n",
    "Este tutorial es un ejecutable [Jupyter notebook](https://jupyter.org) alojado en [Jovian](https://www.jovian.ai). Puede _ejecutar_ este tutorial y experimentar con los ejemplos de código de dos maneras: *usando recursos gratuitos en línea* (recomendado) o *en su computadora*.\n",
    "\n",
    "#### Opción 1: Ejecutar usando recursos en línea gratuitos (1 clic, recomendado)\n",
    "\n",
    "La forma más sencilla de comenzar a ejecutar el código es hacer clic en el botón **Ejecutar** en la parte superior de esta página y seleccionar **Ejecutar en Colab**. [Google Colab](https://colab.research.google.com) es una plataforma en línea gratuita para ejecutar portátiles Jupyter utilizando la infraestructura en la nube de Google. También puede seleccionar \"Ejecutar en Binder\" o \"Ejecutar en Kaggle\" si tiene problemas al ejecutar el cuaderno en Google Colab. \n",
    "\n",
    "\n",
    "#### Opción 2: ejecutar en su computadora localmente\n",
    "\n",
    "Para ejecutar el código en su computadora localmente, deberá configurar [Python](https://www.python.org), descargar el cuaderno e instalar las bibliotecas necesarias. Recomendamos utilizar la distribución [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) de Python. Haga clic en el botón **Ejecutar** en la parte superior de esta página, seleccione la opción **Ejecutar localmente** y siga las instrucciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8bf8cd",
   "metadata": {},
   "source": [
    "### Usando una GPU para un entrenamiento más rápido\n",
    "\n",
    "Puede utilizar una [Unidad de procesamiento de gráficos](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) para entrenar sus modelos más rápido si su plataforma de ejecución está conectada a una GPU fabricada por NVIDIA. Siga estas instrucciones para usar una GPU en la plataforma de su elección:\n",
    "\n",
    "* _Google Colab_: utilice la opción de menú \"Tiempo de ejecución > Cambiar tipo de tiempo de ejecución\" y seleccione \"GPU\" en el menú desplegable \"Acelerador de hardware\".\n",
    "* _Kaggle_: En la sección \"Configuración\" de la barra lateral, seleccione \"GPU\" en el menú desplegable \"Acelerador\". Utilice el botón en la parte superior derecha para abrir la barra lateral.\n",
    "* _Binder_: Las computadoras portátiles que ejecutan Binder no pueden usar una GPU, ya que las máquinas que alimentan Binder no están conectadas a ninguna GPU.\n",
    "* _Linux_: Si su computadora portátil/escritorio tiene una GPU (tarjeta gráfica) NVIDIA, asegúrese de haber instalado los [controladores NVIDIA CUDA] (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index .html).\n",
    "* _Windows_: si su computadora portátil/escritorio tiene una GPU (tarjeta gráfica) NVIDIA, asegúrese de haber instalado los [controladores NVIDIA CUDA] (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows /index.html).\n",
    "* _macOS_: macOS no es compatible con las GPU NVIDIA\n",
    "\n",
    "\n",
    "Si no tiene acceso a una GPU o no está seguro de cuál es, no se preocupe, puede ejecutar todo el código de este tutorial sin una GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269531b9",
   "metadata": {},
   "source": [
    "## Explorando el conjunto de datos CIFAR10\n",
    "\n",
    "En el [tutorial anterior](https://jovian.ml/aakashns/04-feedforward-nn), entrenamos redes neuronales feedfoward con una única capa oculta para clasificar dígitos escritos a mano del [conjunto de datos MNIST](http:// yann.lecun.com/exdb/mnist) con más del 97% de precisión. Para este tutorial, usaremos el conjunto de datos CIFAR10, que consta de 60000 imágenes en color de 32x32 px en 10 clases. Aquí hay algunas imágenes de muestra del conjunto de datos:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/709/1*LyV7_xga4jUHdx4_jHk1PQ.png\" estilo=\"ancho-máximo: 480px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc462bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente y ejecute el comando apropiado para su sistema operativo, si es necesario\n",
    "\n",
    "# Linux/Binder/Windows (sin GPU)\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Linux/Windows (GPU)\n",
    "# pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    " \n",
    "# MacOS (NO GPU)\n",
    "# !pip instalar numpy matplotlib antorcha torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33825efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='05-cifar10-cnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae5950",
   "metadata": {},
   "source": [
    "Descargaremos las imágenes en formato PNG desde [esta página](https://course.fast.ai/datasets), usando algunas funciones auxiliares de los paquetes `torchvision` y `tarfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92196ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el conjunto de datos\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23539f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer del archivo\n",
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6964a",
   "metadata": {},
   "source": [
    "El conjunto de datos se extrae al directorio `data/cifar10`. Contiene 2 carpetas \"train\" y \"test\", que contienen el conjunto de entrenamiento (50000 imágenes) y el conjunto de prueba (10000 imágenes) respectivamente. Cada uno de ellos contiene 10 carpetas, una para cada clase de imágenes. Verifiquemos esto usando `os.listdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f75204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/cifar10'\n",
    "\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbfbf2",
   "metadata": {},
   "source": [
    "Miremos dentro de un par de carpetas, una del conjunto de entrenamiento y otra del conjunto de prueba. Como ejercicio, puedes comprobar que hay el mismo número de imágenes para cada clase, 5000 en el conjunto de entrenamiento y 1000 en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "airplane_files = os.listdir(data_dir + \"/train/airplane\")\n",
    "print('No. of training examples for airplanes:', len(airplane_files))\n",
    "print(airplane_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8680ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_test_files = os.listdir(data_dir + \"/test/ship\")\n",
    "print(\"No. of test examples for ship:\", len(ship_test_files))\n",
    "print(ship_test_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c241b8",
   "metadata": {},
   "source": [
    "Muchos conjuntos de datos de visión por computadora utilizan la estructura de directorios anterior (una carpeta por clase), y la mayoría de las bibliotecas de aprendizaje profundo proporcionan utilidades para trabajar con dichos conjuntos de datos. Podemos usar la clase `ImageFolder` de `torchvision` para cargar los datos como tensores de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1279669",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de14449",
   "metadata": {},
   "source": [
    "Veamos un elemento de muestra del conjunto de datos de entrenamiento. Cada elemento es una tupla que contiene un tensor de imagen y una etiqueta. Dado que los datos constan de imágenes en color de 32x32 px con 3 canales (RGB), cada tensor de imagen tiene la forma \"(3, 32, 32)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = dataset[0]\n",
    "print(img.shape, label)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf07e7e",
   "metadata": {},
   "source": [
    "La lista de clases se almacena en la propiedad `.classes` del conjunto de datos. La etiqueta numérica de cada elemento corresponde al índice de la etiqueta del elemento en la lista de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a71225",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2756569",
   "metadata": {},
   "source": [
    "Podemos ver la imagen usando `matplotlib`, pero necesitamos cambiar las dimensiones del tensor a `(32,32,3)`. Creemos una función auxiliar para mostrar una imagen y su etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af134fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac37247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_example(img, label):\n",
    "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bb94c",
   "metadata": {},
   "source": [
    "Veamos un par de imágenes del conjunto de datos. Como puedes ver, las imágenes de 32x32px son bastante difíciles de identificar, incluso para el ojo humano. Intente cambiar los índices a continuación para ver imágenes diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15441185",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(*dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(*dataset[1099])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b0a1f",
   "metadata": {},
   "source": [
    "### Guarda y sube tu libreta\n",
    "\n",
    "Ya sea que esté ejecutando este cuaderno Jupyter en línea o en su computadora, es esencial guardar su trabajo de vez en cuando. Puede continuar trabajando en un cuaderno guardado más tarde o compartirlo con amigos y colegas para permitirles ejecutar su código. [Jovian](https://jovian.ai/platform-features) ofrece una forma sencilla de guardar y compartir sus cuadernos de Jupyter en línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1ecef",
   "metadata": {},
   "source": [
    "`jovian.commit` carga el cuaderno en su cuenta Jovian, captura el entorno Python y crea un enlace para compartir para su cuaderno, como se muestra arriba. Puede utilizar este enlace para compartir su trabajo y permitir que cualquiera (incluido usted) ejecute sus cuadernos y reproduzca su trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cad7fc",
   "metadata": {},
   "source": [
    "## Conjuntos de datos de capacitación y validación\n",
    "\n",
    "Al crear modelos de aprendizaje automático del mundo real, es bastante común dividir el conjunto de datos en 3 partes:\n",
    "\n",
    "1. **Conjunto de entrenamiento**: se utiliza para entrenar el modelo, es decir, calcular la pérdida y ajustar los pesos del modelo mediante el descenso de gradiente.\n",
    "2. **Conjunto de validación**: se utiliza para evaluar el modelo durante el entrenamiento, ajustar los hiperparámetros (tasa de aprendizaje, etc.) y elegir la mejor versión del modelo.\n",
    "3. **Conjunto de pruebas**: se utiliza para comparar diferentes modelos o diferentes tipos de enfoques de modelado e informar la precisión final del modelo.\n",
    "\n",
    "Dado que no hay un conjunto de validación predefinido, podemos reservar una pequeña porción (5000 imágenes) del conjunto de entrenamiento para usarla como conjunto de validación. Usaremos el método auxiliar `random_split` de PyTorch para hacer esto. Para garantizar que siempre creemos el mismo conjunto de validación, también estableceremos una semilla para el generador de números aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ee799",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43515a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a035b",
   "metadata": {},
   "source": [
    "La biblioteca \"jovian\" también proporciona una API sencilla para registrar parámetros importantes relacionados con el conjunto de datos, el entrenamiento del modelo, los resultados, etc. para facilitar la referencia y comparación entre múltiples experimentos. Registremos `dataset_url`, `val_pct` y `rand_seed` usando `jovian.log_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ccbc8",
   "metadata": {},
   "source": [
    "Ahora podemos crear cargadores de datos para entrenamiento y validación, para cargar los datos en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b319c44",
   "metadata": {},
   "source": [
    "Podemos ver lotes de imágenes del conjunto de datos usando el método `make_grid` de `torchvision`. Cada vez que se ejecuta el siguiente código, obtenemos un bach diferente, ya que el muestreador mezcla los índices antes de crear lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80aabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ebdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49715ef9",
   "metadata": {},
   "source": [
    "Una vez más, guardemos y confirmemos nuestro trabajo usando \"jovian\" antes de continuar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73ec82",
   "metadata": {},
   "source": [
    "Después de la primera confirmación, todas las confirmaciones posteriores registran una nueva versión del cuaderno dentro del mismo proyecto joviano. Puede usar `jovian.commit` para versionar los cuadernos de Jupyter (en lugar de hacer `Archivo > Guardar como`) y mantener organizados sus proyectos de ciencia de datos. Consulte también la pestaña [**Records**](https://jovian.ml/aakashns/05-cifar10-cnn/v/2/records) en la página del proyecto para ver cómo se registra la información usando `jovian.log_dataset `aparece en la interfaz de usuario.\n",
    "\n",
    "<a href=\"https://jovian.ml/aakashns/05-cifar10-cnn/v/2/records\"><img src=\"https://i.imgur.com/h0zkmn9.png\" style=\" ancho:400px\" >>a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb959c2",
   "metadata": {},
   "source": [
    "## Definición del modelo (red neuronal convolucional)\n",
    "\n",
    "En nuestro [tutorial anterior] (https://jovian.ml/aakashns/04-feedforward-nn), definimos una red neuronal profunda con capas completamente conectadas usando `nn.Linear`. Sin embargo, para este tutorial usaremos una red neuronal convolucional, utilizando la clase `nn.Conv2d` de PyTorch.\n",
    "\n",
    "> La convolución 2D es una operación bastante simple en el fondo: se comienza con un núcleo, que es simplemente una pequeña matriz de pesos. Este núcleo se \"desliza\" sobre los datos de entrada 2D, realiza una multiplicación por elementos con la parte de la entrada en la que se encuentra actualmente y luego resume los resultados en un solo píxel de salida. - [Fuente](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"ancho máximo:400px;\">\n",
    "\n",
    "\n",
    "Implementemos una operación de convolución en una imagen de 1 canal con un núcleo de 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3454afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(image, kernel):\n",
    "    ri, ci = image.shape       # image dimensions\n",
    "    rk, ck = kernel.shape      # kernel dimensions\n",
    "    ro, co = ri-rk+1, ci-ck+1  # output dimensions\n",
    "    output = torch.zeros([ro, co])\n",
    "    for i in range(ro): \n",
    "        for j in range(co):\n",
    "            output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = torch.tensor([\n",
    "    [3, 3, 2, 1, 0], \n",
    "    [0, 0, 1, 3, 1], \n",
    "    [3, 1, 2, 2, 3], \n",
    "    [2, 0, 0, 2, 2], \n",
    "    [2, 0, 0, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "sample_kernel = torch.tensor([\n",
    "    [0, 1, 2], \n",
    "    [2, 2, 0], \n",
    "    [0, 1, 2]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "apply_kernel(sample_image, sample_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3d793",
   "metadata": {},
   "source": [
    "Para imágenes multicanal, se aplica un núcleo diferente a cada canal y las salidas se suman por píxeles. \n",
    "\n",
    "Consulte los siguientes artículos para comprender mejor las convoluciones:\n",
    "\n",
    "1. [Comprensión intuitiva de las convoluciones para el aprendizaje profundo] (https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) por Irhum Shafkat\n",
    "2. [Convoluciones en profundidad] (https://sgugger.github.io/convolution-in- Depth.html) de Sylvian Gugger (este artículo implementa convoluciones desde cero)\n",
    "\n",
    "Existen ciertas ventajas que ofrecen las capas convolucionales cuando se trabaja con datos de imágenes:\n",
    "\n",
    "* **Menos parámetros**: se utiliza un pequeño conjunto de parámetros (el núcleo) para calcular los resultados de toda la imagen, por lo que el modelo tiene muchos menos parámetros en comparación con una capa completamente conectada. \n",
    "* **Escasez de conexiones**: en cada capa, cada elemento de salida solo depende de una pequeña cantidad de elementos de entrada, lo que hace que los pases hacia adelante y hacia atrás sean más eficientes.\n",
    "* **Compartición de parámetros e invariancia espacial**: las características aprendidas por un núcleo en una parte de la imagen se pueden usar para detectar patrones similares en una parte diferente de otra imagen.\n",
    "\n",
    "También usaremos capas de [max-pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) para disminuir progresivamente la altura y el ancho de los tensores de salida de cada capa convolucional.\n",
    "\n",
    "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" style=\"max-width:400px;\">\n",
    "\n",
    "Antes de definir el modelo completo, veamos cómo opera en los datos una única capa convolucional seguida de una capa de agrupación máxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29102856",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(2, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae83e63",
   "metadata": {},
   "source": [
    "Consulte la [publicación de Sylvian] (https://sgugger.github.io/convolution-in- Depth.html) para obtener una explicación de `kernel_size`, `stride` y `padding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dl:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = simple_model(images)\n",
    "    print('out.shape:', out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a65ac8",
   "metadata": {},
   "source": [
    "La capa `Conv2d` transforma una imagen de 3 canales en un *mapa de características* de 16 canales, y la capa `MaxPool2d` reduce a la mitad la altura y el ancho. El mapa de características se hace más pequeño a medida que agregamos más capas, hasta que finalmente nos queda un mapa de características pequeño, que se puede aplanar en un vector. Luego podemos agregar algunas capas completamente conectadas al final para obtener un vector de tamaño 10 para cada imagen.\n",
    "\n",
    "<img src=\"https://i.imgur.com/KKtPOKE.png\" estilo=\"ancho-máximo: 540px\">\n",
    "\n",
    "Definamos el modelo extendiendo una clase `ImageClassificationBase` que contiene métodos auxiliares para entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e07e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d5c01",
   "metadata": {},
   "source": [
    "Usaremos `nn.Sequential` para encadenar las capas y funciones de activación en una única arquitectura de red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671266b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10CnnModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b2c78",
   "metadata": {},
   "source": [
    "Verifiquemos que el modelo produzca el resultado esperado en un lote de datos de entrenamiento. Las 10 salidas para cada imagen se pueden interpretar como probabilidades para las 10 clases objetivo (después de aplicar softmax), y la clase con la probabilidad más alta se elige como la etiqueta predicha por el modelo para la imagen de entrada. Consulte la [Parte 3 (regresión logística)](https://jovian.ml/aakashns/03-logistic-regression#C50) para obtener una discusión más detallada sobre cómo interpretar los resultados, aplicar softmax e identificar las etiquetas predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0733df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dl:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = model(images)\n",
    "    print('out.shape:', out.shape)\n",
    "    print('out[0]:', out[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412dbe1",
   "metadata": {},
   "source": [
    "Para usar sin problemas una GPU, si hay una disponible, definimos un par de funciones auxiliares (`get_default_device` y `to_device`) y una clase auxiliar `DeviceDataLoader` para mover nuestro modelo y datos a la GPU según sea necesario. Estos se describen con más detalle en el [tutorial anterior](https://jovian.ml/aakshns/04-feedforward-nn#C21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66767ec5",
   "metadata": {},
   "source": [
    "Según dónde esté ejecutando esta computadora portátil, su dispositivo predeterminado podría ser una CPU (`torch.device('cpu')`) o una GPU (`torch.device('cuda')`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2b7e8",
   "metadata": {},
   "source": [
    "Ahora podemos empaquetar nuestros cargadores de datos de entrenamiento y validación usando `DeviceDataLoader` para transferir automáticamente lotes de datos a la GPU (si está disponible) y usar `to_device` para mover nuestro modelo a la GPU (si está disponible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d86b5e",
   "metadata": {},
   "source": [
    "Una vez más, guardemos y confirmemos el cuaderno antes de continuar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84662b85",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "Definiremos dos funciones: \"ajustar\" y \"evaluar\" para entrenar el modelo usando el descenso de gradiente y evaluar su desempeño en el conjunto de validación. Para obtener un tutorial detallado de estas funciones, consulte el [tutorial anterior](https://jovian.ai/aakashns/03-logistic-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5300d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7b3e7",
   "metadata": {},
   "source": [
    "Antes de comenzar a entrenar, creemos una instancia del modelo una vez más y veamos cómo se desempeña en el conjunto de validación con el conjunto inicial de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cc8d6",
   "metadata": {},
   "source": [
    "La precisión inicial es de alrededor del 10%, que es lo que uno podría esperar de un modelo inicializado aleatoriamente (ya que tiene una probabilidad de 1 entre 10 de obtener una etiqueta correcta al adivinar al azar).\n",
    "\n",
    "Usaremos los siguientes *hiperparámetros* (tasa de aprendizaje, número de épocas, tamaño de lote, etc.) para entrenar nuestro modelo. Como ejercicio, puedes intentar cambiarlos para ver si logras una mayor precisión en menos tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd531afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568e335",
   "metadata": {},
   "source": [
    "Es importante registrar los hiperparámetros de cada experimento que realice, para replicarlo más tarde y compararlo con otros experimentos. Podemos grabarlos usando `jovian.log_hyperparams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdf5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.reset()\n",
    "jovian.log_hyperparams({\n",
    "    'num_epochs': num_epochs,\n",
    "    'opt_func': opt_func.__name__,\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f98c41",
   "metadata": {},
   "source": [
    "Así como hemos registrado los hiperparámetros, también podemos registrar las métricas finales logradas por el modelo usando `jovian.log_metrics` como referencia, análisis y comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f33018",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.log_metrics(train_loss=history[-1]['train_loss'], \n",
    "                   val_loss=history[-1]['val_loss'], \n",
    "                   val_acc=history[-1]['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf062c45",
   "metadata": {},
   "source": [
    "También podemos trazar las precisiones del conjunto de validación para estudiar cómo mejora el modelo con el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61227974",
   "metadata": {},
   "source": [
    "Nuestro modelo alcanza una precisión de alrededor del 75% y, al observar el gráfico, parece poco probable que el modelo alcance una precisión superior al 80% incluso después de un entrenamiento prolongado. Esto sugiere que es posible que necesitemos utilizar un modelo más potente para capturar la relación entre las imágenes y las etiquetas con mayor precisión. Esto se puede hacer agregando más capas convolucionales a nuestro modelo o aumentando el número. de canales en cada capa convolucional, o mediante el uso de técnicas de regularización.\n",
    "\n",
    "También podemos trazar las pérdidas de entrenamiento y validación para estudiar la tendencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c83c1",
   "metadata": {},
   "source": [
    "Inicialmente, tanto las pérdidas de formación como las de validación parecen disminuir con el tiempo. Sin embargo, si entrena el modelo durante el tiempo suficiente, notará que la pérdida de entrenamiento continúa disminuyendo, mientras que la pérdida de validación deja de disminuir e incluso comienza a aumentar después de cierto punto. \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/1QU0m.png\" estilo=\"ancho máximo: 400px;\">\n",
    "\n",
    "Este fenómeno se llama **sobreajuste** y es el no. 1 por qué muchos modelos de aprendizaje automático dan resultados bastante terribles con datos del mundo real. Esto sucede porque el modelo, en un intento por minimizar la pérdida, comienza a aprender patrones que son exclusivos de los datos de entrenamiento, a veces incluso memorizando ejemplos de entrenamiento específicos. Debido a esto, el modelo no se generaliza bien a datos nunca antes vistos.\n",
    "\n",
    "\n",
    "A continuación se presentan algunas estrategias comunes para evitar el sobreajuste:\n",
    "\n",
    "- Recopilar y generar más datos de entrenamiento o agregarles ruido.\n",
    "- Uso de técnicas de regularización como normalización y abandono por lotes.\n",
    "- Detención anticipada del entrenamiento del modelo, cuando la pérdida de validación comienza a aumentar\n",
    "\n",
    "Cubriremos estos temas con más detalle en el próximo tutorial de esta serie y aprenderemos cómo podemos alcanzar una precisión de **más del 90 %** realizando cambios menores pero importantes en nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4903e",
   "metadata": {},
   "source": [
    "Antes de continuar, guardemos nuestro trabajo en la nube usando `jovian.commit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f7f1c",
   "metadata": {},
   "source": [
    "Cuando prueba diferentes experimentos (cambiando la tasa de aprendizaje, el tamaño del lote, el optimizador, etc.) y registra hiperparámetros y métricas con cada versión de su computadora portátil, puede usar [**Comparar**](https://jovian.ml /aakshns/05-cifar10-cnn/compare) en la página del proyecto para analizar qué enfoques están funcionando bien y cuáles no. Usted ordena/filtra por precisión, pérdida, etc., agrega notas para cada versión e incluso invita a colaboradores a contribuir a su proyecto con sus propios experimentos.\n",
    "\n",
    "<a href=\"https://jovian.ml/aakashns/05-cifar10-cnn\"><img src=\"https://i.imgur.com/p1Z3vgN.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d830eb7",
   "metadata": {},
   "source": [
    "## Pruebas con imágenes individuales\n",
    "\n",
    "Si bien hasta ahora hemos estado rastreando la precisión general de un modelo, también es una buena idea observar los resultados del modelo en algunas imágenes de muestra. Probemos nuestro modelo con algunas imágenes del conjunto de datos de prueba predefinido de 10000 imágenes. Comenzamos creando un conjunto de datos de prueba usando la clase `ImageFolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d396d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486debdb",
   "metadata": {},
   "source": [
    "Definamos una función auxiliar `predict_image`, que devuelve la etiqueta predicha para un tensor de imagen único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f510a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741aaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc264e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[1002]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[6153]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717417c4",
   "metadata": {},
   "source": [
    "Identificar dónde nuestro modelo funciona mal puede ayudarnos a mejorarlo, recopilando más datos de entrenamiento, aumentando/disminuyendo la complejidad del modelo y cambiando los hiperparámetros.\n",
    "\n",
    "Como paso final, observemos también la pérdida y precisión general del modelo en el conjunto de prueba y registremos usando \"joviano\". Esperamos que estos valores sean similares a los del conjunto de validación. De lo contrario, es posible que necesitemos un mejor conjunto de validación que tenga datos y distribución similares a los del conjunto de prueba (que a menudo proviene de datos del mundo real)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc1aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04abcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c492ca",
   "metadata": {},
   "source": [
    "## Guardando y cargando el modelo\n",
    "\n",
    "Dado que hemos entrenado nuestro modelo durante mucho tiempo y logramos una precisión razonable, sería una buena idea guardar los pesos del modelo en el disco, para que podamos reutilizar el modelo más adelante y evitar volver a entrenar desde cero. Así es como puedes guardar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cifar10-cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627acf5",
   "metadata": {},
   "source": [
    "El método `.state_dict` devuelve un `OrderedDict` que contiene todos los pesos y matrices de sesgo asignados a los atributos correctos del modelo. Para cargar los pesos del modelo, podemos redefinir el modelo con la misma estructura y usar el método `.load_state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878de669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862491f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_state_dict(torch.load('cifar10-cnn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4eeeed",
   "metadata": {},
   "source": [
    "Solo como control de cordura, verifiquemos que este modelo tenga la misma pérdida y precisión en el conjunto de prueba que antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e56536",
   "metadata": {},
   "source": [
    "Hagamos una confirmación final usando \"jovian\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544e59d",
   "metadata": {},
   "source": [
    "Consulte la pestaña **Archivos** en la página del proyecto para ver o descargar los pesos del modelo entrenado. También puedes descargar todos los archivos juntos usando la opción *Descargar Zip* en el menú desplegable *Clonar*.\n",
    "\n",
    "El trabajo de ciencia de datos a menudo está fragmentado en muchas plataformas diferentes (Git para código, Dropbox/S3 para conjuntos de datos y artefactos, hojas de cálculo para hiperparámetros, métricas, etc.), lo que puede dificultar compartir y reproducir experimentos. Jovian.ml resuelve esto capturando todo lo relacionado con un proyecto de ciencia de datos en una única plataforma, al tiempo que proporciona un flujo de trabajo perfecto para capturar, compartir y reproducir su trabajo. Para saber qué puede hacer con Jovian.ml, consulte los documentos: [https://docs.jovian.ml](https://docs.jovian.ml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb59a48",
   "metadata": {},
   "source": [
    "## Resumen y lecturas adicionales/ejercicios\n",
    "\n",
    "Hemos cubierto mucho terreno en este tutorial. Aquí hay un resumen rápido de los temas:\n",
    "* Introducción al conjunto de datos CIFAR10 para clasificación de imágenes.\n",
    "* Descargar, extraer y cargar un conjunto de datos de imágenes usando `torchvision`\n",
    "* Mostrar lotes aleatorios de imágenes en una cuadrícula usando `torchvision.utils.make_grid`\n",
    "* Creación de una red neuronal convolucional usando las capas `nn.Conv2d` y `nn.MaxPool2d`\n",
    "* Captura de información del conjunto de datos, métricas e hiperparámetros utilizando la biblioteca \"joviana\".\n",
    "* Entrenar una red neuronal convolucional y visualizar las pérdidas y errores.\n",
    "* Comprender el sobreajuste y las estrategias para evitarlo (más sobre esto más adelante)\n",
    "* Generar predicciones sobre imágenes individuales del conjunto de prueba.\n",
    "* Guardar y cargar los pesos del modelo y adjuntarlos a la instantánea del experimento usando `jovian`\n",
    "\n",
    "Hay muchas posibilidades para experimentar aquí y le recomiendo que utilice la naturaleza interactiva de Jupyter para jugar con los distintos parámetros. Aqui hay algunas ideas:\n",
    "* Intente cambiar los hiperparámetros para lograr una mayor precisión en menos épocas. Utilice la tabla de comparación en la página del proyecto Jovian.ml para comparar sus experimentos.\n",
    "* Intente agregar más capas convolucionales o aumentar la cantidad de canales en cada capa convolucional\n",
    "* Intente utilizar una red neuronal de avance y vea cuál es la máxima precisión que puede lograr\n",
    "* Lea acerca de algunas de las estrategias mencionadas anteriormente para reducir el sobreajuste y lograr mejores resultados, e intente implementarlas consultando los documentos de PyTorch.\n",
    "* Modifique este cuaderno para entrenar un modelo para un conjunto de datos diferente (por ejemplo, CIFAR100 o ImageNet)\n",
    "\n",
    "En el próximo tutorial, continuaremos mejorando la precisión de nuestro modelo utilizando técnicas como aumento de datos, normalización por lotes y abandono. También aprenderemos sobre las redes residuales (o ResNets), un cambio pequeño pero crítico en la arquitectura del modelo que aumentará significativamente el rendimiento de nuestro modelo. ¡Manténganse al tanto!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d22466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
