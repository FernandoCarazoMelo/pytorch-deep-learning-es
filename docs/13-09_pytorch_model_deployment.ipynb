{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45184ef",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/09_pytorch_model_deployment.ipynb\" target=\"_parent\"><img src=\"https:// colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/></a>\n",
    "\n",
    "[Ver código fuente](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/09_pytorch_model_deployment.ipynb) | [Ver diapositivas] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05699eda",
   "metadata": {},
   "source": [
    "# 09. Implementación del modelo PyTorch\n",
    "\n",
    "Bienvenido al Proyecto Milestone 3: ¡Implementación del modelo PyTorch!\n",
    "\n",
    "Hemos recorrido un largo camino con nuestro proyecto FoodVision Mini.\n",
    "\n",
    "Pero hasta ahora solo nosotros hemos podido acceder a nuestros modelos PyTorch.\n",
    "\n",
    "¿Qué tal si le damos vida a FoodVision Mini y lo hacemos públicamente accesible?\n",
    "\n",
    "En otras palabras, **¡vamos a implementar nuestro modelo FoodVision Mini en Internet como una aplicación utilizable!**\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-model-deployment-what-were-doing-demo-trimmed-cropped-small.gif\" alt =\"demostración del mini modelo de visión por computadora de Foodvision que se utiliza en un dispositivo móvil para predecir una imagen de sushi y hacerlo bien\" width=900/>\n",
    "\n",
    "*Probando la [versión implementada de FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (lo que vamos a crear) en mi almuerzo. ¡La modelo también acertó 🍣!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abd76d",
   "metadata": {},
   "source": [
    "## ¿Qué es la implementación del modelo de aprendizaje automático?\n",
    "\n",
    "**Implementación del modelo de aprendizaje automático** es el proceso de hacer que su modelo de aprendizaje automático sea accesible para alguien o algo más.\n",
    "\n",
    "Alguien más es una persona que puede interactuar con tu modelo de alguna manera. \n",
    "\n",
    "Por ejemplo, alguien que toma una fotografía de una comida con su teléfono inteligente y luego hace que nuestro modelo FoodVision Mini la clasifique en pizza, filete o sushi.\n",
    "\n",
    "Otra cosa podría ser otro programa, aplicación o incluso otro modelo que interactúe con sus modelos de aprendizaje automático. \n",
    "\n",
    "Por ejemplo, una base de datos bancaria podría depender de un modelo de aprendizaje automático que haga predicciones sobre si una transacción es fraudulenta o no antes de transferir fondos.\n",
    "\n",
    "O un sistema operativo puede reducir su consumo de recursos basándose en un modelo de aprendizaje automático que hace predicciones sobre cuánta energía usa generalmente alguien en momentos específicos del día.\n",
    "\n",
    "Estos casos de uso también se pueden mezclar y combinar.\n",
    "\n",
    "Por ejemplo, el sistema de visión por computadora de un automóvil Tesla interactuará con el programa de planificación de rutas del automóvil (algo más) y luego el programa de planificación de rutas recibirá información y comentarios del conductor (otra persona).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png\" ancho=900 alt=\"dos casos de uso para la implementación de modelos, hacer que su modelo esté disponible para otra persona, por ejemplo, alguien que lo use en una aplicación, o ponerlo a disposición de otra cosa, como otro programa o modelo\"/> \n",
    "\n",
    "*La implementación del modelo de aprendizaje automático implica poner su modelo a disposición de alguien o de algo más. Por ejemplo, alguien podría usar su modelo como parte de una aplicación de reconocimiento de alimentos (como FoodVision Mini o [Nutrify](https://nutrify.app)). Y algo más podría ser otro modelo o programa que utilice su modelo, como un sistema bancario que utilice un modelo de aprendizaje automático para detectar si una transacción es fraudulenta o no.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca5fc3",
   "metadata": {},
   "source": [
    "## ¿Por qué implementar un modelo de aprendizaje automático?\n",
    "\n",
    "Una de las cuestiones filosóficas más importantes en el aprendizaje automático es: \n",
    "\n",
    "<div align=\"centro\">\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-does-it-exist.jpeg\" alt=\"dinosaurio curioso al que a menudo se hace referencia como filósofo raptor que hace la pregunta Si un modelo de aprendizaje automático nunca sale de un cuaderno, ¿existe?\". ancho=300/>\n",
    "</div>\n",
    "\n",
    "Implementar un modelo es tan importante como entrenarlo.\n",
    "\n",
    "Porque aunque puedes tener una idea bastante clara de cómo funcionará tu modelo evaluándolo en un conjunto de pruebas bien diseñado o visualizando sus resultados, nunca sabes realmente cómo funcionará hasta que lo liberas.\n",
    "\n",
    "Hacer que personas que nunca han usado su modelo interactúen con él a menudo revelará casos extremos en los que nunca pensó durante el entrenamiento.\n",
    "\n",
    "Por ejemplo, ¿qué pasaría si alguien subiera una foto que *no* fuera de comida a nuestro modelo FoodVision Mini?\n",
    "\n",
    "Una solución sería crear otro modelo que primero clasifique las imágenes como \"comida\" o \"no comida\" y pase primero la imagen de destino a través de ese modelo (esto es lo que hace [Nutrify](https://nutrify.app)).\n",
    "\n",
    "Luego, si la imagen es de \"comida\", pasa a nuestro modelo FoodVision Mini y se clasifica en pizza, bistec o sushi.\n",
    "\n",
    "Y si \"no es comida\", se muestra un mensaje.\n",
    "\n",
    "Pero ¿y si estas predicciones estuvieran equivocadas?\n",
    "\n",
    "¿Qué pasa entonces?\n",
    "\n",
    "Puedes ver cómo estas preguntas podrían continuar.\n",
    "\n",
    "Por lo tanto, esto resalta la importancia de la implementación del modelo: le ayuda a descubrir errores en su modelo que no son obvios durante el entrenamiento/prueba.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-pytorch-workflow-with-deployment.png\" alt=\"Un flujo de trabajo de PyTorch con implementación de modelo adicional y paso de monitoreo\" ancho=900/>\n",
    "\n",
    "*Cubrimos un flujo de trabajo de PyTorch en [01. Flujo de trabajo de PyTorch](https://www.learnpytorch.io/01_pytorch_workflow/). Pero una vez que se tiene un buen modelo, la implementación es un buen siguiente paso. El monitoreo implica ver cómo funciona su modelo en la división de datos más importante: los datos del mundo real. Para obtener más recursos sobre implementación y monitoreo, consulte [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565ad5e",
   "metadata": {},
   "source": [
    "## Diferentes tipos de implementación de modelos de aprendizaje automático\n",
    "\n",
    "Se podrían escribir libros completos sobre los diferentes tipos de implementación de modelos de aprendizaje automático (y muchos buenos se enumeran en [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and -ingeniería-de-aprendizaje-profundo)).\n",
    "\n",
    "Y el campo aún se está desarrollando en términos de mejores prácticas.\n",
    "\n",
    "Pero me gusta empezar con la pregunta:\n",
    "\n",
    "> \"¿Cuál es el escenario más ideal para utilizar mi modelo de aprendizaje automático?\"\n",
    "\n",
    "Y luego trabaje hacia atrás desde allí.\n",
    "\n",
    "Por supuesto, es posible que no lo sepas de antemano. Pero eres lo suficientemente inteligente como para imaginar esas cosas.\n",
    "\n",
    "En el caso de FoodVision Mini, nuestro escenario ideal podría ser:\n",
    "\n",
    "* Alguien toma una foto en un dispositivo móvil (a través de una aplicación o navegador web).\n",
    "* La predicción vuelve rápidamente.\n",
    "\n",
    "Fácil.\n",
    "\n",
    "Entonces tenemos dos criterios principales:\n",
    "\n",
    "1. El modelo debería funcionar en un dispositivo móvil (esto significa que habrá algunas restricciones informáticas). \n",
    "2. El modelo debe hacer predicciones *rápidas* (porque una aplicación lenta es una aplicación aburrida).\n",
    "\n",
    "Y, por supuesto, según su caso de uso, sus requisitos pueden variar.\n",
    "\n",
    "Puede notar que los dos puntos anteriores se dividen en otras dos preguntas:\n",
    "\n",
    "1. **¿A dónde irá?** - Es decir, ¿dónde se almacenará?\n",
    "2. **¿Cómo va a funcionar?** - Es decir, ¿devuelve predicciones inmediatamente? ¿O vienen más tarde?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png\" alt=\"algunas preguntas para hacer al comenzar la implementación modelos de aprendizaje automático, cuál es el caso de uso ideal del modelo, luego trabaje hacia atrás y pregunte dónde irá mi modelo y cómo funcionará\" width=900/>\n",
    "\n",
    "*Al comenzar a implementar modelos de aprendizaje automático, es útil comenzar preguntando cuál es el caso de uso más ideal y luego trabajar hacia atrás desde allí, preguntando hacia dónde irá el modelo y luego cómo funcionará.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e0e01",
   "metadata": {},
   "source": [
    "### ¿A dónde irá?\n",
    "\n",
    "Cuando implementas tu modelo de aprendizaje automático, ¿dónde reside?\n",
    "\n",
    "El debate principal aquí suele ser en el dispositivo (también llamado borde/en el navegador) o en la nube (una computadora/servidor que no es el dispositivo *real* desde donde alguien/algo llama al modelo). \n",
    "\n",
    "Ambos tienen pros y contras.\n",
    "\n",
    "| **Ubicación de implementación** | **Ventajas** | **Desventajas** | \n",
    "| ----- | ----- | ----- |\n",
    "| **En el dispositivo (borde/en el navegador)** | Puede ser muy rápido (ya que no salen datos del dispositivo) | Potencia informática limitada (los modelos más grandes tardan más en ejecutarse) | \n",
    "| | Preservación de la privacidad (nuevamente, ningún dato tiene que salir del dispositivo) | Espacio de almacenamiento limitado (se requiere un tamaño de modelo más pequeño) | \n",
    "| | No se requiere conexión a Internet (a veces) | A menudo se requieren habilidades específicas del dispositivo | \n",
    "| | | | \n",
    "| **En la nube** | Potencia informática casi ilimitada (puede ampliarse cuando sea necesario) | Los costos pueden salirse de control (si no se aplican límites de escala adecuados) |\n",
    "| | Puede implementar un modelo y usarlo en todas partes (a través de API) | Las predicciones pueden ser más lentas debido a que los datos tienen que salir del dispositivo y las predicciones tienen que regresar (latencia de red) |\n",
    "| | Vínculos con el ecosistema de nube existente | Los datos deben salir del dispositivo (esto puede causar problemas de privacidad) |\n",
    "\n",
    "Hay más detalles sobre estos, pero dejé recursos en el [extracurriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) para obtener más información. \n",
    "\n",
    "Pongamos un ejemplo.\n",
    "\n",
    "Si implementamos FoodVision Mini como una aplicación, queremos que funcione bien y rápido.\n",
    "\n",
    "Entonces, ¿qué modelo preferiríamos? \n",
    "\n",
    "1. Un modelo en el dispositivo que funciona con una precisión del 95 % con un tiempo de inferencia (latencia) de un segundo por predicción.\n",
    "2. Un modelo en la nube que funciona con una precisión del 98 % con un tiempo de inferencia de 10 segundos por predicción (un modelo mejor y más grande, pero lleva más tiempo calcularlo).\n",
    "\n",
    "He inventado estos números, pero muestran una diferencia potencial entre el dispositivo y la nube.\n",
    "\n",
    "La opción 1 podría ser potencialmente un modelo más pequeño, de menor rendimiento y que funcione más rápido porque puede caber en un dispositivo móvil.\n",
    "\n",
    "La opción 2 podría potencialmente ser un modelo más grande y con mayor rendimiento que requiere más computación y almacenamiento, pero tarda un poco más en ejecutarse porque tenemos que enviar datos desde el dispositivo y recuperarlos (por lo que, aunque la predicción real pueda ser rápida, el tiempo de red y la transferencia de datos debe tenerse en cuenta).\n",
    "\n",
    "Para FoodVision Mini, probablemente preferiríamos la opción 1, porque el pequeño impacto en el rendimiento se ve superado con creces por la velocidad de inferencia más rápida.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png\" width=900 alt=\"tesla sistema de visión por computadora en el dispositivo versus en la nube\"/>\n",
    "\n",
    "*En el caso del sistema de visión por computadora de un automóvil Tesla, ¿cuál sería mejor? ¿Un modelo más pequeño que funciona bien en el dispositivo (el modelo está en el automóvil) o un modelo más grande que funciona mejor en la nube? En este caso, preferirías que el modelo estuviera en el auto. El tiempo de red adicional que tomaría para que los datos vayan del automóvil a la nube y luego de regreso al automóvil simplemente no valdría la pena (o incluso sería potencialmente imposible en áreas con mala señal).*\n",
    "\n",
    "> **Nota:** Para ver un ejemplo completo de cómo es implementar un modelo de PyTorch en un dispositivo perimetral, consulte el [tutorial de PyTorch sobre cómo lograr inferencia en tiempo real (30 fps+)](https://pytorch.org/ tutorials/intermediate/realtime_rpi.html) con un modelo de visión por computadora en una Raspberry Pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf30596",
   "metadata": {},
   "source": [
    "### ¿Cómo va a funcionar?\n",
    "\n",
    "Volviendo al caso de uso ideal, cuando implementa su modelo de aprendizaje automático, ¿cómo debería funcionar?\n",
    "\n",
    "Es decir, ¿le gustaría que se le devolvieran las predicciones de inmediato?\n",
    "\n",
    "¿O está bien que sucedan más tarde?\n",
    "\n",
    "Estos dos escenarios generalmente se denominan:\n",
    "\n",
    "* **En línea (en tiempo real)**: las predicciones/inferencias ocurren **inmediatamente**. Por ejemplo, alguien sube una imagen, la imagen se transforma y se devuelven predicciones o alguien realiza una compra y un modelo verifica que la transacción no es fraudulenta para que la compra pueda realizarse.\n",
    "* **Sin conexión (por lotes)**: las predicciones/inferencias ocurren **periódicamente**. Por ejemplo, una aplicación de fotos clasifica sus imágenes en diferentes categorías (como playa, hora de comer, familia, amigos) mientras su dispositivo móvil está enchufado a la carga.\n",
    "\n",
    "> **Nota:** \"Lote\" se refiere a la inferencia que se realiza en varias muestras a la vez. Sin embargo, para agregar un poco de confusión, el procesamiento por lotes puede realizarse inmediatamente/en línea (se clasifican varias imágenes a la vez) y/o fuera de línea (se predicen/entrenan varias imágenes a la vez).  \n",
    "\n",
    "La principal diferencia entre cada ser: las predicciones se realizan de forma inmediata o periódica.\n",
    "\n",
    "Periódicamente también puede tener una escala de tiempo variable, desde cada pocos segundos hasta cada pocas horas o días.\n",
    "\n",
    "Y puedes mezclar y combinar los dos.\n",
    "\n",
    "En el caso de FoodVision Mini, queremos que nuestro proceso de inferencia se realice en línea (en tiempo real), de modo que cuando alguien suba una imagen de pizza, bistec o sushi, los resultados de la predicción se devuelvan inmediatamente (cualquier cosa más lenta de lo que lo haría el tiempo real). hacer una experiencia aburrida).\n",
    "\n",
    "Pero para nuestro proceso de capacitación, está bien que suceda por lotes (fuera de línea), que es lo que hemos estado haciendo a lo largo de los capítulos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df767b0",
   "metadata": {},
   "source": [
    "### Formas de implementar un modelo de aprendizaje automático\n",
    "\n",
    "Hemos analizado un par de opciones para implementar modelos de aprendizaje automático (en el dispositivo y en la nube).\n",
    "\n",
    "Y cada uno de estos tendrá sus requisitos específicos:\n",
    "\n",
    "| **Herramienta/recurso** | **Tipo de implementación** | \n",
    "| ----- | ----- |\n",
    "| [Kit de aprendizaje automático de Google](https://developers.google.com/ml-kit) | En el dispositivo (Android e iOS) | \n",
    "| [Core ML de Apple](https://developer.apple.com/documentation/coreml) y [paquete Python `coremltools`](https://coremltools.readme.io/docs) | En el dispositivo (todos los dispositivos Apple) | \n",
    "| [Sagemaker de Amazon Web Service (AWS)](https://aws.amazon.com/sagemaker/) | Nube | \n",
    "| [Vertex AI de Google Cloud](https://cloud.google.com/vertex-ai) | Nube |\n",
    "| [Aprendizaje automático de Azure de Microsoft](https://azure.microsoft.com/en-au/services/machine-learning/) | Nube |\n",
    "| [Abrazando espacios faciales](https://huggingface.co/spaces) | Nube |\n",
    "| API con [FastAPI](https://fastapi.tiangolo.com) | Servidor en la nube/autohospedado |\n",
    "| API con [TorchServe](https://pytorch.org/serve/) | Servidor en la nube/autohospedado | \n",
    "| [ONNX (Intercambio de redes neuronales abiertas)](https://onnx.ai/index.html) | Muchos/general |\n",
    "| Muchos más... |\n",
    "\n",
    "> **Nota:** Una [interfaz de programación de aplicaciones (API)](https://en.wikipedia.org/wiki/API) es una forma en que dos (o más) programas informáticos interactúan entre sí. Por ejemplo, si su modelo se implementó como API, podría escribir un programa que pudiera enviarle datos y luego recibir predicciones.\n",
    "\n",
    "La opción que elija dependerá en gran medida de lo que esté creando y con quién esté trabajando.\n",
    "\n",
    "Pero con tantas opciones, puede resultar muy intimidante.\n",
    "\n",
    "Así que lo mejor es empezar poco a poco y hacerlo sencillo.\n",
    "\n",
    "Y una de las mejores formas de hacerlo es convertir su modelo de aprendizaje automático en una aplicación de demostración con [Gradio](https://gradio.app) y luego implementarlo en Hugging Face Spaces.\n",
    "\n",
    "Más adelante haremos precisamente eso con FoodVision Mini.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png\" alt=\"herramientas y Lugares para implementar modelos de aprendizaje automático\" width=900/>\n",
    "\n",
    "*Un puñado de lugares y herramientas para alojar e implementar modelos de aprendizaje automático. Hay muchas cosas que me he perdido, así que si desea agregar más, deje una [discusión en GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369bc51",
   "metadata": {},
   "source": [
    "## Qué vamos a cubrir \n",
    "\n",
    "Ya basta de hablar de implementar un modelo de aprendizaje automático.\n",
    "\n",
    "Convirtámonos en ingenieros de aprendizaje automático e implementemos uno.\n",
    "\n",
    "Nuestro objetivo es implementar nuestro modelo FoodVision a través de una aplicación de demostración de Gradio con las siguientes métricas:\n",
    "1. **Rendimiento:** 95%+ precisión.\n",
    "2. **Velocidad:** inferencia en tiempo real de 30 FPS+ (cada predicción tiene una latencia inferior a ~0,03 s).\n",
    "\n",
    "Comenzaremos ejecutando un experimento para comparar nuestros dos mejores modelos hasta el momento: extractores de funciones EffNetB2 y ViT.\n",
    "\n",
    "Luego implementaremos el que se acerque más a nuestras métricas objetivo.\n",
    "\n",
    "Finalmente, terminaremos con un (GRANDE) bono sorpresa.\n",
    "\n",
    "| **Tema** | **Contenido** | \n",
    "| ----- | ----- | \n",
    "| **0. Obteniendo configuración** | Hemos escrito bastante código útil en las últimas secciones, descarguémoslo y asegurémonos de poder usarlo nuevamente. | \n",
    "| **1. Obtener datos** | Descarguemos el conjunto de datos [`pizza_steak_sushi_20_percent.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) para que podamos entrenar nuestros modelos que anteriormente tenían mejor rendimiento en el mismo conjunto de datos. |\n",
    "| **2. Esquema del experimento de implementación del modelo FoodVision Mini** | Incluso en el proyecto del tercer hito, todavía realizaremos múltiples experimentos para ver qué modelo (EffNetB2 o ViT) se acerca más a nuestras métricas objetivo. |\n",
    "| **3. Creando un extractor de funciones EffNetB2** | Un extractor de funciones EfficientNetB2 tuvo el mejor rendimiento en nuestro conjunto de datos de pizza, bistec y sushi en [07. Seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/), vamos a recrearlo como candidato para su implementación. |\n",
    "| **4. Creando un extractor de funciones ViT** | Un extractor de funciones ViT ha sido el modelo con mejor rendimiento hasta ahora en nuestro conjunto de datos de pizza, bistec y sushi en [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/), vamos a recrearlo como candidato para su implementación junto con EffNetB2. |\n",
    "| **5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas** | Hemos creado dos de los modelos con mejor rendimiento hasta el momento. Hagamos predicciones con ellos y realicemos un seguimiento de sus resultados. |\n",
    "| **6. Comparación de resultados de modelos, tiempos de predicción y tamaño** | Comparemos nuestros modelos para ver cuál funciona mejor con nuestros objetivos. | \n",
    "| **7. Dar vida a FoodVision Mini creando una demostración de Gradio** | Uno de nuestros modelos funciona mejor que el otro (en términos de nuestros objetivos), así que ¡convirtámoslo en una demostración de aplicación funcional! |\n",
    "| **8. Convirtiendo nuestra demostración de FoodVision Mini Gradio en una aplicación implementable** | Nuestra demostración de la aplicación Gradio funciona localmente, ¡preparémosla para su implementación! |\n",
    "| **9. Implementando nuestra demostración de Gradio en HuggingFace Spaces** | ¡Llevemos FoodVision Mini a la web y hagámoslo accesible públicamente para todos! |\n",
    "| **10. Creando una GRAN sorpresa** | Hemos creado FoodVision Mini, es hora de dar un paso más. |\n",
    "| **11. Desplegando nuestra GRAN sorpresa** | Implementar una aplicación fue divertido, ¿qué tal si hacemos dos? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cec63e",
   "metadata": {},
   "source": [
    "## ¿Dónde puedes obtener ayuda?\n",
    "\n",
    "Todos los materiales de este curso [están disponibles en GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
    "\n",
    "Si tiene problemas, puede hacer una pregunta en el curso [página de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
    "\n",
    "Y, por supuesto, está la [documentación de PyTorch](https://pytorch.org/docs/stable/index.html) y los [foros de desarrolladores de PyTorch](https://discuss.pytorch.org/), un lugar muy útil para todo lo relacionado con PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598c6e3",
   "metadata": {},
   "source": [
    "## 0. Configuración \n",
    "\n",
    "Como lo hicimos anteriormente, asegurémonos de tener todos los módulos que necesitaremos para esta sección.\n",
    "\n",
    "Importaremos los scripts de Python (como `data_setup.py` y `engine.py`) que creamos en [05. PyTorch se vuelve modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
    "\n",
    "Para hacerlo, descargaremos el directorio [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) del repositorio [`pytorch-deep-learning`]( https://github.com/mrdbourke/pytorch-deep-learning) (si aún no lo tenemos).\n",
    "\n",
    "También obtendremos el paquete [`torchinfo`](https://github.com/TylerYep/torchinfo) si no está disponible. \n",
    "\n",
    "`torchinfo` nos ayudará más adelante a darnos una representación visual de nuestro modelo.\n",
    "\n",
    "Y dado que más adelante usaremos el paquete `torchvision` v0.13 (disponible a partir de julio de 2022), nos aseguraremos de tener las últimas versiones.\n",
    "\n",
    "> **Nota:** Si estás usando Google Colab y aún no tienes una GPU activada, ahora es el momento de activar una a través de `Runtime -> Cambiar tipo de tiempo de ejecución -> Acelerador de hardware -> GPU` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que este portátil se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c43b1d",
   "metadata": {},
   "source": [
    "> **Nota:** Si está utilizando Google Colab y la celda de arriba comienza a instalar varios paquetes de software, es posible que deba reiniciar su tiempo de ejecución después de ejecutar la celda de arriba. Después de reiniciar, puede ejecutar la celda nuevamente y verificar que tenga las versiones correctas de `torch` y `torchvision`.\n",
    "\n",
    "Ahora continuaremos con las importaciones regulares, configurando el código independiente del dispositivo y esta vez también obtendremos [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/ main/helper_functions.py) script de GitHub.\n",
    "\n",
    "El script `helper_functions.py` contiene varias funciones que creamos en secciones anteriores:\n",
    "* `set_seeds()` para configurar las semillas aleatorias (creadas en [07. Sección 0 de seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds )).\n",
    "* `download_data()` para descargar una fuente de datos mediante un enlace (creado en [07. Sección 1 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n",
    "* `plot_loss_curves()` para inspeccionar los resultados del entrenamiento de nuestro modelo (creado en [04. PyTorch Custom Datasets sección 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of- modelo-0))\n",
    "\n",
    "> **Nota:** Puede ser una mejor idea que muchas de las funciones en el script `helper_functions.py` se fusionen en `going_modular/going_modular/utils.py`, tal vez sea una extensión que le gustaría probar ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuar con las importaciones regulares\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Intente obtener torchinfo, instálelo si no funciona\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Intente importar el directorio going_modular, descárguelo de GitHub si no funciona\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3865a8",
   "metadata": {},
   "source": [
    "Finalmente, configuraremos un código independiente del dispositivo para asegurarnos de que nuestros modelos se ejecuten en la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb0e12",
   "metadata": {},
   "source": [
    "## 1. Obtener datos\n",
    "\n",
    "Lo dejamos en [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size) comparando nuestro propio modelo de extractor de funciones Vision Transformer (ViT) con El modelo de extracción de características EfficientNetB2 (EffNetB2) que creamos en [07. Seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it).\n",
    "\n",
    "Y descubrimos que había una ligera diferencia en la comparación.\n",
    "\n",
    "El modelo EffNetB2 se entrenó en el 20 % de los datos de pizza, bistec y sushi de Food101, mientras que el modelo ViT se entrenó en el 10 %.\n",
    "\n",
    "Dado que nuestro objetivo es implementar el mejor modelo para nuestro problema FoodVision Mini, comencemos descargando el [conjunto de datos del 20 % de pizza, bistec y sushi] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main /data/pizza_steak_sushi_20_percent.zip) y entrene un extractor de funciones EffNetB2 y un extractor de funciones ViT en él y luego compare los dos modelos.\n",
    "\n",
    "De esta manera, compararemos manzanas con manzanas (un modelo entrenado en un conjunto de datos con otro modelo entrenado en el mismo conjunto de datos).\n",
    "\n",
    "> **Nota:** El conjunto de datos que estamos descargando es una muestra de todo el [conjunto de datos de Food101](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html#food101) (101 clases de comida con 1.000 imágenes cada una). Más específicamente, 20% se refiere al 20% de imágenes de las clases de pizza, bistec y sushi seleccionadas al azar. Puede ver cómo se creó este conjunto de datos en [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) y más detalles en [ 04. Sección 1 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).\n",
    "\n",
    "Podemos descargar los datos usando la función `download_data()` que creamos en [07. Sección 1 de seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data) de [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning /blob/main/helper_functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargue imágenes de pizza, bistec y sushi desde GitHub\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45e09a",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Ahora que tenemos un conjunto de datos, creemos rutas de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar rutas de directorio para entrenar y probar imágenes\n",
    "train_dir = data_20_percent_path / \"train\"\n",
    "test_dir = data_20_percent_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f7d61",
   "metadata": {},
   "source": [
    "## 2. Esquema del experimento de implementación del modelo FoodVision Mini\n",
    "\n",
    "El modelo implementado ideal, FoodVision Mini, funciona bien y rápido. \n",
    "\n",
    "Nos gustaría que nuestro modelo funcione lo más cerca posible del tiempo real.\n",
    "\n",
    "En este caso, el tiempo real es ~30 FPS (cuadros por segundo) porque eso es [aproximadamente qué tan rápido puede ver el ojo humano] (https://www.healthline.com/health/human-eye-fps) (hay debate sobre esto, pero usemos ~30FPS como nuestro punto de referencia).\n",
    "\n",
    "Y para clasificar tres clases diferentes (pizza, bistec y sushi), nos gustaría un modelo que funcione con una precisión superior al 95 %.\n",
    "\n",
    "Por supuesto, una mayor precisión sería buena, pero esto podría sacrificar la velocidad.\n",
    "\n",
    "Entonces nuestros objetivos son:\n",
    "\n",
    "1. **Rendimiento**: un modelo que funciona con una precisión superior al 95 %.\n",
    "2. **Velocidad**: un modelo que puede clasificar una imagen a ~30 FPS (tiempo de inferencia de 0,03 segundos por imagen, también conocido como latencia).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png\" alt=\"mini objetivos de foodvision en términos de tiempo de desempeño y de inferencia.\" ancho=750/>\n",
    "\n",
    "*Objetivos de implementación de FoodVision Mini. Nos gustaría un modelo de predicción rápida y con buen rendimiento (porque una aplicación lenta es aburrida).*\n",
    "\n",
    "Pondremos énfasis en la velocidad, es decir, preferiríamos un modelo con un rendimiento superior al 90 % a ~30 FPS que un modelo con un rendimiento superior al 95 % a 10 FPS.\n",
    "\n",
    "Para intentar lograr estos resultados, incluyamos nuestros modelos con mejor rendimiento de las secciones anteriores: \n",
    "\n",
    "1. **Extractor de funciones EffNetB2** (EffNetB2 para abreviar): creado originalmente en [07. Sección 7.5 de seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) usando [`torchvision.models.ficientnet_b2()`](https://pytorch.org /vision/stable/models/generated/torchvision.models.ficientnet_b2.html#ficientnet-b2) con capas de `clasificador` ajustadas.\n",
    "2. **Extractor de funciones ViT-B/16** (ViT para abreviar): creado originalmente en [08. Sección 10 de replicación de papel de PyTorch] (https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) usando [`torchvision.models.vit_b_16 ()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) con capas de `cabeza` ajustadas.\n",
    "    * **Nota** ViT-B/16 significa \"Vision Transformer Base, tamaño de parche 16\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png\" alt=\"experimentos de modelado para mini implementaciones de foodvision, uno Modelo de extractor de características effnetb2 y modelo de extractor de características de transformador de visión\" ancho=750 />\n",
    "\n",
    "> **Nota:** Un \"modelo de extracción de características\" a menudo comienza con un modelo que ha sido previamente entrenado en un conjunto de datos similar a su propio problema. Las capas base del modelo previamente entrenado a menudo se dejan congeladas (los patrones/pesos previamente entrenados permanecen iguales) mientras que algunas de las capas superiores (o clasificador/cabeza de clasificación) se personalizan según su propio problema entrenando con sus propios datos. Cubrimos el concepto de un modelo de extracción de características en [06. Sección 3.4 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89626ee3",
   "metadata": {},
   "source": [
    "## 3. Creando un extractor de funciones EffNetB2\n",
    "\n",
    "Primero creamos un modelo de extracción de características EffNetB2 en [07. Sección 7.5 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).\n",
    "\n",
    "Y al final de esa sección vimos que funcionó muy bien.\n",
    "\n",
    "Así que ahora vamos a recrearlo aquí para que podamos comparar sus resultados con un extractor de funciones de ViT entrenado con los mismos datos.\n",
    "\n",
    "Para hacerlo podemos:\n",
    "1. Configure los pesos previamente entrenados como [`weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.ficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights ), donde \"`DEFAULT`\" significa \"mejor disponible actualmente\" (o podría usar `weights=\"DEFAULT\"`). \n",
    "2. Obtenga las transformaciones de la imagen del modelo previamente entrenado a partir de los pesos con el método `transforms()` (los necesitamos para poder convertir nuestras imágenes al mismo formato en el que se entrenó el EffNetB2 previamente entrenado).\n",
    "3. Cree una instancia de modelo previamente entrenada pasando los pesos a una instancia de [`torchvision.models.ficientnet_b2`](https://pytorch.org/vision/stable/models/generated/torchvision.models.ficientnet_b2.html#ficientnet -b2).\n",
    "4. Congele las capas base en el modelo.\n",
    "5. Actualizar el cabezal del clasificador para adaptarlo a nuestros propios datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687129c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configurar pesas EffNetB2 previamente entrenadas\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Obtenga transformaciones EffNetB2\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Configurar el modelo previamente entrenado\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Congele las capas base en el modelo (esto congelará todas las capas para empezar)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8c763",
   "metadata": {},
   "source": [
    "Ahora, para cambiar el encabezado del clasificador, primero inspeccionémoslo usando el atributo \"clasificador\" de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte el cabezal clasificador EffNetB2\n",
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19619350",
   "metadata": {},
   "source": [
    "¡Excelente! Para cambiar el encabezado del clasificador para adaptarlo a nuestro propio problema, reemplacemos la variable `out_features` con el mismo número de clases que tenemos (en nuestro caso, `out_features=3`, una para pizza, bistec, sushi).\n",
    "\n",
    "> **Nota:** Este proceso de cambiar las capas de salida/cabezal clasificador dependerá del problema en el que esté trabajando. Por ejemplo, si quisiera un *número* diferente de salidas o un *tipo* diferente de salida, tendría que cambiar las capas de salida en consecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Actualice el cabezal del clasificador.\n",
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "    nn.Linear(in_features=1408, # keep in_features same \n",
    "              out_features=3)) # change out_features to suit our number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d55f5",
   "metadata": {},
   "source": [
    "¡Hermoso!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e31bd4",
   "metadata": {},
   "source": [
    "### 3.1 Creando una función para hacer un extractor de características EffNetB2\n",
    "Parece que nuestro extractor de funciones EffNetB2 está listo para funcionar; sin embargo, dado que aquí hay bastantes pasos involucrados, ¿qué tal si convertimos el código anterior en una función que podamos reutilizar más adelante?\n",
    "\n",
    "Lo llamaremos `create_effnetb2_model()` y necesitará un número personalizable de clases y un parámetro inicial aleatorio para su reproducibilidad.\n",
    "\n",
    "Idealmente, devolverá un extractor de funciones EffNetB2 junto con sus transformaciones asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # 4. Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 5. Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b91a57",
   "metadata": {},
   "source": [
    "¡Guau! Es una función muy bonita, probémosla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62808c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
    "                                                      seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271ac57",
   "metadata": {},
   "source": [
    "Sin errores, genial, ahora para probarlo realmente, obtengamos un resumen con `torchinfo.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Imprimir resumen del modelo EffNetB2 (descomentar para obtener un resultado completo)\n",
    "# resumen(effnetb2,\n",
    "# tamaño_entrada=(1, 3, 224, 224),\n",
    "# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n",
    "# ancho_columna=20,\n",
    "# row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a5fdd",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-effnetb2-feature-extractor.png\" alt=\"resumen del modelo de extractor de funciones effnetb2\" width=900/ >\n",
    "\n",
    "¡Capas base congeladas, capas superiores entrenables y personalizadas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9837a",
   "metadata": {},
   "source": [
    "### 3.2 Creando cargadores de datos para EffNetB2 \n",
    "\n",
    "Nuestro extractor de funciones EffNetB2 está listo, es hora de crear algunos `DataLoader`.\n",
    "\n",
    "Podemos hacer esto usando la función [`data_setup.create_dataloaders()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) que creamos en [ 05. PyTorch Going Modular sección 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
    "\n",
    "Usaremos un `batch_size` de 32 y transformaremos nuestras imágenes usando `effnetb2_transforms` para que estén en el mismo formato en el que se entrenó nuestro modelo `effnetb2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar cargadores de datos\n",
    "from going_modular.going_modular import data_setup\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071f49b",
   "metadata": {},
   "source": [
    "### 3.3 Entrenamiento del extractor de funciones de EffNetB2\n",
    "\n",
    "Modelo listo, `DataLoader`s listo, ¡entrenemos!\n",
    "\n",
    "Al igual que en [07. Sección 7.6 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code), diez épocas deberían ser suficientes para obtener buenos resultados.\n",
    "\n",
    "Podemos hacerlo creando un optimizador (usaremos [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim .Adam) con una tasa de aprendizaje de `1e-3`), una función de pérdida (usaremos [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch .nn.CrossEntropyLoss.html) para clasificación de clases múltiples) y luego pasarlos junto con nuestro `DataLoader`s al [`engine.train()`](https://github.com/mrdbourke/pytorch-deep -learning/blob/main/going_modular/going_modular/engine.py) función que creamos en [05. PyTorch Going Modular sección 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Optimizador de configuración\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Función de pérdida de configuración\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Establezca semillas para la reproducibilidad y entrene el modelo.\n",
    "set_seeds()\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=10,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7877ba",
   "metadata": {},
   "source": [
    "### 3.4 Inspeccionando las curvas de pérdida de EffNetB2 \n",
    "\n",
    "¡Lindo!\n",
    "\n",
    "Como vimos en 07. Seguimiento de experimentos de PyTorch, el modelo de extracción de características EffNetB2 funciona bastante bien con nuestros datos.\n",
    "\n",
    "Convirtamos sus resultados en curvas de pérdidas para inspeccionarlos más a fondo.\n",
    "\n",
    "> **Nota:** Las curvas de pérdida son una de las mejores formas de visualizar el rendimiento de su modelo. Para obtener más información sobre las curvas de pérdidas, consulte [04. Sección 8 de conjuntos de datos personalizados de PyTorch: ¿Cómo debería ser una curva de pérdida ideal?](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47482f5e",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Esas son algunas curvas de pérdidas bonitas. \n",
    "\n",
    "Parece que nuestro modelo está funcionando bastante bien y tal vez se beneficiaría de un entrenamiento un poco más largo y potencialmente de algo de [aumento de datos](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data -aumento) (para ayudar a prevenir un posible sobreajuste que se produzca debido a un entrenamiento más prolongado)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3a937",
   "metadata": {},
   "source": [
    "### 3.5 Guardar el extractor de funciones de EffNetB2\n",
    "\n",
    "Ahora que tenemos un modelo entrenado con buen rendimiento, guardémoslo en un archivo para poder importarlo y usarlo más tarde.\n",
    "\n",
    "Para guardar nuestro modelo podemos usar la función [`utils.save_model()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py) que creamos en [05. PyTorch Going Modular sección 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).\n",
    "\n",
    "Estableceremos `target_dir` en `\"models\"` y `model_name` en `\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"` (un poco completo, pero al menos sabemos lo que está pasando)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a413d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import utils\n",
    "\n",
    "# guardar el modelo\n",
    "utils.save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd1230",
   "metadata": {},
   "source": [
    "### 3.6 Comprobar el tamaño del extractor de funciones EffNetB2\n",
    "\n",
    "Dado que uno de nuestros criterios para implementar un modelo que impulse FoodVision Mini es la **velocidad** (~30 FPS o mejor), verifiquemos el tamaño de nuestro modelo.\n",
    "\n",
    "¿Por qué comprobar el tamaño?\n",
    "\n",
    "Bueno, aunque no siempre es así, el tamaño de un modelo puede influir en su velocidad de inferencia.\n",
    "\n",
    "Es decir, si un modelo tiene más parámetros, generalmente realiza más operaciones y cada una de estas operaciones requiere cierta potencia informática.\n",
    "\n",
    "Y como nos gustaría que nuestro modelo funcione en dispositivos con potencia informática limitada (por ejemplo, en un dispositivo móvil o en un navegador web), generalmente, cuanto más pequeño sea el tamaño, mejor (siempre que siga funcionando bien en términos de precisión). .\n",
    "\n",
    "Para verificar el tamaño de nuestro modelo en bytes, podemos usar [`pathlib.Path.stat(\"path_to_model\").st_size`](https://docs.python.org/3/library/pathlib.html#pathlib.Path) de Python .stat) y luego podemos convertirlo (aproximadamente) a megabytes dividiéndolo por `(1024*1024)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga el tamaño del modelo en bytes y luego conviértalo a megabytes\n",
    "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e0e07",
   "metadata": {},
   "source": [
    "### 3.7 Recopilación de estadísticas del extractor de funciones de EffNetB2\n",
    "\n",
    "Tenemos algunas estadísticas sobre nuestro modelo de extractor de funciones EffNetB2, como pérdida de prueba, precisión de la prueba y tamaño del modelo. ¿Qué tal si las recopilamos todas en un diccionario para poder compararlas con el próximo extractor de funciones ViT?\n",
    "\n",
    "Y calcularemos uno extra por diversión, el número total de parámetros.\n",
    "\n",
    "Podemos hacerlo contando el número de elementos (o patrones/pesos) en `effnetb2.parameters()`. Accederemos al número de elementos en cada parámetro usando [`torch.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html) (abreviatura de \"número de elementos \") método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be32bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuente el número de parámetros en EffNetB2\n",
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "effnetb2_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ada7f9",
   "metadata": {},
   "source": [
    "¡Excelente!\n",
    "\n",
    "Ahora pongamos todo en un diccionario para poder hacer comparaciones más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario con estadísticas de EffNetB2\n",
    "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": effnetb2_total_params,\n",
    "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0aa115",
   "metadata": {},
   "source": [
    "¡Épico! \n",
    "\n",
    "¡Parece que nuestro modelo EffNetB2 funciona con más del 95% de precisión! \n",
    "\n",
    "Criterio número 1: actuar con una precisión superior al 95%, ¡marca!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf608cd",
   "metadata": {},
   "source": [
    "## 4. Creación de un extractor de funciones ViT\n",
    "\n",
    "Es hora de continuar con nuestros experimentos de modelado de FoodVision Mini.\n",
    "\n",
    "Esta vez vamos a crear un extractor de funciones de ViT.\n",
    "\n",
    "Y lo haremos de la misma manera que el extractor de funciones EffNetB2, excepto que esta vez con [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision. models.vit_b_16.html#torchvision.models.vit_b_16) en lugar de `torchvision.models.ficientnet_b2()`.\n",
    "\n",
    "Comenzaremos creando una función llamada `create_vit_model()` que será muy similar a `create_effnetb2_model()` excepto, por supuesto, que devolverá un modelo extractor de características ViT y transformaciones en lugar de EffNetB2.\n",
    "\n",
    "Otra pequeña diferencia es que la capa de salida de `torchvision.models.vit_b_16()` se llama `cabezas` en lugar de `clasificador`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte la capa de cabezales ViT\n",
    "vit = torchvision.models.vit_b_16()\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f102e",
   "metadata": {},
   "source": [
    "Sabiendo esto, tenemos todas las piezas del rompecabezas que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes:int=3, \n",
    "                     seed:int=42):\n",
    "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
    "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT-B/16 feature extractor model. \n",
    "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
    "    \"\"\"\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze all layers in model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head to suit our needs (this will be trainable)\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
    "                                          out_features=num_classes)) # update to reflect target number of classes\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ea0d8",
   "metadata": {},
   "source": [
    "¡La función de creación de modelos de extracción de características ViT está lista!\n",
    "\n",
    "Probémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo ViT y transformaciones.\n",
    "vit, vit_transforms = create_vit_model(num_classes=3,\n",
    "                                       seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7702cc",
   "metadata": {},
   "source": [
    "Sin errores, ¡es encantador verlo! \n",
    "\n",
    "Ahora obtengamos un resumen atractivo de nuestro modelo ViT usando `torchinfo.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d41dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Imprimir resumen del modelo del extractor de funciones de ViT (descomentar para obtener un resultado completo)\n",
    "# resumen(vit,\n",
    "# tamaño_entrada=(1, 3, 224, 224),\n",
    "# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n",
    "# ancho_columna=20,\n",
    "# row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1fae70",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-vit-feature-extractor-3-classes.png\" width=900 alt=\"extractor de funciones vit con 3 clases de salida\"/>\n",
    "\n",
    "Al igual que nuestro modelo de extracción de funciones EffNetB2, las capas base de nuestro modelo ViT están congeladas y la capa de salida se personaliza según nuestras necesidades. \n",
    "\n",
    "¿Notas la gran diferencia?\n",
    "\n",
    "Nuestro modelo ViT tiene *muchos* más parámetros que nuestro modelo EffNetB2. Quizás esto entre en juego cuando comparemos nuestros modelos en cuanto a velocidad y rendimiento más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a43d6",
   "metadata": {},
   "source": [
    "### 4.1 Crear cargadores de datos para ViT\n",
    "\n",
    "Tenemos nuestro modelo ViT listo, ahora creemos algunos `DataLoader`s para él.\n",
    "\n",
    "Haremos esto de la misma manera que hicimos para EffNetB2 excepto que usaremos `vit_transforms` para transformar nuestras imágenes al mismo formato en el que se entrenó el modelo ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar cargadores de datos ViT\n",
    "from going_modular.going_modular import data_setup\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                       test_dir=test_dir,\n",
    "                                                                                       transform=vit_transforms,\n",
    "                                                                                       batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28c6e4",
   "metadata": {},
   "source": [
    "### 4.2 Extractor de funciones de ViT de entrenamiento\n",
    "\n",
    "Sabes que hora es...\n",
    "\n",
    "...es hora de entrenargggggg (cantado con la misma melodía que la canción [Closing Time](https://youtu.be/xGytDsqkQY8)). \n",
    "\n",
    "Entrenemos nuestro modelo de extracción de características ViT durante 10 épocas usando nuestra función `engine.train()` con `torch.optim.Adam()` y una tasa de aprendizaje de `1e-3` como nuestro optimizador y `torch.nn.CrossEntropyLoss ()` como nuestra función de pérdida.\n",
    "\n",
    "Usaremos nuestra función `set_seeds()` antes del entrenamiento para intentar que nuestros resultados sean lo más reproducibles posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Optimizador de configuración\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Función de pérdida de configuración\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Entrene el modelo ViT con semillas configuradas para lograr reproducibilidad\n",
    "set_seeds()\n",
    "vit_results = engine.train(model=vit,\n",
    "                           train_dataloader=train_dataloader_vit,\n",
    "                           test_dataloader=test_dataloader_vit,\n",
    "                           epochs=10,\n",
    "                           optimizer=optimizer,\n",
    "                           loss_fn=loss_fn,\n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3abbd9",
   "metadata": {},
   "source": [
    "### 4.3 Inspeccionando las curvas de pérdidas de ViT\n",
    "\n",
    "Muy bien, está bien, está bien, modelo ViT entrenado, seamos visuales y veamos algunas curvas de pérdida.\n",
    "\n",
    "> **Nota:** No olvide que puede ver cómo debería verse un conjunto ideal de curvas de pérdida en [04. Sección 8 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2970a1",
   "metadata": {},
   "source": [
    "¡Ohh si! \n",
    "\n",
    "Esas son algunas curvas de pérdidas bonitas. Al igual que nuestro modelo de extracción de funciones EffNetB2, parece que nuestro modelo ViT podría beneficiarse de un tiempo de entrenamiento un poco más largo y tal vez algo de [aumento de datos](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of- transforma-datos-aumento) (para ayudar a prevenir el sobreajuste)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc635d6",
   "metadata": {},
   "source": [
    "### 4.4 Guardar el extractor de funciones de ViT\n",
    "\n",
    "¡Nuestro modelo ViT está funcionando de manera excelente! \n",
    "\n",
    "Así que guardémoslo en un archivo para poder importarlo y usarlo más tarde si lo deseamos.\n",
    "\n",
    "Podemos hacerlo usando la función `utils.save_model()` que creamos en [05. PyTorch Going Modular sección 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo\n",
    "from going_modular.going_modular import utils\n",
    "\n",
    "utils.save_model(model=vit,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cf52c",
   "metadata": {},
   "source": [
    "### 4.5 Comprobar el tamaño del extractor de funciones ViT\n",
    "\n",
    "Y como queremos comparar nuestro modelo EffNetB2 con nuestro modelo ViT en función de una serie de características, averigüemos su tamaño.\n",
    "\n",
    "Para verificar el tamaño de nuestro modelo en bytes, podemos usar `pathlib.Path.stat(\"path_to_model\").st_size` de Python y luego podemos convertirlo (aproximadamente) a megabytes dividiéndolo por `(1024*1024)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga el tamaño del modelo en bytes y luego conviértalo a megabytes\n",
    "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c02a80",
   "metadata": {},
   "source": [
    "Hmm, ¿cómo se compara el tamaño del modelo del extractor de funciones ViT con el tamaño de nuestro modelo EffNetB2?\n",
    "\n",
    "Lo descubriremos en breve cuando comparemos todas las características de nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a22783",
   "metadata": {},
   "source": [
    "### 4.6 Recopilación de estadísticas del extractor de funciones de ViT\n",
    "\n",
    "Reunamos todas las estadísticas del modelo de extracción de funciones de ViT.\n",
    "\n",
    "Lo vimos en el resumen anterior, pero calcularemos su número total de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb262e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de parámetros en ViT\n",
    "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
    "vit_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9deebcb",
   "metadata": {},
   "source": [
    "¡Vaya, eso parece bastante más que nuestro EffNetB2!\n",
    "\n",
    "> **Nota:** Una mayor cantidad de parámetros (o pesos/patrones) generalmente significa que un modelo tiene una mayor *capacidad* de aprender; si realmente utiliza esta capacidad adicional es otra historia. A la luz de esto, nuestro modelo EffNetB2 tiene 7.705.221 parámetros, mientras que nuestro modelo ViT tiene 85.800.963 (11,1 veces más), por lo que podríamos suponer que nuestro modelo ViT tiene más capacidad de aprender, si se le dan más datos (más oportunidades de aprender). Sin embargo, esta mayor capacidad de aprender a menudo viene acompañada de un mayor tamaño del modelo y un mayor tiempo para realizar la inferencia.\n",
    "\n",
    "Ahora creemos un diccionario con algunas características importantes de nuestro modelo ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f32762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionario de estadísticas de ViT\n",
    "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
    "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
    "             \"number_of_parameters\": vit_total_params,\n",
    "             \"model_size (MB)\": pretrained_vit_model_size}\n",
    "\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce72f1",
   "metadata": {},
   "source": [
    "¡Lindo! Parece que nuestro modelo ViT también logra una precisión superior al 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27add0",
   "metadata": {},
   "source": [
    "## 5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas\n",
    "\n",
    "Tenemos un par de modelos entrenados y ambos funcionan bastante bien.\n",
    "\n",
    "Ahora, ¿qué tal si los probamos haciendo lo que nos gustaría que hicieran?\n",
    "\n",
    "Es decir, veamos cómo hacen predicciones (realizando inferencias).\n",
    "\n",
    "Sabemos que nuestros dos modelos funcionan con una precisión superior al 95 % en el conjunto de datos de prueba, pero ¿qué tan rápidos son?\n",
    "\n",
    "Idealmente, si implementamos nuestro modelo FoodVision Mini en un dispositivo móvil para que las personas puedan tomar fotografías de sus alimentos e identificarlos, nos gustaría que las predicciones se realicen en tiempo real (~30 fotogramas por segundo).\n",
    "\n",
    "Por eso nuestro segundo criterio es: un modelo rápido.\n",
    "\n",
    "Para saber cuánto tiempo tarda cada uno de nuestros modelos en inferir el rendimiento, creemos una función llamada `pred_and_store()` para iterar sobre cada una de las imágenes del conjunto de datos de prueba una por una y realizar una predicción. \n",
    "\n",
    "Calcularemos el tiempo de cada una de las predicciones y almacenaremos los resultados en un formato de predicción común: una lista de diccionarios (donde cada elemento de la lista es una predicción única y cada predicción única es un diccionario). \n",
    "\n",
    "> **Nota:** Calculamos las predicciones una por una en lugar de por lotes porque cuando se implementa nuestro modelo, probablemente solo hará una predicción en una imagen a la vez. Es decir, alguien toma una foto y nuestro modelo predice sobre esa única imagen.\n",
    "\n",
    "Como nos gustaría hacer predicciones en todas las imágenes del conjunto de prueba, primero obtengamos una lista de todas las rutas de las imágenes de prueba para que podamos iterar sobre ellas. \n",
    "\n",
    "Para hacerlo, usaremos [`pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))`](https://docs.python.org/3/library/pathlib) de Python. html#basic-use) para encontrar todas las rutas de archivos en un directorio de destino con la extensión `.jpg` (todas nuestras imágenes de prueba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313adfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga todas las rutas de datos de prueba\n",
    "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca242a",
   "metadata": {},
   "source": [
    "### 5.1 Crear una función para hacer predicciones en todo el conjunto de datos de prueba\n",
    "\n",
    "Ahora que tenemos una lista de nuestras rutas de imágenes de prueba, comencemos a trabajar en nuestra función `pred_and_store()`:\n",
    "\n",
    "1. Cree una función que tome una lista de rutas, un modelo PyTorch entrenado, una serie de transformaciones (para preparar imágenes), una lista de nombres de clases de destino y un dispositivo de destino.\n",
    "2. Cree una lista vacía para almacenar diccionarios de predicción (queremos que la función devuelva una lista de diccionarios, uno para cada predicción).\n",
    "3. Recorra las rutas de entrada de destino (los pasos 4 a 14 se realizarán dentro del bucle).\n",
    "4. Cree un diccionario vacío para cada iteración del bucle para almacenar los valores de predicción por muestra.\n",
    "5. Obtenga la ruta de muestra y el nombre de la clase de verdad fundamental (podemos hacer esto infiriendo la clase a partir de la ruta).\n",
    "6. Inicie el temporizador de predicción usando [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer) de Python.\n",
    "7. Abra la imagen usando [`PIL.Image.open(path)`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
    "8. Transforme la imagen para que pueda usarse con el modelo de destino, así como agregue una dimensión por lotes y envíe la imagen al dispositivo de destino.\n",
    "9. Prepare el modelo para la inferencia enviándolo al dispositivo de destino y activando el modo `eval()`.\n",
    "10. Active [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html) y pase la imagen transformada de destino al modelo y calcule la probabilidad de predicción usando ` torch.softmax()` y la etiqueta de destino usando `torch.argmax()`.\n",
    "11. Agregue la probabilidad de predicción y la clase de predicción al diccionario de predicción creado en el paso 4. También asegúrese de que la probabilidad de predicción esté en la CPU para que pueda usarse con bibliotecas que no son de GPU, como NumPy y pandas, para una inspección posterior.\n",
    "12. Finalice el temporizador de predicción iniciado en el paso 6 y agregue el tiempo al diccionario de predicción creado en el paso 4.\n",
    "13. Vea si la clase predicha coincide con la clase de verdad fundamental del paso 5 y agregue el resultado al diccionario de predicción creado en el paso 4.\n",
    "14. Agregue el diccionario de predicciones actualizado a la lista vacía de predicciones creada en el paso 2.\n",
    "15. Devuelve la lista de diccionarios de predicción.\n",
    "\n",
    "¡Muchos pasos, pero nada que no podamos manejar!\n",
    "\n",
    "Vamos a hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc13260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer \n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1. Cree una función para devolver una lista de diccionarios con muestra, etiqueta de verdad, predicción, probabilidad de predicción y tiempo de predicción.\n",
    "def pred_and_store(paths: List[pathlib.Path], \n",
    "                   model: torch.nn.Module,\n",
    "                   transform: torchvision.transforms, \n",
    "                   class_names: List[str], \n",
    "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
    "    \n",
    "    # 2. Create an empty list to store prediction dictionaires\n",
    "    pred_list = []\n",
    "    \n",
    "    # 3. Loop through target paths\n",
    "    for path in tqdm(paths):\n",
    "        \n",
    "        # 4. Create empty dictionary to store prediction information for each sample\n",
    "        pred_dict = {}\n",
    "\n",
    "        # 5. Get the sample path and ground truth class name\n",
    "        pred_dict[\"image_path\"] = path\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "        \n",
    "        # 6. Start the prediction timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # 7. Open image path\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # 8. Transform the image, add batch dimension and put image on target device\n",
    "        transformed_image = transform(img).unsqueeze(0).to(device) \n",
    "        \n",
    "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # 10. Get prediction probability, predicition label and prediction class\n",
    "        with torch.inference_mode():\n",
    "            pred_logit = model(transformed_image) # perform inference on target sample \n",
    "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
    "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
    "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
    "\n",
    "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n",
    "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "            pred_dict[\"pred_class\"] = pred_class\n",
    "            \n",
    "            # 12. End the timer and calculate time per pred\n",
    "            end_time = timer()\n",
    "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
    "\n",
    "        # 13. Does the pred match the true label?\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "\n",
    "        # 14. Add the dictionary to the list of preds\n",
    "        pred_list.append(pred_dict)\n",
    "    \n",
    "    # 15. Return list of prediction dictionaries\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e0706",
   "metadata": {},
   "source": [
    "¡Ho, ho! \n",
    "\n",
    "¡Qué función tan atractiva!\n",
    "\n",
    "Y sabes qué, dado que nuestro `pred_and_store()` es una función de utilidad bastante buena para hacer y almacenar predicciones, podría almacenarse en [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke /pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) para su uso posterior. Podría ser una extensión que le gustaría probar; consulte [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) para obtener ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d53b8",
   "metadata": {},
   "source": [
    "### 5.2 Realización y sincronización de predicciones con EffNetB2\n",
    "\n",
    "¡Es hora de probar nuestra función `pred_and_store()`!\n",
    "\n",
    "Comencemos usándolo para hacer predicciones en todo el conjunto de datos de prueba con nuestro modelo EffNetB2, prestando atención a dos detalles:\n",
    "\n",
    "1. **Dispositivo**: codificaremos el parámetro \"dispositivo\" para usar \"cpu\" porque cuando implementemos nuestro modelo, no siempre tendremos acceso a un dispositivo \"cuda\" (GPU). .\n",
    "    * Hacer predicciones en la CPU también será un buen indicador de la velocidad de inferencia porque generalmente las predicciones en dispositivos con CPU son más lentas que las de los dispositivos con GPU.\n",
    "2. **Transformaciones** - También nos aseguraremos de establecer el parámetro `transform` en `effnetb2_transforms` para asegurarnos de que las imágenes se abran y transformen de la misma manera en la que se entrenó nuestro modelo `effnetb2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d81492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga predicciones en todo el conjunto de datos de prueba con EffNetB2\n",
    "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                          model=effnetb2,\n",
    "                                          transform=effnetb2_transforms,\n",
    "                                          class_names=class_names,\n",
    "                                          device=\"cpu\") # make predictions on CPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257021bd",
   "metadata": {},
   "source": [
    "¡Lindo! ¡Mira esas predicciones volar!\n",
    "\n",
    "Inspeccionemos la primera pareja y veamos cómo se ven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccione los primeros 2 diccionarios de predicción.\n",
    "effnetb2_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fab835",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Parece que nuestra función `pred_and_store()` funcionó bien.\n",
    "\n",
    "Gracias a nuestra lista de estructura de datos de diccionarios, tenemos mucha información útil que podemos inspeccionar más a fondo.\n",
    "\n",
    "Para hacerlo, convierta nuestra lista de diccionarios en un DataFrame de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta test_pred_dicts en un DataFrame\n",
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80be4f0",
   "metadata": {},
   "source": [
    "¡Hermoso!\n",
    "\n",
    "Mire con qué facilidad esos diccionarios de predicción se convierten en un formato estructurado sobre el que podemos realizar análisis.\n",
    "\n",
    "Como encontrar cuántas predicciones nuestro modelo EffNetB2 se equivocó..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar número de predicciones correctas\n",
    "effnetb2_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f0408",
   "metadata": {},
   "source": [
    "Cinco predicciones erróneas de un total de 150, ¡nada mal!\n",
    "\n",
    "¿Y qué tal el tiempo medio de predicción?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentre el tiempo promedio por predicción\n",
    "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551ca3f",
   "metadata": {},
   "source": [
    "Mmmm, ¿cómo cumple ese tiempo promedio de predicción con nuestros criterios de rendimiento de nuestro modelo en tiempo real (~30 FPS o 0,03 segundos por predicción)?\n",
    "\n",
    "> **Nota:** Los tiempos de predicción serán diferentes según los distintos tipos de hardware (por ejemplo, una CPU Intel i9 local frente a una CPU Google Colab). Cuanto mejor y más rápido sea el hardware, generalmente, más rápida será la predicción. Por ejemplo, en mi PC local de aprendizaje profundo con un chip Intel i9, mi tiempo promedio de predicción con EffNetB2 es de alrededor de 0,031 segundos (un poco menos que el tiempo real). Sin embargo, en Google Colab (no estoy seguro de qué hardware de CPU utiliza Colab, pero parece que podría ser un [Intel(R) Xeon(R)](https://stackoverflow.com/questions/47805170/whats-the -hardware-spec-for-google-colaboratory)), mi tiempo promedio de predicción con EffNetB2 es de aproximadamente 0,1396 segundos (3-4 veces más lento).\n",
    "\n",
    "Agreguemos nuestro tiempo promedio por predicción de EffNetB2 a nuestro diccionario `effnetb2_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregue el tiempo de predicción promedio de EffNetB2 al diccionario de estadísticas\n",
    "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d661174",
   "metadata": {},
   "source": [
    "### 5.3 Realizar y cronometrar predicciones con ViT \n",
    "\n",
    "Hemos hecho predicciones con nuestro modelo EffNetB2, ahora hagamos lo mismo con nuestro modelo ViT.\n",
    "\n",
    "Para hacerlo, podemos usar la función `pred_and_store()` que creamos anteriormente, excepto que esta vez pasaremos nuestro modelo `vit` así como `vit_transforms`.\n",
    "\n",
    "Y mantendremos las predicciones en la CPU a través de `device=\"cpu\"` (una extensión natural aquí sería probar los tiempos de predicción en la CPU y en la GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga una lista de diccionarios de predicción con el modelo de extracción de funciones de ViT en imágenes de prueba\n",
    "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                     model=vit,\n",
    "                                     transform=vit_transforms,\n",
    "                                     class_names=class_names,\n",
    "                                     device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c65e9",
   "metadata": {},
   "source": [
    "¡Predicciones hechas!\n",
    "\n",
    "Ahora echemos un vistazo a la primera pareja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55893a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique las primeras predicciones de ViT en el conjunto de datos de prueba\n",
    "vit_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd650a52",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Y al igual que antes, dado que las predicciones de nuestro modelo ViT tienen la forma de una lista de diccionarios, podemos convertirlas fácilmente en un DataFrame de pandas para una inspección más detallada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta vit_test_pred_dicts en un DataFrame\n",
    "import pandas as pd\n",
    "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
    "vit_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32273ad",
   "metadata": {},
   "source": [
    "¿Cuántas predicciones acertó nuestro modelo ViT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb28af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuente el número de predicciones correctas.\n",
    "vit_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f039f3a",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Nuestro modelo ViT funcionó un poco mejor que nuestro modelo EffNetB2 en términos de predicciones correctas, solo dos muestras incorrectas en todo el conjunto de datos de prueba.\n",
    "\n",
    "Como extensión, es posible que desee visualizar las predicciones incorrectas del modelo ViT y ver si hay alguna razón por la cual podrían haberse equivocado.\n",
    "\n",
    "¿Qué tal si calculamos cuánto tiempo tardó el modelo ViT en realizar cada predicción?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bf678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el tiempo promedio por predicción para el modelo ViT\n",
    "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c238746",
   "metadata": {},
   "source": [
    "Bueno, eso parece un poco más lento que el tiempo promedio por predicción de nuestro modelo EffNetB2, pero ¿cómo se ve en términos de nuestro segundo criterio: velocidad?\n",
    "\n",
    "Por ahora, agreguemos el valor a nuestro diccionario `vit_stats` para que podamos compararlo con las estadísticas de nuestro modelo EffNetB2.\n",
    "\n",
    "> **Nota:** El tiempo promedio por valor de predicción dependerá en gran medida del hardware en el que los realice. Por ejemplo, para el modelo ViT, mi tiempo promedio por predicción (en la CPU) fue de 0,0693 a 0,0777 segundos en mi PC local de aprendizaje profundo con una CPU Intel i9. Mientras que en Google Colab, mi tiempo promedio por predicción con el modelo ViT fue de 0,6766 a 0,7113 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be77977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregue el tiempo de predicción promedio para el modelo ViT en la CPU\n",
    "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62926330",
   "metadata": {},
   "source": [
    "## 6. Comparación de resultados de modelos, tiempos de predicción y tamaño\n",
    "\n",
    "Nuestros dos mejores modelos contendientes han sido capacitados y evaluados.\n",
    "\n",
    "Ahora pongámoslos cara a cara y comparemos sus diferentes estadísticas.\n",
    "\n",
    "Para hacerlo, convierta nuestros diccionarios `effnetb2_stats` y `vit_stats` en un DataFrame de pandas.\n",
    "\n",
    "Agregaremos una columna para ver los nombres de los modelos y convertiremos la precisión de la prueba a un porcentaje completo en lugar de decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta los diccionarios de estadísticas en DataFrame\n",
    "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
    "\n",
    "# Agregar columna para nombres de modelos\n",
    "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
    "\n",
    "# Convertir precisión a porcentajes\n",
    "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e6f8e",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Parece que nuestros modelos son bastante parecidos en términos de precisión general de las pruebas, pero ¿cómo se ven en otros campos?\n",
    "\n",
    "Una forma de averiguarlo sería dividir las estadísticas del modelo ViT por las estadísticas del modelo EffNetB2 para descubrir las diferentes proporciones entre los modelos.\n",
    "\n",
    "Creemos otro DataFrame para hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da434fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ViT con EffNetB2 según diferentes características\n",
    "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
    "             columns=[\"ViT to EffNetB2 ratios\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14ef8e",
   "metadata": {},
   "source": [
    "Parece que nuestro modelo ViT supera al modelo EffNetB2 en todas las métricas de rendimiento (pérdida de prueba, donde menor es mejor y precisión de la prueba, donde mayor es mejor), pero a expensas de tener:\n",
    "* 11x+ el número de parámetros.\n",
    "* 11x+ el tamaño del modelo. \n",
    "* 2,5 veces más el tiempo de predicción por imagen.\n",
    "\n",
    "¿Valen la pena estas compensaciones?\n",
    "\n",
    "Quizás si tuviéramos una potencia informática ilimitada, pero para nuestro caso de uso de implementar el modelo FoodVision Mini en un dispositivo más pequeño (por ejemplo, un teléfono móvil), probablemente comenzaríamos con el modelo EffNetB2 para realizar predicciones más rápidas con un rendimiento ligeramente reducido pero dramáticamente más pequeño. tamaño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746fdcb",
   "metadata": {},
   "source": [
    "### 6.1 Visualización del equilibrio entre velocidad y rendimiento \n",
    "\n",
    "Hemos visto que nuestro modelo ViT supera a nuestro modelo EffNetB2 en términos de métricas de rendimiento, como pérdida de prueba y precisión de prueba.\n",
    "\n",
    "Sin embargo, nuestro modelo EffNetB2 realiza predicciones más rápido y tiene un tamaño de modelo mucho más pequeño.\n",
    "\n",
    "> **Nota:** El tiempo de rendimiento o inferencia también suele denominarse \"latencia\".\n",
    "\n",
    "¿Qué tal si hacemos que este hecho sea visual?\n",
    "\n",
    "Podemos hacerlo creando un gráfico con matplotlib:\n",
    "1. Cree un diagrama de dispersión a partir del marco de datos de comparación para comparar los valores `time_per_pred_cpu` y `test_acc` de EffNetB2 y ViT.\n",
    "2. Agregue títulos y etiquetas correspondientes a los datos y personalice el tamaño de fuente por motivos estéticos.\n",
    "3. Anote las muestras en el diagrama de dispersión del paso 1 con sus etiquetas apropiadas (los nombres de los modelos).\n",
    "4. Cree una leyenda basada en los tamaños del modelo (`model_size (MB)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cree un gráfico a partir del marco de datos de comparación de modelos.\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(data=df, \n",
    "                     x=\"time_per_pred_cpu\", \n",
    "                     y=\"test_acc\", \n",
    "                     c=[\"blue\", \"orange\"], # what colours to use?\n",
    "                     s=\"model_size (MB)\") # size the dots by the model sizes\n",
    "\n",
    "# 2. Agregue títulos, etiquetas y personalice el tamaño de fuente por motivos estéticos.\n",
    "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
    "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
    "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "# 3. Anotar con nombres de modelos\n",
    "for index, row in df.iterrows():\n",
    "    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n",
    "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
    "                size=12)\n",
    "\n",
    "# 4. Crea una leyenda basada en los tamaños del modelo.\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
    "model_size_legend = ax.legend(handles, \n",
    "                              labels, \n",
    "                              loc=\"lower right\", \n",
    "                              title=\"Model size (MB)\",\n",
    "                              fontsize=12)\n",
    "\n",
    "# guarda la figura\n",
    "!mdkir images/\n",
    "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
    "\n",
    "# mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6dd1f6",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "La gráfica realmente visualiza la **velocidad versus rendimiento**; en otras palabras, cuando tienes un modelo profundo más grande y de mejor rendimiento (como nuestro modelo ViT), *generalmente* lleva más tiempo realizar la inferencia (mayor latencia).\n",
    "\n",
    "Hay excepciones a la regla y constantemente se publican nuevas investigaciones para ayudar a que los modelos más grandes funcionen más rápido.\n",
    "\n",
    "Y puede resultar tentador simplemente implementar el modelo de *mejor* rendimiento, pero también es bueno tener en cuenta dónde funcionará el modelo.\n",
    "\n",
    "En nuestro caso, las diferencias entre los niveles de rendimiento de nuestro modelo (en la pérdida de prueba y la precisión de la prueba) no son demasiado extremas.\n",
    "\n",
    "Pero como para empezar nos gustaría poner énfasis en la velocidad, seguiremos implementando EffNetB2 ya que es más rápido y ocupa mucho menos espacio.\n",
    "\n",
    "> **Nota:** Los tiempos de predicción serán diferentes según los diferentes tipos de hardware (por ejemplo, Intel i9 frente a CPU de Google Colab frente a GPU), por lo que es importante pensar y probar dónde terminará su modelo. Hacer preguntas como \"¿dónde se ejecutará el modelo?\" o \"¿cuál es el escenario ideal para ejecutar el modelo?\" y luego realizar experimentos para intentar proporcionar respuestas en el camino hacia la implementación es muy útil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da51ad8",
   "metadata": {},
   "source": [
    "## 7. Dar vida a FoodVision Mini creando una demostración de Gradio\n",
    "\n",
    "Hemos decidido que nos gustaría implementar el modelo EffNetB2 (para empezar, esto siempre se puede cambiar más adelante).\n",
    "\n",
    "Entonces, ¿cómo podemos hacer eso?\n",
    "\n",
    "Hay varias formas de implementar un modelo de aprendizaje automático, cada una con casos de uso específicos (como se analizó anteriormente).\n",
    "\n",
    "Nos centraremos en la que quizás sea la forma más rápida y ciertamente una de las más divertidas de implementar un modelo en Internet.\n",
    "\n",
    "Y eso es usando [Gradio](https://gradio.app/).\n",
    "\n",
    "¿Qué es Gradio?\n",
    "\n",
    "La página de inicio lo describe maravillosamente: \n",
    "\n",
    "> Gradio es la forma más rápida de hacer una demostración de su modelo de aprendizaje automático con una interfaz web amigable para que cualquiera pueda usarlo, ¡en cualquier lugar!\n",
    "\n",
    "¿Por qué crear una demostración de tus modelos?\n",
    "\n",
    "Porque las métricas en el conjunto de prueba se ven bien, pero nunca se sabe realmente cómo se desempeña su modelo hasta que lo usa en la naturaleza.\n",
    "\n",
    "¡Así que comencemos a implementar!\n",
    "\n",
    "Comenzaremos importando Gradio con el alias común `gr` y, si no está presente, lo instalaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16950ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar/instalar Gradio\n",
    "try:\n",
    "    import gradio as gr\n",
    "except: \n",
    "    !pip -q install gradio\n",
    "    import gradio as gr\n",
    "    \n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca4692",
   "metadata": {},
   "source": [
    "¡Gradio listo!\n",
    "\n",
    "Convirtamos FoodVision Mini en una aplicación de demostración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ecb68",
   "metadata": {},
   "source": [
    "### 7.1 Descripción general de Gradio\n",
    "\n",
    "La premisa general de Gradio es muy similar a la que hemos ido repitiendo a lo largo del curso.\n",
    "\n",
    "¿Cuáles son nuestras **entradas** y **salidas**?\n",
    "\n",
    "¿Y cómo deberíamos llegar allí?\n",
    "\n",
    "Bueno, eso es lo que hace nuestro modelo de aprendizaje automático.\n",
    "\n",
    "```\n",
    "entradas -> modelo ML -> salidas\n",
    "```\n",
    "\n",
    "En nuestro caso, para FoodVision Mini, nuestras entradas son imágenes de comida, nuestro modelo ML es EffNetB2 y nuestras salidas son clases de comida (pizza, bistec o sushi).\n",
    "\n",
    "```\n",
    "imágenes de alimentos -> EffNetB2 -> salidas\n",
    "```\n",
    "\n",
    "Aunque los conceptos de entradas y salidas pueden vincularse a casi cualquier otro tipo de problema de ML.\n",
    "\n",
    "Sus entradas y salidas pueden ser cualquier combinación de lo siguiente:\n",
    "* Imágenes\n",
    "* Texto\n",
    "* Video\n",
    "* Datos tabulados\n",
    "*Audio\n",
    "* Números\n",
    "* & más\n",
    "\n",
    "Y el modelo de ML que cree dependerá de sus entradas y salidas.\n",
    "\n",
    "Gradio emula este paradigma creando una interfaz ([`gradio.Interface()`](https://gradio.app/docs/#interface-header)) desde las entradas hasta las salidas.\n",
    "\n",
    "```\n",
    "gradio.Interface(fn, entradas, salidas)\n",
    "```\n",
    "\n",
    "Donde, \"fn\" es una función de Python para asignar las \"entradas\" a las \"salidas\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-gradio-workflow.png\" alt=\"flujo de trabajo de gradio de entradas que fluyen hacia algún tipo de modelo o función y luego producir resultados\" width=900/>\n",
    "\n",
    "*Gradio proporciona una clase `Interfaz` muy útil para crear fácilmente entradas -> modelo/función -> flujo de trabajo de salidas donde las entradas y salidas pueden ser casi cualquier cosa que desee. Por ejemplo, puede ingresar Tweets (texto) para ver si tratan sobre aprendizaje automático o no o [ingrese un mensaje de texto para generar imágenes](https://huggingface.co/blog/stable_diffusion).*\n",
    "\n",
    "> **Nota:** Gradio tiene una gran cantidad de posibles opciones de \"entradas\" y \"salidas\" conocidas como \"Componentes\", desde imágenes hasta texto, números, audio, videos y más. Puede verlos todos en la [documentación de componentes de Gradio] (https://gradio.app/docs/#components)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee313d1",
   "metadata": {},
   "source": [
    "### 7.2 Creando una función para mapear nuestras entradas y salidas\n",
    "\n",
    "Para crear nuestra demostración de FoodVision Mini con Gradio, necesitaremos una función para asignar nuestras entradas a nuestras salidas.\n",
    "\n",
    "Anteriormente creamos una función llamada `pred_and_store()` para hacer predicciones con un modelo determinado en una lista de archivos de destino y almacenarlos en una lista de diccionarios.\n",
    "\n",
    "¿Qué tal si creamos una función similar pero esta vez centrándonos en hacer una predicción en una sola imagen con nuestro modelo EffNetB2?\n",
    "\n",
    "Más específicamente, queremos una función que tome una imagen como entrada, la preprocese (transforme), haga una predicción con EffNetB2 y luego devuelva la predicción (pred o etiqueta pred para abreviar), así como la probabilidad de predicción (pred prob).\n",
    "\n",
    "Y ya que estamos aquí, retrocedamos el tiempo que nos llevó hacerlo también:\n",
    "\n",
    "```\n",
    "entrada: imagen -> transformar -> predecir con EffNetB2 -> salida: pred, pred prob, tiempo necesario\n",
    "```\n",
    "\n",
    "Este será nuestro parámetro `fn` para nuestra interfaz Gradio.\n",
    "\n",
    "Primero, asegurémonos de que nuestro modelo EffNetB2 esté en la CPU (ya que nos atenemos a las predicciones solo de CPU, sin embargo, puedes cambiar esto si tienes acceso a una GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponga EffNetB2 en la CPU\n",
    "effnetb2.to(\"cpu\") \n",
    "\n",
    "# Verifique el dispositivo\n",
    "next(iter(effnetb2.parameters())).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e1f3f",
   "metadata": {},
   "source": [
    "Y ahora creemos una función llamada `predict()` para replicar el flujo de trabajo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb99888",
   "metadata": {},
   "source": [
    "¡Hermoso! \n",
    "\n",
    "Ahora veamos nuestra función en acción realizando una predicción en una imagen aleatoria del conjunto de datos de prueba.\n",
    "\n",
    "Comenzaremos obteniendo una lista de todas las rutas de imágenes del directorio de prueba y luego seleccionaremos una al azar.\n",
    "\n",
    "Luego abriremos la imagen seleccionada al azar con [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
    "\n",
    "Finalmente, pasaremos la imagen a nuestra función `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Obtenga una lista de todas las rutas de archivos de imágenes de prueba\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "\n",
    "# Seleccione aleatoriamente una ruta de imagen de prueba\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "# Abra la imagen de destino\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
    "\n",
    "# Predecir sobre la imagen de destino e imprimir los resultados.\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
    "print(f\"Prediction time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db48bb9",
   "metadata": {},
   "source": [
    "¡Lindo!\n",
    "\n",
    "Al ejecutar la celda de arriba varias veces, podemos ver diferentes probabilidades de predicción para cada etiqueta de nuestro modelo EffNetB2, así como el tiempo que tomó cada predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d28a9c",
   "metadata": {},
   "source": [
    "### 7.3 Crear una lista de imágenes de ejemplo\n",
    "\n",
    "Nuestra función `predict()` nos permite ir desde entradas -> transformar -> modelo ML -> salidas.\n",
    "\n",
    "Que es exactamente lo que necesitamos para nuestra demostración de Graido.\n",
    "\n",
    "Pero antes de crear la demostración, creemos una cosa más: una lista de ejemplos.\n",
    "\n",
    "La clase [`Interface`](https://gradio.app/docs/#interface) de Gradio toma una lista de `ejemplos` como parámetro opcional (`gradio.Interface(examples=List[Any])`).\n",
    "\n",
    "Y el formato del parámetro \"ejemplos\" es una lista de listas.\n",
    "\n",
    "Entonces, creemos una lista de listas que contengan rutas de archivos aleatorias a nuestras imágenes de prueba.\n",
    "\n",
    "Tres ejemplos deberían ser suficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6081d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree una lista de entradas de ejemplo para nuestra demostración de Gradio\n",
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cef4a",
   "metadata": {},
   "source": [
    "¡Perfecto!\n",
    "\n",
    "Nuestra demostración de Gradio los mostrará como entradas de ejemplo para nuestra demostración para que las personas puedan probarlo y ver qué hace sin cargar ninguno de sus propios datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccd91e",
   "metadata": {},
   "source": [
    "### 7.4 Construyendo una interfaz Gradio\n",
    "\n",
    "¡Es hora de juntar todo y darle vida a nuestra demostración de FoodVision Mini!\n",
    "\n",
    "Creemos una interfaz Gradio para replicar el flujo de trabajo:\n",
    "\n",
    "```\n",
    "entrada: imagen -> transformar -> predecir con EffNetB2 -> salida: pred, pred prob, tiempo necesario\n",
    "```\n",
    "\n",
    "Lo podemos hacer con la clase [`gradio.Interface()`](https://gradio.app/docs/#interface) con los siguientes parámetros:\n",
    "* `fn` - una función de Python para asignar `entradas` a `salidas`; en nuestro caso, usaremos nuestra función `predict()`.\n",
    "* `inputs`: la entrada a nuestra interfaz, como una imagen usando [`gradio.Image()`](https://gradio.app/docs/#image) o `\"image\"`. \n",
    "* `outputs` - la salida de nuestra interfaz una vez que las `inputs` han pasado por `fn`, como una etiqueta usando [`gradio.Label()`](https://gradio.app/docs/#label ) (para las etiquetas predichas de nuestro modelo) o número usando [`gradio.Number()`](https://gradio.app/docs/#number) (para el tiempo de predicción de nuestro modelo).\n",
    "    * **Nota:** Gradio viene con muchas opciones integradas de `entradas` y `salidas` conocidas como [\"Componentes\"](https://gradio.app/docs/#components).\n",
    "* `ejemplos`: una lista de ejemplos para mostrar en la demostración.\n",
    "* `title` - un título de cadena de la demostración.\n",
    "* `descripción`: una cadena de descripción de la demostración.\n",
    "* `artículo`: una nota de referencia al final de la demostración.\n",
    "\n",
    "Una vez que hayamos creado nuestra instancia de demostración de `gr.Interface()`, podemos darle vida usando [`gradio.Interface().launch()`](https://gradio.app/docs/#launch -header) o el comando `demo.launch()`. \n",
    "\n",
    "¡Fácil!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c456b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Crear cadenas de título, descripción y artículo.\n",
    "title = \"FoodVision Mini 🍕🥩🍣\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Crea la demostración de Gradio\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# ¡Lanza la demostración!\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da65725",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-gradio-running-in-google-colab-and-in-browser.gif\" alt=\" Demostración de Gradio ejecutándose en Google Colab y en la web\" width=750/>\n",
    "\n",
    "*Demo de FoodVision Mini Gradio ejecutándose en Google Colab y en el navegador (el enlace cuando se ejecuta desde Google Colab solo dura 72 horas). Puedes ver la [demo permanente en vivo en Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini).*\n",
    "\n",
    "¡¡¡Guau!!! ¡¡¡Qué demostración tan épica!!!\n",
    "\n",
    "FoodVision Mini ha cobrado vida oficialmente en una interfaz que alguien podría usar y probar.\n",
    "\n",
    "Si configura el parámetro `share=True` en el método `launch()`, Gradio también le proporciona un enlace para compartir como `https://123XYZ.gradio.app` (este enlace es solo un ejemplo y probablemente esté vencido). ) que es válido por 72 horas.\n",
    "\n",
    "El enlace proporciona un proxy a la interfaz de Gradio que inició.\n",
    "\n",
    "Para un alojamiento más permanente, puede cargar su aplicación Gradio en [Hugging Face Spaces](https://huggingface.co/spaces) o en cualquier lugar que ejecute código Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b8927",
   "metadata": {},
   "source": [
    "## 8. Convertir nuestra demostración FoodVision Mini Gradio en una aplicación implementable\n",
    "\n",
    "Hemos visto nuestro modelo FoodVision Mini cobrar vida a través de una demostración de Gradio.\n",
    "\n",
    "Pero ¿y si quisiéramos compartirlo con nuestros amigos?\n",
    "\n",
    "Bueno, podríamos usar el enlace de Gradio proporcionado, sin embargo, el enlace compartido solo dura 72 horas.\n",
    "\n",
    "Para que nuestra demostración de FoodVision Mini sea más permanente, podemos empaquetarla en una aplicación y cargarla en [Hugging Face Spaces](https://huggingface.co/spaces/launch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ff935",
   "metadata": {},
   "source": [
    "### 8.1 ¿Qué es abrazar los espacios faciales?\n",
    "\n",
    "Hugging Face Spaces es un recurso que le permite alojar y compartir aplicaciones de aprendizaje automático.\n",
    "\n",
    "Crear una demostración es una de las mejores formas de mostrar y probar lo que ha hecho.\n",
    "\n",
    "Y Spaces te permite hacer precisamente eso.\n",
    "\n",
    "Puedes pensar en Hugging Face como el GitHub del aprendizaje automático.\n",
    "\n",
    "Si tener un buen portafolio de GitHub muestra tus habilidades de codificación, tener un buen portafolio de Hugging Face puede mostrar tus habilidades de aprendizaje automático.\n",
    "\n",
    "> **Nota:** Hay muchos otros lugares donde podríamos cargar y alojar nuestra aplicación Gradio, como Google Cloud, AWS (Amazon Web Services) u otros proveedores de nube; sin embargo, usaremos Hugging Face Spaces debido a la facilidad de uso y la amplia adopción por parte de la comunidad de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a3777",
   "metadata": {},
   "source": [
    "### 8.2 Estructura de la aplicación Gradio implementada\n",
    "\n",
    "Para cargar nuestra aplicación de demostración Gradio, queremos colocar todo lo relacionado con ella en un solo directorio.\n",
    "\n",
    "Por ejemplo, nuestra demostración podría estar en la ruta `demos/foodvision_mini/` con la estructura de archivos:\n",
    "\n",
    "```\n",
    "población/\n",
    "└── comidavision_mini/\n",
    "    ├── 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
    "    ├── aplicación.py\n",
    "    ├── ejemplos/\n",
    "    │ ├── ejemplo_1.jpg\n",
    "    │ ├── ejemplo_2.jpg\n",
    "    │ └── ejemplo_3.jpg\n",
    "    ├── modelo.py\n",
    "    └── requisitos.txt\n",
    "```\n",
    "\n",
    "Dónde:\n",
    "* `09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth` es nuestro archivo modelo PyTorch entrenado.\n",
    "* `app.py` contiene nuestra aplicación Gradio (similar al código que inició la aplicación).\n",
    "    * **Nota:** `app.py` es el nombre de archivo predeterminado utilizado para Hugging Face Spaces. Si implementas tu aplicación allí, Spaces buscará de manera predeterminada un archivo llamado `app.py` para ejecutar. Esto se puede cambiar en la configuración.\n",
    "* `examples/` contiene imágenes de ejemplo para usar con nuestra aplicación Gradio.\n",
    "* `model.py` contiene la definición del modelo, así como cualquier transformación asociada con el modelo.\n",
    "* `requirements.txt` contiene las dependencias para ejecutar nuestra aplicación, como `torch`, `torchvision` y `gradio`.\n",
    "\n",
    "¿Por qué de esta manera?\n",
    "\n",
    "Porque es uno de los diseños más simples con los que podríamos empezar. \n",
    "\n",
    "Nuestro enfoque es: *¡experimentar, experimentar, experimentar!* \n",
    "\n",
    "Cuanto más rápido podamos realizar experimentos más pequeños, mejores serán los más grandes.\n",
    "\n",
    "Vamos a trabajar para recrear la estructura anterior, pero puedes ver una aplicación de demostración en vivo ejecutándose en Hugging Face Spaces, así como la estructura de archivos:\n",
    "* [Demostración en vivo de Gradio de FoodVision Mini 🍕🥩🍣](https://huggingface.co/spaces/mrdbourke/foodvision_mini).\n",
    "* [Estructura de archivos FoodVision Mini en Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a284c2",
   "metadata": {},
   "source": [
    "### 8.3 Creando una carpeta `demos` para almacenar los archivos de nuestra aplicación FoodVision Mini\n",
    "\n",
    "Para comenzar, primero creemos un directorio `demos/` para almacenar todos los archivos de nuestra aplicación FoodVision Mini.\n",
    "\n",
    "Podemos hacerlo con [`pathlib.Path(\"path_to_dir\")`](https://docs.python.org/3/library/pathlib.html#basic-use) de Python para establecer la ruta del directorio y [`pathlib. Path(\"path_to_dir\").mkdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir) para crearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d63c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Crear una mini ruta de demostración de FoodVision\n",
    "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
    "\n",
    "# Elimine los archivos que ya puedan existir allí y cree un nuevo directorio\n",
    "if foodvision_mini_demo_path.exists():\n",
    "    shutil.rmtree(foodvision_mini_demo_path)\n",
    "    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n",
    "                                    exist_ok=True) # create it even if it already exists?\n",
    "else:\n",
    "    # If the file doesn't exist, create it anyway\n",
    "    foodvision_mini_demo_path.mkdir(parents=True, \n",
    "                                    exist_ok=True)\n",
    "    \n",
    "# Comprueba lo que hay en la carpeta.\n",
    "!ls demos/foodvision_mini/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c75470",
   "metadata": {},
   "source": [
    "### 8.4 Crear una carpeta de imágenes de ejemplo para usar con nuestra demostración de FoodVision Mini\n",
    "\n",
    "Ahora que tenemos un directorio para almacenar nuestros archivos de demostración de FoodVision Mini, agreguemos algunos ejemplos.\n",
    "\n",
    "Tres imágenes de ejemplo del conjunto de datos de prueba deberían ser suficientes.\n",
    "\n",
    "Para hacerlo haremos:\n",
    "1. Cree un directorio `examples/` dentro del directorio `demos/foodvision_mini`.\n",
    "2. Elija tres imágenes aleatorias del conjunto de datos de prueba y recopile sus rutas de archivo en una lista.\n",
    "3. Copie las tres imágenes aleatorias del conjunto de datos de prueba al directorio `demos/foodvision_mini/examples/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Cree un directorio de ejemplos\n",
    "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Recopile tres rutas de imágenes de conjuntos de datos de prueba aleatorias\n",
    "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
    "\n",
    "# 3. Copie las tres imágenes aleatorias al directorio de ejemplos.\n",
    "for example in foodvision_mini_examples:\n",
    "    destination = foodvision_mini_examples_path / example.name\n",
    "    print(f\"[INFO] Copying {example} to {destination}\")\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f0701",
   "metadata": {},
   "source": [
    "Ahora, para verificar que nuestros ejemplos estén presentes, enumeremos el contenido de nuestro directorio `demos/foodvision_mini/examples/` con [`os.listdir()`](https://docs.python.org/3/library/os. html#os.listdir) y luego formatee las rutas de archivo en una lista de listas (para que sea compatible con el parámetro `example` [`gradio.Interface()`](https://gradio.app/docs/#interface) de Gradio) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Obtenga rutas de archivos de ejemplo en una lista de listas\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f8473",
   "metadata": {},
   "source": [
    "### 8.5 Mover nuestro modelo EffNetB2 entrenado a nuestro directorio de demostración FoodVision Mini\n",
    "\n",
    "Anteriormente guardamos nuestro modelo de extractor de funciones FoodVision Mini EffNetB2 en `models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
    "\n",
    "Y en lugar de duplicar los archivos de modelo guardados, muevamos nuestro modelo a nuestro directorio `demos/foodvision_mini`.\n",
    "\n",
    "Podemos hacerlo usando el método [`shutil.move()`](https://docs.python.org/3/library/shutil.html#shutil.move) de Python y pasando `src` (la ruta fuente de el archivo de destino) y `dst` (la ruta de destino del archivo de destino al que se va a mover)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Crear una ruta de origen para nuestro modelo de destino.\n",
    "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
    "\n",
    "# Crear una ruta de destino para nuestro modelo objetivo.\n",
    "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "# Intenta mover el archivo\n",
    "try:\n",
    "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
    "    \n",
    "    # Move the model\n",
    "    shutil.move(src=effnetb2_foodvision_mini_model_path, \n",
    "                dst=effnetb2_foodvision_mini_model_destination)\n",
    "    \n",
    "    print(f\"[INFO] Model move complete.\")\n",
    "\n",
    "# Si el modelo ya ha sido movido, verifique si existe.\n",
    "except:\n",
    "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
    "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a414e8",
   "metadata": {},
   "source": [
    "### 8.6 Convirtiendo nuestro modelo EffNetB2 en un script Python (`model.py`)\n",
    "\n",
    "El `state_dict` de nuestro modelo actual se guarda en `demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
    "\n",
    "Para cargarlo podemos usar `model.load_state_dict()` junto con `torch.load()`.\n",
    "\n",
    "> **Nota:** Para obtener una actualización sobre cómo guardar y cargar un modelo (o el `state_dict` de un modelo en PyTorch, consulte [01. Fundamentos del flujo de trabajo de PyTorch, sección 5: Guardar y cargar un modelo de PyTorch](https://www. learnpytorch.io/01_pytorch_workflow/#5-served-and-loading-a-pytorch-model) o consulte la receta de PyTorch para [¿Qué es un `state_dict` en PyTorch?](https://pytorch.org/tutorials/recipes /recetas/what_is_state_dict.html)\n",
    "\n",
    "Pero antes de que podamos hacer esto, primero necesitamos una forma de crear una instancia de un \"modelo\".\n",
    "\n",
    "Para hacer esto de forma modular, crearemos un script llamado `model.py` que contiene nuestra función `create_effnetb2_model()` que creamos en [sección 3.1: *Creación de una función para crear un extractor de funciones EffNetB2*](https: //www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor).\n",
    "\n",
    "De esa manera podemos importar la función en *otro* script (ver `app.py` a continuación) y luego usarla para crear nuestra instancia de `modelo` EffNetB2, así como obtener sus transformaciones apropiadas.\n",
    "\n",
    "Al igual que en [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/), usaremos el comando mágico `%%writefile path/to/file` para convertir una celda de código en un archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd53a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b6267",
   "metadata": {},
   "source": [
    "### 8.7 Convirtiendo nuestra aplicación FoodVision Mini Gradio en un script Python (`app.py`)\n",
    "\n",
    "Ahora tenemos un script `model.py` así como una ruta a un modelo guardado `state_dict` que podemos cargar.\n",
    "\n",
    "Es hora de construir `app.py`.\n",
    "\n",
    "Lo llamamos `app.py` porque, de forma predeterminada, cuando creas un espacio HuggingFace, busca un archivo llamado `app.py` para ejecutarlo y alojarlo (aunque puedes cambiar esto en la configuración).\n",
    "\n",
    "Nuestro script `app.py` juntará todas las piezas del rompecabezas para crear nuestra demostración de Gradio y tendrá cuatro partes principales: \n",
    "\n",
    "1. **Configuración de importaciones y nombres de clases** - Aquí importaremos las diversas dependencias para nuestra demostración, incluida la función `create_effnetb2_model()` de `model.py`, así como también configuraremos los diferentes nombres de clases para nuestra aplicación FoodVision Mini. . \n",
    "2. **Preparación de modelos y transformaciones**: aquí crearemos una instancia de modelo EffNetB2 junto con las transformaciones que la acompañan y luego cargaremos los pesos/`state_dict` del modelo guardado. Cuando cargamos el modelo, también configuraremos `map_location=torch.device(\"cpu\")` en [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load .html) para que nuestro modelo se cargue en la CPU independientemente del dispositivo en el que se entrenó (hacemos esto porque no necesariamente tendremos una GPU cuando implementemos y obtendremos un error si nuestro modelo está entrenado en GPU pero intente implementarlo en la CPU sin decirlo explícitamente).\n",
    "3. **Función de predicción** - `gradio.Interface()` de Gradio toma un parámetro `fn` para asignar entradas a salidas, nuestra función `predict()` será la misma que definimos anteriormente en la [sección 7.2 : *Creando una función para mapear nuestras entradas y salidas*](https://www.learnpytorch.io/09_pytorch_model_deployment/#72-creating-a-function-to-map-our-inputs-and-outputs), lo hará tome una imagen y luego use las transformaciones cargadas para preprocesarla antes de usar el modelo cargado para hacer una predicción sobre ella.\n",
    "    * **Nota:** Tendremos que crear la lista de ejemplo sobre la marcha mediante el parámetro `examples`. Podemos hacerlo creando una lista de archivos dentro del directorio `examples/` con: `[[\"examples/\" + example] por ejemplo en os.listdir(\"examples\")]`.\n",
    "4. **Aplicación Gradio** - Aquí es donde vivirá la lógica principal de nuestra demostración, crearemos una instancia `gradio.Interface()` llamada `demo` para juntar nuestras entradas, función `predict()` y salidas. ¡Y terminaremos el script llamando a `demo.launch()` para iniciar nuestra demostración de FoodVision Mini!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "# ## 1. Configuración de importaciones y nombres de clases ###\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Configurar nombres de clases\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "# ## 2. Preparación del modelo y transforma ###\n",
    "\n",
    "# Crear modelo EffNetB2\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=3, # len(class_names) would also work\n",
    ")\n",
    "\n",
    "# Cargar pesos guardados\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "# ## 3. Función de predicción ###\n",
    "\n",
    "# Crear función de predicción\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# ## 4. Aplicación Gradio ###\n",
    "\n",
    "# Crear cadenas de título, descripción y artículo.\n",
    "title = \"FoodVision Mini 🍕🥩🍣\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Crear una lista de ejemplos desde el directorio \"examples/\"\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Crea la demostración de Gradio\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    # Create examples list from \"examples/\" directory\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# ¡Lanza la demostración!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79c5b9",
   "metadata": {},
   "source": [
    "### 8.8 Creación de un archivo de requisitos para FoodVision Mini (`requirements.txt`)\n",
    "\n",
    "El último archivo que debemos crear para nuestra aplicación FoodVision Mini es un [archivo `requirements.txt`] (https://learnpython.com/blog/python-requirements-file/).\n",
    "\n",
    "Este será un archivo de texto que contendrá todas las dependencias necesarias para nuestra demostración.\n",
    "\n",
    "Cuando implementemos nuestra aplicación de demostración en Hugging Face Spaces, buscará en este archivo e instalará las dependencias que definimos para que nuestra aplicación pueda ejecutarse.\n",
    "\n",
    "¡La buena noticia es que sólo hay tres!\n",
    "\n",
    "1. `antorcha==1.12.0`\n",
    "2. `torchvision==0.13.0`\n",
    "3. `gradio==3.1.4`\n",
    "\n",
    "\"`==1.12.0`\" indica el número de versión a instalar.\n",
    "\n",
    "Definir el número de versión no es 100% obligatorio, pero lo haremos por ahora, de modo que si se producen actualizaciones importantes en futuras versiones, nuestra aplicación aún se ejecuta (PD: si encuentra algún error, no dude en publicarlo en el curso [Problemas de GitHub](https: //github.com/mrdbourke/pytorch-deep-learning/issues))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9557950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/requirements.txt\n",
    "torch==1.12.0\n",
    "torchvision==0.13.0\n",
    "gradio==3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec66e0",
   "metadata": {},
   "source": [
    "¡Lindo!\n",
    "\n",
    "¡Tenemos oficialmente todos los archivos que necesitamos para implementar nuestra demostración de FoodVision Mini!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2223",
   "metadata": {},
   "source": [
    "## 9. Implementación de nuestra aplicación FoodVision Mini en HuggingFace Spaces\n",
    "\n",
    "Tenemos un archivo que contiene nuestra demostración de FoodVision Mini. Ahora, ¿cómo hacemos para que se ejecute en Hugging Face Spaces?\n",
    "\n",
    "Hay dos opciones principales para cargar en un Hugging Face Space (también llamado [Repositorio de Hugging Face](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories), similares a un repositorio de git): \n",
    "1. [Carga a través de la interfaz web de Hugging Face (más fácil)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui).\n",
    "2. [Carga a través de la línea de comando o terminal] (https://huggingface.co/docs/hub/repositories-getting-started#terminal).\n",
    "    * **Bonificación:** También puedes usar la [biblioteca `huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) para interactuar con Hugging Face, esta sería una buena extensión de las dos opciones anteriores. .\n",
    "\n",
    "No dude en leer la documentación sobre ambas opciones, pero optaremos por la opción dos.\n",
    "\n",
    "> **Nota:** Para alojar cualquier cosa en Hugging Face, deberás [registrarte para obtener una cuenta gratuita de Hugging Face](https://huggingface.co/join)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155f6d8",
   "metadata": {},
   "source": [
    "### 9.1 Descarga de los archivos de nuestra aplicación FoodVision Mini\n",
    "\n",
    "Veamos los archivos de demostración que tenemos dentro de `demos/foodvision_mini`.\n",
    "\n",
    "Para hacerlo, podemos usar el comando `!ls` seguido de la ruta del archivo de destino.\n",
    "\n",
    "`ls` significa \"lista\" y `!` significa que queremos ejecutar el comando en el nivel de shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b068e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls demos/foodvision_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd4675",
   "metadata": {},
   "source": [
    "¡Estos son todos los archivos que hemos creado!\n",
    "\n",
    "Para comenzar a cargar nuestros archivos en Hugging Face, descarguémoslos ahora desde Google Colab (o dondequiera que esté ejecutando este cuaderno).\n",
    "\n",
    "Para hacerlo, primero comprimiremos los archivos en una única carpeta zip mediante el comando: \n",
    "\n",
    "```\n",
    "zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "```\n",
    "\n",
    "Dónde: \n",
    "* `zip` significa \"zip\", como en \"comprima los archivos en el siguiente directorio\". \n",
    "* `-r` significa \"recursivo\", como en \"revisar todos los archivos en el directorio de destino\".\n",
    "* `../foodvision_mini.zip` es el directorio de destino donde nos gustaría comprimir nuestros archivos.\n",
    "* `*` significa \"todos los archivos en el directorio actual\".\n",
    "* `-x` significa \"excluir estos archivos\". \n",
    "\n",
    "Podemos descargar nuestro archivo zip de Google Colab usando [`google.colab.files.download(\"demos/foodvision_mini.zip\")`](https://colab.research.google.com/notebooks/io.ipynb) ( Pondremos esto dentro de un bloque `try` y `except` en caso de que no estemos ejecutando el código dentro de Google Colab y, de ser así, imprimiremos un mensaje que indicará que descarguemos los archivos manualmente).\n",
    "\n",
    "¡Probémoslo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74040c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambie y luego comprima la carpeta foodvision_mini pero excluya ciertos archivos\n",
    "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "\n",
    "# Descargue la aplicación FoodVision Mini comprimida (si se ejecuta en Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"demos/foodvision_mini.zip\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54495e0",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Parece que nuestro comando `zip` fue exitoso.\n",
    "\n",
    "Si está ejecutando este cuaderno en Google Colab, debería ver que un archivo comienza a descargarse en su navegador.\n",
    "\n",
    "De lo contrario, puede ver la carpeta `foodvision_mini.zip` (y más) en el [curso GitHub en el directorio `demos/`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/ población)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63f0e4",
   "metadata": {},
   "source": [
    "### 9.2 Ejecución de nuestra demostración FoodVision Mini localmente\n",
    "\n",
    "Si descarga el archivo `foodvision_mini.zip`, puede probarlo localmente de la siguiente manera:\n",
    "1. Descomprimiendo el archivo.\n",
    "2. Abrir la terminal o una línea de comando.\n",
    "3. Cambiar al directorio `foodvision_mini` (`cd foodvision_mini`).\n",
    "4. Crear un entorno (`python3 -m venv env`).\n",
    "5. Activar el entorno (`source env/bin/activate`).\n",
    "5. Instalar los requisitos (`pip install -r requisitos.txt`, \"`-r`\" es para recursivo).\n",
    "    * **Nota:** Este paso puede tardar entre 5 y 10 minutos dependiendo de tu conexión a Internet. Y si enfrenta errores, es posible que primero necesite actualizar `pip`: `pip install --upgrade pip`.\n",
    "6. Ejecute la aplicación (`python3 app.py`).\n",
    "\n",
    "Esto debería dar como resultado una demostración de Gradio como la que creamos anteriormente ejecutándose localmente en su máquina en una URL como `http://127.0.0.1:7860/`.\n",
    "\n",
    "> **Nota:** Si ejecuta la aplicación localmente y observa que aparece un directorio `marcado/`, contiene muestras que han sido \"marcadas\". \n",
    ">\n",
    "> Por ejemplo, si alguien prueba la demostración y el modelo produce un resultado incorrecto, la muestra se puede \"marcar\" y revisar para más adelante.\n",
    "> \n",
    "> Para obtener más información sobre cómo marcar en Gradio, consulte la [documentación sobre cómo marcar](https://gradio.app/docs/#flagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177828a4",
   "metadata": {},
   "source": [
    "### 9.3 Subir a Hugging Face\n",
    "\n",
    "Hemos verificado que nuestra aplicación FoodVision Mini funciona localmente; sin embargo, lo divertido de crear una demostración de aprendizaje automático es mostrársela a otras personas y permitirles usarla.\n",
    "\n",
    "Para hacerlo, cargaremos nuestra demostración de FoodVision Mini en Hugging Face. \n",
    "\n",
    "> **Nota:** La siguiente serie de pasos utiliza un flujo de trabajo Git (un sistema de seguimiento de archivos). Para obtener más información sobre cómo funciona Git, recomiendo consultar el [tutorial de Git y GitHub para principiantes](https://youtu.be/RGOj5yH7evk) en freeCodeCamp.\n",
    "\n",
    "1. [Regístrese](https://huggingface.co/join) para obtener una cuenta de Hugging Face. \n",
    "2. Inicie un nuevo Hugging Face Space yendo a su perfil y luego [haciendo clic en \"Nuevo espacio\"](https://huggingface.co/new-space).\n",
    "    * **Nota:** Un espacio en Hugging Face también se conoce como \"repositorio de códigos\" (un lugar para almacenar su código/archivos) o \"repositorio\" para abreviar.\n",
    "3. Dale un nombre al Espacio, por ejemplo, el mío se llama `mrdbourke/foodvision_mini`, puedes verlo aquí: https://huggingface.co/spaces/mrdbourke/foodvision_mini\n",
    "4. Seleccione una licencia (yo usé [MIT](https://opensource.org/licenses/MIT)).\n",
    "5. Seleccione Gradio como Space SDK (kit de desarrollo de software). \n",
    "   * **Nota:** Puedes usar otras opciones como Streamlit, pero como nuestra aplicación está construida con Gradio, nos quedaremos con eso.\n",
    "6. Elija si su Espacio es público o privado (seleccioné público porque me gustaría que mi Espacio esté disponible para otros).\n",
    "7. Haga clic en \"Crear espacio\".\n",
    "8. Clone el repositorio localmente ejecutando algo como: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` en la terminal o en el símbolo del sistema.\n",
    "    * **Nota:** También puedes agregar archivos cargándolos en la pestaña \"Archivos y versiones\".\n",
    "9. Copie/mueva el contenido de la carpeta `foodvision_mini` descargada a la carpeta del repositorio clonado.\n",
    "10. Para cargar y rastrear archivos más grandes (por ejemplo, archivos de más de 10 MB o, en nuestro caso, nuestro archivo modelo PyTorch), necesitará [instalar Git LFS](https://git-lfs.github.com/) (que significa para \"almacenamiento de archivos grandes de git\").\n",
    "11. Después de haber instalado Git LFS, puedes activarlo ejecutando `git lfs install`.\n",
    "12. En el directorio `foodvision_mini`, realice un seguimiento de los archivos de más de 10 MB con Git LFS con `git lfs track \"*.file_extension\"`.\n",
    "    * Realice un seguimiento del archivo de modelo EffNetB2 PyTorch con `git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"`.\n",
    "13. Seguimiento de `.gitattributes` (creado automáticamente al clonar desde HuggingFace; este archivo ayudará a garantizar que nuestros archivos más grandes sean rastreados con Git LFS). Puede ver un archivo `.gitattributes` de ejemplo en [FoodVision Mini Hugging Face Space] (https://huggingface.co/spaces/mrdbourke/foodvision_mini/blob/main/.gitattributes).\n",
    "    * `git agregar .gitattributes`\n",
    "14. Agregue el resto de los archivos de la aplicación `foodvision_mini` y confírmelos con: \n",
    "    * `git agregar *`\n",
    "    * `git commit -m \"primer compromiso\"`\n",
    "15. Envíe (cargue) los archivos a Hugging Face:\n",
    "    *`git push`\n",
    "16. Espere de 3 a 5 minutos hasta que se complete la compilación (las compilaciones futuras serán más rápidas) y su aplicación estará activa.\n",
    "\n",
    "Si todo funcionó, debería ver un ejemplo en vivo de nuestra demostración de FoodVision Mini Gradio como este: https://huggingface.co/spaces/mrdbourke/foodvision_mini \n",
    "\n",
    "E incluso podemos insertar nuestra demostración de FoodVision Mini Gradio en nuestra computadora portátil como un [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) con [`IPython.display.IFrame`](https:/ /ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) y un enlace a nuestro espacio en el formato `https://hf.space/embed/[YOUR_USERNAME] /[TU_NOMBRE_ESPACIO]/+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c08623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython es una biblioteca para ayudar a que Python sea interactivo\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Incrustar la demostración de FoodVision Mini Gradio\n",
    "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5335d8",
   "metadata": {},
   "source": [
    "## 10. Creando FoodVision a lo grande\n",
    "\n",
    "Hemos pasado las últimas secciones y capítulos trabajando para darle vida a FoodVision Mini.\n",
    "\n",
    "Y ahora que lo hemos visto funcionar en una demostración en vivo, ¿qué tal si damos un paso más?\n",
    "\n",
    "¿Cómo?\n",
    "\n",
    "¡Visión alimentaria a lo grande!\n",
    "\n",
    "Dado que FoodVision Mini está entrenado con imágenes de pizza, bistec y sushi del [conjunto de datos Food101] (https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) (101 clases de alimentos x 1000 imágenes cada una ), ¿qué tal si hacemos FoodVision Big entrenando un modelo en las 101 clases?\n",
    "\n",
    "¡Pasaremos de tres clases a 101!\n",
    "\n",
    "¡Desde pizza, bistec, sushi hasta pizza, bistec, sushi, hot dog, tarta de manzana, pastel de zanahoria, pastel de chocolate, fuegos franceses, pan de ajo, ramen, nachos, tacos y más!\n",
    "\n",
    "¿Cómo?\n",
    "\n",
    "Bueno, tenemos todos los pasos implementados, todo lo que tenemos que hacer es modificar ligeramente nuestro modelo EffNetB2 y preparar un conjunto de datos diferente.\n",
    "\n",
    "Para finalizar Milestone Project 3, recreemos una demostración de Gradio similar a FoodVision Mini (tres clases) pero para FoodVision Big (101 clases).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-foodvision-mini-to-foodvision-big.png\" alt=\"foodvision mini modele en tres clases: pizza, bistec, sushi y foodvision big en las 101 clases del conjunto de datos food101\" width=900/>\n",
    "\n",
    "*FoodVision Mini funciona con tres clases de alimentos: pizza, bistec y sushi. Y FoodVision Big va un paso más allá para trabajar en 101 clases de alimentos: todas las [clases en el conjunto de datos Food101](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names. TXT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a8045",
   "metadata": {},
   "source": [
    "### 10.1 Creando un modelo y transformaciones para FoodVision Big\n",
    "\n",
    "Al crear FoodVision Mini vimos que el modelo EffNetB2 era un buen equilibrio entre velocidad y rendimiento (funcionó bien a alta velocidad).\n",
    "\n",
    "Así que continuaremos usando el mismo modelo para FoodVision Big.\n",
    "\n",
    "Podemos crear un extractor de funciones EffNetB2 para Food101 usando nuestra función `create_effnetb2_model()` que creamos anteriormente, en la [sección 3.1](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to -make-an-effnetb2-feature-extractor), y pasándole el parámetro `num_classes=101` (ya que Food101 tiene 101 clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree un modelo EffNetB2 capaz de adaptarse a 101 clases para Food101\n",
    "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20507f",
   "metadata": {},
   "source": [
    "¡Hermoso!\n",
    "\n",
    "Ahora obtengamos un resumen de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Obtenga un resumen del extractor de funciones EffNetB2 para Food101 con 101 clases de salida (descomentar para obtener una salida completa)\n",
    "# resumen(effnetb2_food101,\n",
    "# tamaño_entrada=(1, 3, 224, 224),\n",
    "# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n",
    "# ancho_columna=20,\n",
    "# row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1032ad1",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-effnetb2-feature-extractor-101-classes.png\" width=900 alt=\"extractor de funciones effnetb2 con resumen del modelo de 100 clases de salida\"/>\n",
    "                                                                                                                                                      \n",
    "¡Lindo!\n",
    "\n",
    "Vea cómo, al igual que nuestro modelo EffNetB2 para FoodVision Mini, las capas base están congeladas (éstas están previamente entrenadas en ImageNet) y las capas externas (las capas `clasificadoras`) se pueden entrenar con una forma de salida de `[batch_size, 101]` (`101 ` para 101 clases en Food101). \n",
    "\n",
    "Ahora que vamos a tratar con bastante más datos de lo habitual, ¿qué tal si agregamos un poco de aumento de datos a nuestras transformaciones (`effnetb2_transforms`) para aumentar los datos de entrenamiento?\n",
    "\n",
    "> **Nota:** El aumento de datos es una técnica que se utiliza para alterar la apariencia de una muestra de entrenamiento de entrada (por ejemplo, rotar una imagen o sesgarla ligeramente) para aumentar artificialmente la diversidad de un conjunto de datos de entrenamiento y, con suerte, evitar el sobreajuste. Puede ver más sobre el aumento de datos en [04. Sección 6 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation).\n",
    "\n",
    "Compongamos una canalización `torchvision.transforms` para usar [`torchvision.transforms.TrivialAugmentWide()`](https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html) (el mismo aumento de datos utilizado por el equipo de PyTorch en sus [recetas de visión por computadora] (https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break- mejoras de precisión de clave)) así como `effnetb2_transforms` para transformar nuestras imágenes de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree transformaciones de datos de entrenamiento de Food101 (realice solo aumento de datos en las imágenes de entrenamiento)\n",
    "food101_train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.TrivialAugmentWide(),\n",
    "    effnetb2_transforms,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08085acd",
   "metadata": {},
   "source": [
    "¡Épico!\n",
    "\n",
    "Ahora comparemos `food101_train_transforms` (para los datos de entrenamiento) y `effnetb2_transforms` (para los datos de prueba/inferencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training transforms:\\n{food101_train_transforms}\\n\") \n",
    "print(f\"Testing transforms:\\n{effnetb2_transforms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0be74",
   "metadata": {},
   "source": [
    "### 10.2 Obtención de datos para FoodVision Big\n",
    "\n",
    "Para FoodVision Mini, creamos nuestras propias [divisiones de datos personalizadas](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) de todo el conjunto de datos de Food101.\n",
    "\n",
    "Para obtener el conjunto de datos completo de Food101, podemos usar [`torchvision.datasets.Food101()`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html).\n",
    "\n",
    "Primero configuraremos una ruta al directorio `data/` para almacenar las imágenes. \n",
    "\n",
    "Luego descargaremos y transformaremos las divisiones del conjunto de datos de entrenamiento y prueba usando `food101_train_transforms` y `effnetb2_transforms` para transformar cada conjunto de datos respectivamente. \n",
    "\n",
    "> **Nota:** Si está utilizando Google Colab, la siguiente celda tardará entre 3 y 5 minutos en ejecutarse por completo y descargar las imágenes de Food101 desde PyTorch. \n",
    ">\n",
    "> Esto se debe a que se están descargando más de 100.000 imágenes (101 clases x 1000 imágenes por clase). Si reinicia el tiempo de ejecución de Google Colab y regresa a esta celda, las imágenes deberán volver a descargarse. Alternativamente, si está ejecutando este cuaderno localmente, las imágenes se almacenarán en caché y se almacenarán en el directorio especificado por el parámetro `root` de `torchvision.datasets.Food101()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7113f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Directorio de datos de configuración\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Obtenga datos de entrenamiento (~750 imágenes x 101 clases de alimentos)\n",
    "train_data = datasets.Food101(root=data_dir, # path to download data to\n",
    "                              split=\"train\", # dataset split to get\n",
    "                              transform=food101_train_transforms, # perform data augmentation on training data\n",
    "                              download=True) # want to download?\n",
    "\n",
    "# Obtenga datos de prueba (~250 imágenes x 101 clases de alimentos)\n",
    "test_data = datasets.Food101(root=data_dir,\n",
    "                             split=\"test\",\n",
    "                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n",
    "                             download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535c3b6",
   "metadata": {},
   "source": [
    "¡Datos descargados!\n",
    "\n",
    "Ahora podemos obtener una lista de todos los nombres de clases usando `train_data.classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener nombres de clases de Food101\n",
    "food101_class_names = train_data.classes\n",
    "\n",
    "# Ver los primeros 10\n",
    "food101_class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361d8f5",
   "metadata": {},
   "source": [
    "¡Ho, ho! Esas son algunas comidas que suenan deliciosas (aunque nunca he oído hablar de los \"buñuelos\"... actualización: después de una búsqueda rápida en Google, los buñuelos también se ven deliciosos). \n",
    "                                                \n",
    "Puede ver una lista completa de los nombres de las clases Food101 en el curso GitHub en [`extras/food101_class_names.txt`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names. TXT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7227c6",
   "metadata": {},
   "source": [
    "### 10.3 Creación de un subconjunto del conjunto de datos Food101 para experimentar más rápido \n",
    "\n",
    "Esto es opcional.\n",
    "\n",
    "No *necesitamos* crear otro subconjunto del conjunto de datos de Food101, podríamos entrenar y evaluar un modelo en las 101.000 imágenes completas.\n",
    "\n",
    "Pero para seguir entrenando rápido, creemos una división del 20 % de los conjuntos de datos de entrenamiento y prueba.\n",
    "\n",
    "Nuestro objetivo será ver si podemos superar los mejores resultados del [documento Food101] original (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) con solo el 20 % de los datos.\n",
    "\n",
    "Para desglosar los conjuntos de datos que hemos utilizado/utilizaremos:\n",
    "\n",
    "| **Cuadernos(s)** | **Nombre del proyecto** | **Conjunto de datos** | **Número de clases** | **Imágenes de entrenamiento** | **Imágenes de prueba** | \n",
    "| ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| 04, 05, 06, 07, 08 | FoodVision Mini (10% de datos) | Food101 división personalizada | 3 (pizza, bistec, sushi) | 225 | 75 | \n",
    "| 07, 08, 09 | FoodVision Mini (20% de datos) | Food101 división personalizada | 3 (pizza, bistec, sushi) | 450 | 150 |\n",
    "| **09 (este)** | FoodVision Big (20% de datos) | Food101 división personalizada | 101 (todas las clases de Food101) | 15150 | 5050 | \n",
    "| Ampliación | FoodVision grande | Food101 todos los datos | 101 | 75750 | 25250 | \n",
    "\n",
    "¿Puedes ver la tendencia? \n",
    "\n",
    "Así como el tamaño de nuestro modelo aumentó lentamente con el tiempo, también lo hizo el tamaño del conjunto de datos que hemos estado usando para los experimentos.\n",
    "\n",
    "> **Nota:** Para superar realmente los resultados del artículo original de Food101 con el 20 % de los datos, tendríamos que entrenar un modelo con el 20 % de los datos de entrenamiento y luego evaluar nuestro modelo en el conjunto de pruebas *completo* en lugar de que la división que creamos. Dejaré esto como un ejercicio de extensión para que lo pruebes. También le recomiendo que intente entrenar un modelo en todo el conjunto de datos de entrenamiento de Food101.\n",
    "\n",
    "Para dividir nuestro FoodVision Big (20% de datos), creemos una función llamada `split_dataset()` para dividir un conjunto de datos determinado en ciertas proporciones.\n",
    "\n",
    "Podemos usar [`torch.utils.data.random_split()`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) para crear divisiones de tamaños determinados usando ` parámetro de longitudes. \n",
    "\n",
    "El parámetro `longitudes` acepta una lista de longitudes divididas deseadas donde el total de la lista debe ser igual a la longitud total del conjunto de datos.\n",
    "\n",
    "Por ejemplo, con un conjunto de datos de tamaño 100, podría pasar `longitudes=[20, 80]` para recibir una división del 20 % y del 80 %.\n",
    "\n",
    "Querremos que nuestra función devuelva dos divisiones, una con la longitud objetivo (por ejemplo, el 20 % de los datos de entrenamiento) y la otra con la longitud restante (por ejemplo, el 80 % restante de los datos de entrenamiento).\n",
    "\n",
    "Finalmente, estableceremos el parámetro `generador` en un valor `torch.manual_seed()` para mayor reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348192e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n",
    "    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n",
    "\n",
    "    Args:\n",
    "        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n",
    "        split_size (float, optional): How much of the dataset should be split? \n",
    "            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n",
    "        seed (int, optional): Seed for random generator. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n",
    "            random_split_2 is of size (1-split_size)*len(dataset).\n",
    "    \"\"\"\n",
    "    # Create split lengths based on original dataset length\n",
    "    length_1 = int(len(dataset) * split_size) # desired length\n",
    "    length_2 = len(dataset) - length_1 # remaining length\n",
    "        \n",
    "    # Print out info\n",
    "    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n",
    "    \n",
    "    # Create splits with given random seed\n",
    "    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n",
    "                                                                   lengths=[length_1, length_2],\n",
    "                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n",
    "    return random_split_1, random_split_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf24dd",
   "metadata": {},
   "source": [
    "¡Se creó la función de división del conjunto de datos!\n",
    "\n",
    "Ahora probémoslo creando una división del 20% del conjunto de datos de prueba y capacitación de Food101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear capacitación División del 20% de Food101\n",
    "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
    "                                                 split_size=0.2)\n",
    "\n",
    "# Crear pruebas con una división del 20% de Food101\n",
    "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
    "                                                split_size=0.2)\n",
    "\n",
    "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75737af",
   "metadata": {},
   "source": [
    "¡Excelente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bf184",
   "metadata": {},
   "source": [
    "### 10.4 Convirtiendo nuestros conjuntos de datos de Food101 en `DataLoader`s\n",
    "\n",
    "Ahora conviertamos nuestras divisiones del conjunto de datos Food101 20% en `DataLoader` usando `torch.utils.data.DataLoader()`.\n",
    "\n",
    "Estableceremos `shuffle=True` solo para los datos de entrenamiento y el tamaño del lote en `32` para ambos conjuntos de datos.\n",
    "\n",
    "Y estableceremos `num_workers` en `4` si el recuento de CPU está disponible o `2` si no lo está (aunque el valor de `num_workers` es muy experimental y dependerá del hardware que estés usando, hay un [ hilo de discusión activo sobre esto en los foros de PyTorch](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f484ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n",
    "\n",
    "# Crear Food101 20 por ciento de entrenamiento DataLoader\n",
    "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n",
    "                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  num_workers=NUM_WORKERS)\n",
    "# Crear Food101 20 por ciento de prueba DataLoader\n",
    "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n",
    "                                                                 batch_size=BATCH_SIZE,\n",
    "                                                                 shuffle=False,\n",
    "                                                                 num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403df49c",
   "metadata": {},
   "source": [
    "### 10.5 Entrenamiento FoodVision Modelo grande\n",
    "\n",
    "¡El modelo FoodVision Big y `DataLoader`s están listos!\n",
    "\n",
    "Hora de entrenar.\n",
    "\n",
    "Crearemos un optimizador usando `torch.optim.Adam()` y una tasa de aprendizaje de `1e-3`.\n",
    "\n",
    "Y debido a que tenemos tantas clases, también configuraremos una función de pérdida usando `torch.nn.CrossEntropyLoss()` con `label_smoothing=0.1`, en línea con la tecnología de punta de [`torchvision` receta de entrenamiento](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#label-smoothing).\n",
    "\n",
    "¿Qué es [**suavizado de etiquetas**](https://paperswithcode.com/method/label-smoothing)? \n",
    "\n",
    "El suavizado de etiquetas es una técnica de regularización (regularización es otra palabra para describir el proceso de [prevención del sobreajuste](https://www.learnpytorch.io/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting)) que reduce la valor que un modelo le da a cualquier etiqueta y lo distribuye entre las demás etiquetas.\n",
    "\n",
    "En esencia, en lugar de que un modelo se *demasiado confiado* en una sola etiqueta, el suavizado de etiquetas otorga un valor distinto de cero a otras etiquetas para ayudar en la generalización.\n",
    "\n",
    "Por ejemplo, si un modelo *sin* suavizado de etiquetas tuviera los siguientes resultados para 5 clases:\n",
    "\n",
    "```\n",
    "[0, 0, 0,99, 0,01, 0]\n",
    "```\n",
    "\n",
    "Un modelo *con* suavizado de etiquetas puede tener los siguientes resultados:\n",
    "\n",
    "```\n",
    "[0,01, 0,01, 0,96, 0,01, 0,01]\n",
    "```\n",
    "\n",
    "El modelo todavía confía en su predicción de la clase 3, pero dar valores pequeños a las otras etiquetas obliga al modelo a considerar al menos otras opciones.\n",
    "\n",
    "Finalmente, para agilizar las cosas, entrenaremos nuestro modelo durante cinco épocas usando la función `engine.train()` que creamos en [05. PyTorch Going Modular sección 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) con el objetivo de superar al Food101 original resultado del artículo de 56,4% de precisión en el conjunto de prueba.\n",
    "\n",
    "¡Entrenemos a nuestro modelo más grande hasta el momento!\n",
    "\n",
    "> **Nota:** La ejecución de la siguiente celda tardará entre 15 y 20 minutos en Google Colab. Esto se debe a que está entrenando el modelo más grande con la mayor cantidad de datos que hemos usado hasta ahora (15,150 imágenes de entrenamiento, 5050 imágenes de prueba). Y es una de las razones por las que antes decidimos dividir el 20% del conjunto de datos completo de Food101 (para que el entrenamiento no tomara más de una hora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Optimizador de configuración\n",
    "optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "# Función de pérdida de configuración\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n",
    "\n",
    "# Quiere superar el artículo original de Food101 con un 20 % de datos, necesita más del 56,4 % según el conjunto de datos de prueba\n",
    "set_seeds()    \n",
    "effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
    "                                        train_dataloader=train_dataloader_food101_20_percent,\n",
    "                                        test_dataloader=test_dataloader_food101_20_percent,\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        epochs=5,\n",
    "                                        device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788b8b2",
   "metadata": {},
   "source": [
    "Woooo!!!!\n",
    "\n",
    "Parece que superamos los resultados del artículo original de Food101 de 56,4% de precisión con solo el 20% de los datos de entrenamiento (aunque solo evaluamos el 20% de los datos de las pruebas también, para replicar completamente los resultados, podríamos evaluar el 100% de las pruebas). datos). \n",
    "\n",
    "¡Ese es el poder del aprendizaje por transferencia!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d82425",
   "metadata": {},
   "source": [
    "### 10.6 Inspeccionando las curvas de pérdidas del modelo FoodVision Big\n",
    "\n",
    "Hagamos visuales nuestras curvas de pérdidas de FoodVision Big.\n",
    "\n",
    "Podemos hacerlo con la función `plot_loss_curves()` de `helper_functions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# Consulte las curvas de pérdidas de FoodVision Big\n",
    "plot_loss_curves(effnetb2_food101_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cb3eb",
   "metadata": {},
   "source": [
    "¡¡¡Lindo!!!\n",
    "\n",
    "Parece que nuestras técnicas de regularización (aumento de datos y suavizado de etiquetas) ayudaron a evitar que nuestro modelo se sobreajustara (la pérdida de entrenamiento sigue siendo mayor que la pérdida de prueba), lo que indica que nuestro modelo tiene un poco más de capacidad para aprender y podría mejorar con más entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff787e0",
   "metadata": {},
   "source": [
    "### 10.7 Guardar y cargar FoodVision Big\n",
    "\n",
    "Ahora que hemos entrenado nuestro modelo más grande hasta el momento, guardémoslo para poder volver a cargarlo más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import utils\n",
    "\n",
    "# Crear una ruta modelo\n",
    "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n",
    "\n",
    "# Guardar modelo FoodVision Big\n",
    "utils.save_model(model=effnetb2_food101,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=effnetb2_food101_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f9cfa",
   "metadata": {},
   "source": [
    "¡Modelo guardado!\n",
    "\n",
    "Antes de continuar, asegurémonos de poder volver a cargarlo.\n",
    "\n",
    "Lo haremos creando primero una instancia de modelo con `create_effnetb2_model(num_classes=101)` (101 clases para todas las clases de Food101).\n",
    "\n",
    "Y luego cargar el `state_dict()` guardado con [`torch.nn.Module.load_state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict #torch.nn.Module.load_state_dict) y [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b07130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una instancia EffNetB2 compatible con Food101\n",
    "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
    "\n",
    "# Cargue el state_dict() del modelo guardado\n",
    "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc7fb7",
   "metadata": {},
   "source": [
    "### 10.8 Comprobación del tamaño del modelo grande de FoodVision\n",
    "\n",
    "Nuestro modelo FoodVision Big es capaz de clasificar 101 clases frente a las 3 clases de FoodVision Mini, ¡un aumento de 33,6 veces!\n",
    "\n",
    "¿Cómo afecta esto al tamaño del modelo?\n",
    "\n",
    "Vamos a averiguar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c406c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga el tamaño del modelo en bytes y luego conviértalo a megabytes\n",
    "pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9978f",
   "metadata": {},
   "source": [
    "Mmm, parece que el tamaño del modelo se mantuvo prácticamente igual (30 MB para FoodVision Big y 29 MB para FoodVision Mini) a pesar del gran aumento en el número de clases.\n",
    "\n",
    "Esto se debe a que todos los parámetros adicionales para FoodVision Big están *solo* en la última capa (el encabezado del clasificador). \n",
    "\n",
    "Todas las capas base son iguales entre FoodVision Big y FoodVision Mini.\n",
    "\n",
    "Volver arriba y comparar los resúmenes de los modelos dará más detalles.\n",
    "\n",
    "| **Modelo** | **Forma de salida (núm. de clases)** | **Parámetros entrenables** | **Parámetros totales** | **Tamaño del modelo (MB)** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| FoodVision Mini (extractor de funciones EffNetB2) | 3 | 4.227 | 7.705.221 |  29 |\n",
    "| FoodVision Big (extractor de funciones EffNetB2) | 101 | 142.309 | 7.843.303 | 30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cc903",
   "metadata": {},
   "source": [
    "## 11. Convertir nuestro modelo FoodVision Big en una aplicación implementable\n",
    "\n",
    "Tenemos un modelo EffNetB2 entrenado y guardado en el 20% del conjunto de datos de Food101.\n",
    "\n",
    "Y en lugar de dejar que nuestro modelo viva en una carpeta toda su vida, ¡implementémoslo!\n",
    "\n",
    "Implementaremos nuestro modelo FoodVision Big de la misma manera que implementamos nuestro modelo FoodVision Mini, como una demostración de Gradio en Hugging Face Spaces.\n",
    "\n",
    "Para comenzar, creemos un directorio `demos/foodvision_big/` para almacenar nuestros archivos de demostración de FoodVision Big, así como un directorio `demos/foodvision_big/examples` para guardar una imagen de ejemplo con la que probar la demostración.\n",
    "\n",
    "Cuando hayamos terminado tendremos la siguiente estructura de archivos:\n",
    "\n",
    "```\n",
    "población/\n",
    "  comidavision_big/\n",
    "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
    "    aplicación.py\n",
    "    nombres_clase.txt\n",
    "    ejemplos/\n",
    "      ejemplo_1.jpg\n",
    "    modelo.py\n",
    "    requisitos.txt\n",
    "```\n",
    "\n",
    "Dónde:\n",
    "* `09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` es nuestro archivo de modelo PyTorch entrenado.\n",
    "* `app.py` contiene nuestra aplicación FoodVision Big Gradio.\n",
    "* `class_names.txt` contiene todos los nombres de clases de FoodVision Big.\n",
    "* `examples/` contiene imágenes de ejemplo para usar con nuestra aplicación Gradio.\n",
    "* `model.py` contiene la definición del modelo, así como cualquier transformación asociada con el modelo.\n",
    "* `requirements.txt` contiene las dependencias para ejecutar nuestra aplicación, como `torch`, `torchvision` y `gradio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62622187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Crear ruta de demostración de FoodVision Big\n",
    "foodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n",
    "\n",
    "# Haga el directorio de demostración de FoodVision Big\n",
    "foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Directorio de ejemplos de demostración de Make FoodVision Big\n",
    "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522717c",
   "metadata": {},
   "source": [
    "### 11.1 Descargar una imagen de ejemplo y moverla al directorio `ejemplos`\n",
    "\n",
    "Para nuestra imagen de ejemplo, usaremos la fiel [imagen `pizza-dad`] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad. jpeg) (una foto de mi papá comiendo pizza).\n",
    "\n",
    "Así que descarguémoslo del curso GitHub mediante el comando `!wget` y luego podemos moverlo a `demos/foodvision_big/examples` con el comando `!mv` (abreviatura de \"mover\").\n",
    "\n",
    "Mientras estamos aquí, trasladaremos nuestro modelo Food101 EffNetB2 entrenado de `models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` a `demos/foodvision_big` también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff68aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar y mover una imagen de ejemplo\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n",
    "!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n",
    "\n",
    "# Mueva el modelo entrenado a la carpeta de demostración de FoodVision Big (se producirá un error si el modelo ya se ha movido)\n",
    "!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cac84b",
   "metadata": {},
   "source": [
    "### 11.2 Guardar nombres de clases de Food101 en un archivo (`class_names.txt`)\n",
    "\n",
    "Debido a que hay tantas clases en el conjunto de datos Food101, en lugar de almacenarlas como una lista en nuestro archivo `app.py`, guardémoslas en un archivo `.txt` y leámoslas cuando sea necesario.\n",
    "\n",
    "Primero recordaremos cómo se ven revisando `food101_class_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298853dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte los primeros 10 nombres de clases de Food101\n",
    "food101_class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ee04f",
   "metadata": {},
   "source": [
    "Maravilloso, ahora podemos escribirlos en un archivo de texto creando primero una ruta a `demos/foodvision_big/class_names.txt` y luego abriendo un archivo con `open()` de Python y luego escribiendo en él dejando una nueva línea para cada clase. .\n",
    "\n",
    "Idealmente, queremos que los nombres de nuestras clases se guarden como:\n",
    "\n",
    "```\n",
    "tarta de manzana\n",
    "Costillitas\n",
    "baklava\n",
    "Carpaccio de carne\n",
    "tartar_de_carne\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b858df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ruta a los nombres de clases de Food101\n",
    "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
    "\n",
    "# Escriba la lista de nombres de clases de Food101 en el archivo\n",
    "with open(foodvision_big_class_names_path, \"w\") as f:\n",
    "    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
    "    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce674ce",
   "metadata": {},
   "source": [
    "Excelente, ahora asegurémonos de poder leerlos.\n",
    "\n",
    "Para hacerlo usaremos [`open()`](https://www.w3schools.com/python/ref_func_open.asp) de Python en modo lectura (`\"r\"`) y luego usaremos [`readlines( )`](https://www.w3schools.com/python/ref_file_readlines.asp) método para leer cada línea de nuestro archivo `class_names.txt`.\n",
    "\n",
    "Y podemos guardar los nombres de las clases en una lista eliminando el valor de nueva línea de cada uno de ellos con una lista de comprensión y [`strip()`](https://www.w3schools.com/python/ref_string_strip.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abra el archivo de nombres de clases Food101 y lea cada línea en una lista\n",
    "with open(foodvision_big_class_names_path, \"r\") as f:\n",
    "    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n",
    "    \n",
    "# Ver los primeros 5 nombres de clases cargados nuevamente en\n",
    "food101_class_names_loaded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d75617",
   "metadata": {},
   "source": [
    "### 11.3 Convirtiendo nuestro modelo FoodVision Big en un script de Python (`model.py`)\n",
    "\n",
    "Al igual que en la demostración de FoodVision Mini, creemos un script que sea capaz de crear una instancia de un modelo de extracción de características EffNetB2 junto con sus transformaciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c460542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f960c",
   "metadata": {},
   "source": [
    "### 11.4 Convirtiendo nuestra aplicación FoodVision Big Gradio en un script Python (`app.py`)\n",
    "\n",
    "Tenemos un script `model.py` de FoodVision Big, ahora creemos un script `app.py` de FoodVision Big.\n",
    "\n",
    "De nuevo, esto será prácticamente igual que el script `app.py` de FoodVision Mini, excepto que cambiaremos:\n",
    "\n",
    "1. **Configuración de importaciones y nombres de clases** - La variable `class_names` será una lista para todas las clases de Food101 en lugar de pizza, bistec o sushi. Podemos acceder a ellos a través de `demos/foodvision_big/class_names.txt`.\n",
    "2. **Preparación de modelos y transformaciones** - El `modelo` tendrá `num_classes=101` en lugar de `num_classes=3`. También nos aseguraremos de cargar los pesos de `\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"` (nuestra ruta del modelo FoodVision Big).\n",
    "3. **Función de predicción**: seguirá siendo la misma que `app.py` de FoodVision Mini.\n",
    "4. **Aplicación Gradio**: la interfaz de Gradio tendrá diferentes parámetros de \"título\", \"descripción\" y \"artículo\" para reflejar los detalles de FoodVision Big.\n",
    "\n",
    "También nos aseguraremos de guardarlo en `demos/foodvision_big/app.py` usando el comando mágico `%%writefile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a955bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/app.py\n",
    "# ## 1. Configuración de importaciones y nombres de clases ###\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Configurar nombres de clases\n",
    "with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n",
    "    class_names = [food_name.strip() for food_name in  f.readlines()]\n",
    "    \n",
    "# ## 2. Preparación del modelo y transforma ###\n",
    "\n",
    "# Crear modelo\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=101, # could also use len(class_names)\n",
    ")\n",
    "\n",
    "# Cargar pesos guardados\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "# ## 3. Función de predicción ###\n",
    "\n",
    "# Crear función de predicción\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# ## 4. Aplicación Gradio ###\n",
    "\n",
    "# Crear cadenas de título, descripción y artículo.\n",
    "title = \"FoodVision Big 🍔👁\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Crear una lista de ejemplos desde el directorio \"examples/\"\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Crear interfaz Gradio\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
    "        gr.Number(label=\"Prediction time (s)\"),\n",
    "    ],\n",
    "    examples=example_list,\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    ")\n",
    "\n",
    "# ¡Inicia la aplicación!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb8012",
   "metadata": {},
   "source": [
    "### 11.5 Creación de un archivo de requisitos para FoodVision Big (`requirements.txt`)\n",
    "\n",
    "Ahora todo lo que necesitamos es un archivo `requirements.txt` para indicarle a nuestro Hugging Face Space qué dependencias requiere nuestra aplicación FoodVision Big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/requirements.txt\n",
    "torch==1.12.0\n",
    "torchvision==0.13.0\n",
    "gradio==3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddcf8c",
   "metadata": {},
   "source": [
    "### 11.6 Descarga de nuestros archivos de la aplicación FoodVision Big\n",
    "\n",
    "Tenemos todos los archivos que necesitamos para implementar nuestra aplicación FoodVision Big en Hugging Face, ahora comprimámoslos y descargámoslos. \n",
    "\n",
    "Usaremos el mismo proceso que usamos para la aplicación FoodVision Mini anterior en la [sección 9.1: *Descarga de los archivos de nuestra aplicación Foodvision Mini*](https://www.learnpytorch.io/09_pytorch_model_deployment/#91-downloading-our-foodvision -archivos-mini-aplicación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b46e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprima la carpeta foodvision_big pero excluya ciertos archivos\n",
    "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "\n",
    "# Descargue la aplicación FoodVision Big comprimida (si se ejecuta en Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"demos/foodvision_big.zip\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed84eee",
   "metadata": {},
   "source": [
    "### 11.7 Implementación de nuestra aplicación FoodVision Big en HuggingFace Spaces\n",
    "\n",
    "¡Hermoso! \n",
    "\n",
    "¡Es hora de darle vida a nuestro modelo más grande de todo el curso!\n",
    "\n",
    "Implementemos nuestra demostración de FoodVision Big Gradio en Hugging Face Spaces para que podamos probarla de forma interactiva y permitir que otros experimenten la magia de nuestros esfuerzos de aprendizaje automático.\n",
    "\n",
    "> **Nota:** Hay [varias formas de cargar archivos en Hugging Face Spaces](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories). Los siguientes pasos tratan a Hugging Face como un repositorio git para rastrear archivos. Sin embargo, también puede cargar directamente en Hugging Face Spaces a través de la [interfaz web](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui) o por la [biblioteca`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index). \n",
    "\n",
    "La buena noticia es que ya hemos realizado los pasos para hacerlo con FoodVision Mini, así que ahora todo lo que tenemos que hacer es personalizarlos para que se adapten a FoodVision Big:\n",
    "\n",
    "1. [Regístrese](https://huggingface.co/join) para obtener una cuenta de Hugging Face. \n",
    "2. Inicie un nuevo Hugging Face Space yendo a su perfil y luego [haciendo clic en \"Nuevo espacio\"](https://huggingface.co/new-space).\n",
    "    * **Nota:** Un espacio en Hugging Face también se conoce como \"repositorio de códigos\" (un lugar para almacenar su código/archivos) o \"repositorio\" para abreviar.\n",
    "3. Dale un nombre al Espacio, por ejemplo, el mío se llama `mrdbourke/foodvision_big`, puedes verlo aquí: https://huggingface.co/spaces/mrdbourke/foodvision_big\n",
    "4. Seleccione una licencia (yo usé [MIT](https://opensource.org/licenses/MIT)).\n",
    "5. Seleccione Gradio como Space SDK (kit de desarrollo de software). \n",
    "   * **Nota:** Puedes usar otras opciones como Streamlit, pero como nuestra aplicación está construida con Gradio, nos quedaremos con eso.\n",
    "6. Elija si su Espacio es público o privado (seleccioné público porque me gustaría que mi Espacio esté disponible para otros).\n",
    "7. Haga clic en \"Crear espacio\".\n",
    "8. Clone el repositorio localmente ejecutando: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` en la terminal o en el símbolo del sistema.\n",
    "    * **Nota:** También puedes agregar archivos cargándolos en la pestaña \"Archivos y versiones\".\n",
    "9. Copie/mueva el contenido de la carpeta `foodvision_big` descargada a la carpeta del repositorio clonado.\n",
    "10. Para cargar y rastrear archivos más grandes (por ejemplo, archivos de más de 10 MB o, en nuestro caso, nuestro archivo modelo PyTorch), necesitará [instalar Git LFS](https://git-lfs.github.com/) (que significa para \"almacenamiento de archivos grandes de git\").\n",
    "11. Después de haber instalado Git LFS, puedes activarlo ejecutando `git lfs install`.\n",
    "12. En el directorio `foodvision_big`, realice un seguimiento de los archivos de más de 10 MB con Git LFS con `git lfs track \"*.file_extension\"`.\n",
    "    * Realice un seguimiento del archivo de modelo EffNetB2 PyTorch con `git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"`.\n",
    "    * **Nota:** Si recibe algún error al cargar imágenes, es posible que también deba rastrearlas con `git lfs`, por ejemplo, `git lfs track \"examples/04-pizza-dad.jpg\"`\n",
    "13. Seguimiento de `.gitattributes` (creado automáticamente al clonar desde HuggingFace; este archivo ayudará a garantizar que nuestros archivos más grandes sean rastreados con Git LFS). Puede ver un archivo `.gitattributes` de ejemplo en [FoodVision Big Hugging Face Space] (https://huggingface.co/spaces/mrdbourke/foodvision_big/blob/main/.gitattributes).\n",
    "    * `git agregar .gitattributes`\n",
    "14. Agregue el resto de los archivos de la aplicación `foodvision_big` y confírmelos con: \n",
    "    * `git agregar *`\n",
    "    * `git commit -m \"primer compromiso\"`\n",
    "15. Envíe (cargue) los archivos a Hugging Face:\n",
    "    *`git push`\n",
    "16. Espere de 3 a 5 minutos hasta que se complete la compilación (las compilaciones futuras serán más rápidas) y su aplicación estará activa.\n",
    "\n",
    "Si todo funcionó correctamente, ¡nuestra demostración de FoodVision Big Gradio debería estar lista para clasificar!\n",
    "\n",
    "Puedes ver mi versión aquí: https://huggingface.co/spaces/mrdbourke/foodvision_big/\n",
    "\n",
    "O incluso podemos insertar nuestra demostración de FoodVision Big Gradio directamente en nuestro cuaderno como un [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) con [`IPython.display.IFrame`](https: //ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) y un enlace a nuestro espacio en el formato `https://hf.space/embed/[YOUR_USERNAME ]/[TU_NOMBRE_ESPACIO]/+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ae0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython es una biblioteca para ayudar a trabajar con Python de forma interactiva\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Incruste la demostración de FoodVision Big Gradio como un iFrame\n",
    "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847534f7",
   "metadata": {},
   "source": [
    "¡¿¡Cuan genial es eso!?!\n",
    "\n",
    "Hemos recorrido un largo camino desde la construcción de modelos PyTorch para predecir una línea recta... ¡ahora estamos construyendo modelos de visión por computadora accesibles para personas de todo el mundo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443bf2b",
   "metadata": {},
   "source": [
    "## Principales conclusiones\n",
    "\n",
    "* **La implementación es tan importante como la capacitación.** Una vez que tenga un buen modelo de trabajo, su primera pregunta debería ser: ¿cómo puedo implementarlo y hacerlo accesible para otros? La implementación le permite probar su modelo en el mundo real en lugar de en conjuntos de prueba y capacitación privados.\n",
    "* **Tres preguntas para la implementación del modelo de aprendizaje automático:**\n",
    "    1. ¿Cuál es el caso de uso más ideal para el modelo (qué tan bien y qué tan rápido funciona)?\n",
    "    2. ¿A dónde irá el modelo (en el dispositivo o en la nube)?\n",
    "    3. ¿Cómo funcionará el modelo (las predicciones están en línea o fuera de línea)?\n",
    "* **Las opciones de implementación son abundantes.** Pero es mejor empezar de forma sencilla. Una de las mejores formas actuales (digo actual porque estas cosas siempre están cambiando) es usar Gradio para crear una demostración y alojarla en Hugging Face Spaces. Comience de manera simple y amplíelo cuando sea necesario.\n",
    "* **Nunca dejes de experimentar.** Las necesidades de tu modelo de aprendizaje automático probablemente cambiarán con el tiempo, por lo que implementar un único modelo no es el último paso. Es posible que encuentre cambios en el conjunto de datos, por lo que tendrá que actualizar su modelo. O se publica una nueva investigación y hay una mejor arquitectura para usar.\n",
    "    * Por lo tanto, implementar un modelo es un paso excelente, pero probablemente querrás actualizarlo con el tiempo. \n",
    "* **La implementación del modelo de aprendizaje automático es parte de la práctica de ingeniería de MLOps (operaciones de aprendizaje automático).** MLOps es una extensión de DevOps (operaciones de desarrollo) e involucra todas las partes de ingeniería relacionadas con el entrenamiento de un modelo: recopilación y almacenamiento de datos, datos preprocesamiento, implementación de modelos, monitoreo de modelos, control de versiones y más. Es un campo que evoluciona rápidamente, pero existen algunos recursos sólidos para aprender más, muchos de los cuales se encuentran en [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and -ingeniería-de-aprendizaje-profundo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b33cd",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Todos los ejercicios se centran en practicar el código anterior.\n",
    "\n",
    "Debería poder completarlos haciendo referencia a cada sección o siguiendo los recursos vinculados.\n",
    "\n",
    "**Recursos:**\n",
    "\n",
    "* [Cuaderno de plantilla de ejercicios para 09] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/09_pytorch_model_deployment_exercises.ipynb).\n",
    "* [Cuaderno de soluciones de ejemplo para 09](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/09_pytorch_model_deployment_exercise_solutions.ipynb) pruebe los ejercicios *antes* de mirar esto.\n",
    "    * Vea un [video tutorial de las soluciones en YouTube] en vivo (https://youtu.be/jOX5ZCkWO-0) (errores y todo).\n",
    "\n",
    "1. Realice y programe predicciones con ambos modelos de extracción de características en el conjunto de datos de prueba utilizando la GPU (`device=\"cuda\"`). Compare los tiempos de predicción del modelo en GPU y CPU: ¿esto cierra la brecha entre ellos? Por ejemplo, ¿hacer predicciones en la GPU hace que los tiempos de predicción del extractor de funciones de ViT se acerquen más a los tiempos de predicción del extractor de funciones de EffNetB2?\n",
    "    * Encontrarás el código para realizar estos pasos en la [sección 5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas](https://www.learnpytorch.io/09_pytorch_model_deployment/#5-making-predictions-with-our-trained -models-and-timing-them) y [sección 6. Comparación de resultados de modelos, tiempos de predicción y tamaño](https://www.learnpytorch.io/09_pytorch_model_deployment/#6-comparing-model-results-prediction-times-and -tamaño).\n",
    "2. El extractor de funciones de ViT parece tener más capacidad de aprendizaje (debido a más parámetros) que EffNetB2. ¿Cómo le va en la división más grande del 20 % de todo el conjunto de datos de Food101?\n",
    "    * Entrene un extractor de funciones ViT en el conjunto de datos 20% Food101 durante 5 épocas, tal como lo hicimos con EffNetB2 en la sección [10. Creando FoodVision Big](https://www.learnpytorch.io/09_pytorch_model_deployment/#10-creating-foodvision-big).\n",
    "3. Haga predicciones en el conjunto de datos de prueba 20 % de Food101 con el extractor de funciones ViT del ejercicio 2 y encuentre las predicciones \"más incorrectas\".\n",
    "    * Las predicciones serán las que tengan mayor probabilidad de predicción pero con la etiqueta de predicción incorrecta.\n",
    "    * Escribe una oración o dos sobre por qué crees que el modelo se equivocó en estas predicciones.\n",
    "4. Evalúe el extractor de funciones de ViT en todo el conjunto de datos de prueba de Food101 en lugar de solo la versión del 20 %, ¿cómo funciona?\n",
    "    * ¿Supera el mejor resultado del artículo original de Food101 con una precisión del 56,4%?\n",
    "5. Diríjase a [Paperswithcode.com](https://paperswithcode.com/) y busque el modelo actual con mejor rendimiento en el conjunto de datos de Food101.\n",
    "    * ¿Qué modelo de arquitectura utiliza?\n",
    "6. Escriba de 1 a 3 posibles puntos de falla de nuestros modelos FoodVision implementados y cuáles podrían ser algunas posibles soluciones.\n",
    "    * Por ejemplo, ¿qué pasaría si alguien subiera una foto que no fuera de comida a nuestro modelo FoodVision Mini?\n",
    "7. Elija cualquier conjunto de datos de [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) y entrene un modelo de extracción de características usando un modelo de [`torchvision.models`]( https://pytorch.org/vision/stable/models.html) (puede usar uno de los modelos que ya hemos creado, por ejemplo, EffNetB2 o ViT) durante 5 épocas y luego implementar su modelo como una aplicación Gradio en Hugging Face Espacios. \n",
    "    * Es posible que desee elegir un conjunto de datos más pequeño/hacer una división más pequeña para que el entrenamiento no demore demasiado.\n",
    "    * ¡Me encantaría ver tus modelos implementados! Así que asegúrese de compartirlos en Discord o en la [página de debates de GitHub del curso] (https://github.com/mrdbourke/pytorch-deep-learning/discussions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88587451",
   "metadata": {},
   "source": [
    "## Extracurricular\n",
    "\n",
    "* La implementación del modelo de aprendizaje automático es generalmente un desafío de ingeniería en lugar de un desafío de aprendizaje automático puro; consulte la [sección de ingeniería de aprendizaje automático de recursos adicionales de PyTorch] (https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning -y-deep-learning-engineering) para obtener recursos sobre cómo aprender más.\n",
    "    * En el interior encontrará recomendaciones de recursos como el libro de Chip Huyen [*Designing Machine Learning Systems*](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969) ( especialmente el capítulo 7 sobre implementación de modelos) y el [curso Made with ML MLOps] de Goku Mohandas (https://madewithml.com/#mlops).\n",
    "* A medida que empieces a construir más y más proyectos propios, es probable que empieces a usar Git (y potencialmente GitHub) con bastante frecuencia. Para obtener más información sobre ambos, recomiendo el video [*Git y GitHub para principiantes: curso intensivo*](https://youtu.be/RGOj5yH7evk) en el canal de YouTube freeCodeCamp.\n",
    "* Sólo hemos arañado la superficie de lo que es posible con Gradio. Para obtener más información, recomiendo consultar la [documentación completa](https://gradio.app/docs/), especialmente:\n",
    "    * Todos los diferentes tipos de [componentes de entrada y salida] (https://gradio.app/docs/#components).\n",
    "    * La [API de Gradio Blocks](https://gradio.app/docs/#blocks) para flujos de trabajo más avanzados.\n",
    "    * El capítulo del curso Hugging Face sobre [cómo usar Gradio con Hugging Face] (https://huggingface.co/course/chapter9/1).\n",
    "* Los dispositivos Edge no se limitan a teléfonos móviles, incluyen computadoras pequeñas como Raspberry Pi y el equipo de PyTorch tiene un [fantástico tutorial de publicación de blog](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) sobre la implementación un modelo de PyTorch a uno.\n",
    "* Para obtener una guía fantástica sobre el desarrollo de aplicaciones basadas en inteligencia artificial y aprendizaje automático, consulte la [Guía de personas + inteligencia artificial de Google] (https://pair.withgoogle.com/guidebook). Una de mis favoritas es la sección sobre [establecer las expectativas correctas] (https://pair.withgoogle.com/guidebook/patterns#set-the-right-expectations).\n",
    "    * Cubrí más de este tipo de recursos, incluidas guías de Apple, Microsoft y más en la [edición de abril de 2021 de Machine Learning Monthly](https://zerotomastery.io/blog/machine-learning-monthly-april-2021/ ) (un boletín mensual que envío con lo último y lo mejor del campo de ML).\n",
    "* Si desea acelerar el tiempo de ejecución de su modelo en la CPU, debe conocer [TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html), [ONNX](https://pytorch .org/docs/stable/onnx.html) (Open Neural Network Exchange) y [OpenVINO](https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output. HTML). Al pasar de PyTorch puro a modelos ONNX/OpenVINO, he visto un aumento de ~2x+ en el rendimiento.\n",
    "* Para convertir modelos en una API implementable y escalable, consulte la [biblioteca TorchServe](https://pytorch.org/serve/).\n",
    "* Para ver un excelente ejemplo y una justificación de por qué implementar un modelo de aprendizaje automático en el navegador (una forma de implementación perimetral) ofrece varios beneficios (sin retraso en la latencia de transferencia de red), consulte el artículo de Jo Kristian Bergum sobre [*Moving ML Inference from the Cloud hasta el borde*](https://bergum.medium.com/moving-ml-inference-from-the-cloud-to-the-edge-d6f98dbdb2e3)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
