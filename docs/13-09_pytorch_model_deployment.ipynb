{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45184ef",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/09_pytorch_model_deployment.ipynb\" target=\"_parent\"><img src=\"https:// colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/></a>\n",
    "\n",
    "[Ver c칩digo fuente](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/09_pytorch_model_deployment.ipynb) | [Ver diapositivas] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05699eda",
   "metadata": {},
   "source": [
    "# 09. Implementaci칩n del modelo PyTorch\n",
    "\n",
    "Bienvenido al Proyecto Milestone 3: 춰Implementaci칩n del modelo PyTorch!\n",
    "\n",
    "Hemos recorrido un largo camino con nuestro proyecto FoodVision Mini.\n",
    "\n",
    "Pero hasta ahora solo nosotros hemos podido acceder a nuestros modelos PyTorch.\n",
    "\n",
    "쯈u칠 tal si le damos vida a FoodVision Mini y lo hacemos p칰blicamente accesible?\n",
    "\n",
    "En otras palabras, **춰vamos a implementar nuestro modelo FoodVision Mini en Internet como una aplicaci칩n utilizable!**\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-model-deployment-what-were-doing-demo-trimmed-cropped-small.gif\" alt =\"demostraci칩n del mini modelo de visi칩n por computadora de Foodvision que se utiliza en un dispositivo m칩vil para predecir una imagen de sushi y hacerlo bien\" width=900/>\n",
    "\n",
    "*Probando la [versi칩n implementada de FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (lo que vamos a crear) en mi almuerzo. 춰La modelo tambi칠n acert칩 游꼮!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abd76d",
   "metadata": {},
   "source": [
    "## 쯈u칠 es la implementaci칩n del modelo de aprendizaje autom치tico?\n",
    "\n",
    "**Implementaci칩n del modelo de aprendizaje autom치tico** es el proceso de hacer que su modelo de aprendizaje autom치tico sea accesible para alguien o algo m치s.\n",
    "\n",
    "Alguien m치s es una persona que puede interactuar con tu modelo de alguna manera. \n",
    "\n",
    "Por ejemplo, alguien que toma una fotograf칤a de una comida con su tel칠fono inteligente y luego hace que nuestro modelo FoodVision Mini la clasifique en pizza, filete o sushi.\n",
    "\n",
    "Otra cosa podr칤a ser otro programa, aplicaci칩n o incluso otro modelo que interact칰e con sus modelos de aprendizaje autom치tico. \n",
    "\n",
    "Por ejemplo, una base de datos bancaria podr칤a depender de un modelo de aprendizaje autom치tico que haga predicciones sobre si una transacci칩n es fraudulenta o no antes de transferir fondos.\n",
    "\n",
    "O un sistema operativo puede reducir su consumo de recursos bas치ndose en un modelo de aprendizaje autom치tico que hace predicciones sobre cu치nta energ칤a usa generalmente alguien en momentos espec칤ficos del d칤a.\n",
    "\n",
    "Estos casos de uso tambi칠n se pueden mezclar y combinar.\n",
    "\n",
    "Por ejemplo, el sistema de visi칩n por computadora de un autom칩vil Tesla interactuar치 con el programa de planificaci칩n de rutas del autom칩vil (algo m치s) y luego el programa de planificaci칩n de rutas recibir치 informaci칩n y comentarios del conductor (otra persona).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png\" ancho=900 alt=\"dos casos de uso para la implementaci칩n de modelos, hacer que su modelo est칠 disponible para otra persona, por ejemplo, alguien que lo use en una aplicaci칩n, o ponerlo a disposici칩n de otra cosa, como otro programa o modelo\"/> \n",
    "\n",
    "*La implementaci칩n del modelo de aprendizaje autom치tico implica poner su modelo a disposici칩n de alguien o de algo m치s. Por ejemplo, alguien podr칤a usar su modelo como parte de una aplicaci칩n de reconocimiento de alimentos (como FoodVision Mini o [Nutrify](https://nutrify.app)). Y algo m치s podr칤a ser otro modelo o programa que utilice su modelo, como un sistema bancario que utilice un modelo de aprendizaje autom치tico para detectar si una transacci칩n es fraudulenta o no.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca5fc3",
   "metadata": {},
   "source": [
    "## 쯇or qu칠 implementar un modelo de aprendizaje autom치tico?\n",
    "\n",
    "Una de las cuestiones filos칩ficas m치s importantes en el aprendizaje autom치tico es: \n",
    "\n",
    "<div align=\"centro\">\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-does-it-exist.jpeg\" alt=\"dinosaurio curioso al que a menudo se hace referencia como fil칩sofo raptor que hace la pregunta Si un modelo de aprendizaje autom치tico nunca sale de un cuaderno, 쯘xiste?\". ancho=300/>\n",
    "</div>\n",
    "\n",
    "Implementar un modelo es tan importante como entrenarlo.\n",
    "\n",
    "Porque aunque puedes tener una idea bastante clara de c칩mo funcionar치 tu modelo evalu치ndolo en un conjunto de pruebas bien dise침ado o visualizando sus resultados, nunca sabes realmente c칩mo funcionar치 hasta que lo liberas.\n",
    "\n",
    "Hacer que personas que nunca han usado su modelo interact칰en con 칠l a menudo revelar치 casos extremos en los que nunca pens칩 durante el entrenamiento.\n",
    "\n",
    "Por ejemplo, 쯤u칠 pasar칤a si alguien subiera una foto que *no* fuera de comida a nuestro modelo FoodVision Mini?\n",
    "\n",
    "Una soluci칩n ser칤a crear otro modelo que primero clasifique las im치genes como \"comida\" o \"no comida\" y pase primero la imagen de destino a trav칠s de ese modelo (esto es lo que hace [Nutrify](https://nutrify.app)).\n",
    "\n",
    "Luego, si la imagen es de \"comida\", pasa a nuestro modelo FoodVision Mini y se clasifica en pizza, bistec o sushi.\n",
    "\n",
    "Y si \"no es comida\", se muestra un mensaje.\n",
    "\n",
    "Pero 쯫 si estas predicciones estuvieran equivocadas?\n",
    "\n",
    "쯈u칠 pasa entonces?\n",
    "\n",
    "Puedes ver c칩mo estas preguntas podr칤an continuar.\n",
    "\n",
    "Por lo tanto, esto resalta la importancia de la implementaci칩n del modelo: le ayuda a descubrir errores en su modelo que no son obvios durante el entrenamiento/prueba.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-pytorch-workflow-with-deployment.png\" alt=\"Un flujo de trabajo de PyTorch con implementaci칩n de modelo adicional y paso de monitoreo\" ancho=900/>\n",
    "\n",
    "*Cubrimos un flujo de trabajo de PyTorch en [01. Flujo de trabajo de PyTorch](https://www.learnpytorch.io/01_pytorch_workflow/). Pero una vez que se tiene un buen modelo, la implementaci칩n es un buen siguiente paso. El monitoreo implica ver c칩mo funciona su modelo en la divisi칩n de datos m치s importante: los datos del mundo real. Para obtener m치s recursos sobre implementaci칩n y monitoreo, consulte [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565ad5e",
   "metadata": {},
   "source": [
    "## Diferentes tipos de implementaci칩n de modelos de aprendizaje autom치tico\n",
    "\n",
    "Se podr칤an escribir libros completos sobre los diferentes tipos de implementaci칩n de modelos de aprendizaje autom치tico (y muchos buenos se enumeran en [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and -ingenier칤a-de-aprendizaje-profundo)).\n",
    "\n",
    "Y el campo a칰n se est치 desarrollando en t칠rminos de mejores pr치cticas.\n",
    "\n",
    "Pero me gusta empezar con la pregunta:\n",
    "\n",
    "> \"쮺u치l es el escenario m치s ideal para utilizar mi modelo de aprendizaje autom치tico?\"\n",
    "\n",
    "Y luego trabaje hacia atr치s desde all칤.\n",
    "\n",
    "Por supuesto, es posible que no lo sepas de antemano. Pero eres lo suficientemente inteligente como para imaginar esas cosas.\n",
    "\n",
    "En el caso de FoodVision Mini, nuestro escenario ideal podr칤a ser:\n",
    "\n",
    "* Alguien toma una foto en un dispositivo m칩vil (a trav칠s de una aplicaci칩n o navegador web).\n",
    "* La predicci칩n vuelve r치pidamente.\n",
    "\n",
    "F치cil.\n",
    "\n",
    "Entonces tenemos dos criterios principales:\n",
    "\n",
    "1. El modelo deber칤a funcionar en un dispositivo m칩vil (esto significa que habr치 algunas restricciones inform치ticas). \n",
    "2. El modelo debe hacer predicciones *r치pidas* (porque una aplicaci칩n lenta es una aplicaci칩n aburrida).\n",
    "\n",
    "Y, por supuesto, seg칰n su caso de uso, sus requisitos pueden variar.\n",
    "\n",
    "Puede notar que los dos puntos anteriores se dividen en otras dos preguntas:\n",
    "\n",
    "1. **쮸 d칩nde ir치?** - Es decir, 쯗칩nde se almacenar치?\n",
    "2. **쮺칩mo va a funcionar?** - Es decir, 쯗evuelve predicciones inmediatamente? 쯆 vienen m치s tarde?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png\" alt=\"algunas preguntas para hacer al comenzar la implementaci칩n modelos de aprendizaje autom치tico, cu치l es el caso de uso ideal del modelo, luego trabaje hacia atr치s y pregunte d칩nde ir치 mi modelo y c칩mo funcionar치\" width=900/>\n",
    "\n",
    "*Al comenzar a implementar modelos de aprendizaje autom치tico, es 칰til comenzar preguntando cu치l es el caso de uso m치s ideal y luego trabajar hacia atr치s desde all칤, preguntando hacia d칩nde ir치 el modelo y luego c칩mo funcionar치.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e0e01",
   "metadata": {},
   "source": [
    "### 쮸 d칩nde ir치?\n",
    "\n",
    "Cuando implementas tu modelo de aprendizaje autom치tico, 쯗칩nde reside?\n",
    "\n",
    "El debate principal aqu칤 suele ser en el dispositivo (tambi칠n llamado borde/en el navegador) o en la nube (una computadora/servidor que no es el dispositivo *real* desde donde alguien/algo llama al modelo). \n",
    "\n",
    "Ambos tienen pros y contras.\n",
    "\n",
    "| **Ubicaci칩n de implementaci칩n** | **Ventajas** | **Desventajas** | \n",
    "| ----- | ----- | ----- |\n",
    "| **En el dispositivo (borde/en el navegador)** | Puede ser muy r치pido (ya que no salen datos del dispositivo) | Potencia inform치tica limitada (los modelos m치s grandes tardan m치s en ejecutarse) | \n",
    "| | Preservaci칩n de la privacidad (nuevamente, ning칰n dato tiene que salir del dispositivo) | Espacio de almacenamiento limitado (se requiere un tama침o de modelo m치s peque침o) | \n",
    "| | No se requiere conexi칩n a Internet (a veces) | A menudo se requieren habilidades espec칤ficas del dispositivo | \n",
    "| | | | \n",
    "| **En la nube** | Potencia inform치tica casi ilimitada (puede ampliarse cuando sea necesario) | Los costos pueden salirse de control (si no se aplican l칤mites de escala adecuados) |\n",
    "| | Puede implementar un modelo y usarlo en todas partes (a trav칠s de API) | Las predicciones pueden ser m치s lentas debido a que los datos tienen que salir del dispositivo y las predicciones tienen que regresar (latencia de red) |\n",
    "| | V칤nculos con el ecosistema de nube existente | Los datos deben salir del dispositivo (esto puede causar problemas de privacidad) |\n",
    "\n",
    "Hay m치s detalles sobre estos, pero dej칠 recursos en el [extracurriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) para obtener m치s informaci칩n. \n",
    "\n",
    "Pongamos un ejemplo.\n",
    "\n",
    "Si implementamos FoodVision Mini como una aplicaci칩n, queremos que funcione bien y r치pido.\n",
    "\n",
    "Entonces, 쯤u칠 modelo preferir칤amos? \n",
    "\n",
    "1. Un modelo en el dispositivo que funciona con una precisi칩n del 95 % con un tiempo de inferencia (latencia) de un segundo por predicci칩n.\n",
    "2. Un modelo en la nube que funciona con una precisi칩n del 98 % con un tiempo de inferencia de 10 segundos por predicci칩n (un modelo mejor y m치s grande, pero lleva m치s tiempo calcularlo).\n",
    "\n",
    "He inventado estos n칰meros, pero muestran una diferencia potencial entre el dispositivo y la nube.\n",
    "\n",
    "La opci칩n 1 podr칤a ser potencialmente un modelo m치s peque침o, de menor rendimiento y que funcione m치s r치pido porque puede caber en un dispositivo m칩vil.\n",
    "\n",
    "La opci칩n 2 podr칤a potencialmente ser un modelo m치s grande y con mayor rendimiento que requiere m치s computaci칩n y almacenamiento, pero tarda un poco m치s en ejecutarse porque tenemos que enviar datos desde el dispositivo y recuperarlos (por lo que, aunque la predicci칩n real pueda ser r치pida, el tiempo de red y la transferencia de datos debe tenerse en cuenta).\n",
    "\n",
    "Para FoodVision Mini, probablemente preferir칤amos la opci칩n 1, porque el peque침o impacto en el rendimiento se ve superado con creces por la velocidad de inferencia m치s r치pida.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png\" width=900 alt=\"tesla sistema de visi칩n por computadora en el dispositivo versus en la nube\"/>\n",
    "\n",
    "*En el caso del sistema de visi칩n por computadora de un autom칩vil Tesla, 쯖u치l ser칤a mejor? 쯋n modelo m치s peque침o que funciona bien en el dispositivo (el modelo est치 en el autom칩vil) o un modelo m치s grande que funciona mejor en la nube? En este caso, preferir칤as que el modelo estuviera en el auto. El tiempo de red adicional que tomar칤a para que los datos vayan del autom칩vil a la nube y luego de regreso al autom칩vil simplemente no valdr칤a la pena (o incluso ser칤a potencialmente imposible en 치reas con mala se침al).*\n",
    "\n",
    "> **Nota:** Para ver un ejemplo completo de c칩mo es implementar un modelo de PyTorch en un dispositivo perimetral, consulte el [tutorial de PyTorch sobre c칩mo lograr inferencia en tiempo real (30 fps+)](https://pytorch.org/ tutorials/intermediate/realtime_rpi.html) con un modelo de visi칩n por computadora en una Raspberry Pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf30596",
   "metadata": {},
   "source": [
    "### 쮺칩mo va a funcionar?\n",
    "\n",
    "Volviendo al caso de uso ideal, cuando implementa su modelo de aprendizaje autom치tico, 쯖칩mo deber칤a funcionar?\n",
    "\n",
    "Es decir, 쯟e gustar칤a que se le devolvieran las predicciones de inmediato?\n",
    "\n",
    "쯆 est치 bien que sucedan m치s tarde?\n",
    "\n",
    "Estos dos escenarios generalmente se denominan:\n",
    "\n",
    "* **En l칤nea (en tiempo real)**: las predicciones/inferencias ocurren **inmediatamente**. Por ejemplo, alguien sube una imagen, la imagen se transforma y se devuelven predicciones o alguien realiza una compra y un modelo verifica que la transacci칩n no es fraudulenta para que la compra pueda realizarse.\n",
    "* **Sin conexi칩n (por lotes)**: las predicciones/inferencias ocurren **peri칩dicamente**. Por ejemplo, una aplicaci칩n de fotos clasifica sus im치genes en diferentes categor칤as (como playa, hora de comer, familia, amigos) mientras su dispositivo m칩vil est치 enchufado a la carga.\n",
    "\n",
    "> **Nota:** \"Lote\" se refiere a la inferencia que se realiza en varias muestras a la vez. Sin embargo, para agregar un poco de confusi칩n, el procesamiento por lotes puede realizarse inmediatamente/en l칤nea (se clasifican varias im치genes a la vez) y/o fuera de l칤nea (se predicen/entrenan varias im치genes a la vez).  \n",
    "\n",
    "La principal diferencia entre cada ser: las predicciones se realizan de forma inmediata o peri칩dica.\n",
    "\n",
    "Peri칩dicamente tambi칠n puede tener una escala de tiempo variable, desde cada pocos segundos hasta cada pocas horas o d칤as.\n",
    "\n",
    "Y puedes mezclar y combinar los dos.\n",
    "\n",
    "En el caso de FoodVision Mini, queremos que nuestro proceso de inferencia se realice en l칤nea (en tiempo real), de modo que cuando alguien suba una imagen de pizza, bistec o sushi, los resultados de la predicci칩n se devuelvan inmediatamente (cualquier cosa m치s lenta de lo que lo har칤a el tiempo real). hacer una experiencia aburrida).\n",
    "\n",
    "Pero para nuestro proceso de capacitaci칩n, est치 bien que suceda por lotes (fuera de l칤nea), que es lo que hemos estado haciendo a lo largo de los cap칤tulos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df767b0",
   "metadata": {},
   "source": [
    "### Formas de implementar un modelo de aprendizaje autom치tico\n",
    "\n",
    "Hemos analizado un par de opciones para implementar modelos de aprendizaje autom치tico (en el dispositivo y en la nube).\n",
    "\n",
    "Y cada uno de estos tendr치 sus requisitos espec칤ficos:\n",
    "\n",
    "| **Herramienta/recurso** | **Tipo de implementaci칩n** | \n",
    "| ----- | ----- |\n",
    "| [Kit de aprendizaje autom치tico de Google](https://developers.google.com/ml-kit) | En el dispositivo (Android e iOS) | \n",
    "| [Core ML de Apple](https://developer.apple.com/documentation/coreml) y [paquete Python `coremltools`](https://coremltools.readme.io/docs) | En el dispositivo (todos los dispositivos Apple) | \n",
    "| [Sagemaker de Amazon Web Service (AWS)](https://aws.amazon.com/sagemaker/) | Nube | \n",
    "| [Vertex AI de Google Cloud](https://cloud.google.com/vertex-ai) | Nube |\n",
    "| [Aprendizaje autom치tico de Azure de Microsoft](https://azure.microsoft.com/en-au/services/machine-learning/) | Nube |\n",
    "| [Abrazando espacios faciales](https://huggingface.co/spaces) | Nube |\n",
    "| API con [FastAPI](https://fastapi.tiangolo.com) | Servidor en la nube/autohospedado |\n",
    "| API con [TorchServe](https://pytorch.org/serve/) | Servidor en la nube/autohospedado | \n",
    "| [ONNX (Intercambio de redes neuronales abiertas)](https://onnx.ai/index.html) | Muchos/general |\n",
    "| Muchos m치s... |\n",
    "\n",
    "> **Nota:** Una [interfaz de programaci칩n de aplicaciones (API)](https://en.wikipedia.org/wiki/API) es una forma en que dos (o m치s) programas inform치ticos interact칰an entre s칤. Por ejemplo, si su modelo se implement칩 como API, podr칤a escribir un programa que pudiera enviarle datos y luego recibir predicciones.\n",
    "\n",
    "La opci칩n que elija depender치 en gran medida de lo que est칠 creando y con qui칠n est칠 trabajando.\n",
    "\n",
    "Pero con tantas opciones, puede resultar muy intimidante.\n",
    "\n",
    "As칤 que lo mejor es empezar poco a poco y hacerlo sencillo.\n",
    "\n",
    "Y una de las mejores formas de hacerlo es convertir su modelo de aprendizaje autom치tico en una aplicaci칩n de demostraci칩n con [Gradio](https://gradio.app) y luego implementarlo en Hugging Face Spaces.\n",
    "\n",
    "M치s adelante haremos precisamente eso con FoodVision Mini.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png\" alt=\"herramientas y Lugares para implementar modelos de aprendizaje autom치tico\" width=900/>\n",
    "\n",
    "*Un pu침ado de lugares y herramientas para alojar e implementar modelos de aprendizaje autom치tico. Hay muchas cosas que me he perdido, as칤 que si desea agregar m치s, deje una [discusi칩n en GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369bc51",
   "metadata": {},
   "source": [
    "## Qu칠 vamos a cubrir \n",
    "\n",
    "Ya basta de hablar de implementar un modelo de aprendizaje autom치tico.\n",
    "\n",
    "Convirt치monos en ingenieros de aprendizaje autom치tico e implementemos uno.\n",
    "\n",
    "Nuestro objetivo es implementar nuestro modelo FoodVision a trav칠s de una aplicaci칩n de demostraci칩n de Gradio con las siguientes m칠tricas:\n",
    "1. **Rendimiento:** 95%+ precisi칩n.\n",
    "2. **Velocidad:** inferencia en tiempo real de 30 FPS+ (cada predicci칩n tiene una latencia inferior a ~0,03 s).\n",
    "\n",
    "Comenzaremos ejecutando un experimento para comparar nuestros dos mejores modelos hasta el momento: extractores de funciones EffNetB2 y ViT.\n",
    "\n",
    "Luego implementaremos el que se acerque m치s a nuestras m칠tricas objetivo.\n",
    "\n",
    "Finalmente, terminaremos con un (GRANDE) bono sorpresa.\n",
    "\n",
    "| **Tema** | **Contenido** | \n",
    "| ----- | ----- | \n",
    "| **0. Obteniendo configuraci칩n** | Hemos escrito bastante c칩digo 칰til en las 칰ltimas secciones, descargu칠moslo y asegur칠monos de poder usarlo nuevamente. | \n",
    "| **1. Obtener datos** | Descarguemos el conjunto de datos [`pizza_steak_sushi_20_percent.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) para que podamos entrenar nuestros modelos que anteriormente ten칤an mejor rendimiento en el mismo conjunto de datos. |\n",
    "| **2. Esquema del experimento de implementaci칩n del modelo FoodVision Mini** | Incluso en el proyecto del tercer hito, todav칤a realizaremos m칰ltiples experimentos para ver qu칠 modelo (EffNetB2 o ViT) se acerca m치s a nuestras m칠tricas objetivo. |\n",
    "| **3. Creando un extractor de funciones EffNetB2** | Un extractor de funciones EfficientNetB2 tuvo el mejor rendimiento en nuestro conjunto de datos de pizza, bistec y sushi en [07. Seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/), vamos a recrearlo como candidato para su implementaci칩n. |\n",
    "| **4. Creando un extractor de funciones ViT** | Un extractor de funciones ViT ha sido el modelo con mejor rendimiento hasta ahora en nuestro conjunto de datos de pizza, bistec y sushi en [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/), vamos a recrearlo como candidato para su implementaci칩n junto con EffNetB2. |\n",
    "| **5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas** | Hemos creado dos de los modelos con mejor rendimiento hasta el momento. Hagamos predicciones con ellos y realicemos un seguimiento de sus resultados. |\n",
    "| **6. Comparaci칩n de resultados de modelos, tiempos de predicci칩n y tama침o** | Comparemos nuestros modelos para ver cu치l funciona mejor con nuestros objetivos. | \n",
    "| **7. Dar vida a FoodVision Mini creando una demostraci칩n de Gradio** | Uno de nuestros modelos funciona mejor que el otro (en t칠rminos de nuestros objetivos), as칤 que 춰convirt치moslo en una demostraci칩n de aplicaci칩n funcional! |\n",
    "| **8. Convirtiendo nuestra demostraci칩n de FoodVision Mini Gradio en una aplicaci칩n implementable** | Nuestra demostraci칩n de la aplicaci칩n Gradio funciona localmente, 춰prepar칠mosla para su implementaci칩n! |\n",
    "| **9. Implementando nuestra demostraci칩n de Gradio en HuggingFace Spaces** | 춰Llevemos FoodVision Mini a la web y hag치moslo accesible p칰blicamente para todos! |\n",
    "| **10. Creando una GRAN sorpresa** | Hemos creado FoodVision Mini, es hora de dar un paso m치s. |\n",
    "| **11. Desplegando nuestra GRAN sorpresa** | Implementar una aplicaci칩n fue divertido, 쯤u칠 tal si hacemos dos? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cec63e",
   "metadata": {},
   "source": [
    "## 쮻칩nde puedes obtener ayuda?\n",
    "\n",
    "Todos los materiales de este curso [est치n disponibles en GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
    "\n",
    "Si tiene problemas, puede hacer una pregunta en el curso [p치gina de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
    "\n",
    "Y, por supuesto, est치 la [documentaci칩n de PyTorch](https://pytorch.org/docs/stable/index.html) y los [foros de desarrolladores de PyTorch](https://discuss.pytorch.org/), un lugar muy 칰til para todo lo relacionado con PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598c6e3",
   "metadata": {},
   "source": [
    "## 0. Configuraci칩n \n",
    "\n",
    "Como lo hicimos anteriormente, asegur칠monos de tener todos los m칩dulos que necesitaremos para esta secci칩n.\n",
    "\n",
    "Importaremos los scripts de Python (como `data_setup.py` y `engine.py`) que creamos en [05. PyTorch se vuelve modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
    "\n",
    "Para hacerlo, descargaremos el directorio [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) del repositorio [`pytorch-deep-learning`]( https://github.com/mrdbourke/pytorch-deep-learning) (si a칰n no lo tenemos).\n",
    "\n",
    "Tambi칠n obtendremos el paquete [`torchinfo`](https://github.com/TylerYep/torchinfo) si no est치 disponible. \n",
    "\n",
    "`torchinfo` nos ayudar치 m치s adelante a darnos una representaci칩n visual de nuestro modelo.\n",
    "\n",
    "Y dado que m치s adelante usaremos el paquete `torchvision` v0.13 (disponible a partir de julio de 2022), nos aseguraremos de tener las 칰ltimas versiones.\n",
    "\n",
    "> **Nota:** Si est치s usando Google Colab y a칰n no tienes una GPU activada, ahora es el momento de activar una a trav칠s de `Runtime -> Cambiar tipo de tiempo de ejecuci칩n -> Acelerador de hardware -> GPU` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que este port치til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c43b1d",
   "metadata": {},
   "source": [
    "> **Nota:** Si est치 utilizando Google Colab y la celda de arriba comienza a instalar varios paquetes de software, es posible que deba reiniciar su tiempo de ejecuci칩n despu칠s de ejecutar la celda de arriba. Despu칠s de reiniciar, puede ejecutar la celda nuevamente y verificar que tenga las versiones correctas de `torch` y `torchvision`.\n",
    "\n",
    "Ahora continuaremos con las importaciones regulares, configurando el c칩digo independiente del dispositivo y esta vez tambi칠n obtendremos [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/ main/helper_functions.py) script de GitHub.\n",
    "\n",
    "El script `helper_functions.py` contiene varias funciones que creamos en secciones anteriores:\n",
    "* `set_seeds()` para configurar las semillas aleatorias (creadas en [07. Secci칩n 0 de seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds )).\n",
    "* `download_data()` para descargar una fuente de datos mediante un enlace (creado en [07. Secci칩n 1 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n",
    "* `plot_loss_curves()` para inspeccionar los resultados del entrenamiento de nuestro modelo (creado en [04. PyTorch Custom Datasets secci칩n 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of- modelo-0))\n",
    "\n",
    "> **Nota:** Puede ser una mejor idea que muchas de las funciones en el script `helper_functions.py` se fusionen en `going_modular/going_modular/utils.py`, tal vez sea una extensi칩n que le gustar칤a probar ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuar con las importaciones regulares\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Intente obtener torchinfo, inst치lelo si no funciona\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Intente importar el directorio going_modular, desc치rguelo de GitHub si no funciona\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3865a8",
   "metadata": {},
   "source": [
    "Finalmente, configuraremos un c칩digo independiente del dispositivo para asegurarnos de que nuestros modelos se ejecuten en la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb0e12",
   "metadata": {},
   "source": [
    "## 1. Obtener datos\n",
    "\n",
    "Lo dejamos en [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size) comparando nuestro propio modelo de extractor de funciones Vision Transformer (ViT) con El modelo de extracci칩n de caracter칤sticas EfficientNetB2 (EffNetB2) que creamos en [07. Seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it).\n",
    "\n",
    "Y descubrimos que hab칤a una ligera diferencia en la comparaci칩n.\n",
    "\n",
    "El modelo EffNetB2 se entren칩 en el 20 % de los datos de pizza, bistec y sushi de Food101, mientras que el modelo ViT se entren칩 en el 10 %.\n",
    "\n",
    "Dado que nuestro objetivo es implementar el mejor modelo para nuestro problema FoodVision Mini, comencemos descargando el [conjunto de datos del 20 % de pizza, bistec y sushi] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main /data/pizza_steak_sushi_20_percent.zip) y entrene un extractor de funciones EffNetB2 y un extractor de funciones ViT en 칠l y luego compare los dos modelos.\n",
    "\n",
    "De esta manera, compararemos manzanas con manzanas (un modelo entrenado en un conjunto de datos con otro modelo entrenado en el mismo conjunto de datos).\n",
    "\n",
    "> **Nota:** El conjunto de datos que estamos descargando es una muestra de todo el [conjunto de datos de Food101](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html#food101) (101 clases de comida con 1.000 im치genes cada una). M치s espec칤ficamente, 20% se refiere al 20% de im치genes de las clases de pizza, bistec y sushi seleccionadas al azar. Puede ver c칩mo se cre칩 este conjunto de datos en [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) y m치s detalles en [ 04. Secci칩n 1 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).\n",
    "\n",
    "Podemos descargar los datos usando la funci칩n `download_data()` que creamos en [07. Secci칩n 1 de seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data) de [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning /blob/main/helper_functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargue im치genes de pizza, bistec y sushi desde GitHub\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45e09a",
   "metadata": {},
   "source": [
    "춰Maravilloso!\n",
    "\n",
    "Ahora que tenemos un conjunto de datos, creemos rutas de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar rutas de directorio para entrenar y probar im치genes\n",
    "train_dir = data_20_percent_path / \"train\"\n",
    "test_dir = data_20_percent_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f7d61",
   "metadata": {},
   "source": [
    "## 2. Esquema del experimento de implementaci칩n del modelo FoodVision Mini\n",
    "\n",
    "El modelo implementado ideal, FoodVision Mini, funciona bien y r치pido. \n",
    "\n",
    "Nos gustar칤a que nuestro modelo funcione lo m치s cerca posible del tiempo real.\n",
    "\n",
    "En este caso, el tiempo real es ~30 FPS (cuadros por segundo) porque eso es [aproximadamente qu칠 tan r치pido puede ver el ojo humano] (https://www.healthline.com/health/human-eye-fps) (hay debate sobre esto, pero usemos ~30FPS como nuestro punto de referencia).\n",
    "\n",
    "Y para clasificar tres clases diferentes (pizza, bistec y sushi), nos gustar칤a un modelo que funcione con una precisi칩n superior al 95 %.\n",
    "\n",
    "Por supuesto, una mayor precisi칩n ser칤a buena, pero esto podr칤a sacrificar la velocidad.\n",
    "\n",
    "Entonces nuestros objetivos son:\n",
    "\n",
    "1. **Rendimiento**: un modelo que funciona con una precisi칩n superior al 95 %.\n",
    "2. **Velocidad**: un modelo que puede clasificar una imagen a ~30 FPS (tiempo de inferencia de 0,03 segundos por imagen, tambi칠n conocido como latencia).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png\" alt=\"mini objetivos de foodvision en t칠rminos de tiempo de desempe침o y de inferencia.\" ancho=750/>\n",
    "\n",
    "*Objetivos de implementaci칩n de FoodVision Mini. Nos gustar칤a un modelo de predicci칩n r치pida y con buen rendimiento (porque una aplicaci칩n lenta es aburrida).*\n",
    "\n",
    "Pondremos 칠nfasis en la velocidad, es decir, preferir칤amos un modelo con un rendimiento superior al 90 % a ~30 FPS que un modelo con un rendimiento superior al 95 % a 10 FPS.\n",
    "\n",
    "Para intentar lograr estos resultados, incluyamos nuestros modelos con mejor rendimiento de las secciones anteriores: \n",
    "\n",
    "1. **Extractor de funciones EffNetB2** (EffNetB2 para abreviar): creado originalmente en [07. Secci칩n 7.5 de seguimiento de experimentos de PyTorch](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) usando [`torchvision.models.ficientnet_b2()`](https://pytorch.org /vision/stable/models/generated/torchvision.models.ficientnet_b2.html#ficientnet-b2) con capas de `clasificador` ajustadas.\n",
    "2. **Extractor de funciones ViT-B/16** (ViT para abreviar): creado originalmente en [08. Secci칩n 10 de replicaci칩n de papel de PyTorch] (https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) usando [`torchvision.models.vit_b_16 ()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) con capas de `cabeza` ajustadas.\n",
    "    * **Nota** ViT-B/16 significa \"Vision Transformer Base, tama침o de parche 16\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png\" alt=\"experimentos de modelado para mini implementaciones de foodvision, uno Modelo de extractor de caracter칤sticas effnetb2 y modelo de extractor de caracter칤sticas de transformador de visi칩n\" ancho=750 />\n",
    "\n",
    "> **Nota:** Un \"modelo de extracci칩n de caracter칤sticas\" a menudo comienza con un modelo que ha sido previamente entrenado en un conjunto de datos similar a su propio problema. Las capas base del modelo previamente entrenado a menudo se dejan congeladas (los patrones/pesos previamente entrenados permanecen iguales) mientras que algunas de las capas superiores (o clasificador/cabeza de clasificaci칩n) se personalizan seg칰n su propio problema entrenando con sus propios datos. Cubrimos el concepto de un modelo de extracci칩n de caracter칤sticas en [06. Secci칩n 3.4 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89626ee3",
   "metadata": {},
   "source": [
    "## 3. Creando un extractor de funciones EffNetB2\n",
    "\n",
    "Primero creamos un modelo de extracci칩n de caracter칤sticas EffNetB2 en [07. Secci칩n 7.5 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).\n",
    "\n",
    "Y al final de esa secci칩n vimos que funcion칩 muy bien.\n",
    "\n",
    "As칤 que ahora vamos a recrearlo aqu칤 para que podamos comparar sus resultados con un extractor de funciones de ViT entrenado con los mismos datos.\n",
    "\n",
    "Para hacerlo podemos:\n",
    "1. Configure los pesos previamente entrenados como [`weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.ficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights ), donde \"`DEFAULT`\" significa \"mejor disponible actualmente\" (o podr칤a usar `weights=\"DEFAULT\"`). \n",
    "2. Obtenga las transformaciones de la imagen del modelo previamente entrenado a partir de los pesos con el m칠todo `transforms()` (los necesitamos para poder convertir nuestras im치genes al mismo formato en el que se entren칩 el EffNetB2 previamente entrenado).\n",
    "3. Cree una instancia de modelo previamente entrenada pasando los pesos a una instancia de [`torchvision.models.ficientnet_b2`](https://pytorch.org/vision/stable/models/generated/torchvision.models.ficientnet_b2.html#ficientnet -b2).\n",
    "4. Congele las capas base en el modelo.\n",
    "5. Actualizar el cabezal del clasificador para adaptarlo a nuestros propios datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687129c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configurar pesas EffNetB2 previamente entrenadas\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Obtenga transformaciones EffNetB2\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Configurar el modelo previamente entrenado\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Congele las capas base en el modelo (esto congelar치 todas las capas para empezar)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8c763",
   "metadata": {},
   "source": [
    "Ahora, para cambiar el encabezado del clasificador, primero inspeccion칠moslo usando el atributo \"clasificador\" de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte el cabezal clasificador EffNetB2\n",
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19619350",
   "metadata": {},
   "source": [
    "춰Excelente! Para cambiar el encabezado del clasificador para adaptarlo a nuestro propio problema, reemplacemos la variable `out_features` con el mismo n칰mero de clases que tenemos (en nuestro caso, `out_features=3`, una para pizza, bistec, sushi).\n",
    "\n",
    "> **Nota:** Este proceso de cambiar las capas de salida/cabezal clasificador depender치 del problema en el que est칠 trabajando. Por ejemplo, si quisiera un *n칰mero* diferente de salidas o un *tipo* diferente de salida, tendr칤a que cambiar las capas de salida en consecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Actualice el cabezal del clasificador.\n",
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "    nn.Linear(in_features=1408, # keep in_features same \n",
    "              out_features=3)) # change out_features to suit our number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d55f5",
   "metadata": {},
   "source": [
    "춰Hermoso!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e31bd4",
   "metadata": {},
   "source": [
    "### 3.1 Creando una funci칩n para hacer un extractor de caracter칤sticas EffNetB2\n",
    "Parece que nuestro extractor de funciones EffNetB2 est치 listo para funcionar; sin embargo, dado que aqu칤 hay bastantes pasos involucrados, 쯤u칠 tal si convertimos el c칩digo anterior en una funci칩n que podamos reutilizar m치s adelante?\n",
    "\n",
    "Lo llamaremos `create_effnetb2_model()` y necesitar치 un n칰mero personalizable de clases y un par치metro inicial aleatorio para su reproducibilidad.\n",
    "\n",
    "Idealmente, devolver치 un extractor de funciones EffNetB2 junto con sus transformaciones asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # 4. Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 5. Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b91a57",
   "metadata": {},
   "source": [
    "춰Guau! Es una funci칩n muy bonita, prob칠mosla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62808c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
    "                                                      seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271ac57",
   "metadata": {},
   "source": [
    "Sin errores, genial, ahora para probarlo realmente, obtengamos un resumen con `torchinfo.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Imprimir resumen del modelo EffNetB2 (descomentar para obtener un resultado completo)\n",
    "# resumen(effnetb2,\n",
    "# tama침o_entrada=(1, 3, 224, 224),\n",
    "# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n",
    "# ancho_columna=20,\n",
    "# row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a5fdd",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-effnetb2-feature-extractor.png\" alt=\"resumen del modelo de extractor de funciones effnetb2\" width=900/ >\n",
    "\n",
    "춰Capas base congeladas, capas superiores entrenables y personalizadas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9837a",
   "metadata": {},
   "source": [
    "### 3.2 Creando cargadores de datos para EffNetB2 \n",
    "\n",
    "Nuestro extractor de funciones EffNetB2 est치 listo, es hora de crear algunos `DataLoader`.\n",
    "\n",
    "Podemos hacer esto usando la funci칩n [`data_setup.create_dataloaders()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) que creamos en [ 05. PyTorch Going Modular secci칩n 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
    "\n",
    "Usaremos un `batch_size` de 32 y transformaremos nuestras im치genes usando `effnetb2_transforms` para que est칠n en el mismo formato en el que se entren칩 nuestro modelo `effnetb2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar cargadores de datos\n",
    "from going_modular.going_modular import data_setup\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071f49b",
   "metadata": {},
   "source": [
    "### 3.3 Entrenamiento del extractor de funciones de EffNetB2\n",
    "\n",
    "Modelo listo, `DataLoader`s listo, 춰entrenemos!\n",
    "\n",
    "Al igual que en [07. Secci칩n 7.6 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code), diez 칠pocas deber칤an ser suficientes para obtener buenos resultados.\n",
    "\n",
    "Podemos hacerlo creando un optimizador (usaremos [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim .Adam) con una tasa de aprendizaje de `1e-3`), una funci칩n de p칠rdida (usaremos [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch .nn.CrossEntropyLoss.html) para clasificaci칩n de clases m칰ltiples) y luego pasarlos junto con nuestro `DataLoader`s al [`engine.train()`](https://github.com/mrdbourke/pytorch-deep -learning/blob/main/going_modular/going_modular/engine.py) funci칩n que creamos en [05. PyTorch Going Modular secci칩n 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Optimizador de configuraci칩n\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Funci칩n de p칠rdida de configuraci칩n\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Establezca semillas para la reproducibilidad y entrene el modelo.\n",
    "set_seeds()\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=10,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7877ba",
   "metadata": {},
   "source": [
    "### 3.4 Inspeccionando las curvas de p칠rdida de EffNetB2 \n",
    "\n",
    "춰Lindo!\n",
    "\n",
    "Como vimos en 07. Seguimiento de experimentos de PyTorch, el modelo de extracci칩n de caracter칤sticas EffNetB2 funciona bastante bien con nuestros datos.\n",
    "\n",
    "Convirtamos sus resultados en curvas de p칠rdidas para inspeccionarlos m치s a fondo.\n",
    "\n",
    "> **Nota:** Las curvas de p칠rdida son una de las mejores formas de visualizar el rendimiento de su modelo. Para obtener m치s informaci칩n sobre las curvas de p칠rdidas, consulte [04. Secci칩n 8 de conjuntos de datos personalizados de PyTorch: 쮺칩mo deber칤a ser una curva de p칠rdida ideal?](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47482f5e",
   "metadata": {},
   "source": [
    "춰Guau!\n",
    "\n",
    "Esas son algunas curvas de p칠rdidas bonitas. \n",
    "\n",
    "Parece que nuestro modelo est치 funcionando bastante bien y tal vez se beneficiar칤a de un entrenamiento un poco m치s largo y potencialmente de algo de [aumento de datos](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data -aumento) (para ayudar a prevenir un posible sobreajuste que se produzca debido a un entrenamiento m치s prolongado)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3a937",
   "metadata": {},
   "source": [
    "### 3.5 Guardar el extractor de funciones de EffNetB2\n",
    "\n",
    "Ahora que tenemos un modelo entrenado con buen rendimiento, guard칠moslo en un archivo para poder importarlo y usarlo m치s tarde.\n",
    "\n",
    "Para guardar nuestro modelo podemos usar la funci칩n [`utils.save_model()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py) que creamos en [05. PyTorch Going Modular secci칩n 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).\n",
    "\n",
    "Estableceremos `target_dir` en `\"models\"` y `model_name` en `\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"` (un poco completo, pero al menos sabemos lo que est치 pasando)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a413d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import utils\n",
    "\n",
    "# guardar el modelo\n",
    "utils.save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd1230",
   "metadata": {},
   "source": [
    "### 3.6 Comprobar el tama침o del extractor de funciones EffNetB2\n",
    "\n",
    "Dado que uno de nuestros criterios para implementar un modelo que impulse FoodVision Mini es la **velocidad** (~30 FPS o mejor), verifiquemos el tama침o de nuestro modelo.\n",
    "\n",
    "쯇or qu칠 comprobar el tama침o?\n",
    "\n",
    "Bueno, aunque no siempre es as칤, el tama침o de un modelo puede influir en su velocidad de inferencia.\n",
    "\n",
    "Es decir, si un modelo tiene m치s par치metros, generalmente realiza m치s operaciones y cada una de estas operaciones requiere cierta potencia inform치tica.\n",
    "\n",
    "Y como nos gustar칤a que nuestro modelo funcione en dispositivos con potencia inform치tica limitada (por ejemplo, en un dispositivo m칩vil o en un navegador web), generalmente, cuanto m치s peque침o sea el tama침o, mejor (siempre que siga funcionando bien en t칠rminos de precisi칩n). .\n",
    "\n",
    "Para verificar el tama침o de nuestro modelo en bytes, podemos usar [`pathlib.Path.stat(\"path_to_model\").st_size`](https://docs.python.org/3/library/pathlib.html#pathlib.Path) de Python .stat) y luego podemos convertirlo (aproximadamente) a megabytes dividi칠ndolo por `(1024*1024)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga el tama침o del modelo en bytes y luego convi칠rtalo a megabytes\n",
    "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e0e07",
   "metadata": {},
   "source": [
    "### 3.7 Recopilaci칩n de estad칤sticas del extractor de funciones de EffNetB2\n",
    "\n",
    "Tenemos algunas estad칤sticas sobre nuestro modelo de extractor de funciones EffNetB2, como p칠rdida de prueba, precisi칩n de la prueba y tama침o del modelo. 쯈u칠 tal si las recopilamos todas en un diccionario para poder compararlas con el pr칩ximo extractor de funciones ViT?\n",
    "\n",
    "Y calcularemos uno extra por diversi칩n, el n칰mero total de par치metros.\n",
    "\n",
    "Podemos hacerlo contando el n칰mero de elementos (o patrones/pesos) en `effnetb2.parameters()`. Accederemos al n칰mero de elementos en cada par치metro usando [`torch.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html) (abreviatura de \"n칰mero de elementos \") m칠todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be32bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuente el n칰mero de par치metros en EffNetB2\n",
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "effnetb2_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ada7f9",
   "metadata": {},
   "source": [
    "춰Excelente!\n",
    "\n",
    "Ahora pongamos todo en un diccionario para poder hacer comparaciones m치s adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario con estad칤sticas de EffNetB2\n",
    "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": effnetb2_total_params,\n",
    "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0aa115",
   "metadata": {},
   "source": [
    "춰칄pico! \n",
    "\n",
    "춰Parece que nuestro modelo EffNetB2 funciona con m치s del 95% de precisi칩n! \n",
    "\n",
    "Criterio n칰mero 1: actuar con una precisi칩n superior al 95%, 춰marca!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf608cd",
   "metadata": {},
   "source": [
    "## 4. Creaci칩n de un extractor de funciones ViT\n",
    "\n",
    "Es hora de continuar con nuestros experimentos de modelado de FoodVision Mini.\n",
    "\n",
    "Esta vez vamos a crear un extractor de funciones de ViT.\n",
    "\n",
    "Y lo haremos de la misma manera que el extractor de funciones EffNetB2, excepto que esta vez con [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision. models.vit_b_16.html#torchvision.models.vit_b_16) en lugar de `torchvision.models.ficientnet_b2()`.\n",
    "\n",
    "Comenzaremos creando una funci칩n llamada `create_vit_model()` que ser치 muy similar a `create_effnetb2_model()` excepto, por supuesto, que devolver치 un modelo extractor de caracter칤sticas ViT y transformaciones en lugar de EffNetB2.\n",
    "\n",
    "Otra peque침a diferencia es que la capa de salida de `torchvision.models.vit_b_16()` se llama `cabezas` en lugar de `clasificador`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte la capa de cabezales ViT\n",
    "vit = torchvision.models.vit_b_16()\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f102e",
   "metadata": {},
   "source": [
    "Sabiendo esto, tenemos todas las piezas del rompecabezas que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes:int=3, \n",
    "                     seed:int=42):\n",
    "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
    "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT-B/16 feature extractor model. \n",
    "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
    "    \"\"\"\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze all layers in model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head to suit our needs (this will be trainable)\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
    "                                          out_features=num_classes)) # update to reflect target number of classes\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ea0d8",
   "metadata": {},
   "source": [
    "춰La funci칩n de creaci칩n de modelos de extracci칩n de caracter칤sticas ViT est치 lista!\n",
    "\n",
    "Prob칠moslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo ViT y transformaciones.\n",
    "vit, vit_transforms = create_vit_model(num_classes=3,\n",
    "                                       seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7702cc",
   "metadata": {},
   "source": [
    "Sin errores, 춰es encantador verlo! \n",
    "\n",
    "Ahora obtengamos un resumen atractivo de nuestro modelo ViT usando `torchinfo.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d41dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Imprimir resumen del modelo del extractor de funciones de ViT (descomentar para obtener un resultado completo)\n",
    "# resumen(vit,\n",
    "# tama침o_entrada=(1, 3, 224, 224),\n",
    "# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n",
    "# ancho_columna=20,\n",
    "# row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1fae70",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-vit-feature-extractor-3-classes.png\" width=900 alt=\"extractor de funciones vit con 3 clases de salida\"/>\n",
    "\n",
    "Al igual que nuestro modelo de extracci칩n de funciones EffNetB2, las capas base de nuestro modelo ViT est치n congeladas y la capa de salida se personaliza seg칰n nuestras necesidades. \n",
    "\n",
    "쯅otas la gran diferencia?\n",
    "\n",
    "Nuestro modelo ViT tiene *muchos* m치s par치metros que nuestro modelo EffNetB2. Quiz치s esto entre en juego cuando comparemos nuestros modelos en cuanto a velocidad y rendimiento m치s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a43d6",
   "metadata": {},
   "source": [
    "### 4.1 Crear cargadores de datos para ViT\n",
    "\n",
    "Tenemos nuestro modelo ViT listo, ahora creemos algunos `DataLoader`s para 칠l.\n",
    "\n",
    "Haremos esto de la misma manera que hicimos para EffNetB2 excepto que usaremos `vit_transforms` para transformar nuestras im치genes al mismo formato en el que se entren칩 el modelo ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar cargadores de datos ViT\n",
    "from going_modular.going_modular import data_setup\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                       test_dir=test_dir,\n",
    "                                                                                       transform=vit_transforms,\n",
    "                                                                                       batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28c6e4",
   "metadata": {},
   "source": [
    "### 4.2 Extractor de funciones de ViT de entrenamiento\n",
    "\n",
    "Sabes que hora es...\n",
    "\n",
    "...es hora de entrenargggggg (cantado con la misma melod칤a que la canci칩n [Closing Time](https://youtu.be/xGytDsqkQY8)). \n",
    "\n",
    "Entrenemos nuestro modelo de extracci칩n de caracter칤sticas ViT durante 10 칠pocas usando nuestra funci칩n `engine.train()` con `torch.optim.Adam()` y una tasa de aprendizaje de `1e-3` como nuestro optimizador y `torch.nn.CrossEntropyLoss ()` como nuestra funci칩n de p칠rdida.\n",
    "\n",
    "Usaremos nuestra funci칩n `set_seeds()` antes del entrenamiento para intentar que nuestros resultados sean lo m치s reproducibles posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Optimizador de configuraci칩n\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Funci칩n de p칠rdida de configuraci칩n\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Entrene el modelo ViT con semillas configuradas para lograr reproducibilidad\n",
    "set_seeds()\n",
    "vit_results = engine.train(model=vit,\n",
    "                           train_dataloader=train_dataloader_vit,\n",
    "                           test_dataloader=test_dataloader_vit,\n",
    "                           epochs=10,\n",
    "                           optimizer=optimizer,\n",
    "                           loss_fn=loss_fn,\n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3abbd9",
   "metadata": {},
   "source": [
    "### 4.3 Inspeccionando las curvas de p칠rdidas de ViT\n",
    "\n",
    "Muy bien, est치 bien, est치 bien, modelo ViT entrenado, seamos visuales y veamos algunas curvas de p칠rdida.\n",
    "\n",
    "> **Nota:** No olvide que puede ver c칩mo deber칤a verse un conjunto ideal de curvas de p칠rdida en [04. Secci칩n 8 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2970a1",
   "metadata": {},
   "source": [
    "춰Ohh si! \n",
    "\n",
    "Esas son algunas curvas de p칠rdidas bonitas. Al igual que nuestro modelo de extracci칩n de funciones EffNetB2, parece que nuestro modelo ViT podr칤a beneficiarse de un tiempo de entrenamiento un poco m치s largo y tal vez algo de [aumento de datos](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of- transforma-datos-aumento) (para ayudar a prevenir el sobreajuste)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc635d6",
   "metadata": {},
   "source": [
    "### 4.4 Guardar el extractor de funciones de ViT\n",
    "\n",
    "춰Nuestro modelo ViT est치 funcionando de manera excelente! \n",
    "\n",
    "As칤 que guard칠moslo en un archivo para poder importarlo y usarlo m치s tarde si lo deseamos.\n",
    "\n",
    "Podemos hacerlo usando la funci칩n `utils.save_model()` que creamos en [05. PyTorch Going Modular secci칩n 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar el modelo\n",
    "from going_modular.going_modular import utils\n",
    "\n",
    "utils.save_model(model=vit,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cf52c",
   "metadata": {},
   "source": [
    "### 4.5 Comprobar el tama침o del extractor de funciones ViT\n",
    "\n",
    "Y como queremos comparar nuestro modelo EffNetB2 con nuestro modelo ViT en funci칩n de una serie de caracter칤sticas, averig칲emos su tama침o.\n",
    "\n",
    "Para verificar el tama침o de nuestro modelo en bytes, podemos usar `pathlib.Path.stat(\"path_to_model\").st_size` de Python y luego podemos convertirlo (aproximadamente) a megabytes dividi칠ndolo por `(1024*1024)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga el tama침o del modelo en bytes y luego convi칠rtalo a megabytes\n",
    "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c02a80",
   "metadata": {},
   "source": [
    "Hmm, 쯖칩mo se compara el tama침o del modelo del extractor de funciones ViT con el tama침o de nuestro modelo EffNetB2?\n",
    "\n",
    "Lo descubriremos en breve cuando comparemos todas las caracter칤sticas de nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a22783",
   "metadata": {},
   "source": [
    "### 4.6 Recopilaci칩n de estad칤sticas del extractor de funciones de ViT\n",
    "\n",
    "Reunamos todas las estad칤sticas del modelo de extracci칩n de funciones de ViT.\n",
    "\n",
    "Lo vimos en el resumen anterior, pero calcularemos su n칰mero total de par치metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb262e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el n칰mero de par치metros en ViT\n",
    "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
    "vit_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9deebcb",
   "metadata": {},
   "source": [
    "춰Vaya, eso parece bastante m치s que nuestro EffNetB2!\n",
    "\n",
    "> **Nota:** Una mayor cantidad de par치metros (o pesos/patrones) generalmente significa que un modelo tiene una mayor *capacidad* de aprender; si realmente utiliza esta capacidad adicional es otra historia. A la luz de esto, nuestro modelo EffNetB2 tiene 7.705.221 par치metros, mientras que nuestro modelo ViT tiene 85.800.963 (11,1 veces m치s), por lo que podr칤amos suponer que nuestro modelo ViT tiene m치s capacidad de aprender, si se le dan m치s datos (m치s oportunidades de aprender). Sin embargo, esta mayor capacidad de aprender a menudo viene acompa침ada de un mayor tama침o del modelo y un mayor tiempo para realizar la inferencia.\n",
    "\n",
    "Ahora creemos un diccionario con algunas caracter칤sticas importantes de nuestro modelo ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f32762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionario de estad칤sticas de ViT\n",
    "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
    "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
    "             \"number_of_parameters\": vit_total_params,\n",
    "             \"model_size (MB)\": pretrained_vit_model_size}\n",
    "\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce72f1",
   "metadata": {},
   "source": [
    "춰Lindo! Parece que nuestro modelo ViT tambi칠n logra una precisi칩n superior al 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27add0",
   "metadata": {},
   "source": [
    "## 5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas\n",
    "\n",
    "Tenemos un par de modelos entrenados y ambos funcionan bastante bien.\n",
    "\n",
    "Ahora, 쯤u칠 tal si los probamos haciendo lo que nos gustar칤a que hicieran?\n",
    "\n",
    "Es decir, veamos c칩mo hacen predicciones (realizando inferencias).\n",
    "\n",
    "Sabemos que nuestros dos modelos funcionan con una precisi칩n superior al 95 % en el conjunto de datos de prueba, pero 쯤u칠 tan r치pidos son?\n",
    "\n",
    "Idealmente, si implementamos nuestro modelo FoodVision Mini en un dispositivo m칩vil para que las personas puedan tomar fotograf칤as de sus alimentos e identificarlos, nos gustar칤a que las predicciones se realicen en tiempo real (~30 fotogramas por segundo).\n",
    "\n",
    "Por eso nuestro segundo criterio es: un modelo r치pido.\n",
    "\n",
    "Para saber cu치nto tiempo tarda cada uno de nuestros modelos en inferir el rendimiento, creemos una funci칩n llamada `pred_and_store()` para iterar sobre cada una de las im치genes del conjunto de datos de prueba una por una y realizar una predicci칩n. \n",
    "\n",
    "Calcularemos el tiempo de cada una de las predicciones y almacenaremos los resultados en un formato de predicci칩n com칰n: una lista de diccionarios (donde cada elemento de la lista es una predicci칩n 칰nica y cada predicci칩n 칰nica es un diccionario). \n",
    "\n",
    "> **Nota:** Calculamos las predicciones una por una en lugar de por lotes porque cuando se implementa nuestro modelo, probablemente solo har치 una predicci칩n en una imagen a la vez. Es decir, alguien toma una foto y nuestro modelo predice sobre esa 칰nica imagen.\n",
    "\n",
    "Como nos gustar칤a hacer predicciones en todas las im치genes del conjunto de prueba, primero obtengamos una lista de todas las rutas de las im치genes de prueba para que podamos iterar sobre ellas. \n",
    "\n",
    "Para hacerlo, usaremos [`pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))`](https://docs.python.org/3/library/pathlib) de Python. html#basic-use) para encontrar todas las rutas de archivos en un directorio de destino con la extensi칩n `.jpg` (todas nuestras im치genes de prueba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313adfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga todas las rutas de datos de prueba\n",
    "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca242a",
   "metadata": {},
   "source": [
    "### 5.1 Crear una funci칩n para hacer predicciones en todo el conjunto de datos de prueba\n",
    "\n",
    "Ahora que tenemos una lista de nuestras rutas de im치genes de prueba, comencemos a trabajar en nuestra funci칩n `pred_and_store()`:\n",
    "\n",
    "1. Cree una funci칩n que tome una lista de rutas, un modelo PyTorch entrenado, una serie de transformaciones (para preparar im치genes), una lista de nombres de clases de destino y un dispositivo de destino.\n",
    "2. Cree una lista vac칤a para almacenar diccionarios de predicci칩n (queremos que la funci칩n devuelva una lista de diccionarios, uno para cada predicci칩n).\n",
    "3. Recorra las rutas de entrada de destino (los pasos 4 a 14 se realizar치n dentro del bucle).\n",
    "4. Cree un diccionario vac칤o para cada iteraci칩n del bucle para almacenar los valores de predicci칩n por muestra.\n",
    "5. Obtenga la ruta de muestra y el nombre de la clase de verdad fundamental (podemos hacer esto infiriendo la clase a partir de la ruta).\n",
    "6. Inicie el temporizador de predicci칩n usando [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer) de Python.\n",
    "7. Abra la imagen usando [`PIL.Image.open(path)`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
    "8. Transforme la imagen para que pueda usarse con el modelo de destino, as칤 como agregue una dimensi칩n por lotes y env칤e la imagen al dispositivo de destino.\n",
    "9. Prepare el modelo para la inferencia envi치ndolo al dispositivo de destino y activando el modo `eval()`.\n",
    "10. Active [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html) y pase la imagen transformada de destino al modelo y calcule la probabilidad de predicci칩n usando ` torch.softmax()` y la etiqueta de destino usando `torch.argmax()`.\n",
    "11. Agregue la probabilidad de predicci칩n y la clase de predicci칩n al diccionario de predicci칩n creado en el paso 4. Tambi칠n aseg칰rese de que la probabilidad de predicci칩n est칠 en la CPU para que pueda usarse con bibliotecas que no son de GPU, como NumPy y pandas, para una inspecci칩n posterior.\n",
    "12. Finalice el temporizador de predicci칩n iniciado en el paso 6 y agregue el tiempo al diccionario de predicci칩n creado en el paso 4.\n",
    "13. Vea si la clase predicha coincide con la clase de verdad fundamental del paso 5 y agregue el resultado al diccionario de predicci칩n creado en el paso 4.\n",
    "14. Agregue el diccionario de predicciones actualizado a la lista vac칤a de predicciones creada en el paso 2.\n",
    "15. Devuelve la lista de diccionarios de predicci칩n.\n",
    "\n",
    "춰Muchos pasos, pero nada que no podamos manejar!\n",
    "\n",
    "Vamos a hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc13260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer \n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1. Cree una funci칩n para devolver una lista de diccionarios con muestra, etiqueta de verdad, predicci칩n, probabilidad de predicci칩n y tiempo de predicci칩n.\n",
    "def pred_and_store(paths: List[pathlib.Path], \n",
    "                   model: torch.nn.Module,\n",
    "                   transform: torchvision.transforms, \n",
    "                   class_names: List[str], \n",
    "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
    "    \n",
    "    # 2. Create an empty list to store prediction dictionaires\n",
    "    pred_list = []\n",
    "    \n",
    "    # 3. Loop through target paths\n",
    "    for path in tqdm(paths):\n",
    "        \n",
    "        # 4. Create empty dictionary to store prediction information for each sample\n",
    "        pred_dict = {}\n",
    "\n",
    "        # 5. Get the sample path and ground truth class name\n",
    "        pred_dict[\"image_path\"] = path\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "        \n",
    "        # 6. Start the prediction timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # 7. Open image path\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # 8. Transform the image, add batch dimension and put image on target device\n",
    "        transformed_image = transform(img).unsqueeze(0).to(device) \n",
    "        \n",
    "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # 10. Get prediction probability, predicition label and prediction class\n",
    "        with torch.inference_mode():\n",
    "            pred_logit = model(transformed_image) # perform inference on target sample \n",
    "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
    "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
    "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
    "\n",
    "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n",
    "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "            pred_dict[\"pred_class\"] = pred_class\n",
    "            \n",
    "            # 12. End the timer and calculate time per pred\n",
    "            end_time = timer()\n",
    "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
    "\n",
    "        # 13. Does the pred match the true label?\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "\n",
    "        # 14. Add the dictionary to the list of preds\n",
    "        pred_list.append(pred_dict)\n",
    "    \n",
    "    # 15. Return list of prediction dictionaries\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e0706",
   "metadata": {},
   "source": [
    "춰Ho, ho! \n",
    "\n",
    "춰Qu칠 funci칩n tan atractiva!\n",
    "\n",
    "Y sabes qu칠, dado que nuestro `pred_and_store()` es una funci칩n de utilidad bastante buena para hacer y almacenar predicciones, podr칤a almacenarse en [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke /pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) para su uso posterior. Podr칤a ser una extensi칩n que le gustar칤a probar; consulte [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) para obtener ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d53b8",
   "metadata": {},
   "source": [
    "### 5.2 Realizaci칩n y sincronizaci칩n de predicciones con EffNetB2\n",
    "\n",
    "춰Es hora de probar nuestra funci칩n `pred_and_store()`!\n",
    "\n",
    "Comencemos us치ndolo para hacer predicciones en todo el conjunto de datos de prueba con nuestro modelo EffNetB2, prestando atenci칩n a dos detalles:\n",
    "\n",
    "1. **Dispositivo**: codificaremos el par치metro \"dispositivo\" para usar \"cpu\" porque cuando implementemos nuestro modelo, no siempre tendremos acceso a un dispositivo \"cuda\" (GPU). .\n",
    "    * Hacer predicciones en la CPU tambi칠n ser치 un buen indicador de la velocidad de inferencia porque generalmente las predicciones en dispositivos con CPU son m치s lentas que las de los dispositivos con GPU.\n",
    "2. **Transformaciones** - Tambi칠n nos aseguraremos de establecer el par치metro `transform` en `effnetb2_transforms` para asegurarnos de que las im치genes se abran y transformen de la misma manera en la que se entren칩 nuestro modelo `effnetb2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d81492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga predicciones en todo el conjunto de datos de prueba con EffNetB2\n",
    "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                          model=effnetb2,\n",
    "                                          transform=effnetb2_transforms,\n",
    "                                          class_names=class_names,\n",
    "                                          device=\"cpu\") # make predictions on CPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257021bd",
   "metadata": {},
   "source": [
    "춰Lindo! 춰Mira esas predicciones volar!\n",
    "\n",
    "Inspeccionemos la primera pareja y veamos c칩mo se ven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccione los primeros 2 diccionarios de predicci칩n.\n",
    "effnetb2_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fab835",
   "metadata": {},
   "source": [
    "춰Guau!\n",
    "\n",
    "Parece que nuestra funci칩n `pred_and_store()` funcion칩 bien.\n",
    "\n",
    "Gracias a nuestra lista de estructura de datos de diccionarios, tenemos mucha informaci칩n 칰til que podemos inspeccionar m치s a fondo.\n",
    "\n",
    "Para hacerlo, convierta nuestra lista de diccionarios en un DataFrame de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta test_pred_dicts en un DataFrame\n",
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80be4f0",
   "metadata": {},
   "source": [
    "춰Hermoso!\n",
    "\n",
    "Mire con qu칠 facilidad esos diccionarios de predicci칩n se convierten en un formato estructurado sobre el que podemos realizar an치lisis.\n",
    "\n",
    "Como encontrar cu치ntas predicciones nuestro modelo EffNetB2 se equivoc칩..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar n칰mero de predicciones correctas\n",
    "effnetb2_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f0408",
   "metadata": {},
   "source": [
    "Cinco predicciones err칩neas de un total de 150, 춰nada mal!\n",
    "\n",
    "쯏 qu칠 tal el tiempo medio de predicci칩n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentre el tiempo promedio por predicci칩n\n",
    "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551ca3f",
   "metadata": {},
   "source": [
    "Mmmm, 쯖칩mo cumple ese tiempo promedio de predicci칩n con nuestros criterios de rendimiento de nuestro modelo en tiempo real (~30 FPS o 0,03 segundos por predicci칩n)?\n",
    "\n",
    "> **Nota:** Los tiempos de predicci칩n ser치n diferentes seg칰n los distintos tipos de hardware (por ejemplo, una CPU Intel i9 local frente a una CPU Google Colab). Cuanto mejor y m치s r치pido sea el hardware, generalmente, m치s r치pida ser치 la predicci칩n. Por ejemplo, en mi PC local de aprendizaje profundo con un chip Intel i9, mi tiempo promedio de predicci칩n con EffNetB2 es de alrededor de 0,031 segundos (un poco menos que el tiempo real). Sin embargo, en Google Colab (no estoy seguro de qu칠 hardware de CPU utiliza Colab, pero parece que podr칤a ser un [Intel(R) Xeon(R)](https://stackoverflow.com/questions/47805170/whats-the -hardware-spec-for-google-colaboratory)), mi tiempo promedio de predicci칩n con EffNetB2 es de aproximadamente 0,1396 segundos (3-4 veces m치s lento).\n",
    "\n",
    "Agreguemos nuestro tiempo promedio por predicci칩n de EffNetB2 a nuestro diccionario `effnetb2_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregue el tiempo de predicci칩n promedio de EffNetB2 al diccionario de estad칤sticas\n",
    "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d661174",
   "metadata": {},
   "source": [
    "### 5.3 Realizar y cronometrar predicciones con ViT \n",
    "\n",
    "Hemos hecho predicciones con nuestro modelo EffNetB2, ahora hagamos lo mismo con nuestro modelo ViT.\n",
    "\n",
    "Para hacerlo, podemos usar la funci칩n `pred_and_store()` que creamos anteriormente, excepto que esta vez pasaremos nuestro modelo `vit` as칤 como `vit_transforms`.\n",
    "\n",
    "Y mantendremos las predicciones en la CPU a trav칠s de `device=\"cpu\"` (una extensi칩n natural aqu칤 ser칤a probar los tiempos de predicci칩n en la CPU y en la GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga una lista de diccionarios de predicci칩n con el modelo de extracci칩n de funciones de ViT en im치genes de prueba\n",
    "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                     model=vit,\n",
    "                                     transform=vit_transforms,\n",
    "                                     class_names=class_names,\n",
    "                                     device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c65e9",
   "metadata": {},
   "source": [
    "춰Predicciones hechas!\n",
    "\n",
    "Ahora echemos un vistazo a la primera pareja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55893a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique las primeras predicciones de ViT en el conjunto de datos de prueba\n",
    "vit_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd650a52",
   "metadata": {},
   "source": [
    "춰Maravilloso!\n",
    "\n",
    "Y al igual que antes, dado que las predicciones de nuestro modelo ViT tienen la forma de una lista de diccionarios, podemos convertirlas f치cilmente en un DataFrame de pandas para una inspecci칩n m치s detallada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta vit_test_pred_dicts en un DataFrame\n",
    "import pandas as pd\n",
    "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
    "vit_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32273ad",
   "metadata": {},
   "source": [
    "쮺u치ntas predicciones acert칩 nuestro modelo ViT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb28af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuente el n칰mero de predicciones correctas.\n",
    "vit_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f039f3a",
   "metadata": {},
   "source": [
    "춰Guau!\n",
    "\n",
    "Nuestro modelo ViT funcion칩 un poco mejor que nuestro modelo EffNetB2 en t칠rminos de predicciones correctas, solo dos muestras incorrectas en todo el conjunto de datos de prueba.\n",
    "\n",
    "Como extensi칩n, es posible que desee visualizar las predicciones incorrectas del modelo ViT y ver si hay alguna raz칩n por la cual podr칤an haberse equivocado.\n",
    "\n",
    "쯈u칠 tal si calculamos cu치nto tiempo tard칩 el modelo ViT en realizar cada predicci칩n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bf678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el tiempo promedio por predicci칩n para el modelo ViT\n",
    "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c238746",
   "metadata": {},
   "source": [
    "Bueno, eso parece un poco m치s lento que el tiempo promedio por predicci칩n de nuestro modelo EffNetB2, pero 쯖칩mo se ve en t칠rminos de nuestro segundo criterio: velocidad?\n",
    "\n",
    "Por ahora, agreguemos el valor a nuestro diccionario `vit_stats` para que podamos compararlo con las estad칤sticas de nuestro modelo EffNetB2.\n",
    "\n",
    "> **Nota:** El tiempo promedio por valor de predicci칩n depender치 en gran medida del hardware en el que los realice. Por ejemplo, para el modelo ViT, mi tiempo promedio por predicci칩n (en la CPU) fue de 0,0693 a 0,0777 segundos en mi PC local de aprendizaje profundo con una CPU Intel i9. Mientras que en Google Colab, mi tiempo promedio por predicci칩n con el modelo ViT fue de 0,6766 a 0,7113 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be77977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregue el tiempo de predicci칩n promedio para el modelo ViT en la CPU\n",
    "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62926330",
   "metadata": {},
   "source": [
    "## 6. Comparaci칩n de resultados de modelos, tiempos de predicci칩n y tama침o\n",
    "\n",
    "Nuestros dos mejores modelos contendientes han sido capacitados y evaluados.\n",
    "\n",
    "Ahora pong치moslos cara a cara y comparemos sus diferentes estad칤sticas.\n",
    "\n",
    "Para hacerlo, convierta nuestros diccionarios `effnetb2_stats` y `vit_stats` en un DataFrame de pandas.\n",
    "\n",
    "Agregaremos una columna para ver los nombres de los modelos y convertiremos la precisi칩n de la prueba a un porcentaje completo en lugar de decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta los diccionarios de estad칤sticas en DataFrame\n",
    "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
    "\n",
    "# Agregar columna para nombres de modelos\n",
    "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
    "\n",
    "# Convertir precisi칩n a porcentajes\n",
    "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e6f8e",
   "metadata": {},
   "source": [
    "춰Maravilloso!\n",
    "\n",
    "Parece que nuestros modelos son bastante parecidos en t칠rminos de precisi칩n general de las pruebas, pero 쯖칩mo se ven en otros campos?\n",
    "\n",
    "Una forma de averiguarlo ser칤a dividir las estad칤sticas del modelo ViT por las estad칤sticas del modelo EffNetB2 para descubrir las diferentes proporciones entre los modelos.\n",
    "\n",
    "Creemos otro DataFrame para hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da434fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ViT con EffNetB2 seg칰n diferentes caracter칤sticas\n",
    "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
    "             columns=[\"ViT to EffNetB2 ratios\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14ef8e",
   "metadata": {},
   "source": [
    "Parece que nuestro modelo ViT supera al modelo EffNetB2 en todas las m칠tricas de rendimiento (p칠rdida de prueba, donde menor es mejor y precisi칩n de la prueba, donde mayor es mejor), pero a expensas de tener:\n",
    "* 11x+ el n칰mero de par치metros.\n",
    "* 11x+ el tama침o del modelo. \n",
    "* 2,5 veces m치s el tiempo de predicci칩n por imagen.\n",
    "\n",
    "쯌alen la pena estas compensaciones?\n",
    "\n",
    "Quiz치s si tuvi칠ramos una potencia inform치tica ilimitada, pero para nuestro caso de uso de implementar el modelo FoodVision Mini en un dispositivo m치s peque침o (por ejemplo, un tel칠fono m칩vil), probablemente comenzar칤amos con el modelo EffNetB2 para realizar predicciones m치s r치pidas con un rendimiento ligeramente reducido pero dram치ticamente m치s peque침o. tama침o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746fdcb",
   "metadata": {},
   "source": [
    "### 6.1 Visualizaci칩n del equilibrio entre velocidad y rendimiento \n",
    "\n",
    "Hemos visto que nuestro modelo ViT supera a nuestro modelo EffNetB2 en t칠rminos de m칠tricas de rendimiento, como p칠rdida de prueba y precisi칩n de prueba.\n",
    "\n",
    "Sin embargo, nuestro modelo EffNetB2 realiza predicciones m치s r치pido y tiene un tama침o de modelo mucho m치s peque침o.\n",
    "\n",
    "> **Nota:** El tiempo de rendimiento o inferencia tambi칠n suele denominarse \"latencia\".\n",
    "\n",
    "쯈u칠 tal si hacemos que este hecho sea visual?\n",
    "\n",
    "Podemos hacerlo creando un gr치fico con matplotlib:\n",
    "1. Cree un diagrama de dispersi칩n a partir del marco de datos de comparaci칩n para comparar los valores `time_per_pred_cpu` y `test_acc` de EffNetB2 y ViT.\n",
    "2. Agregue t칤tulos y etiquetas correspondientes a los datos y personalice el tama침o de fuente por motivos est칠ticos.\n",
    "3. Anote las muestras en el diagrama de dispersi칩n del paso 1 con sus etiquetas apropiadas (los nombres de los modelos).\n",
    "4. Cree una leyenda basada en los tama침os del modelo (`model_size (MB)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cree un gr치fico a partir del marco de datos de comparaci칩n de modelos.\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(data=df, \n",
    "                     x=\"time_per_pred_cpu\", \n",
    "                     y=\"test_acc\", \n",
    "                     c=[\"blue\", \"orange\"], # what colours to use?\n",
    "                     s=\"model_size (MB)\") # size the dots by the model sizes\n",
    "\n",
    "# 2. Agregue t칤tulos, etiquetas y personalice el tama침o de fuente por motivos est칠ticos.\n",
    "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
    "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
    "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "# 3. Anotar con nombres de modelos\n",
    "for index, row in df.iterrows():\n",
    "    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n",
    "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
    "                size=12)\n",
    "\n",
    "# 4. Crea una leyenda basada en los tama침os del modelo.\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
    "model_size_legend = ax.legend(handles, \n",
    "                              labels, \n",
    "                              loc=\"lower right\", \n",
    "                              title=\"Model size (MB)\",\n",
    "                              fontsize=12)\n",
    "\n",
    "# guarda la figura\n",
    "!mdkir images/\n",
    "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
    "\n",
    "# mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6dd1f6",
   "metadata": {},
   "source": [
    "춰Guau!\n",
    "\n",
    "La gr치fica realmente visualiza la **velocidad versus rendimiento**; en otras palabras, cuando tienes un modelo profundo m치s grande y de mejor rendimiento (como nuestro modelo ViT), *generalmente* lleva m치s tiempo realizar la inferencia (mayor latencia).\n",
    "\n",
    "Hay excepciones a la regla y constantemente se publican nuevas investigaciones para ayudar a que los modelos m치s grandes funcionen m치s r치pido.\n",
    "\n",
    "Y puede resultar tentador simplemente implementar el modelo de *mejor* rendimiento, pero tambi칠n es bueno tener en cuenta d칩nde funcionar치 el modelo.\n",
    "\n",
    "En nuestro caso, las diferencias entre los niveles de rendimiento de nuestro modelo (en la p칠rdida de prueba y la precisi칩n de la prueba) no son demasiado extremas.\n",
    "\n",
    "Pero como para empezar nos gustar칤a poner 칠nfasis en la velocidad, seguiremos implementando EffNetB2 ya que es m치s r치pido y ocupa mucho menos espacio.\n",
    "\n",
    "> **Nota:** Los tiempos de predicci칩n ser치n diferentes seg칰n los diferentes tipos de hardware (por ejemplo, Intel i9 frente a CPU de Google Colab frente a GPU), por lo que es importante pensar y probar d칩nde terminar치 su modelo. Hacer preguntas como \"쯗칩nde se ejecutar치 el modelo?\" o \"쯖u치l es el escenario ideal para ejecutar el modelo?\" y luego realizar experimentos para intentar proporcionar respuestas en el camino hacia la implementaci칩n es muy 칰til."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da51ad8",
   "metadata": {},
   "source": [
    "## 7. Dar vida a FoodVision Mini creando una demostraci칩n de Gradio\n",
    "\n",
    "Hemos decidido que nos gustar칤a implementar el modelo EffNetB2 (para empezar, esto siempre se puede cambiar m치s adelante).\n",
    "\n",
    "Entonces, 쯖칩mo podemos hacer eso?\n",
    "\n",
    "Hay varias formas de implementar un modelo de aprendizaje autom치tico, cada una con casos de uso espec칤ficos (como se analiz칩 anteriormente).\n",
    "\n",
    "Nos centraremos en la que quiz치s sea la forma m치s r치pida y ciertamente una de las m치s divertidas de implementar un modelo en Internet.\n",
    "\n",
    "Y eso es usando [Gradio](https://gradio.app/).\n",
    "\n",
    "쯈u칠 es Gradio?\n",
    "\n",
    "La p치gina de inicio lo describe maravillosamente: \n",
    "\n",
    "> Gradio es la forma m치s r치pida de hacer una demostraci칩n de su modelo de aprendizaje autom치tico con una interfaz web amigable para que cualquiera pueda usarlo, 춰en cualquier lugar!\n",
    "\n",
    "쯇or qu칠 crear una demostraci칩n de tus modelos?\n",
    "\n",
    "Porque las m칠tricas en el conjunto de prueba se ven bien, pero nunca se sabe realmente c칩mo se desempe침a su modelo hasta que lo usa en la naturaleza.\n",
    "\n",
    "춰As칤 que comencemos a implementar!\n",
    "\n",
    "Comenzaremos importando Gradio con el alias com칰n `gr` y, si no est치 presente, lo instalaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16950ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar/instalar Gradio\n",
    "try:\n",
    "    import gradio as gr\n",
    "except: \n",
    "    !pip -q install gradio\n",
    "    import gradio as gr\n",
    "    \n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca4692",
   "metadata": {},
   "source": [
    "춰Gradio listo!\n",
    "\n",
    "Convirtamos FoodVision Mini en una aplicaci칩n de demostraci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ecb68",
   "metadata": {},
   "source": [
    "### 7.1 Descripci칩n general de Gradio\n",
    "\n",
    "La premisa general de Gradio es muy similar a la que hemos ido repitiendo a lo largo del curso.\n",
    "\n",
    "쮺u치les son nuestras **entradas** y **salidas**?\n",
    "\n",
    "쯏 c칩mo deber칤amos llegar all칤?\n",
    "\n",
    "Bueno, eso es lo que hace nuestro modelo de aprendizaje autom치tico.\n",
    "\n",
    "```\n",
    "entradas -> modelo ML -> salidas\n",
    "```\n",
    "\n",
    "En nuestro caso, para FoodVision Mini, nuestras entradas son im치genes de comida, nuestro modelo ML es EffNetB2 y nuestras salidas son clases de comida (pizza, bistec o sushi).\n",
    "\n",
    "```\n",
    "im치genes de alimentos -> EffNetB2 -> salidas\n",
    "```\n",
    "\n",
    "Aunque los conceptos de entradas y salidas pueden vincularse a casi cualquier otro tipo de problema de ML.\n",
    "\n",
    "Sus entradas y salidas pueden ser cualquier combinaci칩n de lo siguiente:\n",
    "* Im치genes\n",
    "* Texto\n",
    "* Video\n",
    "* Datos tabulados\n",
    "*Audio\n",
    "* N칰meros\n",
    "* & m치s\n",
    "\n",
    "Y el modelo de ML que cree depender치 de sus entradas y salidas.\n",
    "\n",
    "Gradio emula este paradigma creando una interfaz ([`gradio.Interface()`](https://gradio.app/docs/#interface-header)) desde las entradas hasta las salidas.\n",
    "\n",
    "```\n",
    "gradio.Interface(fn, entradas, salidas)\n",
    "```\n",
    "\n",
    "Donde, \"fn\" es una funci칩n de Python para asignar las \"entradas\" a las \"salidas\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-gradio-workflow.png\" alt=\"flujo de trabajo de gradio de entradas que fluyen hacia alg칰n tipo de modelo o funci칩n y luego producir resultados\" width=900/>\n",
    "\n",
    "*Gradio proporciona una clase `Interfaz` muy 칰til para crear f치cilmente entradas -> modelo/funci칩n -> flujo de trabajo de salidas donde las entradas y salidas pueden ser casi cualquier cosa que desee. Por ejemplo, puede ingresar Tweets (texto) para ver si tratan sobre aprendizaje autom치tico o no o [ingrese un mensaje de texto para generar im치genes](https://huggingface.co/blog/stable_diffusion).*\n",
    "\n",
    "> **Nota:** Gradio tiene una gran cantidad de posibles opciones de \"entradas\" y \"salidas\" conocidas como \"Componentes\", desde im치genes hasta texto, n칰meros, audio, videos y m치s. Puede verlos todos en la [documentaci칩n de componentes de Gradio] (https://gradio.app/docs/#components)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee313d1",
   "metadata": {},
   "source": [
    "### 7.2 Creando una funci칩n para mapear nuestras entradas y salidas\n",
    "\n",
    "Para crear nuestra demostraci칩n de FoodVision Mini con Gradio, necesitaremos una funci칩n para asignar nuestras entradas a nuestras salidas.\n",
    "\n",
    "Anteriormente creamos una funci칩n llamada `pred_and_store()` para hacer predicciones con un modelo determinado en una lista de archivos de destino y almacenarlos en una lista de diccionarios.\n",
    "\n",
    "쯈u칠 tal si creamos una funci칩n similar pero esta vez centr치ndonos en hacer una predicci칩n en una sola imagen con nuestro modelo EffNetB2?\n",
    "\n",
    "M치s espec칤ficamente, queremos una funci칩n que tome una imagen como entrada, la preprocese (transforme), haga una predicci칩n con EffNetB2 y luego devuelva la predicci칩n (pred o etiqueta pred para abreviar), as칤 como la probabilidad de predicci칩n (pred prob).\n",
    "\n",
    "Y ya que estamos aqu칤, retrocedamos el tiempo que nos llev칩 hacerlo tambi칠n:\n",
    "\n",
    "```\n",
    "entrada: imagen -> transformar -> predecir con EffNetB2 -> salida: pred, pred prob, tiempo necesario\n",
    "```\n",
    "\n",
    "Este ser치 nuestro par치metro `fn` para nuestra interfaz Gradio.\n",
    "\n",
    "Primero, asegur칠monos de que nuestro modelo EffNetB2 est칠 en la CPU (ya que nos atenemos a las predicciones solo de CPU, sin embargo, puedes cambiar esto si tienes acceso a una GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponga EffNetB2 en la CPU\n",
    "effnetb2.to(\"cpu\") \n",
    "\n",
    "# Verifique el dispositivo\n",
    "next(iter(effnetb2.parameters())).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e1f3f",
   "metadata": {},
   "source": [
    "Y ahora creemos una funci칩n llamada `predict()` para replicar el flujo de trabajo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb99888",
   "metadata": {},
   "source": [
    "춰Hermoso! \n",
    "\n",
    "Ahora veamos nuestra funci칩n en acci칩n realizando una predicci칩n en una imagen aleatoria del conjunto de datos de prueba.\n",
    "\n",
    "Comenzaremos obteniendo una lista de todas las rutas de im치genes del directorio de prueba y luego seleccionaremos una al azar.\n",
    "\n",
    "Luego abriremos la imagen seleccionada al azar con [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
    "\n",
    "Finalmente, pasaremos la imagen a nuestra funci칩n `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Obtenga una lista de todas las rutas de archivos de im치genes de prueba\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "\n",
    "# Seleccione aleatoriamente una ruta de imagen de prueba\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "# Abra la imagen de destino\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
    "\n",
    "# Predecir sobre la imagen de destino e imprimir los resultados.\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
    "print(f\"Prediction time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db48bb9",
   "metadata": {},
   "source": [
    "춰Lindo!\n",
    "\n",
    "Al ejecutar la celda de arriba varias veces, podemos ver diferentes probabilidades de predicci칩n para cada etiqueta de nuestro modelo EffNetB2, as칤 como el tiempo que tom칩 cada predicci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d28a9c",
   "metadata": {},
   "source": [
    "### 7.3 Crear una lista de im치genes de ejemplo\n",
    "\n",
    "Nuestra funci칩n `predict()` nos permite ir desde entradas -> transformar -> modelo ML -> salidas.\n",
    "\n",
    "Que es exactamente lo que necesitamos para nuestra demostraci칩n de Graido.\n",
    "\n",
    "Pero antes de crear la demostraci칩n, creemos una cosa m치s: una lista de ejemplos.\n",
    "\n",
    "La clase [`Interface`](https://gradio.app/docs/#interface) de Gradio toma una lista de `ejemplos` como par치metro opcional (`gradio.Interface(examples=List[Any])`).\n",
    "\n",
    "Y el formato del par치metro \"ejemplos\" es una lista de listas.\n",
    "\n",
    "Entonces, creemos una lista de listas que contengan rutas de archivos aleatorias a nuestras im치genes de prueba.\n",
    "\n",
    "Tres ejemplos deber칤an ser suficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6081d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree una lista de entradas de ejemplo para nuestra demostraci칩n de Gradio\n",
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cef4a",
   "metadata": {},
   "source": [
    "춰Perfecto!\n",
    "\n",
    "Nuestra demostraci칩n de Gradio los mostrar치 como entradas de ejemplo para nuestra demostraci칩n para que las personas puedan probarlo y ver qu칠 hace sin cargar ninguno de sus propios datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccd91e",
   "metadata": {},
   "source": [
    "### 7.4 Construyendo una interfaz Gradio\n",
    "\n",
    "춰Es hora de juntar todo y darle vida a nuestra demostraci칩n de FoodVision Mini!\n",
    "\n",
    "Creemos una interfaz Gradio para replicar el flujo de trabajo:\n",
    "\n",
    "```\n",
    "entrada: imagen -> transformar -> predecir con EffNetB2 -> salida: pred, pred prob, tiempo necesario\n",
    "```\n",
    "\n",
    "Lo podemos hacer con la clase [`gradio.Interface()`](https://gradio.app/docs/#interface) con los siguientes par치metros:\n",
    "* `fn` - una funci칩n de Python para asignar `entradas` a `salidas`; en nuestro caso, usaremos nuestra funci칩n `predict()`.\n",
    "* `inputs`: la entrada a nuestra interfaz, como una imagen usando [`gradio.Image()`](https://gradio.app/docs/#image) o `\"image\"`. \n",
    "* `outputs` - la salida de nuestra interfaz una vez que las `inputs` han pasado por `fn`, como una etiqueta usando [`gradio.Label()`](https://gradio.app/docs/#label ) (para las etiquetas predichas de nuestro modelo) o n칰mero usando [`gradio.Number()`](https://gradio.app/docs/#number) (para el tiempo de predicci칩n de nuestro modelo).\n",
    "    * **Nota:** Gradio viene con muchas opciones integradas de `entradas` y `salidas` conocidas como [\"Componentes\"](https://gradio.app/docs/#components).\n",
    "* `ejemplos`: una lista de ejemplos para mostrar en la demostraci칩n.\n",
    "* `title` - un t칤tulo de cadena de la demostraci칩n.\n",
    "* `descripci칩n`: una cadena de descripci칩n de la demostraci칩n.\n",
    "* `art칤culo`: una nota de referencia al final de la demostraci칩n.\n",
    "\n",
    "Una vez que hayamos creado nuestra instancia de demostraci칩n de `gr.Interface()`, podemos darle vida usando [`gradio.Interface().launch()`](https://gradio.app/docs/#launch -header) o el comando `demo.launch()`. \n",
    "\n",
    "춰F치cil!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c456b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Crear cadenas de t칤tulo, descripci칩n y art칤culo.\n",
    "title = \"FoodVision Mini 游꼣游볼游꼮\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Crea la demostraci칩n de Gradio\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# 춰Lanza la demostraci칩n!\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da65725",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-gradio-running-in-google-colab-and-in-browser.gif\" alt=\" Demostraci칩n de Gradio ejecut치ndose en Google Colab y en la web\" width=750/>\n",
    "\n",
    "*Demo de FoodVision Mini Gradio ejecut치ndose en Google Colab y en el navegador (el enlace cuando se ejecuta desde Google Colab solo dura 72 horas). Puedes ver la [demo permanente en vivo en Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini).*\n",
    "\n",
    "춰춰춰Guau!!! 춰춰춰Qu칠 demostraci칩n tan 칠pica!!!\n",
    "\n",
    "FoodVision Mini ha cobrado vida oficialmente en una interfaz que alguien podr칤a usar y probar.\n",
    "\n",
    "Si configura el par치metro `share=True` en el m칠todo `launch()`, Gradio tambi칠n le proporciona un enlace para compartir como `https://123XYZ.gradio.app` (este enlace es solo un ejemplo y probablemente est칠 vencido). ) que es v치lido por 72 horas.\n",
    "\n",
    "El enlace proporciona un proxy a la interfaz de Gradio que inici칩.\n",
    "\n",
    "Para un alojamiento m치s permanente, puede cargar su aplicaci칩n Gradio en [Hugging Face Spaces](https://huggingface.co/spaces) o en cualquier lugar que ejecute c칩digo Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b8927",
   "metadata": {},
   "source": [
    "## 8. Convertir nuestra demostraci칩n FoodVision Mini Gradio en una aplicaci칩n implementable\n",
    "\n",
    "Hemos visto nuestro modelo FoodVision Mini cobrar vida a trav칠s de una demostraci칩n de Gradio.\n",
    "\n",
    "Pero 쯫 si quisi칠ramos compartirlo con nuestros amigos?\n",
    "\n",
    "Bueno, podr칤amos usar el enlace de Gradio proporcionado, sin embargo, el enlace compartido solo dura 72 horas.\n",
    "\n",
    "Para que nuestra demostraci칩n de FoodVision Mini sea m치s permanente, podemos empaquetarla en una aplicaci칩n y cargarla en [Hugging Face Spaces](https://huggingface.co/spaces/launch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ff935",
   "metadata": {},
   "source": [
    "### 8.1 쯈u칠 es abrazar los espacios faciales?\n",
    "\n",
    "Hugging Face Spaces es un recurso que le permite alojar y compartir aplicaciones de aprendizaje autom치tico.\n",
    "\n",
    "Crear una demostraci칩n es una de las mejores formas de mostrar y probar lo que ha hecho.\n",
    "\n",
    "Y Spaces te permite hacer precisamente eso.\n",
    "\n",
    "Puedes pensar en Hugging Face como el GitHub del aprendizaje autom치tico.\n",
    "\n",
    "Si tener un buen portafolio de GitHub muestra tus habilidades de codificaci칩n, tener un buen portafolio de Hugging Face puede mostrar tus habilidades de aprendizaje autom치tico.\n",
    "\n",
    "> **Nota:** Hay muchos otros lugares donde podr칤amos cargar y alojar nuestra aplicaci칩n Gradio, como Google Cloud, AWS (Amazon Web Services) u otros proveedores de nube; sin embargo, usaremos Hugging Face Spaces debido a la facilidad de uso y la amplia adopci칩n por parte de la comunidad de aprendizaje autom치tico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a3777",
   "metadata": {},
   "source": [
    "### 8.2 Estructura de la aplicaci칩n Gradio implementada\n",
    "\n",
    "Para cargar nuestra aplicaci칩n de demostraci칩n Gradio, queremos colocar todo lo relacionado con ella en un solo directorio.\n",
    "\n",
    "Por ejemplo, nuestra demostraci칩n podr칤a estar en la ruta `demos/foodvision_mini/` con la estructura de archivos:\n",
    "\n",
    "```\n",
    "poblaci칩n/\n",
    "較덕較 comidavision_mini/\n",
    "    較럭較 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
    "    較럭較 aplicaci칩n.py\n",
    "    較럭較 ejemplos/\n",
    "    較 較럭較 ejemplo_1.jpg\n",
    "    較 較럭較 ejemplo_2.jpg\n",
    "    較 較덕較 ejemplo_3.jpg\n",
    "    較럭較 modelo.py\n",
    "    較덕較 requisitos.txt\n",
    "```\n",
    "\n",
    "D칩nde:\n",
    "* `09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth` es nuestro archivo modelo PyTorch entrenado.\n",
    "* `app.py` contiene nuestra aplicaci칩n Gradio (similar al c칩digo que inici칩 la aplicaci칩n).\n",
    "    * **Nota:** `app.py` es el nombre de archivo predeterminado utilizado para Hugging Face Spaces. Si implementas tu aplicaci칩n all칤, Spaces buscar치 de manera predeterminada un archivo llamado `app.py` para ejecutar. Esto se puede cambiar en la configuraci칩n.\n",
    "* `examples/` contiene im치genes de ejemplo para usar con nuestra aplicaci칩n Gradio.\n",
    "* `model.py` contiene la definici칩n del modelo, as칤 como cualquier transformaci칩n asociada con el modelo.\n",
    "* `requirements.txt` contiene las dependencias para ejecutar nuestra aplicaci칩n, como `torch`, `torchvision` y `gradio`.\n",
    "\n",
    "쯇or qu칠 de esta manera?\n",
    "\n",
    "Porque es uno de los dise침os m치s simples con los que podr칤amos empezar. \n",
    "\n",
    "Nuestro enfoque es: *춰experimentar, experimentar, experimentar!* \n",
    "\n",
    "Cuanto m치s r치pido podamos realizar experimentos m치s peque침os, mejores ser치n los m치s grandes.\n",
    "\n",
    "Vamos a trabajar para recrear la estructura anterior, pero puedes ver una aplicaci칩n de demostraci칩n en vivo ejecut치ndose en Hugging Face Spaces, as칤 como la estructura de archivos:\n",
    "* [Demostraci칩n en vivo de Gradio de FoodVision Mini 游꼣游볼游꼮](https://huggingface.co/spaces/mrdbourke/foodvision_mini).\n",
    "* [Estructura de archivos FoodVision Mini en Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a284c2",
   "metadata": {},
   "source": [
    "### 8.3 Creando una carpeta `demos` para almacenar los archivos de nuestra aplicaci칩n FoodVision Mini\n",
    "\n",
    "Para comenzar, primero creemos un directorio `demos/` para almacenar todos los archivos de nuestra aplicaci칩n FoodVision Mini.\n",
    "\n",
    "Podemos hacerlo con [`pathlib.Path(\"path_to_dir\")`](https://docs.python.org/3/library/pathlib.html#basic-use) de Python para establecer la ruta del directorio y [`pathlib. Path(\"path_to_dir\").mkdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir) para crearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d63c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Crear una mini ruta de demostraci칩n de FoodVision\n",
    "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
    "\n",
    "# Elimine los archivos que ya puedan existir all칤 y cree un nuevo directorio\n",
    "if foodvision_mini_demo_path.exists():\n",
    "    shutil.rmtree(foodvision_mini_demo_path)\n",
    "    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n",
    "                                    exist_ok=True) # create it even if it already exists?\n",
    "else:\n",
    "    # If the file doesn't exist, create it anyway\n",
    "    foodvision_mini_demo_path.mkdir(parents=True, \n",
    "                                    exist_ok=True)\n",
    "    \n",
    "# Comprueba lo que hay en la carpeta.\n",
    "!ls demos/foodvision_mini/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c75470",
   "metadata": {},
   "source": [
    "### 8.4 Crear una carpeta de im치genes de ejemplo para usar con nuestra demostraci칩n de FoodVision Mini\n",
    "\n",
    "Ahora que tenemos un directorio para almacenar nuestros archivos de demostraci칩n de FoodVision Mini, agreguemos algunos ejemplos.\n",
    "\n",
    "Tres im치genes de ejemplo del conjunto de datos de prueba deber칤an ser suficientes.\n",
    "\n",
    "Para hacerlo haremos:\n",
    "1. Cree un directorio `examples/` dentro del directorio `demos/foodvision_mini`.\n",
    "2. Elija tres im치genes aleatorias del conjunto de datos de prueba y recopile sus rutas de archivo en una lista.\n",
    "3. Copie las tres im치genes aleatorias del conjunto de datos de prueba al directorio `demos/foodvision_mini/examples/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Cree un directorio de ejemplos\n",
    "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Recopile tres rutas de im치genes de conjuntos de datos de prueba aleatorias\n",
    "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
    "\n",
    "# 3. Copie las tres im치genes aleatorias al directorio de ejemplos.\n",
    "for example in foodvision_mini_examples:\n",
    "    destination = foodvision_mini_examples_path / example.name\n",
    "    print(f\"[INFO] Copying {example} to {destination}\")\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f0701",
   "metadata": {},
   "source": [
    "Ahora, para verificar que nuestros ejemplos est칠n presentes, enumeremos el contenido de nuestro directorio `demos/foodvision_mini/examples/` con [`os.listdir()`](https://docs.python.org/3/library/os. html#os.listdir) y luego formatee las rutas de archivo en una lista de listas (para que sea compatible con el par치metro `example` [`gradio.Interface()`](https://gradio.app/docs/#interface) de Gradio) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Obtenga rutas de archivos de ejemplo en una lista de listas\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f8473",
   "metadata": {},
   "source": [
    "### 8.5 Mover nuestro modelo EffNetB2 entrenado a nuestro directorio de demostraci칩n FoodVision Mini\n",
    "\n",
    "Anteriormente guardamos nuestro modelo de extractor de funciones FoodVision Mini EffNetB2 en `models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
    "\n",
    "Y en lugar de duplicar los archivos de modelo guardados, muevamos nuestro modelo a nuestro directorio `demos/foodvision_mini`.\n",
    "\n",
    "Podemos hacerlo usando el m칠todo [`shutil.move()`](https://docs.python.org/3/library/shutil.html#shutil.move) de Python y pasando `src` (la ruta fuente de el archivo de destino) y `dst` (la ruta de destino del archivo de destino al que se va a mover)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Crear una ruta de origen para nuestro modelo de destino.\n",
    "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
    "\n",
    "# Crear una ruta de destino para nuestro modelo objetivo.\n",
    "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "# Intenta mover el archivo\n",
    "try:\n",
    "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
    "    \n",
    "    # Move the model\n",
    "    shutil.move(src=effnetb2_foodvision_mini_model_path, \n",
    "                dst=effnetb2_foodvision_mini_model_destination)\n",
    "    \n",
    "    print(f\"[INFO] Model move complete.\")\n",
    "\n",
    "# Si el modelo ya ha sido movido, verifique si existe.\n",
    "except:\n",
    "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
    "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a414e8",
   "metadata": {},
   "source": [
    "### 8.6 Convirtiendo nuestro modelo EffNetB2 en un script Python (`model.py`)\n",
    "\n",
    "El `state_dict` de nuestro modelo actual se guarda en `demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
    "\n",
    "Para cargarlo podemos usar `model.load_state_dict()` junto con `torch.load()`.\n",
    "\n",
    "> **Nota:** Para obtener una actualizaci칩n sobre c칩mo guardar y cargar un modelo (o el `state_dict` de un modelo en PyTorch, consulte [01. Fundamentos del flujo de trabajo de PyTorch, secci칩n 5: Guardar y cargar un modelo de PyTorch](https://www. learnpytorch.io/01_pytorch_workflow/#5-served-and-loading-a-pytorch-model) o consulte la receta de PyTorch para [쯈u칠 es un `state_dict` en PyTorch?](https://pytorch.org/tutorials/recipes /recetas/what_is_state_dict.html)\n",
    "\n",
    "Pero antes de que podamos hacer esto, primero necesitamos una forma de crear una instancia de un \"modelo\".\n",
    "\n",
    "Para hacer esto de forma modular, crearemos un script llamado `model.py` que contiene nuestra funci칩n `create_effnetb2_model()` que creamos en [secci칩n 3.1: *Creaci칩n de una funci칩n para crear un extractor de funciones EffNetB2*](https: //www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor).\n",
    "\n",
    "De esa manera podemos importar la funci칩n en *otro* script (ver `app.py` a continuaci칩n) y luego usarla para crear nuestra instancia de `modelo` EffNetB2, as칤 como obtener sus transformaciones apropiadas.\n",
    "\n",
    "Al igual que en [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/), usaremos el comando m치gico `%%writefile path/to/file` para convertir una celda de c칩digo en un archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd53a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b6267",
   "metadata": {},
   "source": [
    "### 8.7 Convirtiendo nuestra aplicaci칩n FoodVision Mini Gradio en un script Python (`app.py`)\n",
    "\n",
    "Ahora tenemos un script `model.py` as칤 como una ruta a un modelo guardado `state_dict` que podemos cargar.\n",
    "\n",
    "Es hora de construir `app.py`.\n",
    "\n",
    "Lo llamamos `app.py` porque, de forma predeterminada, cuando creas un espacio HuggingFace, busca un archivo llamado `app.py` para ejecutarlo y alojarlo (aunque puedes cambiar esto en la configuraci칩n).\n",
    "\n",
    "Nuestro script `app.py` juntar치 todas las piezas del rompecabezas para crear nuestra demostraci칩n de Gradio y tendr치 cuatro partes principales: \n",
    "\n",
    "1. **Configuraci칩n de importaciones y nombres de clases** - Aqu칤 importaremos las diversas dependencias para nuestra demostraci칩n, incluida la funci칩n `create_effnetb2_model()` de `model.py`, as칤 como tambi칠n configuraremos los diferentes nombres de clases para nuestra aplicaci칩n FoodVision Mini. . \n",
    "2. **Preparaci칩n de modelos y transformaciones**: aqu칤 crearemos una instancia de modelo EffNetB2 junto con las transformaciones que la acompa침an y luego cargaremos los pesos/`state_dict` del modelo guardado. Cuando cargamos el modelo, tambi칠n configuraremos `map_location=torch.device(\"cpu\")` en [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load .html) para que nuestro modelo se cargue en la CPU independientemente del dispositivo en el que se entren칩 (hacemos esto porque no necesariamente tendremos una GPU cuando implementemos y obtendremos un error si nuestro modelo est치 entrenado en GPU pero intente implementarlo en la CPU sin decirlo expl칤citamente).\n",
    "3. **Funci칩n de predicci칩n** - `gradio.Interface()` de Gradio toma un par치metro `fn` para asignar entradas a salidas, nuestra funci칩n `predict()` ser치 la misma que definimos anteriormente en la [secci칩n 7.2 : *Creando una funci칩n para mapear nuestras entradas y salidas*](https://www.learnpytorch.io/09_pytorch_model_deployment/#72-creating-a-function-to-map-our-inputs-and-outputs), lo har치 tome una imagen y luego use las transformaciones cargadas para preprocesarla antes de usar el modelo cargado para hacer una predicci칩n sobre ella.\n",
    "    * **Nota:** Tendremos que crear la lista de ejemplo sobre la marcha mediante el par치metro `examples`. Podemos hacerlo creando una lista de archivos dentro del directorio `examples/` con: `[[\"examples/\" + example] por ejemplo en os.listdir(\"examples\")]`.\n",
    "4. **Aplicaci칩n Gradio** - Aqu칤 es donde vivir치 la l칩gica principal de nuestra demostraci칩n, crearemos una instancia `gradio.Interface()` llamada `demo` para juntar nuestras entradas, funci칩n `predict()` y salidas. 춰Y terminaremos el script llamando a `demo.launch()` para iniciar nuestra demostraci칩n de FoodVision Mini!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "# ## 1. Configuraci칩n de importaciones y nombres de clases ###\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Configurar nombres de clases\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "# ## 2. Preparaci칩n del modelo y transforma ###\n",
    "\n",
    "# Crear modelo EffNetB2\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=3, # len(class_names) would also work\n",
    ")\n",
    "\n",
    "# Cargar pesos guardados\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "# ## 3. Funci칩n de predicci칩n ###\n",
    "\n",
    "# Crear funci칩n de predicci칩n\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# ## 4. Aplicaci칩n Gradio ###\n",
    "\n",
    "# Crear cadenas de t칤tulo, descripci칩n y art칤culo.\n",
    "title = \"FoodVision Mini 游꼣游볼游꼮\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Crear una lista de ejemplos desde el directorio \"examples/\"\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Crea la demostraci칩n de Gradio\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    # Create examples list from \"examples/\" directory\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# 춰Lanza la demostraci칩n!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79c5b9",
   "metadata": {},
   "source": [
    "### 8.8 Creaci칩n de un archivo de requisitos para FoodVision Mini (`requirements.txt`)\n",
    "\n",
    "El 칰ltimo archivo que debemos crear para nuestra aplicaci칩n FoodVision Mini es un [archivo `requirements.txt`] (https://learnpython.com/blog/python-requirements-file/).\n",
    "\n",
    "Este ser치 un archivo de texto que contendr치 todas las dependencias necesarias para nuestra demostraci칩n.\n",
    "\n",
    "Cuando implementemos nuestra aplicaci칩n de demostraci칩n en Hugging Face Spaces, buscar치 en este archivo e instalar치 las dependencias que definimos para que nuestra aplicaci칩n pueda ejecutarse.\n",
    "\n",
    "춰La buena noticia es que s칩lo hay tres!\n",
    "\n",
    "1. `antorcha==1.12.0`\n",
    "2. `torchvision==0.13.0`\n",
    "3. `gradio==3.1.4`\n",
    "\n",
    "\"`==1.12.0`\" indica el n칰mero de versi칩n a instalar.\n",
    "\n",
    "Definir el n칰mero de versi칩n no es 100% obligatorio, pero lo haremos por ahora, de modo que si se producen actualizaciones importantes en futuras versiones, nuestra aplicaci칩n a칰n se ejecuta (PD: si encuentra alg칰n error, no dude en publicarlo en el curso [Problemas de GitHub](https: //github.com/mrdbourke/pytorch-deep-learning/issues))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9557950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/requirements.txt\n",
    "torch==1.12.0\n",
    "torchvision==0.13.0\n",
    "gradio==3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec66e0",
   "metadata": {},
   "source": [
    "춰Lindo!\n",
    "\n",
    "춰Tenemos oficialmente todos los archivos que necesitamos para implementar nuestra demostraci칩n de FoodVision Mini!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2223",
   "metadata": {},
   "source": [
    "## 9. Implementaci칩n de nuestra aplicaci칩n FoodVision Mini en HuggingFace Spaces\n",
    "\n",
    "Tenemos un archivo que contiene nuestra demostraci칩n de FoodVision Mini. Ahora, 쯖칩mo hacemos para que se ejecute en Hugging Face Spaces?\n",
    "\n",
    "Hay dos opciones principales para cargar en un Hugging Face Space (tambi칠n llamado [Repositorio de Hugging Face](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories), similares a un repositorio de git): \n",
    "1. [Carga a trav칠s de la interfaz web de Hugging Face (m치s f치cil)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui).\n",
    "2. [Carga a trav칠s de la l칤nea de comando o terminal] (https://huggingface.co/docs/hub/repositories-getting-started#terminal).\n",
    "    * **Bonificaci칩n:** Tambi칠n puedes usar la [biblioteca `huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) para interactuar con Hugging Face, esta ser칤a una buena extensi칩n de las dos opciones anteriores. .\n",
    "\n",
    "No dude en leer la documentaci칩n sobre ambas opciones, pero optaremos por la opci칩n dos.\n",
    "\n",
    "> **Nota:** Para alojar cualquier cosa en Hugging Face, deber치s [registrarte para obtener una cuenta gratuita de Hugging Face](https://huggingface.co/join)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155f6d8",
   "metadata": {},
   "source": [
    "### 9.1 Descarga de los archivos de nuestra aplicaci칩n FoodVision Mini\n",
    "\n",
    "Veamos los archivos de demostraci칩n que tenemos dentro de `demos/foodvision_mini`.\n",
    "\n",
    "Para hacerlo, podemos usar el comando `!ls` seguido de la ruta del archivo de destino.\n",
    "\n",
    "`ls` significa \"lista\" y `!` significa que queremos ejecutar el comando en el nivel de shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b068e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls demos/foodvision_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd4675",
   "metadata": {},
   "source": [
    "춰Estos son todos los archivos que hemos creado!\n",
    "\n",
    "Para comenzar a cargar nuestros archivos en Hugging Face, descargu칠moslos ahora desde Google Colab (o dondequiera que est칠 ejecutando este cuaderno).\n",
    "\n",
    "Para hacerlo, primero comprimiremos los archivos en una 칰nica carpeta zip mediante el comando: \n",
    "\n",
    "```\n",
    "zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "```\n",
    "\n",
    "D칩nde: \n",
    "* `zip` significa \"zip\", como en \"comprima los archivos en el siguiente directorio\". \n",
    "* `-r` significa \"recursivo\", como en \"revisar todos los archivos en el directorio de destino\".\n",
    "* `../foodvision_mini.zip` es el directorio de destino donde nos gustar칤a comprimir nuestros archivos.\n",
    "* `*` significa \"todos los archivos en el directorio actual\".\n",
    "* `-x` significa \"excluir estos archivos\". \n",
    "\n",
    "Podemos descargar nuestro archivo zip de Google Colab usando [`google.colab.files.download(\"demos/foodvision_mini.zip\")`](https://colab.research.google.com/notebooks/io.ipynb) ( Pondremos esto dentro de un bloque `try` y `except` en caso de que no estemos ejecutando el c칩digo dentro de Google Colab y, de ser as칤, imprimiremos un mensaje que indicar치 que descarguemos los archivos manualmente).\n",
    "\n",
    "춰Prob칠moslo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74040c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambie y luego comprima la carpeta foodvision_mini pero excluya ciertos archivos\n",
    "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "\n",
    "# Descargue la aplicaci칩n FoodVision Mini comprimida (si se ejecuta en Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"demos/foodvision_mini.zip\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54495e0",
   "metadata": {},
   "source": [
    "춰Guau!\n",
    "\n",
    "Parece que nuestro comando `zip` fue exitoso.\n",
    "\n",
    "Si est치 ejecutando este cuaderno en Google Colab, deber칤a ver que un archivo comienza a descargarse en su navegador.\n",
    "\n",
    "De lo contrario, puede ver la carpeta `foodvision_mini.zip` (y m치s) en el [curso GitHub en el directorio `demos/`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/ poblaci칩n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63f0e4",
   "metadata": {},
   "source": [
    "### 9.2 Ejecuci칩n de nuestra demostraci칩n FoodVision Mini localmente\n",
    "\n",
    "Si descarga el archivo `foodvision_mini.zip`, puede probarlo localmente de la siguiente manera:\n",
    "1. Descomprimiendo el archivo.\n",
    "2. Abrir la terminal o una l칤nea de comando.\n",
    "3. Cambiar al directorio `foodvision_mini` (`cd foodvision_mini`).\n",
    "4. Crear un entorno (`python3 -m venv env`).\n",
    "5. Activar el entorno (`source env/bin/activate`).\n",
    "5. Instalar los requisitos (`pip install -r requisitos.txt`, \"`-r`\" es para recursivo).\n",
    "    * **Nota:** Este paso puede tardar entre 5 y 10 minutos dependiendo de tu conexi칩n a Internet. Y si enfrenta errores, es posible que primero necesite actualizar `pip`: `pip install --upgrade pip`.\n",
    "6. Ejecute la aplicaci칩n (`python3 app.py`).\n",
    "\n",
    "Esto deber칤a dar como resultado una demostraci칩n de Gradio como la que creamos anteriormente ejecut치ndose localmente en su m치quina en una URL como `http://127.0.0.1:7860/`.\n",
    "\n",
    "> **Nota:** Si ejecuta la aplicaci칩n localmente y observa que aparece un directorio `marcado/`, contiene muestras que han sido \"marcadas\". \n",
    ">\n",
    "> Por ejemplo, si alguien prueba la demostraci칩n y el modelo produce un resultado incorrecto, la muestra se puede \"marcar\" y revisar para m치s adelante.\n",
    "> \n",
    "> Para obtener m치s informaci칩n sobre c칩mo marcar en Gradio, consulte la [documentaci칩n sobre c칩mo marcar](https://gradio.app/docs/#flagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177828a4",
   "metadata": {},
   "source": [
    "### 9.3 Subir a Hugging Face\n",
    "\n",
    "Hemos verificado que nuestra aplicaci칩n FoodVision Mini funciona localmente; sin embargo, lo divertido de crear una demostraci칩n de aprendizaje autom치tico es mostr치rsela a otras personas y permitirles usarla.\n",
    "\n",
    "Para hacerlo, cargaremos nuestra demostraci칩n de FoodVision Mini en Hugging Face. \n",
    "\n",
    "> **Nota:** La siguiente serie de pasos utiliza un flujo de trabajo Git (un sistema de seguimiento de archivos). Para obtener m치s informaci칩n sobre c칩mo funciona Git, recomiendo consultar el [tutorial de Git y GitHub para principiantes](https://youtu.be/RGOj5yH7evk) en freeCodeCamp.\n",
    "\n",
    "1. [Reg칤strese](https://huggingface.co/join) para obtener una cuenta de Hugging Face. \n",
    "2. Inicie un nuevo Hugging Face Space yendo a su perfil y luego [haciendo clic en \"Nuevo espacio\"](https://huggingface.co/new-space).\n",
    "    * **Nota:** Un espacio en Hugging Face tambi칠n se conoce como \"repositorio de c칩digos\" (un lugar para almacenar su c칩digo/archivos) o \"repositorio\" para abreviar.\n",
    "3. Dale un nombre al Espacio, por ejemplo, el m칤o se llama `mrdbourke/foodvision_mini`, puedes verlo aqu칤: https://huggingface.co/spaces/mrdbourke/foodvision_mini\n",
    "4. Seleccione una licencia (yo us칠 [MIT](https://opensource.org/licenses/MIT)).\n",
    "5. Seleccione Gradio como Space SDK (kit de desarrollo de software). \n",
    "   * **Nota:** Puedes usar otras opciones como Streamlit, pero como nuestra aplicaci칩n est치 construida con Gradio, nos quedaremos con eso.\n",
    "6. Elija si su Espacio es p칰blico o privado (seleccion칠 p칰blico porque me gustar칤a que mi Espacio est칠 disponible para otros).\n",
    "7. Haga clic en \"Crear espacio\".\n",
    "8. Clone el repositorio localmente ejecutando algo como: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` en la terminal o en el s칤mbolo del sistema.\n",
    "    * **Nota:** Tambi칠n puedes agregar archivos carg치ndolos en la pesta침a \"Archivos y versiones\".\n",
    "9. Copie/mueva el contenido de la carpeta `foodvision_mini` descargada a la carpeta del repositorio clonado.\n",
    "10. Para cargar y rastrear archivos m치s grandes (por ejemplo, archivos de m치s de 10 MB o, en nuestro caso, nuestro archivo modelo PyTorch), necesitar치 [instalar Git LFS](https://git-lfs.github.com/) (que significa para \"almacenamiento de archivos grandes de git\").\n",
    "11. Despu칠s de haber instalado Git LFS, puedes activarlo ejecutando `git lfs install`.\n",
    "12. En el directorio `foodvision_mini`, realice un seguimiento de los archivos de m치s de 10 MB con Git LFS con `git lfs track \"*.file_extension\"`.\n",
    "    * Realice un seguimiento del archivo de modelo EffNetB2 PyTorch con `git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"`.\n",
    "13. Seguimiento de `.gitattributes` (creado autom치ticamente al clonar desde HuggingFace; este archivo ayudar치 a garantizar que nuestros archivos m치s grandes sean rastreados con Git LFS). Puede ver un archivo `.gitattributes` de ejemplo en [FoodVision Mini Hugging Face Space] (https://huggingface.co/spaces/mrdbourke/foodvision_mini/blob/main/.gitattributes).\n",
    "    * `git agregar .gitattributes`\n",
    "14. Agregue el resto de los archivos de la aplicaci칩n `foodvision_mini` y conf칤rmelos con: \n",
    "    * `git agregar *`\n",
    "    * `git commit -m \"primer compromiso\"`\n",
    "15. Env칤e (cargue) los archivos a Hugging Face:\n",
    "    *`git push`\n",
    "16. Espere de 3 a 5 minutos hasta que se complete la compilaci칩n (las compilaciones futuras ser치n m치s r치pidas) y su aplicaci칩n estar치 activa.\n",
    "\n",
    "Si todo funcion칩, deber칤a ver un ejemplo en vivo de nuestra demostraci칩n de FoodVision Mini Gradio como este: https://huggingface.co/spaces/mrdbourke/foodvision_mini \n",
    "\n",
    "E incluso podemos insertar nuestra demostraci칩n de FoodVision Mini Gradio en nuestra computadora port치til como un [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) con [`IPython.display.IFrame`](https:/ /ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) y un enlace a nuestro espacio en el formato `https://hf.space/embed/[YOUR_USERNAME] /[TU_NOMBRE_ESPACIO]/+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c08623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython es una biblioteca para ayudar a que Python sea interactivo\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Incrustar la demostraci칩n de FoodVision Mini Gradio\n",
    "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5335d8",
   "metadata": {},
   "source": [
    "## 10. Creando FoodVision a lo grande\n",
    "\n",
    "Hemos pasado las 칰ltimas secciones y cap칤tulos trabajando para darle vida a FoodVision Mini.\n",
    "\n",
    "Y ahora que lo hemos visto funcionar en una demostraci칩n en vivo, 쯤u칠 tal si damos un paso m치s?\n",
    "\n",
    "쮺칩mo?\n",
    "\n",
    "춰Visi칩n alimentaria a lo grande!\n",
    "\n",
    "Dado que FoodVision Mini est치 entrenado con im치genes de pizza, bistec y sushi del [conjunto de datos Food101] (https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) (101 clases de alimentos x 1000 im치genes cada una ), 쯤u칠 tal si hacemos FoodVision Big entrenando un modelo en las 101 clases?\n",
    "\n",
    "춰Pasaremos de tres clases a 101!\n",
    "\n",
    "춰Desde pizza, bistec, sushi hasta pizza, bistec, sushi, hot dog, tarta de manzana, pastel de zanahoria, pastel de chocolate, fuegos franceses, pan de ajo, ramen, nachos, tacos y m치s!\n",
    "\n",
    "쮺칩mo?\n",
    "\n",
    "Bueno, tenemos todos los pasos implementados, todo lo que tenemos que hacer es modificar ligeramente nuestro modelo EffNetB2 y preparar un conjunto de datos diferente.\n",
    "\n",
    "Para finalizar Milestone Project 3, recreemos una demostraci칩n de Gradio similar a FoodVision Mini (tres clases) pero para FoodVision Big (101 clases).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-foodvision-mini-to-foodvision-big.png\" alt=\"foodvision mini modele en tres clases: pizza, bistec, sushi y foodvision big en las 101 clases del conjunto de datos food101\" width=900/>\n",
    "\n",
    "*FoodVision Mini funciona con tres clases de alimentos: pizza, bistec y sushi. Y FoodVision Big va un paso m치s all치 para trabajar en 101 clases de alimentos: todas las [clases en el conjunto de datos Food101](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names. TXT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a8045",
   "metadata": {},
   "source": [
    "### 10.1 Creando un modelo y transformaciones para FoodVision Big\n",
    "\n",
    "Al crear FoodVision Mini vimos que el modelo EffNetB2 era un buen equilibrio entre velocidad y rendimiento (funcion칩 bien a alta velocidad).\n",
    "\n",
    "As칤 que continuaremos usando el mismo modelo para FoodVision Big.\n",
    "\n",
    "Podemos crear un extractor de funciones EffNetB2 para Food101 usando nuestra funci칩n `create_effnetb2_model()` que creamos anteriormente, en la [secci칩n 3.1](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to -make-an-effnetb2-feature-extractor), y pas치ndole el par치metro `num_classes=101` (ya que Food101 tiene 101 clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree un modelo EffNetB2 capaz de adaptarse a 101 clases para Food101\n",
    "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20507f",
   "metadata": {},
   "source": [
    "춰Hermoso!\n",
    "\n",
    "Ahora obtengamos un resumen de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fcfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Obtenga un resumen del extractor de funciones EffNetB2 para Food101 con 101 clases de salida (descomentar para obtener una salida completa)\n",
    "# resumen(effnetb2_food101,\n",
    "# tama침o_entrada=(1, 3, 224, 224),\n",
    "# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n",
    "# ancho_columna=20,\n",
    "# row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1032ad1",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-effnetb2-feature-extractor-101-classes.png\" width=900 alt=\"extractor de funciones effnetb2 con resumen del modelo de 100 clases de salida\"/>\n",
    "                                                                                                                                                      \n",
    "춰Lindo!\n",
    "\n",
    "Vea c칩mo, al igual que nuestro modelo EffNetB2 para FoodVision Mini, las capas base est치n congeladas (칠stas est치n previamente entrenadas en ImageNet) y las capas externas (las capas `clasificadoras`) se pueden entrenar con una forma de salida de `[batch_size, 101]` (`101 ` para 101 clases en Food101). \n",
    "\n",
    "Ahora que vamos a tratar con bastante m치s datos de lo habitual, 쯤u칠 tal si agregamos un poco de aumento de datos a nuestras transformaciones (`effnetb2_transforms`) para aumentar los datos de entrenamiento?\n",
    "\n",
    "> **Nota:** El aumento de datos es una t칠cnica que se utiliza para alterar la apariencia de una muestra de entrenamiento de entrada (por ejemplo, rotar una imagen o sesgarla ligeramente) para aumentar artificialmente la diversidad de un conjunto de datos de entrenamiento y, con suerte, evitar el sobreajuste. Puede ver m치s sobre el aumento de datos en [04. Secci칩n 6 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation).\n",
    "\n",
    "Compongamos una canalizaci칩n `torchvision.transforms` para usar [`torchvision.transforms.TrivialAugmentWide()`](https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html) (el mismo aumento de datos utilizado por el equipo de PyTorch en sus [recetas de visi칩n por computadora] (https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break- mejoras de precisi칩n de clave)) as칤 como `effnetb2_transforms` para transformar nuestras im치genes de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree transformaciones de datos de entrenamiento de Food101 (realice solo aumento de datos en las im치genes de entrenamiento)\n",
    "food101_train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.TrivialAugmentWide(),\n",
    "    effnetb2_transforms,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08085acd",
   "metadata": {},
   "source": [
    "춰칄pico!\n",
    "\n",
    "Ahora comparemos `food101_train_transforms` (para los datos de entrenamiento) y `effnetb2_transforms` (para los datos de prueba/inferencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training transforms:\\n{food101_train_transforms}\\n\") \n",
    "print(f\"Testing transforms:\\n{effnetb2_transforms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0be74",
   "metadata": {},
   "source": [
    "### 10.2 Obtenci칩n de datos para FoodVision Big\n",
    "\n",
    "Para FoodVision Mini, creamos nuestras propias [divisiones de datos personalizadas](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) de todo el conjunto de datos de Food101.\n",
    "\n",
    "Para obtener el conjunto de datos completo de Food101, podemos usar [`torchvision.datasets.Food101()`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html).\n",
    "\n",
    "Primero configuraremos una ruta al directorio `data/` para almacenar las im치genes. \n",
    "\n",
    "Luego descargaremos y transformaremos las divisiones del conjunto de datos de entrenamiento y prueba usando `food101_train_transforms` y `effnetb2_transforms` para transformar cada conjunto de datos respectivamente. \n",
    "\n",
    "> **Nota:** Si est치 utilizando Google Colab, la siguiente celda tardar치 entre 3 y 5 minutos en ejecutarse por completo y descargar las im치genes de Food101 desde PyTorch. \n",
    ">\n",
    "> Esto se debe a que se est치n descargando m치s de 100.000 im치genes (101 clases x 1000 im치genes por clase). Si reinicia el tiempo de ejecuci칩n de Google Colab y regresa a esta celda, las im치genes deber치n volver a descargarse. Alternativamente, si est치 ejecutando este cuaderno localmente, las im치genes se almacenar치n en cach칠 y se almacenar치n en el directorio especificado por el par치metro `root` de `torchvision.datasets.Food101()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7113f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Directorio de datos de configuraci칩n\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Obtenga datos de entrenamiento (~750 im치genes x 101 clases de alimentos)\n",
    "train_data = datasets.Food101(root=data_dir, # path to download data to\n",
    "                              split=\"train\", # dataset split to get\n",
    "                              transform=food101_train_transforms, # perform data augmentation on training data\n",
    "                              download=True) # want to download?\n",
    "\n",
    "# Obtenga datos de prueba (~250 im치genes x 101 clases de alimentos)\n",
    "test_data = datasets.Food101(root=data_dir,\n",
    "                             split=\"test\",\n",
    "                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n",
    "                             download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535c3b6",
   "metadata": {},
   "source": [
    "춰Datos descargados!\n",
    "\n",
    "Ahora podemos obtener una lista de todos los nombres de clases usando `train_data.classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener nombres de clases de Food101\n",
    "food101_class_names = train_data.classes\n",
    "\n",
    "# Ver los primeros 10\n",
    "food101_class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361d8f5",
   "metadata": {},
   "source": [
    "춰Ho, ho! Esas son algunas comidas que suenan deliciosas (aunque nunca he o칤do hablar de los \"bu침uelos\"... actualizaci칩n: despu칠s de una b칰squeda r치pida en Google, los bu침uelos tambi칠n se ven deliciosos). \n",
    "                                                \n",
    "Puede ver una lista completa de los nombres de las clases Food101 en el curso GitHub en [`extras/food101_class_names.txt`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names. TXT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7227c6",
   "metadata": {},
   "source": [
    "### 10.3 Creaci칩n de un subconjunto del conjunto de datos Food101 para experimentar m치s r치pido \n",
    "\n",
    "Esto es opcional.\n",
    "\n",
    "No *necesitamos* crear otro subconjunto del conjunto de datos de Food101, podr칤amos entrenar y evaluar un modelo en las 101.000 im치genes completas.\n",
    "\n",
    "Pero para seguir entrenando r치pido, creemos una divisi칩n del 20 % de los conjuntos de datos de entrenamiento y prueba.\n",
    "\n",
    "Nuestro objetivo ser치 ver si podemos superar los mejores resultados del [documento Food101] original (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) con solo el 20 % de los datos.\n",
    "\n",
    "Para desglosar los conjuntos de datos que hemos utilizado/utilizaremos:\n",
    "\n",
    "| **Cuadernos(s)** | **Nombre del proyecto** | **Conjunto de datos** | **N칰mero de clases** | **Im치genes de entrenamiento** | **Im치genes de prueba** | \n",
    "| ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| 04, 05, 06, 07, 08 | FoodVision Mini (10% de datos) | Food101 divisi칩n personalizada | 3 (pizza, bistec, sushi) | 225 | 75 | \n",
    "| 07, 08, 09 | FoodVision Mini (20% de datos) | Food101 divisi칩n personalizada | 3 (pizza, bistec, sushi) | 450 | 150 |\n",
    "| **09 (este)** | FoodVision Big (20% de datos) | Food101 divisi칩n personalizada | 101 (todas las clases de Food101) | 15150 | 5050 | \n",
    "| Ampliaci칩n | FoodVision grande | Food101 todos los datos | 101 | 75750 | 25250 | \n",
    "\n",
    "쯇uedes ver la tendencia? \n",
    "\n",
    "As칤 como el tama침o de nuestro modelo aument칩 lentamente con el tiempo, tambi칠n lo hizo el tama침o del conjunto de datos que hemos estado usando para los experimentos.\n",
    "\n",
    "> **Nota:** Para superar realmente los resultados del art칤culo original de Food101 con el 20 % de los datos, tendr칤amos que entrenar un modelo con el 20 % de los datos de entrenamiento y luego evaluar nuestro modelo en el conjunto de pruebas *completo* en lugar de que la divisi칩n que creamos. Dejar칠 esto como un ejercicio de extensi칩n para que lo pruebes. Tambi칠n le recomiendo que intente entrenar un modelo en todo el conjunto de datos de entrenamiento de Food101.\n",
    "\n",
    "Para dividir nuestro FoodVision Big (20% de datos), creemos una funci칩n llamada `split_dataset()` para dividir un conjunto de datos determinado en ciertas proporciones.\n",
    "\n",
    "Podemos usar [`torch.utils.data.random_split()`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) para crear divisiones de tama침os determinados usando ` par치metro de longitudes. \n",
    "\n",
    "El par치metro `longitudes` acepta una lista de longitudes divididas deseadas donde el total de la lista debe ser igual a la longitud total del conjunto de datos.\n",
    "\n",
    "Por ejemplo, con un conjunto de datos de tama침o 100, podr칤a pasar `longitudes=[20, 80]` para recibir una divisi칩n del 20 % y del 80 %.\n",
    "\n",
    "Querremos que nuestra funci칩n devuelva dos divisiones, una con la longitud objetivo (por ejemplo, el 20 % de los datos de entrenamiento) y la otra con la longitud restante (por ejemplo, el 80 % restante de los datos de entrenamiento).\n",
    "\n",
    "Finalmente, estableceremos el par치metro `generador` en un valor `torch.manual_seed()` para mayor reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348192e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n",
    "    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n",
    "\n",
    "    Args:\n",
    "        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n",
    "        split_size (float, optional): How much of the dataset should be split? \n",
    "            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n",
    "        seed (int, optional): Seed for random generator. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n",
    "            random_split_2 is of size (1-split_size)*len(dataset).\n",
    "    \"\"\"\n",
    "    # Create split lengths based on original dataset length\n",
    "    length_1 = int(len(dataset) * split_size) # desired length\n",
    "    length_2 = len(dataset) - length_1 # remaining length\n",
    "        \n",
    "    # Print out info\n",
    "    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n",
    "    \n",
    "    # Create splits with given random seed\n",
    "    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n",
    "                                                                   lengths=[length_1, length_2],\n",
    "                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n",
    "    return random_split_1, random_split_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf24dd",
   "metadata": {},
   "source": [
    "춰Se cre칩 la funci칩n de divisi칩n del conjunto de datos!\n",
    "\n",
    "Ahora prob칠moslo creando una divisi칩n del 20% del conjunto de datos de prueba y capacitaci칩n de Food101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear capacitaci칩n Divisi칩n del 20% de Food101\n",
    "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
    "                                                 split_size=0.2)\n",
    "\n",
    "# Crear pruebas con una divisi칩n del 20% de Food101\n",
    "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
    "                                                split_size=0.2)\n",
    "\n",
    "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75737af",
   "metadata": {},
   "source": [
    "춰Excelente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bf184",
   "metadata": {},
   "source": [
    "### 10.4 Convirtiendo nuestros conjuntos de datos de Food101 en `DataLoader`s\n",
    "\n",
    "Ahora conviertamos nuestras divisiones del conjunto de datos Food101 20% en `DataLoader` usando `torch.utils.data.DataLoader()`.\n",
    "\n",
    "Estableceremos `shuffle=True` solo para los datos de entrenamiento y el tama침o del lote en `32` para ambos conjuntos de datos.\n",
    "\n",
    "Y estableceremos `num_workers` en `4` si el recuento de CPU est치 disponible o `2` si no lo est치 (aunque el valor de `num_workers` es muy experimental y depender치 del hardware que est칠s usando, hay un [ hilo de discusi칩n activo sobre esto en los foros de PyTorch](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f484ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n",
    "\n",
    "# Crear Food101 20 por ciento de entrenamiento DataLoader\n",
    "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n",
    "                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  num_workers=NUM_WORKERS)\n",
    "# Crear Food101 20 por ciento de prueba DataLoader\n",
    "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n",
    "                                                                 batch_size=BATCH_SIZE,\n",
    "                                                                 shuffle=False,\n",
    "                                                                 num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403df49c",
   "metadata": {},
   "source": [
    "### 10.5 Entrenamiento FoodVision Modelo grande\n",
    "\n",
    "춰El modelo FoodVision Big y `DataLoader`s est치n listos!\n",
    "\n",
    "Hora de entrenar.\n",
    "\n",
    "Crearemos un optimizador usando `torch.optim.Adam()` y una tasa de aprendizaje de `1e-3`.\n",
    "\n",
    "Y debido a que tenemos tantas clases, tambi칠n configuraremos una funci칩n de p칠rdida usando `torch.nn.CrossEntropyLoss()` con `label_smoothing=0.1`, en l칤nea con la tecnolog칤a de punta de [`torchvision` receta de entrenamiento](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#label-smoothing).\n",
    "\n",
    "쯈u칠 es [**suavizado de etiquetas**](https://paperswithcode.com/method/label-smoothing)? \n",
    "\n",
    "El suavizado de etiquetas es una t칠cnica de regularizaci칩n (regularizaci칩n es otra palabra para describir el proceso de [prevenci칩n del sobreajuste](https://www.learnpytorch.io/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting)) que reduce la valor que un modelo le da a cualquier etiqueta y lo distribuye entre las dem치s etiquetas.\n",
    "\n",
    "En esencia, en lugar de que un modelo se *demasiado confiado* en una sola etiqueta, el suavizado de etiquetas otorga un valor distinto de cero a otras etiquetas para ayudar en la generalizaci칩n.\n",
    "\n",
    "Por ejemplo, si un modelo *sin* suavizado de etiquetas tuviera los siguientes resultados para 5 clases:\n",
    "\n",
    "```\n",
    "[0, 0, 0,99, 0,01, 0]\n",
    "```\n",
    "\n",
    "Un modelo *con* suavizado de etiquetas puede tener los siguientes resultados:\n",
    "\n",
    "```\n",
    "[0,01, 0,01, 0,96, 0,01, 0,01]\n",
    "```\n",
    "\n",
    "El modelo todav칤a conf칤a en su predicci칩n de la clase 3, pero dar valores peque침os a las otras etiquetas obliga al modelo a considerar al menos otras opciones.\n",
    "\n",
    "Finalmente, para agilizar las cosas, entrenaremos nuestro modelo durante cinco 칠pocas usando la funci칩n `engine.train()` que creamos en [05. PyTorch Going Modular secci칩n 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) con el objetivo de superar al Food101 original resultado del art칤culo de 56,4% de precisi칩n en el conjunto de prueba.\n",
    "\n",
    "춰Entrenemos a nuestro modelo m치s grande hasta el momento!\n",
    "\n",
    "> **Nota:** La ejecuci칩n de la siguiente celda tardar치 entre 15 y 20 minutos en Google Colab. Esto se debe a que est치 entrenando el modelo m치s grande con la mayor cantidad de datos que hemos usado hasta ahora (15,150 im치genes de entrenamiento, 5050 im치genes de prueba). Y es una de las razones por las que antes decidimos dividir el 20% del conjunto de datos completo de Food101 (para que el entrenamiento no tomara m치s de una hora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Optimizador de configuraci칩n\n",
    "optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "# Funci칩n de p칠rdida de configuraci칩n\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n",
    "\n",
    "# Quiere superar el art칤culo original de Food101 con un 20 % de datos, necesita m치s del 56,4 % seg칰n el conjunto de datos de prueba\n",
    "set_seeds()    \n",
    "effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
    "                                        train_dataloader=train_dataloader_food101_20_percent,\n",
    "                                        test_dataloader=test_dataloader_food101_20_percent,\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        epochs=5,\n",
    "                                        device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788b8b2",
   "metadata": {},
   "source": [
    "Woooo!!!!\n",
    "\n",
    "Parece que superamos los resultados del art칤culo original de Food101 de 56,4% de precisi칩n con solo el 20% de los datos de entrenamiento (aunque solo evaluamos el 20% de los datos de las pruebas tambi칠n, para replicar completamente los resultados, podr칤amos evaluar el 100% de las pruebas). datos). \n",
    "\n",
    "춰Ese es el poder del aprendizaje por transferencia!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d82425",
   "metadata": {},
   "source": [
    "### 10.6 Inspeccionando las curvas de p칠rdidas del modelo FoodVision Big\n",
    "\n",
    "Hagamos visuales nuestras curvas de p칠rdidas de FoodVision Big.\n",
    "\n",
    "Podemos hacerlo con la funci칩n `plot_loss_curves()` de `helper_functions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# Consulte las curvas de p칠rdidas de FoodVision Big\n",
    "plot_loss_curves(effnetb2_food101_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cb3eb",
   "metadata": {},
   "source": [
    "춰춰춰Lindo!!!\n",
    "\n",
    "Parece que nuestras t칠cnicas de regularizaci칩n (aumento de datos y suavizado de etiquetas) ayudaron a evitar que nuestro modelo se sobreajustara (la p칠rdida de entrenamiento sigue siendo mayor que la p칠rdida de prueba), lo que indica que nuestro modelo tiene un poco m치s de capacidad para aprender y podr칤a mejorar con m치s entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff787e0",
   "metadata": {},
   "source": [
    "### 10.7 Guardar y cargar FoodVision Big\n",
    "\n",
    "Ahora que hemos entrenado nuestro modelo m치s grande hasta el momento, guard칠moslo para poder volver a cargarlo m치s tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import utils\n",
    "\n",
    "# Crear una ruta modelo\n",
    "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n",
    "\n",
    "# Guardar modelo FoodVision Big\n",
    "utils.save_model(model=effnetb2_food101,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=effnetb2_food101_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f9cfa",
   "metadata": {},
   "source": [
    "춰Modelo guardado!\n",
    "\n",
    "Antes de continuar, asegur칠monos de poder volver a cargarlo.\n",
    "\n",
    "Lo haremos creando primero una instancia de modelo con `create_effnetb2_model(num_classes=101)` (101 clases para todas las clases de Food101).\n",
    "\n",
    "Y luego cargar el `state_dict()` guardado con [`torch.nn.Module.load_state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict #torch.nn.Module.load_state_dict) y [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b07130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una instancia EffNetB2 compatible con Food101\n",
    "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
    "\n",
    "# Cargue el state_dict() del modelo guardado\n",
    "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc7fb7",
   "metadata": {},
   "source": [
    "### 10.8 Comprobaci칩n del tama침o del modelo grande de FoodVision\n",
    "\n",
    "Nuestro modelo FoodVision Big es capaz de clasificar 101 clases frente a las 3 clases de FoodVision Mini, 춰un aumento de 33,6 veces!\n",
    "\n",
    "쮺칩mo afecta esto al tama침o del modelo?\n",
    "\n",
    "Vamos a averiguar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c406c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Obtenga el tama침o del modelo en bytes y luego convi칠rtalo a megabytes\n",
    "pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9978f",
   "metadata": {},
   "source": [
    "Mmm, parece que el tama침o del modelo se mantuvo pr치cticamente igual (30 MB para FoodVision Big y 29 MB para FoodVision Mini) a pesar del gran aumento en el n칰mero de clases.\n",
    "\n",
    "Esto se debe a que todos los par치metros adicionales para FoodVision Big est치n *solo* en la 칰ltima capa (el encabezado del clasificador). \n",
    "\n",
    "Todas las capas base son iguales entre FoodVision Big y FoodVision Mini.\n",
    "\n",
    "Volver arriba y comparar los res칰menes de los modelos dar치 m치s detalles.\n",
    "\n",
    "| **Modelo** | **Forma de salida (n칰m. de clases)** | **Par치metros entrenables** | **Par치metros totales** | **Tama침o del modelo (MB)** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| FoodVision Mini (extractor de funciones EffNetB2) | 3 | 4.227 | 7.705.221 |  29 |\n",
    "| FoodVision Big (extractor de funciones EffNetB2) | 101 | 142.309 | 7.843.303 | 30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cc903",
   "metadata": {},
   "source": [
    "## 11. Convertir nuestro modelo FoodVision Big en una aplicaci칩n implementable\n",
    "\n",
    "Tenemos un modelo EffNetB2 entrenado y guardado en el 20% del conjunto de datos de Food101.\n",
    "\n",
    "Y en lugar de dejar que nuestro modelo viva en una carpeta toda su vida, 춰implement칠moslo!\n",
    "\n",
    "Implementaremos nuestro modelo FoodVision Big de la misma manera que implementamos nuestro modelo FoodVision Mini, como una demostraci칩n de Gradio en Hugging Face Spaces.\n",
    "\n",
    "Para comenzar, creemos un directorio `demos/foodvision_big/` para almacenar nuestros archivos de demostraci칩n de FoodVision Big, as칤 como un directorio `demos/foodvision_big/examples` para guardar una imagen de ejemplo con la que probar la demostraci칩n.\n",
    "\n",
    "Cuando hayamos terminado tendremos la siguiente estructura de archivos:\n",
    "\n",
    "```\n",
    "poblaci칩n/\n",
    "  comidavision_big/\n",
    "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
    "    aplicaci칩n.py\n",
    "    nombres_clase.txt\n",
    "    ejemplos/\n",
    "      ejemplo_1.jpg\n",
    "    modelo.py\n",
    "    requisitos.txt\n",
    "```\n",
    "\n",
    "D칩nde:\n",
    "* `09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` es nuestro archivo de modelo PyTorch entrenado.\n",
    "* `app.py` contiene nuestra aplicaci칩n FoodVision Big Gradio.\n",
    "* `class_names.txt` contiene todos los nombres de clases de FoodVision Big.\n",
    "* `examples/` contiene im치genes de ejemplo para usar con nuestra aplicaci칩n Gradio.\n",
    "* `model.py` contiene la definici칩n del modelo, as칤 como cualquier transformaci칩n asociada con el modelo.\n",
    "* `requirements.txt` contiene las dependencias para ejecutar nuestra aplicaci칩n, como `torch`, `torchvision` y `gradio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62622187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Crear ruta de demostraci칩n de FoodVision Big\n",
    "foodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n",
    "\n",
    "# Haga el directorio de demostraci칩n de FoodVision Big\n",
    "foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Directorio de ejemplos de demostraci칩n de Make FoodVision Big\n",
    "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522717c",
   "metadata": {},
   "source": [
    "### 11.1 Descargar una imagen de ejemplo y moverla al directorio `ejemplos`\n",
    "\n",
    "Para nuestra imagen de ejemplo, usaremos la fiel [imagen `pizza-dad`] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad. jpeg) (una foto de mi pap치 comiendo pizza).\n",
    "\n",
    "As칤 que descargu칠moslo del curso GitHub mediante el comando `!wget` y luego podemos moverlo a `demos/foodvision_big/examples` con el comando `!mv` (abreviatura de \"mover\").\n",
    "\n",
    "Mientras estamos aqu칤, trasladaremos nuestro modelo Food101 EffNetB2 entrenado de `models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` a `demos/foodvision_big` tambi칠n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff68aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar y mover una imagen de ejemplo\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n",
    "!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n",
    "\n",
    "# Mueva el modelo entrenado a la carpeta de demostraci칩n de FoodVision Big (se producir치 un error si el modelo ya se ha movido)\n",
    "!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cac84b",
   "metadata": {},
   "source": [
    "### 11.2 Guardar nombres de clases de Food101 en un archivo (`class_names.txt`)\n",
    "\n",
    "Debido a que hay tantas clases en el conjunto de datos Food101, en lugar de almacenarlas como una lista en nuestro archivo `app.py`, guard칠moslas en un archivo `.txt` y le치moslas cuando sea necesario.\n",
    "\n",
    "Primero recordaremos c칩mo se ven revisando `food101_class_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298853dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte los primeros 10 nombres de clases de Food101\n",
    "food101_class_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ee04f",
   "metadata": {},
   "source": [
    "Maravilloso, ahora podemos escribirlos en un archivo de texto creando primero una ruta a `demos/foodvision_big/class_names.txt` y luego abriendo un archivo con `open()` de Python y luego escribiendo en 칠l dejando una nueva l칤nea para cada clase. .\n",
    "\n",
    "Idealmente, queremos que los nombres de nuestras clases se guarden como:\n",
    "\n",
    "```\n",
    "tarta de manzana\n",
    "Costillitas\n",
    "baklava\n",
    "Carpaccio de carne\n",
    "tartar_de_carne\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b858df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ruta a los nombres de clases de Food101\n",
    "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
    "\n",
    "# Escriba la lista de nombres de clases de Food101 en el archivo\n",
    "with open(foodvision_big_class_names_path, \"w\") as f:\n",
    "    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
    "    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce674ce",
   "metadata": {},
   "source": [
    "Excelente, ahora asegur칠monos de poder leerlos.\n",
    "\n",
    "Para hacerlo usaremos [`open()`](https://www.w3schools.com/python/ref_func_open.asp) de Python en modo lectura (`\"r\"`) y luego usaremos [`readlines( )`](https://www.w3schools.com/python/ref_file_readlines.asp) m칠todo para leer cada l칤nea de nuestro archivo `class_names.txt`.\n",
    "\n",
    "Y podemos guardar los nombres de las clases en una lista eliminando el valor de nueva l칤nea de cada uno de ellos con una lista de comprensi칩n y [`strip()`](https://www.w3schools.com/python/ref_string_strip.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abra el archivo de nombres de clases Food101 y lea cada l칤nea en una lista\n",
    "with open(foodvision_big_class_names_path, \"r\") as f:\n",
    "    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n",
    "    \n",
    "# Ver los primeros 5 nombres de clases cargados nuevamente en\n",
    "food101_class_names_loaded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d75617",
   "metadata": {},
   "source": [
    "### 11.3 Convirtiendo nuestro modelo FoodVision Big en un script de Python (`model.py`)\n",
    "\n",
    "Al igual que en la demostraci칩n de FoodVision Mini, creemos un script que sea capaz de crear una instancia de un modelo de extracci칩n de caracter칤sticas EffNetB2 junto con sus transformaciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c460542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f960c",
   "metadata": {},
   "source": [
    "### 11.4 Convirtiendo nuestra aplicaci칩n FoodVision Big Gradio en un script Python (`app.py`)\n",
    "\n",
    "Tenemos un script `model.py` de FoodVision Big, ahora creemos un script `app.py` de FoodVision Big.\n",
    "\n",
    "De nuevo, esto ser치 pr치cticamente igual que el script `app.py` de FoodVision Mini, excepto que cambiaremos:\n",
    "\n",
    "1. **Configuraci칩n de importaciones y nombres de clases** - La variable `class_names` ser치 una lista para todas las clases de Food101 en lugar de pizza, bistec o sushi. Podemos acceder a ellos a trav칠s de `demos/foodvision_big/class_names.txt`.\n",
    "2. **Preparaci칩n de modelos y transformaciones** - El `modelo` tendr치 `num_classes=101` en lugar de `num_classes=3`. Tambi칠n nos aseguraremos de cargar los pesos de `\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"` (nuestra ruta del modelo FoodVision Big).\n",
    "3. **Funci칩n de predicci칩n**: seguir치 siendo la misma que `app.py` de FoodVision Mini.\n",
    "4. **Aplicaci칩n Gradio**: la interfaz de Gradio tendr치 diferentes par치metros de \"t칤tulo\", \"descripci칩n\" y \"art칤culo\" para reflejar los detalles de FoodVision Big.\n",
    "\n",
    "Tambi칠n nos aseguraremos de guardarlo en `demos/foodvision_big/app.py` usando el comando m치gico `%%writefile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a955bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/app.py\n",
    "# ## 1. Configuraci칩n de importaciones y nombres de clases ###\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Configurar nombres de clases\n",
    "with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n",
    "    class_names = [food_name.strip() for food_name in  f.readlines()]\n",
    "    \n",
    "# ## 2. Preparaci칩n del modelo y transforma ###\n",
    "\n",
    "# Crear modelo\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=101, # could also use len(class_names)\n",
    ")\n",
    "\n",
    "# Cargar pesos guardados\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "# ## 3. Funci칩n de predicci칩n ###\n",
    "\n",
    "# Crear funci칩n de predicci칩n\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# ## 4. Aplicaci칩n Gradio ###\n",
    "\n",
    "# Crear cadenas de t칤tulo, descripci칩n y art칤culo.\n",
    "title = \"FoodVision Big 游꼢游녜\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Crear una lista de ejemplos desde el directorio \"examples/\"\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Crear interfaz Gradio\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
    "        gr.Number(label=\"Prediction time (s)\"),\n",
    "    ],\n",
    "    examples=example_list,\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    ")\n",
    "\n",
    "# 춰Inicia la aplicaci칩n!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb8012",
   "metadata": {},
   "source": [
    "### 11.5 Creaci칩n de un archivo de requisitos para FoodVision Big (`requirements.txt`)\n",
    "\n",
    "Ahora todo lo que necesitamos es un archivo `requirements.txt` para indicarle a nuestro Hugging Face Space qu칠 dependencias requiere nuestra aplicaci칩n FoodVision Big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/requirements.txt\n",
    "torch==1.12.0\n",
    "torchvision==0.13.0\n",
    "gradio==3.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddcf8c",
   "metadata": {},
   "source": [
    "### 11.6 Descarga de nuestros archivos de la aplicaci칩n FoodVision Big\n",
    "\n",
    "Tenemos todos los archivos que necesitamos para implementar nuestra aplicaci칩n FoodVision Big en Hugging Face, ahora comprim치moslos y descarg치moslos. \n",
    "\n",
    "Usaremos el mismo proceso que usamos para la aplicaci칩n FoodVision Mini anterior en la [secci칩n 9.1: *Descarga de los archivos de nuestra aplicaci칩n Foodvision Mini*](https://www.learnpytorch.io/09_pytorch_model_deployment/#91-downloading-our-foodvision -archivos-mini-aplicaci칩n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b46e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprima la carpeta foodvision_big pero excluya ciertos archivos\n",
    "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "\n",
    "# Descargue la aplicaci칩n FoodVision Big comprimida (si se ejecuta en Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"demos/foodvision_big.zip\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed84eee",
   "metadata": {},
   "source": [
    "### 11.7 Implementaci칩n de nuestra aplicaci칩n FoodVision Big en HuggingFace Spaces\n",
    "\n",
    "춰Hermoso! \n",
    "\n",
    "춰Es hora de darle vida a nuestro modelo m치s grande de todo el curso!\n",
    "\n",
    "Implementemos nuestra demostraci칩n de FoodVision Big Gradio en Hugging Face Spaces para que podamos probarla de forma interactiva y permitir que otros experimenten la magia de nuestros esfuerzos de aprendizaje autom치tico.\n",
    "\n",
    "> **Nota:** Hay [varias formas de cargar archivos en Hugging Face Spaces](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories). Los siguientes pasos tratan a Hugging Face como un repositorio git para rastrear archivos. Sin embargo, tambi칠n puede cargar directamente en Hugging Face Spaces a trav칠s de la [interfaz web](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui) o por la [biblioteca`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index). \n",
    "\n",
    "La buena noticia es que ya hemos realizado los pasos para hacerlo con FoodVision Mini, as칤 que ahora todo lo que tenemos que hacer es personalizarlos para que se adapten a FoodVision Big:\n",
    "\n",
    "1. [Reg칤strese](https://huggingface.co/join) para obtener una cuenta de Hugging Face. \n",
    "2. Inicie un nuevo Hugging Face Space yendo a su perfil y luego [haciendo clic en \"Nuevo espacio\"](https://huggingface.co/new-space).\n",
    "    * **Nota:** Un espacio en Hugging Face tambi칠n se conoce como \"repositorio de c칩digos\" (un lugar para almacenar su c칩digo/archivos) o \"repositorio\" para abreviar.\n",
    "3. Dale un nombre al Espacio, por ejemplo, el m칤o se llama `mrdbourke/foodvision_big`, puedes verlo aqu칤: https://huggingface.co/spaces/mrdbourke/foodvision_big\n",
    "4. Seleccione una licencia (yo us칠 [MIT](https://opensource.org/licenses/MIT)).\n",
    "5. Seleccione Gradio como Space SDK (kit de desarrollo de software). \n",
    "   * **Nota:** Puedes usar otras opciones como Streamlit, pero como nuestra aplicaci칩n est치 construida con Gradio, nos quedaremos con eso.\n",
    "6. Elija si su Espacio es p칰blico o privado (seleccion칠 p칰blico porque me gustar칤a que mi Espacio est칠 disponible para otros).\n",
    "7. Haga clic en \"Crear espacio\".\n",
    "8. Clone el repositorio localmente ejecutando: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` en la terminal o en el s칤mbolo del sistema.\n",
    "    * **Nota:** Tambi칠n puedes agregar archivos carg치ndolos en la pesta침a \"Archivos y versiones\".\n",
    "9. Copie/mueva el contenido de la carpeta `foodvision_big` descargada a la carpeta del repositorio clonado.\n",
    "10. Para cargar y rastrear archivos m치s grandes (por ejemplo, archivos de m치s de 10 MB o, en nuestro caso, nuestro archivo modelo PyTorch), necesitar치 [instalar Git LFS](https://git-lfs.github.com/) (que significa para \"almacenamiento de archivos grandes de git\").\n",
    "11. Despu칠s de haber instalado Git LFS, puedes activarlo ejecutando `git lfs install`.\n",
    "12. En el directorio `foodvision_big`, realice un seguimiento de los archivos de m치s de 10 MB con Git LFS con `git lfs track \"*.file_extension\"`.\n",
    "    * Realice un seguimiento del archivo de modelo EffNetB2 PyTorch con `git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"`.\n",
    "    * **Nota:** Si recibe alg칰n error al cargar im치genes, es posible que tambi칠n deba rastrearlas con `git lfs`, por ejemplo, `git lfs track \"examples/04-pizza-dad.jpg\"`\n",
    "13. Seguimiento de `.gitattributes` (creado autom치ticamente al clonar desde HuggingFace; este archivo ayudar치 a garantizar que nuestros archivos m치s grandes sean rastreados con Git LFS). Puede ver un archivo `.gitattributes` de ejemplo en [FoodVision Big Hugging Face Space] (https://huggingface.co/spaces/mrdbourke/foodvision_big/blob/main/.gitattributes).\n",
    "    * `git agregar .gitattributes`\n",
    "14. Agregue el resto de los archivos de la aplicaci칩n `foodvision_big` y conf칤rmelos con: \n",
    "    * `git agregar *`\n",
    "    * `git commit -m \"primer compromiso\"`\n",
    "15. Env칤e (cargue) los archivos a Hugging Face:\n",
    "    *`git push`\n",
    "16. Espere de 3 a 5 minutos hasta que se complete la compilaci칩n (las compilaciones futuras ser치n m치s r치pidas) y su aplicaci칩n estar치 activa.\n",
    "\n",
    "Si todo funcion칩 correctamente, 춰nuestra demostraci칩n de FoodVision Big Gradio deber칤a estar lista para clasificar!\n",
    "\n",
    "Puedes ver mi versi칩n aqu칤: https://huggingface.co/spaces/mrdbourke/foodvision_big/\n",
    "\n",
    "O incluso podemos insertar nuestra demostraci칩n de FoodVision Big Gradio directamente en nuestro cuaderno como un [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) con [`IPython.display.IFrame`](https: //ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) y un enlace a nuestro espacio en el formato `https://hf.space/embed/[YOUR_USERNAME ]/[TU_NOMBRE_ESPACIO]/+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ae0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython es una biblioteca para ayudar a trabajar con Python de forma interactiva\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Incruste la demostraci칩n de FoodVision Big Gradio como un iFrame\n",
    "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847534f7",
   "metadata": {},
   "source": [
    "춰춰Cuan genial es eso!?!\n",
    "\n",
    "Hemos recorrido un largo camino desde la construcci칩n de modelos PyTorch para predecir una l칤nea recta... 춰ahora estamos construyendo modelos de visi칩n por computadora accesibles para personas de todo el mundo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443bf2b",
   "metadata": {},
   "source": [
    "## Principales conclusiones\n",
    "\n",
    "* **La implementaci칩n es tan importante como la capacitaci칩n.** Una vez que tenga un buen modelo de trabajo, su primera pregunta deber칤a ser: 쯖칩mo puedo implementarlo y hacerlo accesible para otros? La implementaci칩n le permite probar su modelo en el mundo real en lugar de en conjuntos de prueba y capacitaci칩n privados.\n",
    "* **Tres preguntas para la implementaci칩n del modelo de aprendizaje autom치tico:**\n",
    "    1. 쮺u치l es el caso de uso m치s ideal para el modelo (qu칠 tan bien y qu칠 tan r치pido funciona)?\n",
    "    2. 쮸 d칩nde ir치 el modelo (en el dispositivo o en la nube)?\n",
    "    3. 쮺칩mo funcionar치 el modelo (las predicciones est치n en l칤nea o fuera de l칤nea)?\n",
    "* **Las opciones de implementaci칩n son abundantes.** Pero es mejor empezar de forma sencilla. Una de las mejores formas actuales (digo actual porque estas cosas siempre est치n cambiando) es usar Gradio para crear una demostraci칩n y alojarla en Hugging Face Spaces. Comience de manera simple y ampl칤elo cuando sea necesario.\n",
    "* **Nunca dejes de experimentar.** Las necesidades de tu modelo de aprendizaje autom치tico probablemente cambiar치n con el tiempo, por lo que implementar un 칰nico modelo no es el 칰ltimo paso. Es posible que encuentre cambios en el conjunto de datos, por lo que tendr치 que actualizar su modelo. O se publica una nueva investigaci칩n y hay una mejor arquitectura para usar.\n",
    "    * Por lo tanto, implementar un modelo es un paso excelente, pero probablemente querr치s actualizarlo con el tiempo. \n",
    "* **La implementaci칩n del modelo de aprendizaje autom치tico es parte de la pr치ctica de ingenier칤a de MLOps (operaciones de aprendizaje autom치tico).** MLOps es una extensi칩n de DevOps (operaciones de desarrollo) e involucra todas las partes de ingenier칤a relacionadas con el entrenamiento de un modelo: recopilaci칩n y almacenamiento de datos, datos preprocesamiento, implementaci칩n de modelos, monitoreo de modelos, control de versiones y m치s. Es un campo que evoluciona r치pidamente, pero existen algunos recursos s칩lidos para aprender m치s, muchos de los cuales se encuentran en [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and -ingenier칤a-de-aprendizaje-profundo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b33cd",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Todos los ejercicios se centran en practicar el c칩digo anterior.\n",
    "\n",
    "Deber칤a poder completarlos haciendo referencia a cada secci칩n o siguiendo los recursos vinculados.\n",
    "\n",
    "**Recursos:**\n",
    "\n",
    "* [Cuaderno de plantilla de ejercicios para 09] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/09_pytorch_model_deployment_exercises.ipynb).\n",
    "* [Cuaderno de soluciones de ejemplo para 09](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/09_pytorch_model_deployment_exercise_solutions.ipynb) pruebe los ejercicios *antes* de mirar esto.\n",
    "    * Vea un [video tutorial de las soluciones en YouTube] en vivo (https://youtu.be/jOX5ZCkWO-0) (errores y todo).\n",
    "\n",
    "1. Realice y programe predicciones con ambos modelos de extracci칩n de caracter칤sticas en el conjunto de datos de prueba utilizando la GPU (`device=\"cuda\"`). Compare los tiempos de predicci칩n del modelo en GPU y CPU: 쯘sto cierra la brecha entre ellos? Por ejemplo, 쯛acer predicciones en la GPU hace que los tiempos de predicci칩n del extractor de funciones de ViT se acerquen m치s a los tiempos de predicci칩n del extractor de funciones de EffNetB2?\n",
    "    * Encontrar치s el c칩digo para realizar estos pasos en la [secci칩n 5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas](https://www.learnpytorch.io/09_pytorch_model_deployment/#5-making-predictions-with-our-trained -models-and-timing-them) y [secci칩n 6. Comparaci칩n de resultados de modelos, tiempos de predicci칩n y tama침o](https://www.learnpytorch.io/09_pytorch_model_deployment/#6-comparing-model-results-prediction-times-and -tama침o).\n",
    "2. El extractor de funciones de ViT parece tener m치s capacidad de aprendizaje (debido a m치s par치metros) que EffNetB2. 쮺칩mo le va en la divisi칩n m치s grande del 20 % de todo el conjunto de datos de Food101?\n",
    "    * Entrene un extractor de funciones ViT en el conjunto de datos 20% Food101 durante 5 칠pocas, tal como lo hicimos con EffNetB2 en la secci칩n [10. Creando FoodVision Big](https://www.learnpytorch.io/09_pytorch_model_deployment/#10-creating-foodvision-big).\n",
    "3. Haga predicciones en el conjunto de datos de prueba 20 % de Food101 con el extractor de funciones ViT del ejercicio 2 y encuentre las predicciones \"m치s incorrectas\".\n",
    "    * Las predicciones ser치n las que tengan mayor probabilidad de predicci칩n pero con la etiqueta de predicci칩n incorrecta.\n",
    "    * Escribe una oraci칩n o dos sobre por qu칠 crees que el modelo se equivoc칩 en estas predicciones.\n",
    "4. Eval칰e el extractor de funciones de ViT en todo el conjunto de datos de prueba de Food101 en lugar de solo la versi칩n del 20 %, 쯖칩mo funciona?\n",
    "    * 쯉upera el mejor resultado del art칤culo original de Food101 con una precisi칩n del 56,4%?\n",
    "5. Dir칤jase a [Paperswithcode.com](https://paperswithcode.com/) y busque el modelo actual con mejor rendimiento en el conjunto de datos de Food101.\n",
    "    * 쯈u칠 modelo de arquitectura utiliza?\n",
    "6. Escriba de 1 a 3 posibles puntos de falla de nuestros modelos FoodVision implementados y cu치les podr칤an ser algunas posibles soluciones.\n",
    "    * Por ejemplo, 쯤u칠 pasar칤a si alguien subiera una foto que no fuera de comida a nuestro modelo FoodVision Mini?\n",
    "7. Elija cualquier conjunto de datos de [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) y entrene un modelo de extracci칩n de caracter칤sticas usando un modelo de [`torchvision.models`]( https://pytorch.org/vision/stable/models.html) (puede usar uno de los modelos que ya hemos creado, por ejemplo, EffNetB2 o ViT) durante 5 칠pocas y luego implementar su modelo como una aplicaci칩n Gradio en Hugging Face Espacios. \n",
    "    * Es posible que desee elegir un conjunto de datos m치s peque침o/hacer una divisi칩n m치s peque침a para que el entrenamiento no demore demasiado.\n",
    "    * 춰Me encantar칤a ver tus modelos implementados! As칤 que aseg칰rese de compartirlos en Discord o en la [p치gina de debates de GitHub del curso] (https://github.com/mrdbourke/pytorch-deep-learning/discussions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88587451",
   "metadata": {},
   "source": [
    "## Extracurricular\n",
    "\n",
    "* La implementaci칩n del modelo de aprendizaje autom치tico es generalmente un desaf칤o de ingenier칤a en lugar de un desaf칤o de aprendizaje autom치tico puro; consulte la [secci칩n de ingenier칤a de aprendizaje autom치tico de recursos adicionales de PyTorch] (https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning -y-deep-learning-engineering) para obtener recursos sobre c칩mo aprender m치s.\n",
    "    * En el interior encontrar치 recomendaciones de recursos como el libro de Chip Huyen [*Designing Machine Learning Systems*](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969) ( especialmente el cap칤tulo 7 sobre implementaci칩n de modelos) y el [curso Made with ML MLOps] de Goku Mohandas (https://madewithml.com/#mlops).\n",
    "* A medida que empieces a construir m치s y m치s proyectos propios, es probable que empieces a usar Git (y potencialmente GitHub) con bastante frecuencia. Para obtener m치s informaci칩n sobre ambos, recomiendo el video [*Git y GitHub para principiantes: curso intensivo*](https://youtu.be/RGOj5yH7evk) en el canal de YouTube freeCodeCamp.\n",
    "* S칩lo hemos ara침ado la superficie de lo que es posible con Gradio. Para obtener m치s informaci칩n, recomiendo consultar la [documentaci칩n completa](https://gradio.app/docs/), especialmente:\n",
    "    * Todos los diferentes tipos de [componentes de entrada y salida] (https://gradio.app/docs/#components).\n",
    "    * La [API de Gradio Blocks](https://gradio.app/docs/#blocks) para flujos de trabajo m치s avanzados.\n",
    "    * El cap칤tulo del curso Hugging Face sobre [c칩mo usar Gradio con Hugging Face] (https://huggingface.co/course/chapter9/1).\n",
    "* Los dispositivos Edge no se limitan a tel칠fonos m칩viles, incluyen computadoras peque침as como Raspberry Pi y el equipo de PyTorch tiene un [fant치stico tutorial de publicaci칩n de blog](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) sobre la implementaci칩n un modelo de PyTorch a uno.\n",
    "* Para obtener una gu칤a fant치stica sobre el desarrollo de aplicaciones basadas en inteligencia artificial y aprendizaje autom치tico, consulte la [Gu칤a de personas + inteligencia artificial de Google] (https://pair.withgoogle.com/guidebook). Una de mis favoritas es la secci칩n sobre [establecer las expectativas correctas] (https://pair.withgoogle.com/guidebook/patterns#set-the-right-expectations).\n",
    "    * Cubr칤 m치s de este tipo de recursos, incluidas gu칤as de Apple, Microsoft y m치s en la [edici칩n de abril de 2021 de Machine Learning Monthly](https://zerotomastery.io/blog/machine-learning-monthly-april-2021/ ) (un bolet칤n mensual que env칤o con lo 칰ltimo y lo mejor del campo de ML).\n",
    "* Si desea acelerar el tiempo de ejecuci칩n de su modelo en la CPU, debe conocer [TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html), [ONNX](https://pytorch .org/docs/stable/onnx.html) (Open Neural Network Exchange) y [OpenVINO](https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output. HTML). Al pasar de PyTorch puro a modelos ONNX/OpenVINO, he visto un aumento de ~2x+ en el rendimiento.\n",
    "* Para convertir modelos en una API implementable y escalable, consulte la [biblioteca TorchServe](https://pytorch.org/serve/).\n",
    "* Para ver un excelente ejemplo y una justificaci칩n de por qu칠 implementar un modelo de aprendizaje autom치tico en el navegador (una forma de implementaci칩n perimetral) ofrece varios beneficios (sin retraso en la latencia de transferencia de red), consulte el art칤culo de Jo Kristian Bergum sobre [*Moving ML Inference from the Cloud hasta el borde*](https://bergum.medium.com/moving-ml-inference-from-the-cloud-to-the-edge-d6f98dbdb2e3)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
