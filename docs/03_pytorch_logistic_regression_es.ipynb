{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Clasificación de Imágenes con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Parte 3 de \"Aprendizaje profundo con Pytorch: de cero a GAN\"\n",
    "\n",
    "Esta serie de didácticas es una introducción práctica y amena para principiantes en aprender en profundidad con la ayuda de [PyTorch](https://pytorch.org), una biblioteca de recursos de neuronas de código abierto. Estos tutoriales adoptan un enfoque práctico centrado en la codificación. La mejor manera de aprender hardware es ejecutar el código y experimentarlo usted mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Este tutorial cubre los siguientes temas:\n",
    "    \n",
    "* Trabajar con imágenes en PyTorch (usando el conjunto de datos MNIST)\n",
    "* Dividir un conjunto de datos en conjuntos de entrenamiento, validación y prueba\n",
    "* Creación de modelos PyTorch con lógica personalizada mediante la ampliación de la clase `nn.Module`\n",
    "* Interpretar los resultados del modelo como probabilidades utilizando Softmax y seleccionando etiquetas predichas\n",
    "* Elegir una métrica de evaluación útil (precisión) y una función de pérdida (entropía cruzada) para problemas de clasificación\n",
    "* Configuración de un ciclo de entrenamiento que también evalúa el modelo utilizando el conjunto de validación\n",
    "* Probar el modelo manualmente en ejemplos seleccionados al azar* Guardar y cargar puntos de control del modelo para evitar volver a entrenar desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Cómo ejecutar el código\n",
    "\n",
    "Este tutorial es un ejecutable [Jupyter notebook](https://jupyter.org) alojado en [Jovian](https://www.jovian.ai). Puede _ejecutar_ este tutorial y experimentar con los ejemplos de código de varias maneras: *usando recursos gratuitos en línea* (recomendado) o *en su computadora*.\n",
    "\n",
    "## Opción 1: Ejecutar usando recursos en línea gratuitos (1-clic, recomendado)\n",
    "\n",
    "La forma más fácil de comenzar a ejecutar el código es hacer clic en el botón **Ejecutar** en la parte superior de esta página y seleccionar **Ejecutar en Colab**. [Google Colab](https://colab.research.google.com) es una plataforma en línea gratuita para ejecutar portátiles Jupyter utilizando la infraestructura de nube de Google. También puede seleccionar \"Ejecutar en Binder\" o \"Ejecutar en Kaggle\" si tiene problemas para ejecutar el cuaderno en Google Colab.\n",
    "\n",
    "\n",
    "## Opción 2: Ejecutar en su computadora localmente\n",
    "\n",
    "Para ejecutar el código en su computadora localmente, deberá configurar [Python](https://www.python.org), descargar el cuaderno e instalar las bibliotecas requeridas. Recomendamos usar la distribución [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) de Python. Haga clic en el botón **Ejecutar** en la parte superior de esta página, seleccione la opción **Ejecutar localmente** y siga las instrucciones.\n",
    "\n",
    "> **Cuadernos de Jupyter**: Este tutorial es un [Cuaderno de Jupyter](https://jupyter.org) - un documento hecho de _celdas_. Cada celda puede contener código escrito en Python o explicaciones en inglés sencillo. Puede ejecutar celdas de código y ver los resultados, por ejemplo, números, mensajes, gráficos, tablas, archivos, etc., instantáneamente dentro del cuaderno. Jupyter es una poderosa plataforma para la experimentación y el análisis. No tenga miedo de perder el tiempo con el código y romper cosas: aprenderá mucho al encontrar y corregir errores. Puede utilizar la opción de menú \"Kernel > Reiniciar y borrar salida\" o \"Editar > Borrar salidas\" para borrar todas las salidas y empezar de nuevo desde arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Trabajar con imágenes\n",
    "\n",
    "En este tutorial, usaremos nuestro conocimiento existente de PyTorch y la regresión lineal para resolver un tipo de problema muy diferente: *clasificación de imágenes*. Usaremos la famosa [*Base de datos de dígitos escritos a mano del MNIST*](http://yann.lecun.com/exdb/mnist/) como nuestro conjunto de datos de entrenamiento. Consiste en imágenes en escala de grises de 28 px por 28 px de dígitos escritos a mano (0 a 9) y etiquetas para cada imagen que indican qué dígito representa. Aquí hay algunas imágenes de muestra del conjunto de datos:\n",
    "\n",
    "![muestra-mnist](https://i.imgur.com/CAYnuo1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Comenzamos instalando e importando `torch` y `torchvision`. `torchvision` contiene algunas utilidades para trabajar con datos de imagen. También proporciona clases auxiliares para descargar e importar conjuntos de datos populares como MNIST automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o_xViEWvfTTV"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389,
     "referenced_widgets": [
      "d7bc8179054c448c8decb471100d8edf",
      "3e158b4f6fc045a799a5d2848b7ef224",
      "7de6379c81224840ab318d1c5dbe2573",
      "8d85fb898cc4482794a273f9f40a7716",
      "ab0c7dba3efe488e962a95f77fdff901",
      "5e3647b0ec134d6cb7c76347435756d1",
      "41db56c1758644e28cc891ed17b9111e",
      "b278158fce8d4b619f02689a2d671560",
      "c755370c4edf4661b591e61de544ad8f",
      "392aac024b8845c58ab75d00893b4e42",
      "c50520b03cd0415c931144f086b2007a",
      "4ea3550eaeab40969e6a835721528c35",
      "12d3ec215dcc4b6794e3bbdef44602ce",
      "28ef332369de4d358909a586a1743c61",
      "332d4f3a80d84f07ad997aa55afd7f45",
      "69e7677bfdb3471d925d0a3eca1df5a7",
      "b4213321d97e4f54b356d0b466253e4c",
      "0e03dd08224f4e8c9c2f8d975358cdb7",
      "90fba2e9dcee4b5eaf94776dcd05b01a",
      "ef3735b9a82347edb1c193fcc1dc1ec2",
      "b91d4ce036c24e6fb29c9e3a2914e0bd",
      "96b3cf3c2b8c41ae832e3f33dcf13453",
      "57ea6daaa8484c59b3930783d0a9cf20",
      "043acb1db455426e8f56a61d6f39b222",
      "02a48991f3f2405db5dd2968af3177bb",
      "7113a3efba564850899ad46ec33a1ce0",
      "be465e46f1fa44c69c9dd8ff2d173543",
      "ef4e2e0ed447424b8695c1a66cec0210",
      "b612c47359234c3a8d59430c65605047",
      "7ffd56bd6fdb48efb48c338dfe983bfb",
      "eab337afee944438b7e9db9a2b7fd61b",
      "876cca63cea043f99904cd7a28f67e26"
     ]
    },
    "id": "q1Skz7dlfTTV",
    "outputId": "cfc70a7f-bbb0-4c05-e160-24c0f43bc0c7"
   },
   "outputs": [],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Cuando esta instrucción se ejecuta por primera vez, descarga los datos en el directorio `data/` al lado del cuaderno y crea un PyTorch `Dataset`. En ejecuciones posteriores, la descarga se omite porque los datos ya se han descargado. Vamos a comprobar el tamaño del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El conjunto de datos tiene 60 000 imágenes que usaremos para entrenar el modelo. También hay un conjunto de prueba adicional de 10 000 imágenes que se utilizan para evaluar modelos y reportar métricas en documentos e informes. Podemos crear el conjunto de datos de prueba usando la clase `MNIST` pasando `train=False` al constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLqZQOlWfTTW",
    "outputId": "2a1c7697-0294-4c64-d3f3-e8608a9f1fcb"
   },
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Veamos un elemento de muestra del conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZmSZeFGfTTW",
    "outputId": "f38133b3-f015-4ab9-9773-3d196e17f9a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x1A79439D198>, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pX4OKgeqYr55",
    "outputId": "100d11fd-66a3-4c59-f965-ff4bacb201a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x1A79439D240>, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Es un par, que consiste en una imagen de 28x28px y una etiqueta. La imagen es un objeto de la clase `PIL.Image.Image`, que forma parte de la biblioteca de imágenes de Python [Pillow](https://pillow.readthedocs.io/en/stable/). Podemos ver la imagen dentro de Jupyter usando [`matplotlib`](https://matplotlib.org/), la biblioteca de gráficos y gráficos de facto para la ciencia de datos en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "edyBIgXPfTTX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La instrucción `%matplotlib inline` indica a Jupyter que queremos trazar los gráficos dentro del cuaderno. Sin esta línea, Jupyter mostrará la imagen en una ventana emergente. Las declaraciones que comienzan con `%` se denominan comandos mágicos y se utilizan para configurar el comportamiento del propio Jupyter. Puede encontrar una lista completa de comandos mágicos aquí: https://ipython.readthedocs.io/en/stable/interactive/magics.html.\n",
    "\n",
    "Veamos un par de imágenes del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "3cxwpTQLfTTX",
    "outputId": "8d1b6cff-b7e9-4d3f-e7c8-5d54437f11f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "35ELPbM7fTTX",
    "outputId": "c5b3867e-1aa4-4fc1-d879-09bd51e40acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Es obvio que estas imágenes tienen un tamaño relativamente pequeño, y reconocer los dígitos a veces puede ser un desafío incluso para el ojo humano. Si bien es útil mirar estas imágenes, solo hay un problema aquí: PyTorch no sabe cómo trabajar con imágenes. Necesitamos convertir las imágenes en tensores. Podemos hacer esto especificando una transformación mientras creamos nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FBnad_UDfTTX"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los conjuntos de datos de PyTorch nos permiten especificar una o más funciones de transformación que se aplican a las imágenes a medida que se cargan. El módulo `torchvision.transforms` contiene muchas de estas funciones predefinidas. Usaremos la transformación `ToTensor` para convertir imágenes en tensores PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hYvBQ3LzfTTX"
   },
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', \n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCZ4h7jMfTTX",
    "outputId": "31b3ff15-d208-4f4d-b0ce-e7c496539e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La imagen ahora se convierte a un tensor de 1x28x28. La primera dimensión rastrea los canales de color. Las dimensiones segunda y tercera representan píxeles a lo largo de la altura y el ancho de la imagen, respectivamente. Dado que las imágenes en el conjunto de datos MNIST están en escala de grises, solo hay un canal. Otros conjuntos de datos tienen imágenes con color, en cuyo caso hay tres canales: rojo, verde y azul (RGB).\n",
    "\n",
    "Veamos algunos valores de muestra dentro del tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prZFgdtefTTX",
    "outputId": "7a27e0b1-d787-4ea3-edb0-9fe8509366e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0,10:15,10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los valores varían de 0 a 1, con `0` representando el negro, `1` el blanco y los valores entre diferentes tonos de gris. También podemos trazar el tensor como una imagen usando `plt.imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Fw-NvsqpfTTX",
    "outputId": "6ebc701b-949d-4c21-efbe-9caaa38d732f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the image by passing in the 28x28 matrix\n",
    "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Tenga en cuenta que necesitamos pasar solo la matriz de 28x28 a `plt.imshow`, sin una dimensión de canal. También pasamos un mapa de colores (`cmap=gray`) para indicar que queremos ver una imagen en escala de grises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Conjuntos de datos de entrenamiento y validación\n",
    "\n",
    "Al construir modelos de aprendizaje automático del mundo real, es bastante común dividir el conjunto de datos en tres partes:\n",
    "\n",
    "1. **Conjunto de entrenamiento**: se utiliza para entrenar el modelo, es decir, calcular la pérdida y ajustar los pesos del modelo mediante el descenso de gradiente.\n",
    "2. **Conjunto de validación**: se utiliza para evaluar el modelo durante el entrenamiento, ajustar los hiperparámetros (tasa de aprendizaje, etc.) y elegir la mejor versión del modelo.\n",
    "3. **Conjunto de prueba**: se utiliza para comparar diferentes modelos o enfoques e informar sobre la precisión final del modelo.\n",
    "\n",
    "En el conjunto de datos del MNIST, hay 60 000 imágenes de entrenamiento y 10 000 imágenes de prueba. El conjunto de prueba está estandarizado para que diferentes investigadores puedan informar los resultados de sus modelos contra la misma colección de imágenes.\n",
    "\n",
    "Dado que no hay un conjunto de validación predefinido, debemos dividir manualmente las 60 000 imágenes en conjuntos de datos de entrenamiento y validación. Reservemos 10 000 imágenes elegidas al azar para la validación. Podemos hacer esto usando el método `random_spilt` de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dcE0OPBfTTX",
    "outputId": "0d97e992-46c5-4a06-e8fb-236419ca2559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Es esencial elegir una muestra aleatoria para crear un conjunto de validación. Los datos de entrenamiento a menudo se ordenan por las etiquetas de destino, es decir, imágenes de 0, seguidas de 1, seguidas de 2, etc. Si creamos un conjunto de validación utilizando el último 20% de las imágenes, solo constaría de 8 y 9. Por el contrario, el conjunto de entrenamiento no contendría 8 ni 9. Tal entrenamiento-validación haría imposible entrenar un modelo útil.\n",
    "\n",
    "Ahora podemos crear cargadores de datos para ayudarnos a cargar los datos en lotes. Usaremos un tamaño de lote de 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pD_oD7BTfTTX"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Configuramos `shuffle=True` para el cargador de datos de entrenamiento para garantizar que los lotes generados en cada época sean diferentes. Esta aleatorización ayuda a generalizar y acelerar el proceso de entrenamiento. Por otro lado, dado que el cargador de datos de validación se usa solo para evaluar el modelo, no es necesario mezclar las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Ahora que hemos preparado nuestros cargadores de datos, podemos definir nuestro modelo.\n",
    "\n",
    "*Un modelo de**regresión logística**es casi idéntico a un modelo de regresión lineal. Contiene matrices de ponderación y sesgo, y la salida se obtiene mediante operaciones matriciales simples (`pred = x @ w.t() + b`).* Como hicimos con la regresión lineal, podemos usar `nn.Linear` para crear el modelo en lugar de crear e inicializar manualmente las matrices.\n",
    "\n",
    "*Dado que `nn.Linear` espera que cada ejemplo de entrenamiento sea un vector, cada tensor de imagen `1x28x28` se _aplana_ en un vector de tamaño 784 `(28*28)` antes de pasar al modelo.\n",
    "\n",
    "* El resultado de cada imagen es un vector de tamaño 10, en el que cada elemento representa la probabilidad de una etiqueta de destino en particular (es decir, de 0 a 9). La etiqueta predicha para una imagen es simplemente la que tiene la probabilidad más alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zzVbD7pifTTY"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(in_features = input_size, out_features = num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Por supuesto, este modelo es mucho más grande que nuestro modelo anterior en términos de número de parámetros. Echemos un vistazo a los pesos y sesgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3syrwEJYfNib",
    "outputId": "5ffab68c-989f-4425-b7af-a8f99057659e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2udKz11JfPcD",
    "outputId": "6e434dd9-7f3e-46b1-b640-ff49d30662aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias\n",
    "model.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAj8gVdUfTTY",
    "outputId": "bc83c418-59a4-4fca-e4fc-ebe5dd90c3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0272,  0.0254,  0.0070,  ...,  0.0271,  0.0071, -0.0239],\n",
       "        [ 0.0008, -0.0183, -0.0132,  ..., -0.0297, -0.0195, -0.0028],\n",
       "        [-0.0193, -0.0279,  0.0178,  ...,  0.0179, -0.0306, -0.0301],\n",
       "        ...,\n",
       "        [ 0.0315, -0.0104, -0.0265,  ..., -0.0106,  0.0091,  0.0350],\n",
       "        [-0.0015,  0.0148, -0.0252,  ..., -0.0261,  0.0177, -0.0202],\n",
       "        [ 0.0005,  0.0014, -0.0202,  ..., -0.0184,  0.0351, -0.0174]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRnviQ34fTTY",
    "outputId": "8c530856-2acd-40a9-8043-3048525f7fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0130,  0.0238,  0.0274,  0.0117,  0.0307, -0.0181,  0.0007, -0.0185,\n",
       "        -0.0080,  0.0238], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Aunque hay un total de 7850 parámetros aquí, conceptualmente, nada ha cambiado hasta ahora. Probemos y generemos algunos resultados usando nuestro modelo. Tomaremos el primer lote de 100 imágenes de nuestro conjunto de datos y las pasaremos a nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoTzJOy0fmjW",
    "outputId": "48c807e0-24cc-4b6b-d281-46fb1148e5a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lb = train_ds[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEey1T8Vfq4E",
    "outputId": "9ad632a2-0547-429e-cae0-db46c013ae29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a79b094198>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Td3uzb-ZjhDA",
    "outputId": "7c084fbe-bf40-42e6-b944-f7c56db8bc84"
   },
   "outputs": [],
   "source": [
    "list(train_loader)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "qIuxYOy3fTTY",
    "outputId": "ecdeb054-1227-45d8-b000-c830455e908f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 9, 3, 5, 3, 4, 3, 0, 6, 4, 5, 1, 1, 2, 7, 3, 6, 0, 2, 1, 9, 2, 5,\n",
      "        0, 4, 7, 1, 9, 8, 0, 6, 6, 1, 3, 2, 5, 0, 3, 4, 0, 5, 9, 2, 4, 5, 5, 6,\n",
      "        0, 0, 5, 2, 3, 7, 0, 4, 3, 0, 8, 8, 2, 2, 3, 4, 6, 1, 2, 8, 6, 2, 1, 4,\n",
      "        0, 5, 5, 0, 9, 9, 4, 1, 9, 6, 6, 3, 3, 1, 3, 3, 5, 8, 3, 1, 5, 3, 4, 4,\n",
      "        2, 7, 6, 7, 5, 0, 1, 9, 2, 3, 7, 9, 5, 1, 4, 5, 9, 3, 8, 7, 1, 0, 0, 9,\n",
      "        5, 7, 3, 8, 7, 2, 3, 3])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "#     outputs = model(images)\n",
    "#     print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9m-JsBr6lSO8",
    "outputId": "9d437758-74a2-4beb-f725-5151a1496145"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3_ktSR1lVJQ",
    "outputId": "774d2f7c-e443-4235-8375-318fd2257b0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.reshape(batch_size, 784).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El código anterior genera un error porque nuestros datos de entrada no tienen la forma correcta. Nuestras imágenes tienen la forma 1x28x28, pero necesitamos que sean vectores de tamaño 784, es decir, necesitamos aplanarlos. Usaremos el método `.reshape` de un tensor, que nos permitirá 'ver' eficientemente cada imagen como un vector plano sin realmente crear una copia de los datos subyacentes. Para incluir esta funcionalidad adicional dentro de nuestro modelo, necesitamos definir un modelo personalizado extendiendo la clase `nn.Module` de PyTorch.\n",
    "\n",
    "Una clase en Python proporciona un \"modelo\" para crear objetos. Veamos un ejemplo de definición de una nueva clase en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "c2HWol7qfTTY"
   },
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "  \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Dentro del método constructor `__init__`, instanciamos los pesos y sesgos usando `nn.Linear`. Y dentro del método `forward`, que se invoca cuando pasamos un lote de entradas al modelo, aplanamos el tensor de entrada y lo pasamos a `self.linear`.\n",
    "\n",
    "`xb.reshape(-1, 28*28)` indica a PyTorch que queremos una*vista*del tensor `xb` con dos dimensiones. La longitud a lo largo de la segunda dimensión es 28\\*28 (es decir, 784). Un argumento para `.reshape` se puede establecer en `-1` (en este caso, la primera dimensión) para permitir que PyTorch lo descubra automáticamente en función de la forma del tensor original.\n",
    "\n",
    "Tenga en cuenta que el modelo ya no tiene los atributos `.weight` y `.bias` (ya que ahora están dentro del atributo `.linear`), pero tiene un método `.parameters` que devuelve una lista que contiene los pesos y el sesgo ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voZF2M_3nVnI",
    "outputId": "26655cbc-0d5b-4534-cbb1-c902b9b65afe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZ4B-gGJjXFB",
    "outputId": "2463dafd-6642-40f6-8fc7-6cd49c7fb0be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0203,  0.0183,  0.0007,  ..., -0.0008, -0.0156,  0.0218],\n",
       "        [ 0.0298,  0.0048, -0.0126,  ...,  0.0071,  0.0040, -0.0052],\n",
       "        [-0.0020,  0.0249, -0.0317,  ..., -0.0075, -0.0352,  0.0043],\n",
       "        ...,\n",
       "        [-0.0084, -0.0184, -0.0218,  ..., -0.0340, -0.0063,  0.0113],\n",
       "        [ 0.0085,  0.0116, -0.0288,  ..., -0.0040, -0.0013, -0.0339],\n",
       "        [ 0.0096, -0.0070,  0.0032,  ..., -0.0012,  0.0349, -0.0125]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JciejPo9fTTY",
    "outputId": "97d4b205-3b50-49f2-d5af-9f991f5c70e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0203,  0.0183,  0.0007,  ..., -0.0008, -0.0156,  0.0218],\n",
       "         [ 0.0298,  0.0048, -0.0126,  ...,  0.0071,  0.0040, -0.0052],\n",
       "         [-0.0020,  0.0249, -0.0317,  ..., -0.0075, -0.0352,  0.0043],\n",
       "         ...,\n",
       "         [-0.0084, -0.0184, -0.0218,  ..., -0.0340, -0.0063,  0.0113],\n",
       "         [ 0.0085,  0.0116, -0.0288,  ..., -0.0040, -0.0013, -0.0339],\n",
       "         [ 0.0096, -0.0070,  0.0032,  ..., -0.0012,  0.0349, -0.0125]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0191, -0.0302, -0.0273, -0.0281, -0.0226,  0.0268,  0.0181,  0.0354,\n",
       "         -0.0002, -0.0301], requires_grad=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "model.parameters()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Podemos usar nuestro nuevo modelo personalizado de la misma manera que antes. Veamos si funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJ6FGOvYfTTZ",
    "outputId": "48150d99-1f81-44b9-bbcb-cc92ab813ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs :\n",
      " tensor([[ 0.2190, -0.0496,  0.0746, -0.0122, -0.1212, -0.3471, -0.2582,  0.0699,\n",
      "         -0.2685,  0.2092],\n",
      "        [ 0.2571, -0.0107, -0.1650,  0.1359,  0.0424, -0.3832,  0.1630,  0.0675,\n",
      "         -0.6183,  0.1319]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLWHV-6ojx-y",
    "outputId": "78af6da7-2a78-4660-e322-6ea93a66c37b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2190, -0.0496,  0.0746,  ...,  0.0699, -0.2685,  0.2092],\n",
       "        [ 0.2571, -0.0107, -0.1650,  ...,  0.0675, -0.6183,  0.1319],\n",
       "        [ 0.1358, -0.1044,  0.0111,  ...,  0.2700, -0.1633, -0.0345],\n",
       "        ...,\n",
       "        [ 0.2713, -0.1081, -0.1103,  ...,  0.1579, -0.0792,  0.2572],\n",
       "        [-0.0306, -0.2571, -0.1545,  ...,  0.3003,  0.0127,  0.1174],\n",
       "        [ 0.3300, -0.1934, -0.0621,  ...,  0.3359, -0.3857,  0.1580]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Para cada una de las 100 imágenes de entrada, obtenemos 10 salidas, una para cada clase. Como se discutió anteriormente, nos gustaría que estos resultados representen probabilidades. Los elementos de cada fila de salida deben estar entre 0 y 1 y sumar 1, lo cual no es el caso.\n",
    "\n",
    "Para convertir las filas de salida en probabilidades, usamos la función softmax, que tiene la siguiente fórmula:\n",
    "\n",
    "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
    "\n",
    "Primero, reemplazamos cada elemento `yi` en una fila de salida por `e^yi`, haciendo que todos los elementos sean positivos.\n",
    "\n",
    "![](https://www.montereyinstitute.org/courses/DevelopmentalMath/COURSE_TEXT2_RESOURCE/U18_L1_T1_text_final_6_files/image001.png)\n",
    "\n",
    "\n",
    "\n",
    "Luego, los dividimos por su suma para asegurarnos de que suman 1. El vector resultante puede interpretarse como probabilidades.\n",
    "\n",
    "\n",
    "\n",
    "Si bien es fácil implementar la función softmax (¡debe probarla!), usaremos la implementación que se proporciona dentro de PyTorch porque funciona bien con tensores multidimensionales (una lista de filas de salida en nuestro caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "q2XZB9CDkmKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.0, 7.0, 5.0, 9.0]\n",
      "[0.14, 1096.63, 148.41, 8103.08]\n",
      "[1.4976049018747874e-05, 0.11730846168163916, 0.0158756816776598, 0.8668008805916823]\n",
      "tensor([-2.,  7.,  5.,  9.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4477e-05, 1.1731e-01, 1.5876e-02, 8.6680e-01])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "c = [-2., 7., 5., 9.]\n",
    "print(c)\n",
    "c_exp = [round(math.exp(i), 2) for i in c]\n",
    "print(c_exp)\n",
    "softmax = [(j/sum(c_exp)) for j in c_exp]\n",
    "print(softmax)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "print(torch.as_tensor(c))\n",
    "F.softmax(torch.as_tensor(c), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Wc7d6WD0fTTZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La función softmax está incluida en el paquete `torch.nn.function` y requiere que especifiquemos una dimensión a lo largo de la cual se debe aplicar la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xvjcBVTo14n",
    "outputId": "34f2c507-4278-4f76-91cf-9ef33073ee87",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2190, -0.0496,  0.0746, -0.0122, -0.1212, -0.3471, -0.2582,  0.0699,\n",
       "         -0.2685,  0.2092],\n",
       "        [ 0.2571, -0.0107, -0.1650,  0.1359,  0.0424, -0.3832,  0.1630,  0.0675,\n",
       "         -0.6183,  0.1319],\n",
       "        [ 0.1358, -0.1044,  0.0111,  0.1517,  0.1973, -0.2435, -0.1137,  0.2700,\n",
       "         -0.1633, -0.0345]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "tenemos 10 salidas para 128 imágenes (lote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/700/0*fr4sfcnDc0KsXA8P.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]]\n",
      "[3 3 3]\n",
      "[3 6]\n",
      "[1 2]\n",
      "[1 1 1]\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,1,1], [2,2,2]])\n",
    "print(a)\n",
    "print(a.sum(axis=0))\n",
    "print(a.sum(axis=1))\n",
    "print(a[:,0])\n",
    "print(a[0])\n",
    "print(a[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(F.softmax(outputs, dim=1).sum(axis=1))\n",
    "print(\"\\n\")\n",
    "print(F.softmax(outputs, dim=0).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3neZ1YGfTTZ",
    "outputId": "4e499c5c-6c61-40f0-f05b-6f8d46f0e61a"
   },
   "outputs": [],
   "source": [
    "# Apply softmax for each output row\n",
    "probs = F.softmax(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1284, 0.0981, 0.1111, 0.1019, 0.0914, 0.0729, 0.0797, 0.1106, 0.0788,\n",
       "        0.1271], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3neZ1YGfTTZ",
    "outputId": "4e499c5c-6c61-40f0-f05b-6f8d46f0e61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.1284, 0.0981, 0.1111, 0.1019, 0.0914, 0.0729, 0.0797, 0.1106, 0.0788,\n",
      "         0.1271],\n",
      "        [0.1303, 0.0997, 0.0854, 0.1154, 0.1051, 0.0687, 0.1186, 0.1078, 0.0543,\n",
      "         0.1149]])\n",
      "Sum:  0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Finalmente, podemos determinar la etiqueta predicha para cada imagen simplemente eligiendo el índice del elemento con la probabilidad más alta en cada fila de salida. Podemos hacer esto usando `torch.max`, que devuelve el elemento más grande de cada fila y el índice correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o9a4hAufTTZ",
    "outputId": "ffe611f9-df20-4bbc-fb1c-6178faac90cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0])\n",
      "tensor([0.1284, 0.1303], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs[:2], dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o9a4hAufTTZ",
    "outputId": "ffe611f9-df20-4bbc-fb1c-6178faac90cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 7, 3, 7, 3, 0, 3, 6, 3, 9, 3, 0, 7, 6, 7, 7, 9, 0, 3, 3, 9, 9, 3,\n",
      "        9, 7, 3, 9, 7, 3, 0, 3, 9, 7, 7, 0, 0, 3, 0, 3, 3, 0, 7, 6, 7, 0, 7, 4,\n",
      "        0, 3, 9, 9, 9, 0, 0, 0, 3, 0, 6, 9, 9, 7, 3, 9, 3, 6, 9, 3, 9, 0, 0, 3,\n",
      "        9, 0, 6, 0, 3, 9, 6, 7, 9, 0, 6, 3, 9, 7, 0, 3, 3, 3, 3, 6, 0, 0, 9, 3,\n",
      "        7, 4, 9, 3, 6, 0, 7, 7, 6, 6, 9, 7, 0, 0, 9, 7, 0, 9, 7, 0, 9, 3, 0, 9,\n",
      "        6, 9, 0, 6, 0, 0, 7, 7])\n",
      "tensor([0.1284, 0.1303, 0.1279, 0.1646, 0.1446, 0.1247, 0.1309, 0.1304, 0.1168,\n",
      "        0.1262, 0.1311, 0.1348, 0.1226, 0.1286, 0.1290, 0.1308, 0.1365, 0.1402,\n",
      "        0.1334, 0.1496, 0.1278, 0.1408, 0.1220, 0.1240, 0.1402, 0.1419, 0.1375,\n",
      "        0.1193, 0.1208, 0.1503, 0.1325, 0.1407, 0.1487, 0.1294, 0.1537, 0.1301,\n",
      "        0.1318, 0.1449, 0.1186, 0.1189, 0.1378, 0.1347, 0.1287, 0.1297, 0.1450,\n",
      "        0.1541, 0.1425, 0.1224, 0.1235, 0.1480, 0.1701, 0.1385, 0.1539, 0.1290,\n",
      "        0.1370, 0.1264, 0.1346, 0.1346, 0.1262, 0.1475, 0.1373, 0.1309, 0.1403,\n",
      "        0.1578, 0.1289, 0.1213, 0.1445, 0.1387, 0.1568, 0.1280, 0.1352, 0.1356,\n",
      "        0.1682, 0.1351, 0.1312, 0.1339, 0.1321, 0.1440, 0.1374, 0.1310, 0.1439,\n",
      "        0.1330, 0.1305, 0.1162, 0.1504, 0.1415, 0.1531, 0.1209, 0.1504, 0.1402,\n",
      "        0.1446, 0.1217, 0.1424, 0.1462, 0.1189, 0.1466, 0.1504, 0.1200, 0.1485,\n",
      "        0.1296, 0.1316, 0.1305, 0.1234, 0.1463, 0.1296, 0.1400, 0.1681, 0.1365,\n",
      "        0.1533, 0.1225, 0.1502, 0.1296, 0.1475, 0.1105, 0.1503, 0.1335, 0.1257,\n",
      "        0.1256, 0.1490, 0.1302, 0.1212, 0.1468, 0.1242, 0.1323, 0.1152, 0.1266,\n",
      "        0.1411, 0.1308], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Esto está mal, solo para probar la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o9a4hAufTTZ",
    "outputId": "ffe611f9-df20-4bbc-fb1c-6178faac90cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1303, 0.0997, 0.1111, 0.1154, 0.1051, 0.0729, 0.1186, 0.1106, 0.0788,\n",
      "        0.1271], grad_fn=<MaxBackward0>)\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "aux1, aux2 = torch.max(probs[:2], dim=0)\n",
    "print(aux1)\n",
    "print(aux2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Los números impresos arriba son las etiquetas predichas para el primer lote de imágenes de entrenamiento. Vamos a compararlos con las etiquetas reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsT85mhffTTZ",
    "outputId": "bc61f2f3-3213-4538-9733-579ac3cd8cef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 6, 6, 6, 1, 8, 1, 7, 9, 7, 4, 6, 5, 1, 5, 5, 0, 8, 2, 1, 0, 6, 1,\n",
       "        7, 5, 4, 4, 2, 1, 4, 7, 9, 3, 3, 0, 8, 2, 4, 8, 6, 9, 9, 1, 5, 2, 6, 7,\n",
       "        9, 7, 0, 0, 0, 6, 8, 6, 4, 5, 5, 0, 0, 8, 6, 0, 5, 1, 0, 9, 0, 4, 6, 1,\n",
       "        0, 4, 9, 6, 3, 0, 1, 5, 4, 6, 1, 5, 0, 8, 6, 1, 8, 4, 8, 7, 0, 3, 3, 2,\n",
       "        6, 1, 7, 4, 1, 6, 3, 3, 1, 1, 0, 3, 3, 1, 0, 8, 4, 7, 8, 5, 5, 5, 4, 7,\n",
       "        1, 7, 8, 1, 4, 0, 0, 6])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La mayoría de las etiquetas previstas son diferentes de las etiquetas reales. Esto se debe a que comenzamos con pesos y sesgos inicializados aleatoriamente. Necesitamos entrenar el modelo, es decir, ajustar los pesos usando el gradiente descendente para hacer mejores predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Métrica de evaluación y función de pérdida\n",
    "Al igual que con la regresión lineal, necesitamos una forma de evaluar qué tan bien está funcionando nuestro modelo. Una forma natural de hacer esto sería encontrar el porcentaje de etiquetas que se predijeron correctamente, es decir,. la **precisión** de las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wG0IBPVMp3UE",
    "outputId": "ae1ba45f-bfc3-4e35-d4d4-0a0ecefdb919"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2190, -0.0496,  0.0746, -0.0122, -0.1212, -0.3471, -0.2582,  0.0699,\n",
       "         -0.2685,  0.2092],\n",
       "        [ 0.2571, -0.0107, -0.1650,  0.1359,  0.0424, -0.3832,  0.1630,  0.0675,\n",
       "         -0.6183,  0.1319]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHgMnCL4qGDt",
    "outputId": "90f55abc-6496-46bf-faaf-edf7fe22fac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHgMnCL4qGDt",
    "outputId": "90f55abc-6496-46bf-faaf-edf7fe22fac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(preds == labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "11hDOTR1fTTZ"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El operador `==` realiza una comparación por elementos de dos tensores con la misma forma y devuelve un tensor de la misma forma, que contiene `Verdadero` para elementos desiguales y `Falso` para elementos iguales. Pasar el resultado a `torch.sum` devuelve el número de etiquetas que se predijeron correctamente. Finalmente, dividimos por el número total de imágenes para obtener la precisión.\n",
    "\n",
    "Tenga en cuenta que no necesitamos aplicar softmax a las salidas ya que sus resultados tienen el mismo orden relativo. Esto se debe a que `e^x` es una función creciente, es decir, si `y1 > y2`, entonces `e^y1 > e^y2`. Lo mismo ocurre después de promediar los valores para obtener el softmax.\n",
    "\n",
    "Calculemos la precisión del modelo actual en el primer lote de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwVAxpZXfTTZ",
    "outputId": "d7a56d59-a6b9-4209-c7f3-fba67d0b268c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0391)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEyaxGIBrNFm",
    "outputId": "2dab272b-2825-470a-91d6-30e39c5b1a43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1284, 0.0981, 0.1111,  ..., 0.1106, 0.0788, 0.1271],\n",
       "        [0.1303, 0.0997, 0.0854,  ..., 0.1078, 0.0543, 0.1149],\n",
       "        [0.1118, 0.0880, 0.0987,  ..., 0.1279, 0.0829, 0.0943],\n",
       "        ...,\n",
       "        [0.1266, 0.0866, 0.0864,  ..., 0.1130, 0.0892, 0.1248],\n",
       "        [0.1014, 0.0808, 0.0896,  ..., 0.1411, 0.1059, 0.1175],\n",
       "        [0.1301, 0.0771, 0.0879,  ..., 0.1308, 0.0636, 0.1095]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La precisión es una forma excelente para nosotros (los humanos) de evaluar el modelo. Sin embargo, no podemos usarla como una función de pérdida para optimizar nuestro modelo mediante el descenso de gradiente por las siguientes razones:\n",
    "\n",
    "1. No es una función diferenciable. `torch.max` y `==` son operaciones no continuas y no diferenciables, por lo que no podemos usar la precisión para calcular gradientes con los pesos y sesgos.\n",
    "\n",
    "2. No tiene en cuenta las probabilidades reales predichas por el modelo, por lo que no puede proporcionar suficiente retroalimentación para mejoras incrementales.\n",
    "\n",
    "Por estas razones, la precisión se usa a menudo como una **métrica de evaluación** para la clasificación, pero no como una función de pérdida. Una función de pérdida comúnmente utilizada para problemas de clasificación es la **entropía cruzada**, que tiene la siguiente fórmula:\n",
    "\n",
    "![entropía cruzada](https://i.imgur.com/VDRDl1D.png)\n",
    "\n",
    "Si bien parece complicado, en realidad es bastante simple:\n",
    "\n",
    "*Para cada fila de salida, elija la probabilidad pronosticada para la etiqueta correcta. Por ejemplo, si las probabilidades pronosticadas para una imagen son `[0.1, 0.3, 0.2, ...]` y la etiqueta correcta es `1`, elegimos el elemento correspondiente `0.3` e ignoramos el resto.* Luego, toma el [logaritmo](https://en.wikipedia.org/wiki/Logarithm) de la probabilidad seleccionada. Si la probabilidad es alta, es decir, cercana a 1, entonces su logaritmo es un valor negativo muy pequeño, cercano a 0. Y si la probabilidad es baja (cerca de 0), entonces el logaritmo es un valor negativo muy grande. También multiplicamos el resultado por -1, lo que da como resultado un gran valor positivo de la pérdida por malas predicciones.\n",
    "\n",
    "![](https://www.intmath.com/blog/wp-content/images/2019/05/log10.png)\n",
    "\n",
    "* Finalmente, tome el promedio de la entropía cruzada en todas las filas de salida para obtener la pérdida total de un lote de datos.\n",
    "\n",
    "A diferencia de la precisión, la entropía cruzada es una función continua y diferenciable. También proporciona retroalimentación útil para mejoras incrementales en el modelo (una probabilidad ligeramente mayor para la etiqueta correcta conduce a una pérdida menor). Estos dos factores hacen que la entropía cruzada sea una mejor opción para la función de pérdida.\n",
    "\n",
    "Como era de esperar, PyTorch proporciona una implementación de entropía cruzada eficiente y compatible con tensores como parte del paquete `torch.nn.funcional`. Además, también realiza softmax internamente, por lo que podemos pasar directamente los resultados del modelo sin convertirlos en probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkkyxKR1r8nm",
    "outputId": "7e13a471-37df-4549-e02d-1a807971176d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2190, -0.0496,  0.0746,  ...,  0.0699, -0.2685,  0.2092],\n",
       "        [ 0.2571, -0.0107, -0.1650,  ...,  0.0675, -0.6183,  0.1319],\n",
       "        [ 0.1358, -0.1044,  0.0111,  ...,  0.2700, -0.1633, -0.0345],\n",
       "        ...,\n",
       "        [ 0.2713, -0.1081, -0.1103,  ...,  0.1579, -0.0792,  0.2572],\n",
       "        [-0.0306, -0.2571, -0.1545,  ...,  0.3003,  0.0127,  0.1174],\n",
       "        [ 0.3300, -0.1934, -0.0621,  ...,  0.3359, -0.3857,  0.1580]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "OqWUQpANfTTZ"
   },
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuESG0hbfTTZ",
    "outputId": "10804014-e2d7-4b7c-c56d-b56761ffb464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3799, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Sabemos que la entropía cruzada es el logaritmo negativo de la probabilidad predicha de la etiqueta correcta promediada sobre todas las muestras de entrenamiento. Por lo tanto, una forma de interpretar el número resultante, p. `2.23` es mirar `e^-2.23` que es alrededor de `0.1` como la probabilidad predicha de la etiqueta correcta, en promedio. *Cuanto menor sea la pérdida, mejor será el modelo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09255626006372962\n",
      "4.5399929762484854e-05\n",
      "0.1353352832366127\n",
      "0.36787944117144233\n",
      "0.8187307530779818\n"
     ]
    }
   ],
   "source": [
    "print(math.exp(-loss.item()))\n",
    "print(math.exp(-10))\n",
    "print(math.exp(-2))\n",
    "print(math.exp(-1))\n",
    "print(math.exp(-0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Entrenando al modelo\n",
    "\n",
    "Ahora que hemos definido los cargadores de datos, el modelo, la función de pérdida y el optimizador, estamos listos para entrenar el modelo. El proceso de entrenamiento es idéntico a la regresión lineal, con la adición de una \"fase de validación\" para evaluar el modelo en cada época. Así es como se ve en pseudocódigo:\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    # Fase de entrenamiento\n",
    "    for batch in train_loader:\n",
    "        # Generar predicciones\n",
    "        # Calcular pérdida\n",
    "        # Calcular gradientes\n",
    "        # Actualizar pesos\n",
    "        # Restablecer gradientes\n",
    "    \n",
    "    # Fase de validación\n",
    "    for batch in val_loader:\n",
    "        # Generar predicciones\n",
    "        # Calcular pérdida\n",
    "        # Calcular métricas (precisión, etc.)\n",
    "    # Calcule la pérdida de validación promedio y las métricas\n",
    "    \n",
    "    # Época de registro, pérdida y métricas para inspección\n",
    "```\n",
    "\n",
    "Algunas partes del ciclo de entrenamiento son específicas del problema específico que estamos resolviendo (por ejemplo, función de pérdida, métricas, etc.), mientras que otras son genéricas y se pueden aplicar a cualquier problema de aprendizaje profundo.\n",
    "\n",
    "Incluiremos las partes independientes del problema dentro de una función llamada `fit`, que se usará para entrenar el modelo. Las partes específicas del problema se implementarán agregando nuevos métodos a la clase `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "1cv7a7ukfTTZ"
   },
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La función `fit` registra la pérdida de validación y la métrica de cada época. Devuelve un historial de la formación, útil para la depuración y visualización.\n",
    "\n",
    "Las configuraciones como el tamaño del lote, la tasa de aprendizaje, etc. (llamadas hiperparámetros), deben elegirse con anticipación mientras se entrenan los modelos de aprendizaje automático. Elegir los hiperparámetros correctos es fundamental para entrenar un modelo razonablemente preciso en un tiempo razonable. Es un área activa de investigación y experimentación en aprendizaje automático. Siéntase libre de probar diferentes tasas de aprendizaje y ver cómo afecta el proceso de capacitación.\n",
    "\n",
    "\n",
    "Definamos la función `evaluate`, utilizada en la fase de validación de `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "N064K5H9ujtQ"
   },
   "outputs": [],
   "source": [
    "l1 = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5W8a_hmFujqa",
    "outputId": "ec0a2dfb-9b6a-48f7-86b3-5ee852af0072"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = [x*2 for x in l1]\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgWjXs5mfTTZ"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Finalmente, redefinamos la clase `MnistModel` para incluir los métodos adicionales `training_step`, `validation_step`, `validation_epoch_end` y `epoch_end` usados ​​por `fit` y `evaluar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbJN7ICGfTTZ"
   },
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Antes de entrenar el modelo, veamos cómo funciona el modelo en el conjunto de validación con el conjunto inicial de pesos y sesgos inicializados aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNUBy9qTfTTZ",
    "outputId": "774e8e20-68c2-4a30-ada9-4e0df23285ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_acc': 0.10977056622505188, 'val_loss': 2.3349318504333496}"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "La precisión inicial es de alrededor del 10 %, lo que cabría esperar de un modelo inicializado aleatoriamente (ya que tiene una probabilidad de 1 en 10 de obtener una etiqueta correcta adivinando aleatoriamente).\n",
    "\n",
    "Ahora estamos listos para entrenar el modelo. Entrenemos durante cinco épocas y veamos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQRahsa6fTTZ",
    "outputId": "864bc5a4-c235-458c-e316-764cae8bb587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9552, val_acc: 0.6153\n",
      "Epoch [1], val_loss: 1.6839, val_acc: 0.7270\n",
      "Epoch [2], val_loss: 1.4819, val_acc: 0.7587\n",
      "Epoch [3], val_loss: 1.3295, val_acc: 0.7791\n",
      "Epoch [4], val_loss: 1.2124, val_acc: 0.7969\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "¡Es un gran resultado! Con solo 5 épocas de entrenamiento, nuestro modelo ha alcanzado una precisión de más del 80 % en el conjunto de validación. Veamos si podemos mejorar eso entrenando para algunas épocas más. Intente cambiar las tasas de aprendizaje y el número de épocas en cada una de las celdas a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJvMd9CufTTa",
    "outputId": "c15d370b-4d0b-4644-8ddb-965117f03ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.1205, val_acc: 0.8081\n",
      "Epoch [1], val_loss: 1.0467, val_acc: 0.8165\n",
      "Epoch [2], val_loss: 0.9862, val_acc: 0.8237\n",
      "Epoch [3], val_loss: 0.9359, val_acc: 0.8281\n",
      "Epoch [4], val_loss: 0.8934, val_acc: 0.8322\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hON9V8GBfTTa",
    "outputId": "08129337-153d-4eaa-eb13-1ee86089d60f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.8569, val_acc: 0.8371\n",
      "Epoch [1], val_loss: 0.8254, val_acc: 0.8393\n",
      "Epoch [2], val_loss: 0.7977, val_acc: 0.8420\n",
      "Epoch [3], val_loss: 0.7733, val_acc: 0.8447\n",
      "Epoch [4], val_loss: 0.7515, val_acc: 0.8470\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5ALmwZjfTTa",
    "outputId": "340bd5b7-e290-4cbe-9a62-f5901f09bf63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7320, val_acc: 0.8494\n",
      "Epoch [1], val_loss: 0.7144, val_acc: 0.8512\n",
      "Epoch [2], val_loss: 0.6985, val_acc: 0.8528\n",
      "Epoch [3], val_loss: 0.6839, val_acc: 0.8543\n",
      "Epoch [4], val_loss: 0.6706, val_acc: 0.8557\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Si bien la precisión continúa aumentando a medida que entrenamos para más épocas, las mejoras se reducen con cada época. Visualicemos esto usando un gráfico de líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "swNRQSa0fTTa",
    "outputId": "fa726f42-fc0a-4a74-e660-e105192777b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Está bastante claro en la imagen de arriba que el modelo probablemente no cruzará el umbral de precisión del 90%, incluso después de entrenar durante mucho tiempo. Una posible razón de esto es que la tasa de aprendizaje puede ser demasiado alta. Los parámetros del modelo pueden estar \"rebotando\" alrededor del conjunto óptimo de parámetros para la pérdida más baja. Puede intentar reducir la tasa de aprendizaje y el entrenamiento durante algunas épocas más para ver si ayuda.\n",
    "\n",
    "La razón más probable de que **el modelo simplemente no sea lo suficientemente potente**. Si recuerdas nuestra hipótesis inicial, hemos asumido que la salida (en este caso las probabilidades de clase) es una **función lineal** de la entrada (intensidades de píxeles), obtenida al realizar una multiplicación matricial con los pesos matriz y sumando el sesgo. Esta es una suposición bastante débil, ya que es posible que en realidad no exista una relación lineal entre las intensidades de los píxeles en una imagen y el dígito que representa. Si bien funciona razonablemente bien para un conjunto de datos simple como MNIST (lo que nos brinda una precisión del 85 %), necesitamos modelos más sofisticados que puedan capturar relaciones no lineales entre los píxeles de la imagen y las etiquetas para tareas complejas como reconocer objetos cotidianos, animales, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Pruebas con imágenes individuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Si bien hasta ahora hemos estado rastreando la precisión general de un modelo, también es una buena idea observar los resultados del modelo en algunas imágenes de muestra. Probemos nuestro modelo con algunas imágenes del conjunto de datos de prueba predefinido de 10000 imágenes. Comenzamos recreando el conjunto de datos de prueba con la transformación `ToTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqcHUQK3fTTa"
   },
   "outputs": [],
   "source": [
    "# Define test dataset\n",
    "test_dataset = MNIST(root='data/', \n",
    "                     train=False,\n",
    "                     transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Aquí hay una imagen de muestra del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "yzEdFHq6fTTa",
    "outputId": "4e0faa6b-789f-46c4-9c25-27441665844c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Definamos una función auxiliar `predict_image`, que devuelve la etiqueta pronosticada para un solo tensor de imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FfeYQaOfTTa"
   },
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "`img.unsqueeze` simplemente agrega otra dimensión al comienzo del tensor 1x28x28, convirtiéndolo en un tensor 1x1x28x28, que el modelo ve como un lote que contiene una sola imagen.\n",
    "\n",
    "Vamos a probarlo con algunas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "sPNcp52ifTTa",
    "outputId": "3f209ee0-d70e-4631-bc01-bf67165a8347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 , Predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Nrj-w1COfTTa",
    "outputId": "63571287-3d8e-430f-c146-38167c355d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 , Predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EPrF7gB4fTTb",
    "outputId": "f315cb3c-7eb9-47aa-95ee-c4e917f28bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9 , Predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[193]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "eetYJdoFfTTb",
    "outputId": "da8c6613-355a-4479-cb49-574236d2de5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 , Predicted: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[1839]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Identificar dónde funciona mal nuestro modelo puede ayudarnos a mejorar el modelo mediante la recopilación de más datos de entrenamiento, el aumento o la disminución de la complejidad del modelo y el cambio de los parámetros exagerados.\n",
    "\n",
    "Como paso final, veamos también la pérdida general y la precisión del modelo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bdjaeG-fTTb",
    "outputId": "c4479ab7-ee92-46d4-ac0d-9217d6873f43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_acc': 0.86083984375, 'val_loss': 0.6424765586853027}"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Esperamos que esto sea similar a la precisión/pérdida en el conjunto de validación. De lo contrario, es posible que necesitemos un mejor conjunto de validación que tenga datos y una distribución similares a los del conjunto de prueba (que a menudo proviene de datos del mundo real)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqC3HEZJfTTb",
    "lang": "en"
   },
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xESC-106fTTb"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "El método `.state_dict` devuelve un `OrderedDict` que contiene todas las matrices de ponderación y sesgo asignadas a los atributos correctos del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elbp48SCfTTb",
    "outputId": "84f92a49-910c-4466-8b96-ab267026f3ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0057,  0.0222, -0.0220,  ..., -0.0021, -0.0115, -0.0308],\n",
       "                      [-0.0353,  0.0083, -0.0307,  ...,  0.0345, -0.0087,  0.0200],\n",
       "                      [ 0.0104,  0.0158,  0.0225,  ...,  0.0255,  0.0227, -0.0346],\n",
       "                      ...,\n",
       "                      [-0.0097, -0.0173, -0.0154,  ...,  0.0025, -0.0274, -0.0276],\n",
       "                      [ 0.0272, -0.0156,  0.0029,  ...,  0.0217,  0.0286, -0.0114],\n",
       "                      [-0.0018, -0.0293, -0.0191,  ..., -0.0297,  0.0291,  0.0212]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0322,  0.1078, -0.0008, -0.0159,  0.0346,  0.0235, -0.0047,  0.0277,\n",
       "                      -0.0684, -0.0356]))])"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Para cargar los pesos del modelo, podemos instanciar un nuevo objeto de la clase `MnistModel` y usar el método `.load_state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO666r7_1rbW"
   },
   "outputs": [],
   "source": [
    "model2 = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUvYwe4Q1six",
    "outputId": "63adc692-57b8-458d-a926-dc969bf491db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0168, -0.0088, -0.0010,  ..., -0.0233,  0.0253,  0.0161],\n",
       "                      [-0.0139, -0.0039,  0.0011,  ...,  0.0178, -0.0125, -0.0090],\n",
       "                      [-0.0341, -0.0001,  0.0089,  ..., -0.0282,  0.0181,  0.0251],\n",
       "                      ...,\n",
       "                      [-0.0274, -0.0289, -0.0180,  ..., -0.0197, -0.0173,  0.0262],\n",
       "                      [ 0.0318,  0.0125,  0.0178,  ..., -0.0192,  0.0083, -0.0032],\n",
       "                      [-0.0264, -0.0261,  0.0058,  ..., -0.0005,  0.0135,  0.0287]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0014, -0.0254, -0.0085, -0.0081, -0.0333,  0.0109, -0.0128, -0.0342,\n",
       "                       0.0204,  0.0232]))])"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHac9s6e1vyS",
    "outputId": "bd0ef06a-ded0-4d91-c0a1-620b310bc89e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_acc': 0.08339843899011612, 'val_loss': 2.325232744216919}"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvR1g8ggfTTb",
    "outputId": "2cb76bb5-b26a-472c-d2b3-3f872ebb8c7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0057,  0.0222, -0.0220,  ..., -0.0021, -0.0115, -0.0308],\n",
       "                      [-0.0353,  0.0083, -0.0307,  ...,  0.0345, -0.0087,  0.0200],\n",
       "                      [ 0.0104,  0.0158,  0.0225,  ...,  0.0255,  0.0227, -0.0346],\n",
       "                      ...,\n",
       "                      [-0.0097, -0.0173, -0.0154,  ...,  0.0025, -0.0274, -0.0276],\n",
       "                      [ 0.0272, -0.0156,  0.0029,  ...,  0.0217,  0.0286, -0.0114],\n",
       "                      [-0.0018, -0.0293, -0.0191,  ..., -0.0297,  0.0291,  0.0212]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0322,  0.1078, -0.0008, -0.0159,  0.0346,  0.0235, -0.0047,  0.0277,\n",
       "                      -0.0684, -0.0356]))])"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Solo como una verificación de cordura, verifiquemos que este modelo tenga la misma pérdida y precisión en el conjunto de prueba que antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UynZ4aSLfTTb",
    "outputId": "ed0184c3-6239-4610-b707-09b2e4cbd49d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_acc': 0.86083984375, 'val_loss': 0.6424765586853027}"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model2, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Como paso final, podemos guardar y confirmar nuestro trabajo utilizando la biblioteca `jovian`. Junto con el cuaderno, también podemos acoplar las pesas de nuestro modelo entrenado, para poder utilizarlo más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "## Otras lecturas\n",
    "\n",
    "\n",
    "Aquí hay algunas referencias para leer más:\n",
    "*Para un tratamiento más matemático, consulte el popular curso [Aprendizaje automático](https://www.coursera.org/lecture/machine-learning/classification-wlPeP) en Coursera. La mayoría de las imágenes utilizadas en esta serie de tutoriales han sido tomadas de este curso.* El ciclo de entrenamiento definido en este cuaderno se inspiró en [Cuadernos de desarrollo de FastAI](https://github.com/fastai/fastai_docs/blob/master/dev_nb/001a_nn_basics.ipynb) que contienen una gran cantidad de otras cosas útiles si puede leer y comprender el código.\n",
    "* Para una inmersión profunda en softmax y entropía cruzada, consulte [esta publicación de blog en DeepNotes] (https://deepnotes.io/softmax-crossentropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBom6So2kj2j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "03-logistic-regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
