{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb196ad",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb\" target=\"_parent\"><img src=\"https:// colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/></a>\n",
    "\n",
    "[Ver código fuente](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb) | [Ver diapositivas](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/04_pytorch_custom_datasets.pdf) | [Ver vídeo tutorial](https://youtu.be/Z_ikDlimN6A?t=71010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6abc1",
   "metadata": {},
   "source": [
    "# 04. Conjuntos de datos personalizados de PyTorch\n",
    "\n",
    "En el último cuaderno, [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/), analizamos cómo crear modelos de visión por computadora en un conjunto de datos integrado en PyTorch (FashionMNIST).\n",
    "\n",
    "Los pasos que tomamos son similares en muchos problemas diferentes del aprendizaje automático.\n",
    "\n",
    "Encuentre un conjunto de datos, convierta el conjunto de datos en números, cree un modelo (o encuentre un modelo existente) para encontrar patrones en esos números que puedan usarse para la predicción.\n",
    "\n",
    "PyTorch tiene muchos conjuntos de datos integrados que se utilizan para una gran cantidad de puntos de referencia de aprendizaje automático; sin embargo, a menudo querrás usar tu propio **conjunto de datos personalizado**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c74a9",
   "metadata": {},
   "source": [
    "## ¿Qué es un conjunto de datos personalizado?\n",
    "\n",
    "Un **conjunto de datos personalizado** es una colección de datos relacionados con un problema específico en el que estás trabajando.\n",
    "\n",
    "En esencia, un **conjunto de datos personalizado** puede estar compuesto por casi cualquier cosa.\n",
    "\n",
    "Por ejemplo, si estuviéramos creando una aplicación de clasificación de imágenes de alimentos como [Nutrify](https://nutrify.app), nuestro conjunto de datos personalizado podrían ser imágenes de alimentos.\n",
    "\n",
    "O si intentáramos crear un modelo para clasificar si una reseña basada en texto en un sitio web fue positiva o negativa, nuestro conjunto de datos personalizado podría ser ejemplos de reseñas de clientes existentes y sus calificaciones.\n",
    "\n",
    "O si intentáramos crear una aplicación de clasificación de sonido, nuestro conjunto de datos personalizado podría ser muestras de sonido junto con sus etiquetas de muestra.\n",
    "\n",
    "O si intentáramos crear un sistema de recomendación para los clientes que compran cosas en nuestro sitio web, nuestro conjunto de datos personalizado podría ser ejemplos de productos que otras personas han comprado.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pytorch-domain-libraries.png\" alt=\"Se pueden usar diferentes bibliotecas de dominio de pytorch para PyTorch específico problemas\" ancho=1000/>\n",
    "\n",
    "*PyTorch incluye muchas funciones existentes para cargar en varios conjuntos de datos personalizados en [`TorchVision`](https://pytorch.org/vision/stable/index.html), [`TorchText`](https://pytorch.org /text/stable/index.html), [`TorchAudio`](https://pytorch.org/audio/stable/index.html) y [`TorchRec`](https://pytorch.org/torchrec/) bibliotecas de dominio.*\n",
    "\n",
    "Pero a veces estas funciones existentes pueden no ser suficientes.\n",
    "\n",
    "En ese caso, siempre podemos crear una subclase de `torch.utils.data.Dataset` y personalizarla a nuestro gusto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea81b0",
   "metadata": {},
   "source": [
    "## Qué vamos a cubrir\n",
    "\n",
    "Aplicaremos el flujo de trabajo de PyTorch que cubrimos en el [cuaderno 01] (https://www.learnpytorch.io/01_pytorch_workflow/) y el [cuaderno 02] (https://www.learnpytorch.io/02_pytorch_classification/) a un problema de visión por computadora.\n",
    "\n",
    "Pero en lugar de utilizar un conjunto de datos PyTorch incorporado, usaremos nuestro propio conjunto de datos de imágenes de pizza, bistec y sushi.\n",
    "\n",
    "El objetivo será cargar estas imágenes y luego construir un modelo para entrenarlas y predecirlas.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pytorch-food-vision-layout.png\" alt=\"construyendo un canal para cargar imágenes de alimentos y luego construir un modelo de pytorch para clasificar esas imágenes de comida\" ancho=800 />\n",
    "\n",
    "*Qué vamos a construir. Usaremos `torchvision.datasets` así como nuestra propia clase `Dataset` personalizada para cargar imágenes de alimentos y luego construiremos un modelo de visión por computadora PyTorch para, con suerte, poder clasificarlas.*\n",
    "\n",
    "Específicamente, cubriremos:\n",
    "\n",
    "| **Tema** | **Contenido** |\n",
    "| ----- | ----- |\n",
    "| **0. Importación de PyTorch y configuración de código independiente del dispositivo** | Carguemos PyTorch y luego sigamos las mejores prácticas para configurar nuestro código para que sea independiente del dispositivo.  |\n",
    "| **1. Obtener datos** | Usaremos nuestro propio **conjunto de datos personalizado** de imágenes de pizza, bistec y sushi. |\n",
    "| **2. Conviértete en uno con los datos (preparación de datos)** | Al comienzo de cualquier problema nuevo de aprendizaje automático, es fundamental comprender los datos con los que se está trabajando. Aquí tomaremos algunos pasos para determinar qué datos tenemos. |\n",
    "| **3. Transformación de datos** |A menudo, los datos que obtienes no estarán 100% listos para usar con un modelo de aprendizaje automático. Aquí veremos algunos pasos que podemos seguir para *transformar* nuestras imágenes para que estén listas para ser usado con un modelo. | \n",
    "| **4. Cargando datos con `ImageFolder` (opción 1)** | PyTorch tiene muchas funciones de carga de datos integradas para tipos de datos comunes. `ImageFolder` es útil si nuestras imágenes están en formato de clasificación de imágenes estándar. |\n",
    "| **5. Cargando datos de imagen con un `Conjunto de datos`** personalizado | ¿Qué pasaría si PyTorch no tuviera una función incorporada para cargar datos? Aquí es donde podemos crear nuestra propia subclase personalizada de `torch.utils.data.Dataset`. |\n",
    "| **6. Otras formas de transformaciones (aumento de datos)** | El aumento de datos es una técnica común para ampliar la diversidad de sus datos de entrenamiento. Aquí exploraremos algunas de las funciones de aumento de datos integradas de `torchvision`. |\n",
    "| **7. Modelo 0: TinyVGG sin aumento de datos** | En esta etapa, tendremos nuestros datos listos, construyamos un modelo capaz de ajustarlos. También crearemos algunas funciones de entrenamiento y prueba para entrenar y evaluar nuestro modelo. |\n",
    "| **8. Explorando curvas de pérdidas** | Las curvas de pérdida son una excelente manera de ver cómo su modelo se entrena o mejora con el tiempo. También son una buena manera de ver si su modelo está **bajo ajuste** o **sobreajuste**. |\n",
    "| **9. Modelo 1: TinyVGG con aumento de datos** | Hasta ahora, hemos probado un modelo *sin*, ¿qué tal si probamos uno *con* aumento de datos? |\n",
    "| **10. Comparar resultados de modelos** | Comparemos las curvas de pérdida de nuestros diferentes modelos y veamos cuál funcionó mejor y analicemos algunas opciones para mejorar el rendimiento. |\n",
    "| **11. Hacer una predicción en una imagen personalizada** | Nuestro modelo está entrenado en un conjunto de datos de imágenes de pizza, bistec y sushi. En esta sección cubriremos cómo usar nuestro modelo entrenado para predecir en una imagen *fuera* de nuestro conjunto de datos existente. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271f3be",
   "metadata": {},
   "source": [
    "## ¿Dónde puedes conseguir ayuda?\n",
    "\n",
    "Todos los materiales de este curso [en vivo en GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
    "\n",
    "Si tiene problemas, también puede hacer una pregunta en el curso [página de debates de GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
    "\n",
    "Y, por supuesto, está la [documentación de PyTorch](https://pytorch.org/docs/stable/index.html) y los [foros de desarrolladores de PyTorch](https://discuss.pytorch.org/), un lugar muy útil para todo lo relacionado con PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b85a19",
   "metadata": {},
   "source": [
    "## 0. Importación de PyTorch y configuración de código independiente del dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a81a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Nota: este portátil requiere antorcha >= 1.10.0\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e8470",
   "metadata": {},
   "source": [
    "Y ahora sigamos las mejores prácticas y configuremos el código independiente del dispositivo.\n",
    "\n",
    "> **Nota:** Si estás usando Google Colab y aún no tienes una GPU activada, ahora es el momento de activar una a través de `Runtime -> Cambiar tipo de tiempo de ejecución -> Acelerador de hardware -> GPU` . Si hace esto, es probable que su tiempo de ejecución se reinicie y tendrá que ejecutar todas las celdas anteriores yendo a \"Tiempo de ejecución -> Ejecutar antes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar código independiente del dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0860df1",
   "metadata": {},
   "source": [
    "## 1. Obtener datos\n",
    "\n",
    "Lo primero es lo primero que necesitamos algunos datos.\n",
    "\n",
    "Y como todo buen programa de cocina, ya nos tienen preparados algunos datos.\n",
    "\n",
    "Vamos a empezar poco a poco.\n",
    "\n",
    "Porque todavía no buscamos entrenar el modelo más grande ni utilizar el conjunto de datos más grande.\n",
    "\n",
    "El aprendizaje automático es un proceso iterativo: comience poco a poco, haga que algo funcione y aumente cuando sea necesario.\n",
    "\n",
    "Los datos que usaremos son un subconjunto del [conjunto de datos Food101] (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).\n",
    "\n",
    "Food101 es un popular punto de referencia de visión por computadora, ya que contiene 1000 imágenes de 101 tipos diferentes de alimentos, con un total de 101 000 imágenes (75 750 de tren y 25 250 de prueba).\n",
    "\n",
    "¿Puedes pensar en 101 alimentos diferentes?\n",
    "\n",
    "¿Se te ocurre un programa informático para clasificar 101 alimentos?\n",
    "\n",
    "Puedo.\n",
    "\n",
    "¡Un modelo de aprendizaje automático! \n",
    "\n",
    "Específicamente, un modelo de visión por computadora de PyTorch como el que cubrimos en el [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/).\n",
    "\n",
    "Sin embargo, en lugar de 101 clases de comida, comenzaremos con 3: pizza, bistec y sushi.\n",
    "\n",
    "Y en lugar de 1000 imágenes por clase, comenzaremos con un 10% aleatorio (comience poco a poco, aumente cuando sea necesario).\n",
    "\n",
    "Si desea ver de dónde provienen los datos, consulte los siguientes recursos:\n",
    "* Original [conjunto de datos de Food101 y sitio web en papel] (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).\n",
    "* [`torchvision.datasets.Food101`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html): la versión de los datos que descargué para este cuaderno.\n",
    "* [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb): un cuaderno que usé para formatear el conjunto de datos de Food101 para usarlo este cuaderno.\n",
    "* [`data/pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip): el archivo zip de imágenes de pizza, bistec y sushi de Food101 , creado con el cuaderno vinculado anteriormente.\n",
    "\n",
    "Escribamos un código para descargar los datos formateados de GitHub.\n",
    "\n",
    "> **Nota:** El conjunto de datos que vamos a utilizar ha sido formateado previamente para el uso que nos gustaría utilizar. Sin embargo, a menudo tendrás que formatear tus propios conjuntos de datos para cualquier problema en el que estés trabajando. Esta es una práctica habitual en el mundo del aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta de configuración a la carpeta de datos\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# Si la carpeta de imágenes no existe, descárgala y prepárala...\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\") \n",
    "        zip_ref.extractall(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d1fc2",
   "metadata": {},
   "source": [
    "## 2. Conviértete en uno con los datos (preparación de datos)\n",
    "\n",
    "¡Conjunto de datos descargado!\n",
    "\n",
    "Es hora de volverse uno con ello.\n",
    "\n",
    "Este es otro paso importante antes de construir un modelo.\n",
    "\n",
    "Como dijo Abraham Lossfunction...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-abraham-lossfunction.png\" alt=\"tweet de mrdbourke, si tuviera ocho horas para construir un modelo de aprendizaje automático, pasaría las primeras 6 horas preparando mi conjunto de datos\" width=800/>\n",
    "\n",
    "*La preparación de datos es primordial. Antes de construir un modelo, vuélvete uno con los datos. Pregunte: ¿Qué estoy tratando de hacer aquí? Fuente: [@mrdbourke Twitter](https://twitter.com/mrdbourke).*\n",
    "\n",
    "¿Qué es inspeccionar los datos y volverse uno con ellos? \n",
    "\n",
    "Antes de comenzar un proyecto o construir cualquier tipo de modelo, es importante saber con qué datos estás trabajando.\n",
    "\n",
    "En nuestro caso, tenemos imágenes de pizza, bistec y sushi en formato de clasificación de imágenes estándar.\n",
    "\n",
    "El formato de clasificación de imágenes contiene clases separadas de imágenes en directorios separados titulados con un nombre de clase particular.\n",
    "\n",
    "Por ejemplo, todas las imágenes de `pizza` están contenidas en el directorio `pizza/`.\n",
    "\n",
    "Este formato es popular en muchos puntos de referencia de clasificación de imágenes diferentes, incluido [ImageNet](https://www.image-net.org/) (de los conjuntos de datos de puntos de referencia de visión por computadora más populares).\n",
    "\n",
    "Puede ver un ejemplo del formato de almacenamiento a continuación, los números de las imágenes son arbitrarios.\n",
    "\n",
    "```\n",
    "pizza_steak_sushi/ <- carpeta del conjunto de datos general\n",
    "    tren/ <- imágenes de entrenamiento\n",
    "        pizza/ <- nombre de clase como nombre de carpeta\n",
    "            imagen01.jpeg\n",
    "            imagen02.jpeg\n",
    "            ...\n",
    "        bife/\n",
    "            imagen24.jpeg\n",
    "            imagen25.jpeg\n",
    "            ...\n",
    "        Sushi/\n",
    "            imagen37.jpeg\n",
    "            ...\n",
    "    prueba/ <- imágenes de prueba\n",
    "        pizza/\n",
    "            imagen101.jpeg\n",
    "            imagen102.jpeg\n",
    "            ...\n",
    "        bife/\n",
    "            imagen154.jpeg\n",
    "            imagen155.jpeg\n",
    "            ...\n",
    "        Sushi/\n",
    "            imagen167.jpeg\n",
    "            ...\n",
    "```\n",
    "\n",
    "El objetivo será **tomar esta estructura de almacenamiento de datos y convertirla en un conjunto de datos utilizable con PyTorch**.\n",
    "\n",
    "> **Nota:** La estructura de los datos con los que trabaja variará según el problema en el que esté trabajando. Pero la premisa sigue siendo: volverse uno con los datos y luego encontrar la manera de convertirlos en un conjunto de datos compatible con PyTorch.\n",
    "\n",
    "Podemos inspeccionar lo que hay en nuestro directorio de datos escribiendo una pequeña función auxiliar para recorrer cada uno de los subdirectorios y contar los archivos presentes.\n",
    "\n",
    "Para hacerlo, usaremos la [`os.walk()`] incorporada de Python (https://docs.python.org/3/library/os.html#os.walk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15aaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Walks through dir_path returning its contents.\n",
    "  Args:\n",
    "    dir_path (str or pathlib.Path): target directory\n",
    "  \n",
    "  Returns:\n",
    "    A print out of:\n",
    "      number of subdiretories in dir_path\n",
    "      number of images (files) in each subdirectory\n",
    "      name of each subdirectory\n",
    "  \"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a591c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b4dc1",
   "metadata": {},
   "source": [
    "¡Excelente!\n",
    "\n",
    "Parece que tenemos alrededor de 75 imágenes por clase de capacitación y 25 imágenes por clase de prueba.\n",
    "\n",
    "Eso debería ser suficiente para empezar.\n",
    "\n",
    "Recuerde, estas imágenes son subconjuntos del conjunto de datos original de Food101.\n",
    "\n",
    "Puede ver cómo se crearon en el [cuaderno de creación de datos] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb).\n",
    "\n",
    "Mientras estamos en eso, configuremos nuestras rutas de capacitación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar rutas de tren y pruebas.\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e883782",
   "metadata": {},
   "source": [
    "### 2.1 Visualizar una imagen\n",
    "\n",
    "Bien, hemos visto cómo se formatea nuestra estructura de directorios.\n",
    "\n",
    "Ahora, siguiendo el espíritu del explorador de datos, es hora de *¡visualizar, visualizar, visualizar!*\n",
    "\n",
    "Escribamos un código para:\n",
    "1. Obtenga todas las rutas de las imágenes usando [`pathlib.Path.glob()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob) para encontrar todas las archivos que terminan en `.jpg`. \n",
    "2. Elija una ruta de imagen aleatoria usando [`random.choice()`](https://docs.python.org/3/library/random.html#random.choice) de Python.\n",
    "3. Obtenga el nombre de la clase de imagen usando [`pathlib.Path.parent.stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.parent).\n",
    "4. Y como estamos trabajando con imágenes, abriremos la ruta de la imagen aleatoria usando [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image. html#PIL.Image.open) (PIL significa Biblioteca de imágenes de Python).\n",
    "5. Luego mostraremos la imagen e imprimiremos algunos metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Establecer semilla\n",
    "random.seed(42) # <- try changing this and see what happens\n",
    "\n",
    "# 1. Obtenga todas las rutas de las imágenes (* significa \"cualquier combinación\")\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "# 2. Obtener ruta de imagen aleatoria\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# 3. Obtener la clase de imagen a partir del nombre de la ruta (la clase de imagen es el nombre del directorio donde está almacenada la imagen)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# 4. Abrir imagen\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# 5. Imprimir metadatos\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\") \n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc75d4",
   "metadata": {},
   "source": [
    "Podemos hacer lo mismo con [`matplotlib.pyplot.imshow()`](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.imshow.html), excepto que tenemos que convertir la imagen. a una matriz NumPy primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertir la imagen en una matriz\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Trazar la imagen con matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ea9de",
   "metadata": {},
   "source": [
    "## 3. Transformar datos \n",
    "\n",
    "Ahora, ¿qué pasaría si quisiéramos cargar los datos de nuestra imagen en PyTorch?\n",
    "\n",
    "Antes de que podamos usar nuestros datos de imagen con PyTorch, necesitamos:\n",
    "\n",
    "1. Convertirlo en tensores (representaciones numéricas de nuestras imágenes).\n",
    "2. Conviértalo en `torch.utils.data.Dataset` y posteriormente en `torch.utils.data.DataLoader`; los llamaremos `Dataset` y `DataLoader` para abreviar.\n",
    "\n",
    "Hay varios tipos diferentes de conjuntos de datos y cargadores de conjuntos de datos prediseñados para PyTorch, según el problema en el que esté trabajando. \n",
    "\n",
    "| **Espacio problemático** | **Conjuntos de datos y funciones prediseñados** |\n",
    "| ----- | ----- |\n",
    "| **Visión** | [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) |\n",
    "| **Audio** | [`torchaudio.datasets`](https://pytorch.org/audio/stable/datasets.html) |\n",
    "| **Texto** | [`torchtext.datasets`](https://pytorch.org/text/stable/datasets.html) |\n",
    "| **Sistema de recomendación** | [`torchrec.datasets`](https://pytorch.org/torchrec/torchrec.datasets.html) |\n",
    "\n",
    "Dado que estamos trabajando con un problema de visión, veremos `torchvision.datasets` para nuestras funciones de carga de datos, así como [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms .html) para preparar nuestros datos.\n",
    "\n",
    "Importemos algunas bibliotecas base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4795f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f17c22",
   "metadata": {},
   "source": [
    "### 3.1 Transformando datos con `torchvision.transforms`\n",
    "\n",
    "Tenemos carpetas de imágenes, pero antes de poder usarlas con PyTorch, necesitamos convertirlas en tensores.\n",
    "\n",
    "Una de las formas en que podemos hacer esto es usando el módulo `torchvision.transforms`.\n",
    "\n",
    "`torchvision.transforms` contiene muchos métodos prediseñados para formatear imágenes, convertirlas en tensores e incluso manipularlas para **aumento de datos** (la práctica de alterar datos para dificultar el aprendizaje de un modelo, veremos esto más adelante) propósitos. \n",
    "\n",
    "Para adquirir experiencia con `torchvision.transforms`, escribamos una serie de pasos de transformación que:\n",
    "1. Cambie el tamaño de las imágenes usando [`transforms.Resize()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize) (de aproximadamente 512x512 a 64x64 , la misma forma que las imágenes en el [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/)).\n",
    "2. Voltee nuestras imágenes aleatoriamente en horizontal usando [`transforms.RandomHorizontalFlip()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip) (esto podría considerarse una forma de aumento de datos porque cambiará artificialmente los datos de nuestra imagen).\n",
    "3. Convierta nuestras imágenes de una imagen PIL a un tensor de PyTorch usando [`transforms.ToTensor()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms. A Tensor).\n",
    "\n",
    "Podemos compilar todos estos pasos usando [`torchvision.transforms.Compose()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e29c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir transformación para imagen\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3b41f",
   "metadata": {},
   "source": [
    "Ahora que tenemos una composición de transformaciones, escribamos una función para probarlas en varias imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths. \n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "plot_transformed_images(image_path_list, \n",
    "                        transform=data_transform, \n",
    "                        n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b2e97",
   "metadata": {},
   "source": [
    "¡Lindo!\n",
    "\n",
    "Ahora tenemos una manera de convertir nuestras imágenes en tensores usando `torchvision.transforms`.\n",
    "\n",
    "También manipulamos su tamaño y orientación si es necesario (algunos modelos prefieren imágenes de diferentes tamaños y formas).\n",
    "\n",
    "Generalmente, cuanto mayor sea la forma de la imagen, más información podrá recuperar un modelo.\n",
    "\n",
    "Por ejemplo, una imagen de tamaño `[256, 256, 3]` tendrá 16 veces más píxeles que una imagen de tamaño `[64, 64, 3]` (`(256*256*3)/(64*64* 3)=16`).\n",
    "\n",
    "Sin embargo, la desventaja es que más píxeles requieren más cálculos.\n",
    "\n",
    "> **Ejercicio:** Intente comentar una de las transformaciones en `data_transform` y ejecute la función de trazado `plot_transformed_images()` nuevamente, ¿qué sucede?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000608f4",
   "metadata": {},
   "source": [
    "## 4. Opción 1: cargar datos de imagen usando [`ImageFolder`](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder)\n",
    "\n",
    "Muy bien, es hora de convertir nuestros datos de imagen en un \"Conjunto de datos\" capaz de usarse con PyTorch.\n",
    "\n",
    "Dado que nuestros datos están en formato de clasificación de imágenes estándar, podemos usar la clase [`torchvision.datasets.ImageFolder`](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets .Carpeta de imágenes).\n",
    "\n",
    "Donde podemos pasarle la ruta del archivo de un directorio de imágenes de destino, así como una serie de transformaciones que nos gustaría realizar en nuestras imágenes.\n",
    "\n",
    "Probémoslo en nuestras carpetas de datos `train_dir` y `test_dir` pasando `transform=data_transform` para convertir nuestras imágenes en tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilice ImageFolder para crear conjuntos de datos\n",
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                 transform=data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b689e95",
   "metadata": {},
   "source": [
    "¡Hermoso!\n",
    "\n",
    "Parece que PyTorch ha registrado nuestro \"Conjunto de datos\".\n",
    "\n",
    "Inspeccionémoslos revisando los atributos `classes` y `class_to_idx`, así como la duración de nuestros conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener nombres de clases como una lista\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646678be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También puede obtener nombres de clases como un dictado.\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea7d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba las longitudes\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cfc13",
   "metadata": {},
   "source": [
    "¡Lindo! Parece que podremos usarlos como referencia para más adelante.\n",
    "\n",
    "¿Qué tal nuestras imágenes y etiquetas?\n",
    "\n",
    "¿Como se ven?\n",
    "\n",
    "Podemos indexar nuestros `train_data` y `test_data` `Dataset` para encontrar muestras y sus etiquetas de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c75b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d2fa1",
   "metadata": {},
   "source": [
    "Nuestras imágenes ahora tienen la forma de un tensor (con forma `[3, 64, 64]`) y las etiquetas tienen la forma de un número entero relacionado con una clase específica (como lo indica el atributo `class_to_idx`).\n",
    "\n",
    "¿Qué tal si trazamos un tensor de imagen única usando `matplotlib`?\n",
    "\n",
    "Primero tendremos que permutar (reorganizar el orden de sus dimensiones) para que sea compatible.\n",
    "\n",
    "En este momento, las dimensiones de nuestra imagen están en el formato `CHW` (canales de color, alto, ancho) pero `matplotlib` prefiere `HWC` (alto, ancho, canales de color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06004300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizar el orden de las dimensiones.\n",
    "img_permute = img.permute(1, 2, 0)\n",
    "\n",
    "# Imprime diferentes formas (antes y después de la permutación)\n",
    "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n",
    "\n",
    "# Trazar la imagen\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b147052",
   "metadata": {},
   "source": [
    "Observe que la imagen ahora está más pixelada (menos calidad).\n",
    "\n",
    "Esto se debe a que se cambió su tamaño de \"512x512\" a \"64x64\" píxeles.\n",
    "\n",
    "La intuición aquí es que si crees que la imagen es más difícil de reconocer lo que está sucediendo, es probable que al modelo también le resulte más difícil entenderlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611360f8",
   "metadata": {},
   "source": [
    "### 4.1 Convertir imágenes cargadas en `DataLoader`'s\n",
    "\n",
    "Tenemos nuestras imágenes como `Dataset` de PyTorch, pero ahora convirtámoslas en `DataLoader`.\n",
    "\n",
    "Lo haremos usando [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Convertir nuestro \"Conjunto de datos\" en \"Cargador de datos\" los hace iterables para que un modelo pueda aprender las relaciones entre muestras y objetivos (características y etiquetas).\n",
    "\n",
    "Para simplificar las cosas, usaremos `batch_size=1` y `num_workers=1`.\n",
    "\n",
    "¿Qué es \"num_workers\"?\n",
    "\n",
    "Buena pregunta.\n",
    "\n",
    "Define cuántos subprocesos se crearán para cargar sus datos.\n",
    "\n",
    "Piénselo así: cuanto mayor sea el valor establecido en `num_workers`, más potencia de cálculo utilizará PyTorch para cargar sus datos.\n",
    "\n",
    "Personalmente, normalmente lo configuro en el número total de CPU en mi máquina a través de [`os.cpu_count()`](https://docs.python.org/3/library/os.html#os.cpu_count) de Python.\n",
    "\n",
    "Esto garantiza que el `DataLoader` reclute tantos núcleos como sea posible para cargar datos.\n",
    "\n",
    "> **Nota:** Hay más parámetros con los que puede familiarizarse usando `torch.utils.data.DataLoader` en la [documentación de PyTorch](https://pytorch.org/docs/stable/data.html#torch .utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta conjuntos de datos de entrenamiento y prueba en cargadores de datos\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=1, # how many samples per batch?\n",
    "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=1, \n",
    "                             num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77667ca4",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Ahora nuestros datos son iterables.\n",
    "\n",
    "Probémoslo y comprobemos las formas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# El tamaño del lote ahora será 1, intente cambiar el parámetro de tamaño de lote anterior y vea qué sucede\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a4dbf",
   "metadata": {},
   "source": [
    "Ahora podríamos usar estos `DataLoader` con un bucle de entrenamiento y prueba para entrenar un modelo.\n",
    "\n",
    "Pero antes de hacerlo, veamos otra opción para cargar imágenes (o casi cualquier otro tipo de datos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e555b",
   "metadata": {},
   "source": [
    "## 5. Opción 2: cargar datos de imagen con un `conjunto de datos` personalizado\n",
    "\n",
    "¿Qué pasaría si no existiera un creador de `conjunto de datos` prediseñado como [`torchvision.datasets.ImageFolder()`](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageFolder)?\n",
    "\n",
    "¿O no existía uno para su problema específico?\n",
    "\n",
    "Bueno, podrías construir el tuyo propio.\n",
    "\n",
    "Pero espera, ¿cuáles son los pros y los contras de crear tu propia forma personalizada de cargar \"conjuntos de datos\"?\n",
    "\n",
    "| Ventajas de crear un \"conjunto de datos\" personalizado | Desventajas de crear un `conjunto de datos` personalizado |\n",
    "| ----- | ----- |\n",
    "| Puede crear un \"conjunto de datos\" a partir de casi cualquier cosa. | Aunque *podrías* crear un `Conjunto de datos` a partir de casi cualquier cosa, eso no significa que vaya a funcionar. | \n",
    "| No se limita a las funciones de \"conjunto de datos\" prediseñadas de PyTorch. | El uso de un \"conjunto de datos\" personalizado a menudo resulta en escribir más código, lo que podría ser propenso a errores o problemas de rendimiento. |\n",
    "\n",
    "Para ver esto en acción, trabajemos para replicar `torchvision.datasets.ImageFolder()` subclasificando `torch.utils.data.Dataset` (la clase base para todos los `Dataset` en PyTorch). \n",
    "\n",
    "Comenzaremos importando los módulos que necesitamos:\n",
    "* El `os` de Python para tratar con directorios (nuestros datos se almacenan en directorios).\n",
    "* `pathlib` de Python para tratar con rutas de archivos (cada una de nuestras imágenes tiene una ruta de archivo única).\n",
    "* `antorcha` para todo lo relacionado con PyTorch.\n",
    "* Clase `Imagen` de PIL para cargar imágenes.\n",
    "* `torch.utils.data.Dataset` para subclasificar y crear nuestro propio `Dataset` personalizado.\n",
    "* `torchvision.transforms` para convertir nuestras imágenes en tensores.\n",
    "* Varios tipos del módulo `typing` de Python para agregar sugerencias de tipo a nuestro código.\n",
    "\n",
    "> **Nota:** Puede personalizar los siguientes pasos para su propio conjunto de datos. La premisa sigue siendo: escriba código para cargar sus datos en el formato que desee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de938042",
   "metadata": {},
   "source": [
    "¿Recuerda cómo nuestras instancias de `torchvision.datasets.ImageFolder()` nos permitieron usar los atributos `classes` y `class_to_idx`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8274f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia de torchvision.datasets.ImageFolder()\n",
    "train_data.classes, train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f54281",
   "metadata": {},
   "source": [
    "### 5.1 Creando una función auxiliar para obtener nombres de clases\n",
    "\n",
    "Escribamos una función auxiliar capaz de crear una lista de nombres de clases y un diccionario de nombres de clases y sus índices dada una ruta de directorio.\n",
    "\n",
    "Para hacerlo, haremos:\n",
    "1. Obtenga los nombres de las clases usando `os.scandir()` para recorrer un directorio de destino (idealmente el directorio está en formato de clasificación de imágenes estándar).\n",
    "2. Genera un error si no se encuentran los nombres de las clases (si esto sucede, es posible que haya algún problema con la estructura del directorio).\n",
    "3. Convierta los nombres de las clases en un diccionario de etiquetas numéricas, una para cada clase.\n",
    "\n",
    "Veamos un pequeño ejemplo del paso 1 antes de escribir la función completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de configuración para el directorio de destino\n",
    "target_directory = train_dir\n",
    "print(f\"Target directory: {target_directory}\")\n",
    "\n",
    "# Obtenga los nombres de las clases del directorio de destino\n",
    "class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\n",
    "print(f\"Class names found: {class_names_found}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0feae29",
   "metadata": {},
   "source": [
    "¡Excelente!\n",
    "\n",
    "¿Qué tal si lo convertimos en una función completa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df610cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear función para buscar clases en el directorio de destino\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "    \n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "    \n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    \n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "        \n",
    "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a5e7e",
   "metadata": {},
   "source": [
    "¡Luciendo bien!\n",
    "\n",
    "Ahora probemos nuestra función `find_classes()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb246272",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classes(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f03e5",
   "metadata": {},
   "source": [
    "¡Guau! ¡Luciendo bien!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f39dcf2",
   "metadata": {},
   "source": [
    "### 5.2 Crear un `Conjunto de datos` personalizado para replicar `ImageFolder`\n",
    "\n",
    "Ahora estamos listos para crear nuestro propio \"conjunto de datos\" personalizado.\n",
    "\n",
    "Construiremos uno para replicar la funcionalidad de `torchvision.datasets.ImageFolder()`. \n",
    "\n",
    "Esta será una buena práctica y, además, revelará algunos de los pasos necesarios para crear su propio \"conjunto de datos\" personalizado.\n",
    "\n",
    "Será bastante código... ¡pero nada que no podamos manejar!\n",
    "\n",
    "Vamos a desglosarlo:\n",
    "1. Subclase `torch.utils.data.Dataset`.\n",
    "2. Inicialice nuestra subclase con un parámetro `targ_dir` (el directorio de datos de destino) y un parámetro `transform` (para que tengamos la opción de transformar nuestros datos si es necesario).\n",
    "3. Cree varios atributos para `paths` (las rutas de nuestras imágenes de destino), `transform` (las transformaciones que nos gustaría usar, esta puede ser `Ninguna`), `classes` y `class_to_idx` (de nuestro `find_classes ()` función).\n",
    "4. Cree una función para cargar imágenes desde un archivo y devolverlas, esto podría ser usando `PIL` o [`torchvision.io`](https://pytorch.org/vision/stable/io.html#image) (para entrada/salida de datos de visión). \n",
    "5. Sobrescriba el método `__len__` de `torch.utils.data.Dataset` para devolver el número de muestras en el `Dataset`. Esto se recomienda pero no es obligatorio. Esto es para que puedas llamar a `len (Conjunto de datos)`.\n",
    "6. Sobrescriba el método `__getitem__` de `torch.utils.data.Dataset` para devolver una única muestra del `Dataset`; esto es obligatorio.\n",
    "\n",
    "¡Vamos a hacerlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f127e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escriba una clase de conjunto de datos personalizada (hereda de torch.utils.data.Dataset)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclase torch.utils.data.Dataset\n",
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    # 2. Initialize with a targ_dir and transform (optional) parameter\n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # Get all image paths\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "    # 4. Make function to load images\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"Opens an image via a path and returns it.\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.paths)\n",
    "    \n",
    "    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "            return img, class_idx # return data, label (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecabcc",
   "metadata": {},
   "source": [
    "¡Guau! Un montón de código para cargar en nuestras imágenes.\n",
    "\n",
    "Esta es una de las desventajas de crear su propio \"conjunto de datos\" personalizado.\n",
    "\n",
    "Sin embargo, ahora que lo hemos escrito una vez, podemos moverlo a un archivo `.py` como `data_loader.py` junto con algunas otras funciones de datos útiles y reutilizarlo más adelante. \n",
    "\n",
    "Antes de probar nuestra nueva clase `ImageFolderCustom`, creemos algunas transformaciones para preparar nuestras imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e93ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aumentar los datos del tren\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# No aumente los datos de prueba, solo remodele\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744dd3a",
   "metadata": {},
   "source": [
    "¡Ahora llega la hora de la verdad!\n",
    "\n",
    "Convirtamos nuestras imágenes de entrenamiento (contenidas en `train_dir`) y nuestras imágenes de prueba (contenidas en `test_dir`) en `Dataset` usando nuestra propia clase `ImageFolderCustom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d28e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n",
    "                                      transform=train_transforms)\n",
    "test_data_custom = ImageFolderCustom(targ_dir=test_dir, \n",
    "                                     transform=test_transforms)\n",
    "train_data_custom, test_data_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6293fe4",
   "metadata": {},
   "source": [
    "Hmm... no hay errores, ¿funcionó?\n",
    "\n",
    "Intentemos llamar a `len()` en nuestro nuevo `Dataset` y busquemos los atributos `classes` y `class_to_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a27758",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_custom), len(test_data_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ea2c3",
   "metadata": {},
   "source": [
    "`len(test_data_custom) == len(test_data)` y `len(test_data_custom) == len(test_data)` ¡¡¡Sí!!!\n",
    "\n",
    "Parece que funcionó.\n",
    "\n",
    "También podríamos verificar la igualdad con el `Dataset` creado por la clase `torchvision.datasets.ImageFolder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ec9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique la igualdad entre nuestro conjunto de datos personalizado y el conjunto de datos ImageFolder\n",
    "print((len(train_data_custom) == len(train_data)) & (len(test_data_custom) == len(test_data)))\n",
    "print(train_data_custom.classes == train_data.classes)\n",
    "print(train_data_custom.class_to_idx == train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fcc84",
   "metadata": {},
   "source": [
    "¡Ho, ho!\n",
    "\n",
    "¡Míranos ir!\n",
    "\n",
    "¡Tres \"verdaderos\"!\n",
    "\n",
    "No hay nada mejor que eso.\n",
    "\n",
    "¿Qué tal si lo llevamos a un nivel superior y trazamos algunas imágenes aleatorias para probar nuestra anulación `__getitem__`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad3812",
   "metadata": {},
   "source": [
    "### 5.3 Crear una función para mostrar imágenes aleatorias\n",
    "\n",
    "¡Sabes que hora es!\n",
    "\n",
    "Es hora de ponernos el sombrero de explorador de datos y *¡visualizar, visualizar, visualizar!*\n",
    "\n",
    "Creemos una función auxiliar llamada `display_random_images()` que nos ayuda a visualizar imágenes en nuestro `Conjunto de datos'.\n",
    "\n",
    "Específicamente:\n",
    "1. Tome un `Conjunto de datos` y una serie de otros parámetros como `clases` (los nombres de nuestras clases de destino), la cantidad de imágenes para mostrar (`n`) y una semilla aleatoria. \n",
    "2. Para evitar que la visualización se salga de control, limitaremos \"n\" a 10 imágenes.\n",
    "3. Establezca la semilla aleatoria para parcelas reproducibles (si se establece \"semilla\"). \n",
    "4. Obtenga una lista de índices de muestra aleatorios (podemos usar `random.sample()` de Python para esto) para trazar.\n",
    "5. Configure un gráfico `matplotlib`.\n",
    "6. Recorra los índices de muestra aleatorios que se encuentran en el paso 4 y grábelos con `matplotlib`.\n",
    "7. Asegúrese de que las imágenes de muestra tengan la forma \"HWC\" (alto, ancho, canales de color) para que podamos trazarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876216c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Incorpore un conjunto de datos y una lista de nombres de clases.\n",
    "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
    "                          classes: List[str] = None,\n",
    "                          n: int = 10,\n",
    "                          display_shape: bool = True,\n",
    "                          seed: int = None):\n",
    "    \n",
    "    # 2. Adjust display if n too high\n",
    "    if n > 10:\n",
    "        n = 10\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "    \n",
    "    # 3. Set random seed\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # 4. Get random sample indexes\n",
    "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    # 5. Setup plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # 6. Loop through samples and display random samples \n",
    "    for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "\n",
    "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
    "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
    "\n",
    "        # Plot adjusted samples\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(targ_image_adjust)\n",
    "        plt.axis(\"off\")\n",
    "        if classes:\n",
    "            title = f\"class: {classes[targ_label]}\"\n",
    "            if display_shape:\n",
    "                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6848ab1",
   "metadata": {},
   "source": [
    "¡Qué función tan atractiva!\n",
    "\n",
    "Probémoslo primero con el `Dataset` que creamos con `torchvision.datasets.ImageFolder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar imágenes aleatorias del conjunto de datos creado por ImageFolder\n",
    "display_random_images(train_data, \n",
    "                      n=5, \n",
    "                      classes=class_names,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb122f8",
   "metadata": {},
   "source": [
    "Y ahora con el `Dataset` que creamos con nuestro propio `ImageFolderCustom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar imágenes aleatorias del conjunto de datos ImageFolderCustom\n",
    "display_random_images(train_data_custom, \n",
    "                      n=12, \n",
    "                      classes=class_names,\n",
    "                      seed=None) # Try setting the seed for reproducible images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad484a7",
   "metadata": {},
   "source": [
    "¡¡¡Lindo!!!\n",
    "\n",
    "Parece que nuestro `ImageFolderCustom` está funcionando tal como nos gustaría."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6604a47",
   "metadata": {},
   "source": [
    "### 5.4 Convierta imágenes cargadas personalizadas en `DataLoader` \n",
    "\n",
    "Tenemos una manera de convertir nuestras imágenes sin procesar en `Dataset` (características asignadas a etiquetas o `X` asignadas a `y`) a través de nuestra clase `ImageFolderCustom`.\n",
    "\n",
    "Ahora, ¿cómo podríamos convertir nuestro \"Conjunto de datos\" personalizado en un \"Cargador de datos\"?\n",
    "\n",
    "Si adivinaste usando `torch.utils.data.DataLoader()`, ¡estarías en lo cierto!\n",
    "\n",
    "Debido a que la subclase de nuestro `Dataset` personalizado `torch.utils.data.Dataset`, podemos usarla directamente con `torch.utils.data.DataLoader()`.\n",
    "\n",
    "Y podemos hacerlo usando pasos muy similares a los anteriores, excepto que esta vez usaremos nuestro `Conjunto de datos` personalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta el tren y pruebe conjuntos de datos personalizados en DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n",
    "                                     batch_size=1, # how many samples per batch?\n",
    "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "                                     shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n",
    "                                    batch_size=1, \n",
    "                                    num_workers=0, \n",
    "                                    shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader_custom, test_dataloader_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d845ed6",
   "metadata": {},
   "source": [
    "¿Las formas de las muestras son iguales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener imagen y etiqueta de DataLoader personalizado\n",
    "img_custom, label_custom = next(iter(train_dataloader_custom))\n",
    "\n",
    "# El tamaño del lote ahora será 1, intente cambiar el parámetro de tamaño de lote anterior y vea qué sucede\n",
    "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label_custom.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15a2ea",
   "metadata": {},
   "source": [
    "¡Seguro lo hacen!\n",
    "\n",
    "Ahora analicemos otras formas de transformaciones de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853002a",
   "metadata": {},
   "source": [
    "## 6. Otras formas de transformaciones (aumento de datos)\n",
    "\n",
    "Ya hemos visto un par de transformaciones en nuestros datos, pero hay muchas más.\n",
    "\n",
    "Puede verlos todos en la [documentación `torchvision.transforms`] (https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "El propósito de las transformaciones es alterar sus imágenes de alguna manera.\n",
    "\n",
    "Eso puede convertir sus imágenes en un tensor (como hemos visto antes).\n",
    "\n",
    "O recortarlo o borrar aleatoriamente una parte o rotarla aleatoriamente.\n",
    "\n",
    "Realizar este tipo de transformaciones a menudo se denomina **aumento de datos**.\n",
    "\n",
    "**Aumento de datos** es el proceso de alterar tus datos de tal manera que *artificialmente* aumentes la diversidad de tu conjunto de entrenamiento.\n",
    "\n",
    "Se espera que entrenar un modelo con este conjunto de datos *artificialmente* alterado dé como resultado un modelo que sea capaz de realizar una mejor *generalización* (los patrones que aprende son más sólidos para futuros ejemplos no vistos).\n",
    "\n",
    "Puede ver muchos ejemplos diferentes de aumento de datos realizado en imágenes usando `torchvision.transforms` en el [ejemplo de ilustración de transformaciones] de PyTorch (https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#ilustracion-of-transforms ).\n",
    "\n",
    "Pero probemos uno nosotros mismos.\n",
    "\n",
    "El aprendizaje automático consiste en aprovechar el poder de la aleatoriedad y las investigaciones muestran que las transformaciones aleatorias (como [`transforms.RandAugment()`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#randaugment) y [ `transforms.TrivialAugmentWide()`](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#trivialaugmentwide)) generalmente funcionan mejor que las transformaciones seleccionadas cuidadosamente.\n",
    "\n",
    "La idea detrás de [TrivialAugment](https://arxiv.org/abs/2103.10158) es... bueno, trivial. \n",
    "\n",
    "Tiene un conjunto de transformaciones y elige aleatoriamente una cantidad de ellas para realizarlas en una imagen y en una magnitud aleatoria entre un rango determinado (una magnitud más alta significa más intensidad).\n",
    "\n",
    "El equipo de PyTorch incluso [usó TrivialAugment para entrenar sus últimos modelos de visión de última generación](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using -torchvision-latest-primitives/#break-down-of-key-accuracy-improvements).\n",
    "\n",
    "![aumento de datos de aumento trivial que se utiliza para la capacitación de vanguardia de PyTorch] (https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-trivial-augment-being-using-in -PyTorch-resize.png)\n",
    "\n",
    "*TrivialAugment fue uno de los ingredientes utilizados en una reciente actualización de capacitación de última generación para varios modelos de visión de PyTorch.*\n",
    "\n",
    "¿Qué tal si lo probamos en algunas de nuestras propias imágenes?\n",
    "\n",
    "El parámetro principal al que prestar atención en `transforms.TrivialAugmentWide()` es `num_magnitude_bins=31`.\n",
    "\n",
    "Define qué parte de un rango se seleccionará un valor de intensidad para aplicar una determinada transformación, siendo \"0\" ningún rango y \"31\" siendo el rango máximo (la mayor probabilidad de obtener la mayor intensidad). \n",
    "\n",
    "Podemos incorporar `transforms.TrivialAugmentWide()` en `transforms.Compose()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n",
    "    transforms.ToTensor() # use ToTensor() last to get everything between 0 & 1\n",
    "])\n",
    "\n",
    "# No es necesario realizar un aumento en los datos de prueba.\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66b2f5",
   "metadata": {},
   "source": [
    "> **Nota:** Generalmente no se realizan aumentos de datos en el conjunto de prueba. La idea del aumento de datos es aumentar *artificialmente* la diversidad del conjunto de entrenamiento para predecir mejor en el conjunto de prueba. \n",
    ">\n",
    "> Sin embargo, debe asegurarse de que las imágenes de su conjunto de prueba se transformen en tensores. También dimensionamos las imágenes de prueba al mismo tamaño que nuestras imágenes de entrenamiento; sin embargo, se puede realizar inferencia en imágenes de diferentes tamaños si es necesario (aunque esto puede alterar el rendimiento).\n",
    "\n",
    "Hermoso, ahora tenemos una transformación de entrenamiento (con aumento de datos) y una transformación de prueba (sin aumento de datos).\n",
    "\n",
    "¡Probemos nuestro aumento de datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todas las rutas de imágenes\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "\n",
    "# Trazar imágenes aleatorias\n",
    "plot_transformed_images(\n",
    "    image_paths=image_path_list,\n",
    "    transform=train_transforms,\n",
    "    n=3,\n",
    "    seed=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36980773",
   "metadata": {},
   "source": [
    "Intente ejecutar la celda de arriba varias veces y vea cómo la imagen original cambia a medida que pasa por la transformación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5620de",
   "metadata": {},
   "source": [
    "## 7. Modelo 0: TinyVGG sin aumento de datos\n",
    "\n",
    "Muy bien, hemos visto cómo convertir nuestros datos de imágenes en carpetas a tensores transformados.\n",
    "\n",
    "Ahora construyamos un modelo de visión por computadora para ver si podemos clasificar si una imagen es de pizza, bistec o sushi.\n",
    "\n",
    "Para comenzar, comenzaremos con una transformación simple, solo cambiaremos el tamaño de las imágenes a \"(64, 64)\" y las convertiremos en tensores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46d2fc",
   "metadata": {},
   "source": [
    "### 7.1 Creando transformaciones y cargando datos para el Modelo 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear transformación simple\n",
    "simple_transform = transforms.Compose([ \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313a073",
   "metadata": {},
   "source": [
    "Excelente, ahora tenemos una transformación simple:\n",
    "1. Cargue los datos, convirtiendo primero cada una de nuestras carpetas de entrenamiento y prueba en un `Conjunto de datos` con `torchvision.datasets.ImageFolder()` \n",
    "2. Luego, en un `DataLoader` usando `torch.utils.data.DataLoader()`.\n",
    "    * Configuraremos `batch_size=32` y `num_workers` en tantas CPU como sea posible en nuestra máquina (esto dependerá de qué máquina esté usando)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d38ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar y transformar datos\n",
    "from torchvision import datasets\n",
    "train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\n",
    "test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n",
    "\n",
    "# 2. Convierta datos en DataLoaders\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configurar el tamaño del lote y el número de trabajadores\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
    "\n",
    "# Crear cargador de datos\n",
    "train_dataloader_simple = DataLoader(train_data_simple, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader_simple = DataLoader(test_data_simple, \n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=NUM_WORKERS)\n",
    "\n",
    "train_dataloader_simple, test_dataloader_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426efab",
   "metadata": {},
   "source": [
    "¡Se ha creado `DataLoader`! \n",
    "\n",
    "Construyamos un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47981b63",
   "metadata": {},
   "source": [
    "### 7.2 Crear clase de modelo TinyVGG\n",
    "\n",
    "En [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn), utilizamos el modelo TinyVGG del [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "Recreemos el mismo modelo, excepto que esta vez usaremos imágenes en color en lugar de escala de grises (`in_channels=3` en lugar de `in_channels=1` para píxeles RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85758af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*16*16,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e5e70a",
   "metadata": {},
   "source": [
    "> **Nota:** Una de las formas de acelerar la computación de los modelos de aprendizaje profundo en una GPU es aprovechar la **fusión de operadores**.\n",
    ">\n",
    "> Esto significa que en el método `forward()` de nuestro modelo anterior, en lugar de llamar a un bloque de capa y reasignar `x` cada vez, llamamos a cada bloque en sucesión (consulte la última línea del método `forward()` en el modelo anterior como ejemplo).\n",
    ">\n",
    "> Esto ahorra el tiempo dedicado a reasignar `x` (memoria pesada) y se centra únicamente en calcular en `x`.\n",
    "> \n",
    "> Consulte [*Cómo hacer que el aprendizaje profundo funcione mejor desde los primeros principios*](https://horace.io/brrr_intro.html) de Horace He para conocer más formas de acelerar los modelos de aprendizaje automático.\n",
    "\n",
    "¡Ese sí que es un modelo bonito!\n",
    "\n",
    "¿Qué tal si lo probamos con un pase hacia adelante en una sola imagen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e7062",
   "metadata": {},
   "source": [
    "### 7.3 Pruebe un pase hacia adelante en una sola imagen (para probar el modelo)\n",
    "\n",
    "Una buena forma de probar un modelo es hacer un pase directo a un solo dato.\n",
    "\n",
    "También es una forma práctica de probar las formas de entrada y salida de nuestras diferentes capas.\n",
    "\n",
    "Para hacer un pase hacia adelante en una sola imagen, hagamos lo siguiente:\n",
    "1. Obtenga un lote de imágenes y etiquetas del `DataLoader`.\n",
    "2. Obtenga una sola imagen del lote y \"descomprima()\" la imagen para que tenga un tamaño de lote de \"1\" (para que su forma se ajuste al modelo).\n",
    "3. Realice una inferencia en una sola imagen (asegurándose de enviar la imagen al \"dispositivo\" de destino).\n",
    "4. Imprima lo que está sucediendo y convierta los logits de salida sin procesar del modelo en probabilidades de predicción con `torch.softmax()` (ya que estamos trabajando con datos de múltiples clases) y convierta las probabilidades de predicción en etiquetas de predicción con `torch.argmax( )`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aafaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obtenga un lote de imágenes y etiquetas del DataLoader\n",
    "img_batch, label_batch = next(iter(train_dataloader_simple))\n",
    "\n",
    "# 2. Obtenga una sola imagen del lote y descomprima la imagen para que su forma se ajuste al modelo.\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# 3. Realice un pase hacia adelante en una sola imagen.\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_0(img_single.to(device))\n",
    "    \n",
    "# 4. Imprima lo que está sucediendo y convierta los logits del modelo -> problemas pred -> etiqueta pred\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09622eef",
   "metadata": {},
   "source": [
    "Maravilloso, parece que nuestro modelo está generando lo que esperábamos.\n",
    "\n",
    "Puede ejecutar la celda de arriba varias veces y cada vez se predecirá una imagen diferente.\n",
    "\n",
    "Y probablemente notará que las predicciones a menudo son erróneas.\n",
    "\n",
    "Esto es de esperarse porque el modelo aún no ha sido entrenado y esencialmente se trata de adivinar usando pesos aleatorios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40281a14",
   "metadata": {},
   "source": [
    "### 7.4 Utilice `torchinfo` para tener una idea de las formas que atraviesan nuestro modelo.\n",
    "\n",
    "Imprimir nuestro modelo con `print(model)` nos da una idea de lo que está pasando con nuestro modelo.\n",
    "\n",
    "Y podemos imprimir las formas de nuestros datos a través del método `forward()`.\n",
    "\n",
    "Sin embargo, una forma útil de obtener información de nuestro modelo es usar [`torchinfo`](https://github.com/TylerYep/torchinfo).\n",
    "\n",
    "`torchinfo` viene con un método `summary()` que toma un modelo de PyTorch así como un `input_shape` y devuelve lo que sucede cuando un tensor se mueve a través de su modelo.\n",
    "\n",
    "> **Nota:** Si estás utilizando Google Colab, necesitarás instalar `torchinfo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instale torchinfo si no está disponible, impórtelo si lo está\n",
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "    \n",
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f162cd2",
   "metadata": {},
   "source": [
    "¡Lindo! \n",
    "\n",
    "La salida de `torchinfo.summary()` nos brinda una gran cantidad de información sobre nuestro modelo.\n",
    "\n",
    "Como \"parámetros totales\", el número total de parámetros en nuestro modelo, el \"tamaño total estimado (MB)\", que es el tamaño de nuestro modelo.\n",
    "\n",
    "También puede ver el cambio en las formas de entrada y salida a medida que los datos de un determinado `input_size` se mueven a través de nuestro modelo.\n",
    "\n",
    "En este momento, nuestros números de parámetros y el tamaño total del modelo son bajos. \n",
    "\n",
    "Esto porque estamos comenzando con un modelo pequeño.\n",
    "\n",
    "Y si necesitamos aumentar su tamaño más adelante, podemos hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26ccd5",
   "metadata": {},
   "source": [
    "### 7.5 Crear funciones de tren y bucle de prueba \n",
    "\n",
    "Tenemos datos y tenemos un modelo.\n",
    "\n",
    "Ahora creemos algunas funciones de bucle de prueba y entrenamiento para entrenar nuestro modelo con los datos de entrenamiento y evaluar nuestro modelo con los datos de prueba.\n",
    "\n",
    "Y para asegurarnos de que podamos volver a utilizar estos bucles de entrenamiento y prueba, los pondremos en funcionamiento.\n",
    "\n",
    "En concreto vamos a realizar tres funciones:\n",
    "1. `train_step()`: toma un modelo, un `DataLoader`, una función de pérdida y un optimizador y entrena el modelo en el `DataLoader`.\n",
    "2. `test_step()`: toma un modelo, un `DataLoader` y una función de pérdida y evalúa el modelo en el `DataLoader`.\n",
    "3. `train()`: realiza 1. y 2. juntos durante un número determinado de épocas y devuelve un diccionario de resultados.\n",
    "\n",
    "> **Nota:** También cubrimos los pasos de un bucle de optimización de PyTorch en [cuaderno 01](https://www.learnpytorch.io/01_pytorch_workflow/#creating-an-optimization-loop-in-pytorch). como la [Canción de bucle de optimización de PyTorch no oficial] (https://youtu.be/Nutpusq_AFw) y hemos creado funciones similares en el [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/#62-functionizing- bucles de entrenamiento y prueba).\n",
    "\n",
    "Comencemos construyendo `train_step()`.\n",
    "\n",
    "Debido a que estamos tratando con lotes en el `DataLoader`, acumularemos los valores de precisión y pérdida del modelo durante el entrenamiento (sumándolos para cada lote) y luego los ajustaremos al final antes de devolverlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5009a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc42c5",
   "metadata": {},
   "source": [
    "¡Guau! Función `train_step()` realizada.\n",
    "\n",
    "Ahora hagamos lo mismo con la función `test_step()`.\n",
    "\n",
    "La principal diferencia aquí será que `test_step()` no aceptará un optimizador y, por lo tanto, no realizará un descenso de gradiente.\n",
    "\n",
    "Pero como haremos inferencias, nos aseguraremos de activar el administrador de contexto `torch.inference_mode()` para hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ba1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1dec7",
   "metadata": {},
   "source": [
    "¡Excelente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6bc8db",
   "metadata": {},
   "source": [
    "### 7.6 Creando una función `train()` para combinar `train_step()` y `test_step()`\n",
    "\n",
    "Ahora necesitamos una manera de juntar nuestras funciones `train_step()` y `test_step()`.\n",
    "\n",
    "Para hacerlo, los empaquetaremos en una función `train()`.\n",
    "\n",
    "Esta función entrenará el modelo y lo evaluará.\n",
    "\n",
    "Específicamente:\n",
    "1. Tome un modelo, un `DataLoader` para conjuntos de entrenamiento y prueba, un optimizador, una función de pérdida y cuántas épocas realizar cada paso de entrenamiento y prueba.\n",
    "2. Cree un diccionario de resultados vacío para los valores `train_loss`, `train_acc`, `test_loss` y `test_acc` (podemos llenarlo a medida que avanza el entrenamiento).\n",
    "3. Recorra las funciones de los pasos de prueba y entrenamiento durante varias épocas.\n",
    "4. Imprime lo que sucede al final de cada época.\n",
    "5. Actualice el diccionario de resultados vacío con las métricas actualizadas en cada época.\n",
    "6. Devuelva el relleno\n",
    "\n",
    "Para realizar un seguimiento de la cantidad de épocas por las que hemos pasado, importemos `tqdm` desde `tqdm.auto` ([`tqdm`](https://github.com/tqdm/tqdm) es uno de los más populares Las bibliotecas de barra de progreso para Python y `tqdm.auto` deciden automáticamente qué tipo de barra de progreso es mejor para su entorno informático, por ejemplo, Jupyter Notebook frente a script de Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Considere varios parámetros necesarios para los pasos de capacitación y prueba.\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260514b",
   "metadata": {},
   "source": [
    "### 7.7 Entrenar y evaluar el modelo 0\n",
    "\n",
    "Muy bien, muy bien, tenemos todos los ingredientes que necesitamos para entrenar y evaluar nuestro modelo.\n",
    "\n",
    "¡Es hora de juntar nuestro modelo `TinyVGG`, las funciones `DataLoader` y `train()` para ver si podemos construir un modelo capaz de discernir entre pizza, bistec y sushi!\n",
    "\n",
    "Recreemos `model_0` (no es necesario, pero lo haremos para completarlo) y luego llamemos a nuestra función `train()` pasando los parámetros necesarios.\n",
    "\n",
    "Para que nuestros experimentos sean rápidos, entrenaremos nuestro modelo durante **5 épocas** (aunque puedes aumentar esto si lo deseas).\n",
    "\n",
    "En cuanto a un **optimizador** y una **función de pérdida**, usaremos `torch.nn.CrossEntropyLoss()` (ya que estamos trabajando con datos de clasificación de clases múltiples) y `torch.optim.Adam( )` con una tasa de aprendizaje de `1e-3` respectivamente.\n",
    "\n",
    "Para ver cuánto tardan las cosas, importaremos el método [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer) de Python para calcular el tiempo de entrenamiento. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer semillas aleatorias\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Establecer número de épocas\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Recrea una instancia de TinyVGG\n",
    "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "# Función de pérdida de configuración y optimizador.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
    "\n",
    "# iniciar el cronómetro\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Modelo de tren_0\n",
    "model_0_results = train(model=model_0, \n",
    "                        train_dataloader=train_dataloader_simple,\n",
    "                        test_dataloader=test_dataloader_simple,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# Finalice el cronómetro e imprima cuánto tiempo tardó\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ed5f6",
   "metadata": {},
   "source": [
    "Mmm...\n",
    "\n",
    "Parece que nuestro modelo funcionó bastante mal.\n",
    "\n",
    "Pero por ahora está bien, seguiremos perseverando.\n",
    "\n",
    "¿Cuáles son algunas formas en las que podrías mejorarlo?\n",
    "\n",
    "> **Nota:** Consulte la sección [*Mejorar un modelo (desde la perspectiva del modelo)* en el cuaderno 02](https://www.learnpytorch.io/02_pytorch_classification/#5-improving-a-model-from -a-model-perspective) para obtener ideas sobre cómo mejorar nuestro modelo TinyVGG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacec337",
   "metadata": {},
   "source": [
    "### 7.8 Trazar las curvas de pérdidas del Modelo 0\n",
    "\n",
    "Según las impresiones de nuestro entrenamiento `model_0`, no parecía que le fuera muy bien.\n",
    "\n",
    "Pero podemos evaluarlo mejor trazando las **curvas de pérdida** del modelo. \n",
    "\n",
    "**Las curvas de pérdida** muestran los resultados del modelo a lo largo del tiempo.\n",
    "\n",
    "Y son una excelente manera de ver cómo se desempeña su modelo en diferentes conjuntos de datos (por ejemplo, entrenamiento y prueba).\n",
    "\n",
    "Creemos una función para trazar los valores en nuestro diccionario `model_0_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a08ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique las claves model_0_results\n",
    "model_0_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff512429",
   "metadata": {},
   "source": [
    "Necesitaremos extraer cada una de estas claves y convertirlas en una trama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b95d4",
   "metadata": {},
   "source": [
    "Bien, probemos nuestra función `plot_loss_curves()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e90a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_0_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37885786",
   "metadata": {},
   "source": [
    "Vaya.\n",
    "\n",
    "Parece que las cosas están por todos lados...\n",
    "\n",
    "Pero lo sabíamos porque los resultados impresos de nuestro modelo durante el entrenamiento no eran muy prometedores.\n",
    "\n",
    "Podría intentar entrenar el modelo durante más tiempo y ver qué sucede cuando traza una curva de pérdidas en un horizonte temporal más largo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d675a3",
   "metadata": {},
   "source": [
    "## 8. ¿Cómo debería ser una curva de pérdidas ideal?\n",
    "\n",
    "Observar las curvas de pérdida de prueba y entrenamiento es una excelente manera de ver si su modelo está **sobreajustado**.\n",
    "\n",
    "Un modelo de sobreajuste es aquel que funciona mejor (a menudo por un margen considerable) en el conjunto de entrenamiento que en el conjunto de validación/prueba.\n",
    "\n",
    "Si su pérdida de entrenamiento es mucho menor que su pérdida de prueba, su modelo está **sobreajustado**.\n",
    "\n",
    "Es decir, se aprenden demasiado bien los patrones en el entrenamiento y esos patrones no se generalizan a los datos de prueba.\n",
    "\n",
    "El otro lado es cuando tu pérdida de entrenamiento y pruebas no es tan baja como te gustaría, esto se considera **insuficiencia**.\n",
    "\n",
    "La posición ideal para una curva de pérdida de entrenamiento y prueba es que se alineen estrechamente entre sí.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg\" alt=\"diferentes curvas de pérdida de entrenamiento y pruebas que ilustra el sobreajuste, el desajuste y las curvas de pérdida ideales\" width=\"800\"/>\n",
    "\n",
    "*Izquierda: si tus curvas de pérdida de entrenamiento y pruebas no son tan bajas como te gustaría, esto se considera **insuficiencia**. *Medio:* Cuando su pérdida de prueba/validación es mayor que su pérdida de entrenamiento, esto se considera **sobreajuste**. *Derecha:* El escenario ideal es cuando las curvas de pérdida de entrenamiento y prueba se alinean con el tiempo. Esto significa que su modelo se está generalizando bien. Hay más combinaciones y diferentes cosas que las curvas de pérdida pueden hacer; para obtener más información sobre esto, consulte la [guía de interpretación de curvas de pérdida] de Google (https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8dc136",
   "metadata": {},
   "source": [
    "### 8.1 Cómo lidiar con el sobreajuste\n",
    "\n",
    "Dado que el principal problema con el sobreajuste es que su modelo se ajusta *demasiado bien* a los datos de entrenamiento, querrá utilizar técnicas para \"controlarlo\".\n",
    "\n",
    "Una técnica común para prevenir el sobreajuste se conoce como [**regularización**](https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html).\n",
    "\n",
    "Me gusta pensar en esto como \"hacer que nuestros modelos sean más regulares\", es decir, capaces de ajustar *más* tipos de datos.\n",
    "\n",
    "Analicemos algunos métodos para evitar el sobreajuste.\n",
    "\n",
    "| **Método para evitar el sobreajuste** | **¿Qué es?** |\n",
    "| ----- | ----- |\n",
    "| **Obtener más datos** | Tener más datos le da al modelo más oportunidades de aprender patrones, patrones que pueden ser más generalizables a nuevos ejemplos. | \n",
    "| **Simplifica tu modelo** | Si el modelo actual ya está sobreajustando los datos de entrenamiento, puede ser un modelo demasiado complicado. Esto significa que está aprendiendo demasiado bien los patrones de los datos y no puede generalizar bien a datos invisibles. Una forma de simplificar un modelo es reducir la cantidad de capas que utiliza o reducir la cantidad de unidades ocultas en cada capa. | \n",
    "| **Usar aumento de datos** | [**Aumento de datos**](https://developers.google.com/machine-learning/glossary#data-augmentation) manipula los datos de entrenamiento de una manera que al modelo le resulta más difícil aprender, ya que agrega artificialmente más variedad. a los datos. Si un modelo es capaz de aprender patrones en datos aumentados, es posible que pueda generalizar mejor a datos invisibles. |\n",
    "| **Usar aprendizaje por transferencia** | [**Transferir aprendizaje**](https://developers.google.com/machine-learning/glossary#transfer-learning) implica aprovechar los patrones (también llamados pesos previamente entrenados) que un modelo ha aprendido a usar como base para su propia tarea. En nuestro caso, podríamos usar un modelo de visión por computadora previamente entrenado en una gran variedad de imágenes y luego modificarlo ligeramente para que esté más especializado en imágenes de alimentos. |\n",
    "| **Usar capas de abandono** | Las capas de abandono eliminan aleatoriamente las conexiones entre capas ocultas en las redes neuronales, lo que simplifica efectivamente un modelo pero también mejora las conexiones restantes. Consulte [`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) para obtener más información. | \n",
    "| **Usar disminución de la tasa de aprendizaje** | La idea aquí es disminuir lentamente la tasa de aprendizaje a medida que se entrena un modelo. Esto es similar a alcanzar una moneda en el respaldo de un sofá. Cuanto más te acercas, más pequeños son tus pasos. Lo mismo ocurre con la tasa de aprendizaje: cuanto más te acerques a [**convergencia**](https://developers.google.com/machine-learning/glossary#convergence), más pequeñas querrás que sean tus actualizaciones de peso. .  |\n",
    "| **Utilice la parada anticipada** | [**Detención temprana**](https://developers.google.com/machine-learning/glossary#early_stopping) detiene el entrenamiento del modelo *antes* de que comience a sobreajustarse. Por ejemplo, digamos que la pérdida del modelo ha dejado de disminuir durante las últimas 10 épocas (este número es arbitrario), es posible que desee detener el entrenamiento del modelo aquí e ir con los pesos del modelo que tuvieron la pérdida más baja (10 épocas anteriores). |\n",
    "\n",
    "Existen más métodos para abordar el sobreajuste, pero estos son algunos de los principales.\n",
    "\n",
    "A medida que comience a construir modelos cada vez más profundos, descubrirá que debido a que los aprendizajes profundos son *tan buenos* para aprender patrones en los datos, lidiar con el sobreajuste es uno de los principales problemas del aprendizaje profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccb5c1",
   "metadata": {},
   "source": [
    "### 8.2 Cómo lidiar con el desajuste \n",
    "\n",
    "Cuando un modelo es [**underfitting**](https://developers.google.com/machine-learning/glossary#underfitting), se considera que tiene un poder predictivo deficiente en los conjuntos de entrenamiento y prueba.\n",
    "\n",
    "En esencia, un modelo insuficiente no logrará reducir los valores de pérdida al nivel deseado.\n",
    "\n",
    "En este momento, al observar nuestras curvas de pérdida actuales, consideré que nuestro modelo \"TinyVGG\", \"model_0\", no se ajustaba a los datos.\n",
    "\n",
    "La idea principal detrás de lidiar con el desajuste es *aumentar* el poder predictivo de su modelo.\n",
    "\n",
    "Hay varias formas de hacer esto.\n",
    "\n",
    "| **Método para evitar el desajuste** | **¿Qué es?** |\n",
    "| ----- | ----- |\n",
    "| **Agregue más capas/unidades a su modelo** | Si su modelo no se ajusta lo suficiente, es posible que no tenga la capacidad suficiente para *aprender* los patrones/pesos/representaciones requeridos de los datos para que sean predictivos. Una forma de agregar más poder predictivo a su modelo es aumentar la cantidad de capas/unidades ocultas dentro de esas capas. | \n",
    "| **Ajustar la tasa de aprendizaje** | Quizás la tasa de aprendizaje de su modelo sea demasiado alta para empezar. Y está tratando de actualizar demasiado sus pesos en cada época, y a su vez no aprende nada. En este caso, puede reducir la tasa de aprendizaje y ver qué sucede. |\n",
    "| **Usar aprendizaje por transferencia** | El aprendizaje por transferencia es capaz de prevenir el sobreajuste y el desajuste. Implica utilizar los patrones de un modelo que ya funcionaba y ajustarlos a su propio problema. |\n",
    "| **Entrena por más tiempo** | A veces, un modelo simplemente necesita más tiempo para aprender las representaciones de datos. Si descubre que en sus experimentos más pequeños su modelo no está aprendiendo nada, tal vez dejarlo entrenar durante más épocas pueda dar como resultado un mejor rendimiento. |\n",
    "| **Utilice menos regularización** | Quizás su modelo no se ajuste lo suficiente porque está tratando de evitar un ajuste excesivo. Reprimir las técnicas de regularización puede ayudar a que su modelo se ajuste mejor a los datos. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515b063",
   "metadata": {},
   "source": [
    "### 8.3 El equilibrio entre sobreajuste y desajuste\n",
    "\n",
    "Ninguno de los métodos discutidos anteriormente son soluciones mágicas, es decir, no siempre funcionan.\n",
    "\n",
    "Y prevenir el sobreajuste y el desajuste es posiblemente el área más activa de la investigación sobre el aprendizaje automático.\n",
    "\n",
    "Dado que todo el mundo quiere que sus modelos se ajusten mejor (menos subajuste), pero no tan bien, no generalizan bien ni funcionan en el mundo real (menos sobreajuste).\n",
    "\n",
    "Existe una delgada línea entre el sobreajuste y el desajuste.\n",
    "\n",
    "Porque demasiado de cada uno puede causar el otro.\n",
    "\n",
    "El aprendizaje por transferencia es quizás una de las técnicas más poderosas cuando se trata de lidiar con el sobreajuste y el desajuste de sus propios problemas.\n",
    "\n",
    "En lugar de elaborar manualmente diferentes técnicas de sobreajuste y desajuste, el aprendizaje por transferencia le permite tomar un modelo que ya funciona en un espacio problemático similar al suyo (por ejemplo, uno de [paperswithcode.com/sota](https://paperswithcode.com/sota) o [ Modelos de Hugging Face](https://huggingface.co/models)) y aplíquelo a su propio conjunto de datos.\n",
    "\n",
    "Veremos el poder del aprendizaje por transferencia en un cuaderno posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bec0b1",
   "metadata": {},
   "source": [
    "## 9. Modelo 1: TinyVGG con aumento de datos\n",
    "\n",
    "¡Es hora de probar otro modelo!\n",
    "\n",
    "Esta vez, carguemos los datos y usemos **aumento de datos** para ver si mejora nuestros resultados de alguna manera.\n",
    "\n",
    "Primero, componeremos una transformación de entrenamiento para incluir `transforms.TrivialAugmentWide()`, además de cambiar el tamaño y convertir nuestras imágenes en tensores.\n",
    "\n",
    "Haremos lo mismo para una transformación de prueba excepto sin el aumento de datos.\n",
    "\n",
    "### 9.1 Crear transformación con aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea transformación de entrenamiento con TrivialAugment\n",
    "train_transform_trivial_augment = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "# Crear transformación de prueba (sin aumento de datos)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae65f13",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Ahora convirtamos nuestras imágenes en `Dataset` usando `torchvision.datasets.ImageFolder()` y luego en `DataLoader` con `torch.utils.data.DataLoader()`.\n",
    "\n",
    "### 9.2 Crear y probar `Dataset` y `DataLoader`\n",
    "\n",
    "Nos aseguraremos de que el `Dataset` del tren use `train_transform_trivial_augment` y el `Dataset` de prueba use `test_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta carpetas de imágenes en conjuntos de datos\n",
    "train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\n",
    "test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "train_data_augmented, test_data_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2030d",
   "metadata": {},
   "source": [
    "Y crearemos `DataLoader` con `batch_size=32` y con `num_workers` configurados según el número de CPU disponibles en nuestra máquina (podemos obtener esto usando `os.cpu_count()` de Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57040d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta conjuntos de datos en DataLoader\n",
    "import os\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataloader_augmented = DataLoader(train_data_augmented, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True,\n",
    "                                        num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader_simple = DataLoader(test_data_simple, \n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=NUM_WORKERS)\n",
    "\n",
    "train_dataloader_augmented, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f60d4",
   "metadata": {},
   "source": [
    "### 9.3 Construir y entrenar el Modelo 1\n",
    "\n",
    "¡Datos cargados!\n",
    "\n",
    "Ahora, para construir nuestro próximo modelo, `model_1`, podemos reutilizar nuestra clase `TinyVGG` de antes. \n",
    "\n",
    "Nos aseguraremos de enviarlo al dispositivo de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree model_1 y envíelo al dispositivo de destino\n",
    "torch.manual_seed(42)\n",
    "model_1 = TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(train_data_augmented.classes)).to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49900140",
   "metadata": {},
   "source": [
    "¡Modelo listo!\n",
    "\n",
    "¡Es hora de entrenar!\n",
    "\n",
    "Como ya tenemos funciones para el bucle de entrenamiento (`train_step()`) y el bucle de prueba (`test_step()`) y una función para juntarlos en `train()`, reutilicémoslas.\n",
    "\n",
    "Usaremos la misma configuración que `model_0` con solo variar el parámetro `train_dataloader`:\n",
    "* Entrena durante 5 épocas.\n",
    "* Utilice `train_dataloader=train_dataloader_augmented` como datos de entrenamiento en `train()`.\n",
    "* Utilice `torch.nn.CrossEntropyLoss()` como función de pérdida (ya que estamos trabajando con clasificación de clases múltiples).\n",
    "* Utilice `torch.optim.Adam()` con `lr=0.001` como tasa de aprendizaje como optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer semillas aleatorias\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Establecer número de épocas\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Función de pérdida de configuración y optimizador.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n",
    "\n",
    "# iniciar el cronómetro\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Modelo de tren_1\n",
    "model_1_results = train(model=model_1, \n",
    "                        train_dataloader=train_dataloader_augmented,\n",
    "                        test_dataloader=test_dataloader_simple,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# Finalice el cronómetro e imprima cuánto tiempo tardó\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd08589",
   "metadata": {},
   "source": [
    "Mmm...\n",
    "\n",
    "No parece que nuestro modelo haya vuelto a funcionar muy bien.\n",
    "\n",
    "Veamos sus curvas de pérdidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac51434",
   "metadata": {},
   "source": [
    "### 9.4 Trazar las curvas de pérdidas del Modelo 1\n",
    "\n",
    "Como tenemos los resultados de `model_1` guardados en un diccionario de resultados, `model_1_results`, podemos trazarlos usando `plot_loss_curves()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34aed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf856b",
   "metadata": {},
   "source": [
    "Guau...\n",
    "\n",
    "Estos tampoco tienen muy buena pinta...\n",
    "\n",
    "¿Nuestro modelo está **insuficiente** o **sobreajustado**?\n",
    "\n",
    "¿O ambos?\n",
    "\n",
    "Idealmente nos gustaría que tuviera mayor precisión y menor pérdida, ¿verdad?\n",
    "\n",
    "¿Cuáles son algunos métodos que podría intentar utilizar para lograrlos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7863d58",
   "metadata": {},
   "source": [
    "## 10. Comparar los resultados del modelo\n",
    "\n",
    "Aunque nuestros modelos funcionan bastante mal, aún podemos escribir código para compararlos.\n",
    "\n",
    "Primero convirtamos los resultados de nuestro modelo en pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_0_df = pd.DataFrame(model_0_results)\n",
    "model_1_df = pd.DataFrame(model_1_results)\n",
    "model_0_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3175c38",
   "metadata": {},
   "source": [
    "Y ahora podemos escribir un código de trazado usando `matplotlib` para visualizar los resultados de `model_0` y `model_1` juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fa7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar una trama\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Obtener número de épocas\n",
    "epochs = range(len(model_0_df))\n",
    "\n",
    "# Trama de pérdida del tren\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Pérdida de prueba de trama\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Trazar la precisión del tren\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
    "plt.title(\"Train Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Precisión de la prueba de trazado\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
    "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14ee90",
   "metadata": {},
   "source": [
    "Parece que nuestros modelos tuvieron un desempeño igualmente pobre y fueron algo esporádicos (las métricas suben y bajan bruscamente).\n",
    "\n",
    "Si construyeras \"model_2\", ¿qué harías diferente para intentar mejorar el rendimiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3743d2",
   "metadata": {},
   "source": [
    "## 11. Haz una predicción sobre una imagen personalizada.\n",
    "\n",
    "Si ha entrenado un modelo en un determinado conjunto de datos, es probable que desee hacer una predicción con sus propios datos personalizados.\n",
    "\n",
    "En nuestro caso, dado que hemos entrenado un modelo con imágenes de pizza, bistec y sushi, ¿cómo podríamos usar nuestro modelo para hacer una predicción sobre una de nuestras propias imágenes?\n",
    "\n",
    "Para hacerlo, podemos cargar una imagen y luego **preprocesarla de una manera que coincida con el tipo de datos con los que se entrenó nuestro modelo**.\n",
    "\n",
    "En otras palabras, tendremos que convertir nuestra propia imagen personalizada en un tensor y asegurarnos de que esté en el tipo de datos correcto antes de pasarla a nuestro modelo.\n",
    "\n",
    "Comencemos descargando una imagen personalizada.\n",
    "\n",
    "Dado que nuestro modelo predice si una imagen contiene pizza, bistec o sushi, descarguemos una foto de [mi papá dando el visto bueno a una pizza grande de Learn PyTorch for Deep Learning GitHub](https://github.com/mrdbourke/ pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg).\n",
    "\n",
    "Descargamos la imagen usando el módulo `solicitudes` de Python.\n",
    "\n",
    "> **Nota:** Si estás usando Google Colab, también puedes cargar una imagen a la sesión actual yendo al menú del lado izquierdo -> Archivos -> Cargar en el almacenamiento de la sesión. Sin embargo, tenga cuidado, esta imagen se eliminará cuando finalice su sesión de Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2568564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar imagen personalizada\n",
    "import requests\n",
    "\n",
    "# Configurar ruta de imagen personalizada\n",
    "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
    "\n",
    "# Descarga la imagen si aún no existe\n",
    "if not custom_image_path.is_file():\n",
    "    with open(custom_image_path, \"wb\") as f:\n",
    "        # When downloading from GitHub, need to use the \"raw\" file link\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "        print(f\"Downloading {custom_image_path}...\")\n",
    "        f.write(request.content)\n",
    "else:\n",
    "    print(f\"{custom_image_path} already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410602ad",
   "metadata": {},
   "source": [
    "### 11.1 Cargando una imagen personalizada con PyTorch\n",
    "\n",
    "¡Excelente!\n",
    "\n",
    "Parece que tenemos una imagen personalizada descargada y lista para usar en `data/04-pizza-dad.jpeg`. \n",
    "\n",
    "Es hora de cargarlo.\n",
    "\n",
    "`torchvision` de PyTorch tiene varios métodos de entrada y salida (\"IO\" o \"io\" para abreviar) para leer y escribir imágenes y videos en [`torchvision.io`](https://pytorch.org/vision/stable/io .html).\n",
    "\n",
    "Como queremos cargar una imagen, usaremos [`torchvision.io.read_image()`](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io .read_image).\n",
    "\n",
    "Este método leerá una imagen JPEG o PNG y la convertirá en un `torch.Tensor` tridimensional RGB o en escala de grises con valores del tipo de datos `uint8` en el rango `[0, 255]`.\n",
    "\n",
    "Probémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Leer en imagen personalizada\n",
    "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
    "\n",
    "# Imprimir datos de imagen\n",
    "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
    "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
    "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616e448",
   "metadata": {},
   "source": [
    "¡Lindo! Parece que nuestra imagen está en formato tensorial; sin embargo, ¿este formato de imagen es compatible con nuestro modelo?\n",
    "\n",
    "Nuestro tensor `custom_image` es del tipo de datos `torch.uint8` y sus valores están entre `[0, 255]`.\n",
    "\n",
    "Pero nuestro modelo toma tensores de imagen del tipo de datos `torch.float32` y con valores entre `[0, 1]`.\n",
    "\n",
    "Entonces, antes de usar nuestra imagen personalizada con nuestro modelo, **tendremos que convertirla al mismo formato que los datos con los que se entrena nuestro modelo**.\n",
    "\n",
    "Si no hacemos esto, nuestro modelo generará un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intente hacer una predicción sobre la imagen en formato uint8 (esto generará un error)\n",
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    model_1(custom_image_uint8.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd7a89",
   "metadata": {},
   "source": [
    "Si intentamos hacer una predicción sobre una imagen en un tipo de datos diferente al que se entrenó nuestro modelo, obtenemos un error como el siguiente:\n",
    "\n",
    "> `RuntimeError: El tipo de entrada (torch.cuda.ByteTensor) y el tipo de peso (torch.cuda.FloatTensor) deben ser los mismos`\n",
    "\n",
    "Arreglemos este problema convirtiendo nuestra imagen personalizada al mismo tipo de datos en el que se entrenó nuestro modelo (`torch.float32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargue una imagen personalizada y convierta los valores del tensor a float32\n",
    "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
    "\n",
    "# Divida los valores de píxeles de la imagen por 255 para obtenerlos entre [0, 1]\n",
    "custom_image = custom_image / 255. \n",
    "\n",
    "# Imprimir datos de imagen\n",
    "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
    "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
    "print(f\"Custom image dtype: {custom_image.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1118357",
   "metadata": {},
   "source": [
    "### 11.2 Predicción de imágenes personalizadas con un modelo PyTorch entrenado\n",
    "\n",
    "Hermoso, parece que nuestros datos de imagen ahora están en el mismo formato en el que se entrenó nuestro modelo.\n",
    "\n",
    "Excepto por una cosa...\n",
    "\n",
    "Es \"forma\".\n",
    "\n",
    "Nuestro modelo fue entrenado en imágenes con forma \"[3, 64, 64]\", mientras que nuestra imagen personalizada actualmente es \"[3, 4032, 3024]\". \n",
    "\n",
    "¿Cómo podemos asegurarnos de que nuestra imagen personalizada tenga la misma forma que las imágenes en las que se entrenó nuestro modelo?\n",
    "\n",
    "¿Hay algún `torchvision.transforms` que pueda ayudar?\n",
    "\n",
    "Antes de responder esa pregunta, tracemos la imagen con `matplotlib` para asegurarnos de que se vea bien. Recuerde que tendremos que permutar las dimensiones de `CHW` a `HWC` para adaptarlas a los requisitos de `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ee343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazar imagen personalizada\n",
    "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
    "plt.title(f\"Image shape: {custom_image.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb79a6",
   "metadata": {},
   "source": [
    "¡Dos pulgares arriba!\n",
    "\n",
    "Ahora bien, ¿cómo podríamos hacer que nuestra imagen tenga el mismo tamaño que las imágenes en las que se entrenó nuestro modelo?\n",
    "\n",
    "Una forma de hacerlo es con `torchvision.transforms.Resize()`.\n",
    "\n",
    "Compongamos una canalización de transformación para hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear canal de transformación para cambiar el tamaño de la imagen\n",
    "custom_image_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "])\n",
    "\n",
    "# Transformar imagen de destino\n",
    "custom_image_transformed = custom_image_transform(custom_image)\n",
    "\n",
    "# Imprime la forma original y la nueva forma.\n",
    "print(f\"Original shape: {custom_image.shape}\")\n",
    "print(f\"New shape: {custom_image_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e74c72",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Finalmente hagamos una predicción sobre nuestra propia imagen personalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    custom_image_pred = model_1(custom_image_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4ed26",
   "metadata": {},
   "source": [
    "Oh Dios mío...\n",
    "\n",
    "A pesar de nuestros preparativos, nuestra imagen y modelo personalizados están en diferentes dispositivos.\n",
    "\n",
    "Y obtenemos el error:\n",
    "\n",
    "> `RuntimeError: Se esperaba que todos los tensores estuvieran en el mismo dispositivo, pero encontré al menos dos dispositivos, cpu y cuda:0. (al comprobar el peso del argumento en el método wrapper___slow_conv2d_forward)`\n",
    "\n",
    "Arreglemos eso poniendo nuestra `custom_image_transformed` en el dispositivo de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff738fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    custom_image_pred = model_1(custom_image_transformed.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f92a2c",
   "metadata": {},
   "source": [
    "¿Ahora que?\n",
    "\n",
    "Parece que estamos recibiendo un error de forma.\n",
    "\n",
    "¿Por qué podría ser esto?\n",
    "\n",
    "Convertimos nuestra imagen personalizada para que tenga el mismo tamaño que las imágenes en las que se entrenó nuestro modelo...\n",
    "\n",
    "Oh espera...\n",
    "\n",
    "Hay una dimensión que nos olvidamos.\n",
    "\n",
    "El tamaño del lote.\n",
    "\n",
    "Nuestro modelo espera tensores de imagen con una dimensión de tamaño de lote al inicio (\"NCHW\" donde \"N\" es el tamaño de lote).\n",
    "\n",
    "Excepto que nuestra imagen personalizada actualmente es solo \"CHW\". \n",
    "\n",
    "Podemos agregar una dimensión de tamaño de lote usando `torch.unsqueeze(dim=0)` para agregar una dimensión adicional a nuestra imagen y *finalmente* hacer una predicción. \n",
    "\n",
    "Básicamente, le indicaremos a nuestro modelo que prediga en una sola imagen (una imagen con un `batch_size` de 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    # Add an extra dimension to image\n",
    "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
    "    \n",
    "    # Print out different shapes\n",
    "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
    "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
    "    \n",
    "    # Make a prediction on image with an extra dimension\n",
    "    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f27c2e",
   "metadata": {},
   "source": [
    "¡¡¡Sí!!!\n",
    "\n",
    "¡Parece que funcionó!\n",
    "\n",
    "> **Nota:** Lo que acabamos de analizar son tres de los problemas clásicos y más comunes de aprendizaje profundo y PyTorch:\n",
    "> 1. **Tipos de datos incorrectos**: nuestro modelo espera `torch.float32` donde nuestra imagen personalizada original era `uint8`.\n",
    "> 2. **Dispositivo incorrecto**: nuestro modelo estaba en el \"dispositivo\" de destino (en nuestro caso, la GPU), mientras que nuestros datos de destino aún no se habían movido al \"dispositivo\" de destino.\n",
    "> 3. **Formas incorrectas**: nuestro modelo esperaba una imagen de entrada con la forma `[N, C, H, W]` o `[batch_size, color_channels, height, width]` mientras que nuestro tensor de imagen personalizado tenía la forma ` [canales_color, alto, ancho]`.\n",
    ">\n",
    "> Tenga en cuenta que estos errores no son solo para predecir en imágenes personalizadas. \n",
    ">\n",
    "> Estarán presentes en casi todos los tipos de datos (texto, audio, datos estructurados) y problemas con los que trabaje.\n",
    "\n",
    "Ahora echemos un vistazo a las predicciones de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71074f84",
   "metadata": {},
   "source": [
    "Muy bien, estos todavía están en *forma logit* (las salidas sin procesar de un modelo se llaman logits).\n",
    "\n",
    "Convirtámoslos de logits -> probabilidades de predicción -> etiquetas de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b015aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprima logs de predicción\n",
    "print(f\"Prediction logits: {custom_image_pred}\")\n",
    "\n",
    "# Convertir logits -> probabilidades de predicción (usando torch.softmax() para clasificación de clases múltiples)\n",
    "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
    "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
    "\n",
    "# Convertir probabilidades de predicción -> etiquetas de predicción\n",
    "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
    "print(f\"Prediction label: {custom_image_pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b2733",
   "metadata": {},
   "source": [
    "¡Está bien!\n",
    "\n",
    "Luciendo bien.\n",
    "\n",
    "Pero, por supuesto, nuestra etiqueta de predicción todavía está en forma de índice/tensor.\n",
    "\n",
    "Podemos convertirlo en una predicción de nombre de clase de cadena indexando en la lista `class_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentra la etiqueta prevista\n",
    "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
    "custom_image_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe43a92",
   "metadata": {},
   "source": [
    "Guau.\n",
    "\n",
    "Parece que el modelo hace la predicción correcta, a pesar de que tuvo un desempeño deficiente según nuestras métricas de evaluación.\n",
    "\n",
    "> **Nota:** El modelo en su forma actual predecirá \"pizza\", \"filete\" o \"sushi\" sin importar la imagen que se le dé. Si quisieras que tu modelo predijera en una clase diferente, tendrías que entrenarlo para hacerlo.\n",
    "\n",
    "Pero si verificamos `custom_image_pred_probs`, notaremos que el modelo otorga casi el mismo peso (los valores son similares) a cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los valores de las probabilidades de predicción son bastante similares.\n",
    "custom_image_pred_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d102c",
   "metadata": {},
   "source": [
    "Tener probabilidades de predicción tan similares podría significar un par de cosas:\n",
    "1. El modelo intenta predecir las tres clases al mismo tiempo (puede haber una imagen que contenga pizza, bistec y sushi).\n",
    "2. El modelo no sabe realmente lo que quiere predecir y, a su vez, simplemente asigna valores similares a cada una de las clases.\n",
    "\n",
    "Nuestro caso es el número 2, dado que nuestro modelo está mal entrenado, básicamente se trata de *adivinar* la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441a766",
   "metadata": {},
   "source": [
    "### 11.3 Armar la predicción de imágenes personalizadas: construir una función\n",
    "\n",
    "Realizar todos los pasos anteriores cada vez que desee hacer una predicción sobre una imagen personalizada rápidamente se volvería tedioso.\n",
    "\n",
    "Así que juntémoslos todos en una función que podamos usar fácilmente una y otra vez.\n",
    "\n",
    "Específicamente, hagamos una función que:\n",
    "1. Toma una ruta de imagen de destino y la convierte al tipo de datos correcto para nuestro modelo (`torch.float32`).\n",
    "2. Se asegura de que los valores de píxeles de la imagen de destino estén en el rango `[0, 1]`.\n",
    "3. Transforma la imagen de destino si es necesario.\n",
    "4. Se asegura de que el modelo esté en el dispositivo de destino.\n",
    "5. Realiza una predicción sobre la imagen de destino con un modelo entrenado (asegurándose de que la imagen tenga el tamaño correcto y esté en el mismo dispositivo que el modelo).\n",
    "6. Convierte los logits de salida del modelo en probabilidades de predicción.\n",
    "7. Convierte las probabilidades de predicción en etiquetas de predicción.\n",
    "8. Traza la imagen de destino junto con la predicción del modelo y la probabilidad de predicción.\n",
    "\n",
    "¡Unos pocos pasos, pero lo tenemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe509020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_plot_image(model: torch.nn.Module, \n",
    "                        image_path: str, \n",
    "                        class_names: List[str] = None, \n",
    "                        transform=None,\n",
    "                        device: torch.device = device):\n",
    "    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n",
    "    \n",
    "    # 1. Load in image and convert the tensor values to float32\n",
    "    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
    "    \n",
    "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
    "    target_image = target_image / 255. \n",
    "    \n",
    "    # 3. Transform if necessary\n",
    "    if transform:\n",
    "        target_image = transform(target_image)\n",
    "    \n",
    "    # 4. Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "    \n",
    "    # 5. Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Add an extra dimension to the image\n",
    "        target_image = target_image.unsqueeze(dim=0)\n",
    "    \n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(target_image.to(device))\n",
    "        \n",
    "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # 7. Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "    \n",
    "    # 8. Plot the image alongside the prediction and prediction probability\n",
    "    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n",
    "    if class_names:\n",
    "        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
    "    else: \n",
    "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23568974",
   "metadata": {},
   "source": [
    "Qué función tan bonita, probémosla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0483b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred en nuestra imagen personalizada\n",
    "pred_and_plot_image(model=model_1,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names,\n",
    "                    transform=custom_image_transform,\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6faa42b",
   "metadata": {},
   "source": [
    "¡Dos pulgares arriba otra vez!\n",
    "\n",
    "Parece que nuestro modelo acertó en la predicción con solo adivinar.\n",
    "\n",
    "Sin embargo, este no será siempre el caso con otras imágenes...\n",
    "\n",
    "La imagen también está pixelada porque cambiamos su tamaño a \"[64, 64]\" usando \"custom_image_transform\".\n",
    "\n",
    "> **Ejercicio:** Intenta hacer una predicción con una de tus propias imágenes de pizza, bistec o sushi y observa qué sucede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d9b36",
   "metadata": {},
   "source": [
    "## Principales conclusiones\n",
    "\n",
    "Hemos cubierto bastante en este módulo.\n",
    "\n",
    "Resumámoslo con algunos puntos.\n",
    "\n",
    "* PyTorch tiene muchas funciones integradas para manejar todo tipo de datos, desde visión hasta texto, audio y sistemas de recomendación.\n",
    "* Si las funciones de carga de datos integradas de PyTorch no se adaptan a sus necesidades, puede escribir código para crear sus propios conjuntos de datos personalizados subclasificando `torch.utils.data.Dataset`.\n",
    "* `torch.utils.data.DataLoader`' en PyTorch ayuda a convertir su `Dataset` en iterables que se pueden usar al entrenar y probar un modelo.\n",
    "* Gran parte del aprendizaje automático trata del equilibrio entre **sobreajuste** y **desajuste** (anteriormente analizamos diferentes métodos para cada uno, por lo que un buen ejercicio sería investigar más y escribir código para probar las diferentes técnicas). ).\n",
    "* Es posible predecir sus propios datos personalizados con un modelo entrenado, siempre y cuando formatee los datos en un formato similar al formato en el que se entrenó el modelo. Asegúrese de ocuparse de los tres grandes errores de PyTorch y de aprendizaje profundo:\n",
    "    1. **Tipos de datos incorrectos**: su modelo esperaba `torch.float32` cuando sus datos son `torch.uint8`.\n",
    "    2. **Formas de datos incorrectas**: su modelo esperaba `[batch_size, color_channels, height, width]` cuando sus datos son `[color_channels, height, width]`.\n",
    "    3. **Dispositivos incorrectos**: su modelo está en la GPU pero sus datos están en la CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73bdfb",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Todos los ejercicios se centran en practicar el código de las secciones anteriores.\n",
    "\n",
    "Debería poder completarlos haciendo referencia a cada sección o siguiendo los recursos vinculados.\n",
    "\n",
    "Todos los ejercicios deben completarse utilizando [código independiente del dispositivo](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n",
    "\n",
    "**Recursos:**\n",
    "* [Cuaderno de plantilla de ejercicios para 04](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/04_pytorch_custom_datasets_exercises.ipynb)\n",
    "* [Cuaderno de soluciones de ejemplo para 04](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/04_pytorch_custom_datasets_exercise_solutions.ipynb) (pruebe los ejercicios *antes* de mirar esto)\n",
    "\n",
    "1. Nuestros modelos tienen un rendimiento deficiente (no se ajustan bien a los datos). ¿Cuáles son 3 métodos para prevenir el desajuste? Escríbelas y explica cada una con una frase.\n",
    "2. Recrea las funciones de carga de datos que creamos en las secciones 1, 2, 3 y 4. Deberías tener el `DataLoader` preparado y probado listo para usar.\n",
    "3. Recrea el `model_0` que construimos en la sección 7.\n",
    "4. Cree funciones de entrenamiento y prueba para `model_0`.\n",
    "5. Intenta entrenar el modelo que hiciste en el ejercicio 3 durante 5, 20 y 50 épocas, ¿qué pasa con los resultados?\n",
    "    * Utilice `torch.optim.Adam()` con una tasa de aprendizaje de 0,001 como optimizador. \n",
    "6. Duplica la cantidad de unidades ocultas en tu modelo y entrénalo durante 20 épocas, ¿qué pasa con los resultados?\n",
    "7. Duplica los datos que estás usando con tu modelo y entrénalo durante 20 épocas, ¿qué pasa con los resultados?\n",
    "    * **Nota:** Puede utilizar el [cuaderno de creación de datos personalizado](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) para ampliar su conjunto de datos de Food101 .\n",
    "    * También puede encontrar el [conjunto de datos de datos dobles ya formateados (subconjunto del 20% en lugar del 10%) en GitHub](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip ), deberá escribir el código de descarga como en el ejercicio 2 para incluirlo en este cuaderno.\n",
    "8. Haz una predicción sobre tu propia imagen personalizada de pizza/filete/sushi (incluso puedes descargar una de Internet) y comparte tu predicción. \n",
    "    * ¿El modelo que entrenó en el ejercicio 7 lo hace bien? \n",
    "    * Si no, ¿qué crees que podrías hacer para mejorarlo?\n",
    "\n",
    "## Extracurricular\n",
    "\n",
    "* Para practicar su conocimiento de los `Dataset` y `DataLoader` de PyTorch a través de PyTorch [cuaderno tutorial de conjuntos de datos y cargadores de datos] (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "* Dedique 10 minutos a leer la [documentación de PyTorch `torchvision.transforms`] (https://pytorch.org/vision/stable/transforms.html).\n",
    "    * Puede ver demostraciones de transformaciones en acción en el [tutorial de ilustraciones de transformaciones](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#Illustration-of-transforms). \n",
    "* Dedique 10 minutos a leer la [documentación `torchvision.datasets`] de PyTorch (https://pytorch.org/vision/stable/datasets.html).\n",
    "    * ¿Cuáles son algunos conjuntos de datos que le llaman la atención?\n",
    "    * ¿Cómo podrías intentar construir un modelo sobre estos?\n",
    "* [TorchData está actualmente en versión beta](https://pytorch.org/data/beta/index.html) (a partir de abril de 2022), será una forma futura de cargar datos en PyTorch, pero puedes comenzar a Échale un vistazo ahora. \n",
    "* Para acelerar los modelos de aprendizaje profundo, puede hacer algunos trucos para mejorar la computación, la memoria y los cálculos generales. Para obtener más información, lea la publicación [*Cómo hacer que el aprendizaje profundo sea mejor desde los primeros principios*](https://horace.io/brrr_intro .html) de Horace He."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
