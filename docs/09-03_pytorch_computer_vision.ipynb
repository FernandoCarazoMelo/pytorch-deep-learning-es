{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b03626",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb\" target=\"_parent\"><img src=\"https:// colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/></a>\n",
    "\n",
    "[Ver código fuente](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb) | [Ver diapositivas](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/03_pytorch_computer_vision.pdf) | [Ver vídeo tutorial](https://youtu.be/Z_ikDlimN6A?t=50417)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe4bf2",
   "metadata": {},
   "source": [
    "# 03. Visión por computadora PyTorch\n",
    "\n",
    "[Visión por computadora](https://en.wikipedia.org/wiki/Computer_vision) es el arte de enseñarle a ver a una computadora.\n",
    "\n",
    "Por ejemplo, podría implicar la construcción de un modelo para clasificar si una foto es de un gato o de un perro ([clasificación binaria](https://developers.google.com/machine-learning/glossary#binary-classification)).\n",
    "\n",
    "O si una foto es de un gato, un perro o una gallina ([clasificación multiclase](https://developers.google.com/machine-learning/glossary#multi-class-classification)).\n",
    "\n",
    "O identificar dónde aparece un automóvil en un cuadro de video ([detección de objetos](https://en.wikipedia.org/wiki/Object_detection)).\n",
    "\n",
    "O descubrir dónde se pueden separar los diferentes objetos de una imagen ([segmentación panóptica](https://arxiv.org/abs/1801.00868)).\n",
    "\n",
    "![ejemplos de problemas de visión por computadora](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png)\n",
    "*Ejemplos de problemas de visión por computadora para clasificación binaria, clasificación multiclase, detección y segmentación de objetos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea4c95",
   "metadata": {},
   "source": [
    "## ¿Dónde se utiliza la visión por computadora?\n",
    "\n",
    "Si usa un teléfono inteligente, ya ha utilizado la visión por computadora.\n",
    "\n",
    "Las aplicaciones de cámara y fotografía utilizan [visión por computadora para mejorar](https://machinelearning.apple.com/research/panoptic-segmentation) y ordenan imágenes.\n",
    "\n",
    "Los automóviles modernos utilizan [visión por computadora](https://youtu.be/j0z4FweCy4M?t=2989) para evitar otros automóviles y mantenerse dentro de las líneas de los carriles.\n",
    "\n",
    "Los fabricantes utilizan la visión por computadora para identificar defectos en varios productos.\n",
    "\n",
    "Las cámaras de seguridad utilizan visión por computadora para detectar posibles intrusos.\n",
    "\n",
    "En esencia, cualquier cosa que pueda describirse en un sentido visual puede ser un posible problema de visión por computadora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589167e",
   "metadata": {},
   "source": [
    "## Qué vamos a cubrir\n",
    "\n",
    "Aplicaremos el flujo de trabajo de PyTorch que hemos estado aprendiendo en las últimas secciones a la visión por computadora.\n",
    "\n",
    "![un flujo de trabajo de PyTorch con enfoque en visión por computadora](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png)\n",
    "\n",
    "Específicamente, cubriremos:\n",
    "\n",
    "| **Tema** | **Contenido** |\n",
    "| ----- | ----- |\n",
    "| **0. Bibliotecas de visión por computadora en PyTorch** | PyTorch tiene un montón de bibliotecas de visión por computadora útiles integradas, echémosle un vistazo.  |\n",
    "| **1. Cargar datos** | Para practicar la visión por computadora, comenzaremos con algunas imágenes de diferentes prendas de vestir de [FashionMNIST] (https://github.com/zalandoresearch/fashion-mnist). |\n",
    "| **2. Preparar datos** | Tenemos algunas imágenes, carguémoslas con un [PyTorch `DataLoader`](https://pytorch.org/docs/stable/data.html) para que podamos usarlas con nuestro bucle de entrenamiento. |\n",
    "| **3. Modelo 0: construcción de un modelo de referencia** | Aquí crearemos un modelo de clasificación de múltiples clases para aprender patrones en los datos, también elegiremos una **función de pérdida**, un **optimizador** y crearemos un **bucle de entrenamiento**. | \n",
    "| **4. Hacer predicciones y evaluar el modelo 0** | Hagamos algunas predicciones con nuestro modelo de referencia y evalúémoslas. |\n",
    "| **5. Configurar código independiente del dispositivo para modelos futuros** | Es una buena práctica escribir código independiente del dispositivo, así que configurémoslo. |\n",
    "| **6. Modelo 1: Agregar no linealidad** | Experimentar es una gran parte del aprendizaje automático. Intentemos mejorar nuestro modelo de referencia agregando capas no lineales. |\n",
    "| **7. Modelo 2: Red neuronal convolucional (CNN)** | Es hora de especificar la visión por computadora e introducir la poderosa arquitectura de red neuronal convolucional. |\n",
    "| **8. Comparando nuestros modelos** | Hemos construido tres modelos diferentes, comparémoslos. |\n",
    "| **9. Evaluando nuestro mejor modelo** | Hagamos algunas predicciones sobre imágenes aleatorias y evaluemos nuestro mejor modelo. |\n",
    "| **10. Haciendo una matriz de confusión** | Una matriz de confusión es una excelente manera de evaluar un modelo de clasificación; veamos cómo podemos crear una. |\n",
    "| **11. Guardar y cargar el modelo con mejor rendimiento** | Dado que es posible que queramos usar nuestro modelo para más adelante, guardémoslo y asegurémonos de que se vuelva a cargar correctamente. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19702966",
   "metadata": {},
   "source": [
    "## ¿Dónde puedes conseguir ayuda?\n",
    "\n",
    "Todos los materiales de este curso [en vivo en GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
    "\n",
    "Si tiene problemas, también puede hacer una pregunta en el curso [página de debates de GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
    "\n",
    "Y, por supuesto, está la [documentación de PyTorch](https://pytorch.org/docs/stable/index.html) y los [foros de desarrolladores de PyTorch](https://discuss.pytorch.org/), un lugar muy útil para todo lo relacionado con PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de173a8e",
   "metadata": {},
   "source": [
    "## 0. Bibliotecas de visión por computadora en PyTorch\n",
    "\n",
    "Antes de comenzar a escribir código, hablemos de algunas bibliotecas de visión por computadora de PyTorch que debe conocer.\n",
    "\n",
    "| Módulo PyTorch | ¿Qué hace? |\n",
    "| ----- | ----- |\n",
    "| [`torchvision`](https://pytorch.org/vision/stable/index.html) | Contiene conjuntos de datos, arquitecturas de modelos y transformaciones de imágenes que se utilizan a menudo para problemas de visión por computadora. |\n",
    "| [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) | Aquí encontrará muchos conjuntos de datos de visión por computadora de ejemplo para una variedad de problemas, desde clasificación de imágenes, detección de objetos, subtítulos de imágenes, clasificación de videos y más. También contiene [una serie de clases base para crear conjuntos de datos personalizados](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |\n",
    "| [`torchvision.models`](https://pytorch.org/vision/stable/models.html) | Este módulo contiene arquitecturas de modelos de visión por computadora de buen rendimiento y de uso común implementadas en PyTorch; puede usarlas con sus propios problemas. | \n",
    "| [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) | A menudo, las imágenes deben transformarse (convertirse en números/procesarse/aumentarse) antes de usarse con un modelo; las transformaciones de imágenes comunes se encuentran aquí. | \n",
    "| [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | Clase de conjunto de datos base para PyTorch.  | \n",
    "| [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) | Crea un iterable de Python sobre un conjunto de datos (creado con `torch.utils.data.Dataset`). |\n",
    "\n",
    "> **Nota:** Las clases `torch.utils.data.Dataset` y `torch.utils.data.DataLoader` no son solo para visión por computadora en PyTorch, sino que son capaces de manejar muchos tipos diferentes de datos.\n",
    "\n",
    "Ahora que hemos cubierto algunas de las bibliotecas de visión por computadora de PyTorch más importantes, importemos las dependencias relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Importar visión de antorcha\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Importar matplotlib para visualización\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Consultar versiones\n",
    "# Nota: su versión de PyTorch no debe ser inferior a 1.10.0 y la versión de torchvision no debe ser inferior a 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4211d",
   "metadata": {},
   "source": [
    "## 1. Obtener un conjunto de datos\n",
    "\n",
    "Para comenzar a trabajar en un problema de visión por computadora, obtengamos un conjunto de datos de visión por computadora.\n",
    "\n",
    "Vamos a empezar con FashionMNIST.\n",
    "\n",
    "MNIST significa Instituto Nacional Modificado de Estándares y Tecnología.\n",
    "\n",
    "El [conjunto de datos MNIST original] (https://en.wikipedia.org/wiki/MNIST_database) contiene miles de ejemplos de dígitos escritos a mano (del 0 al 9) y se utilizó para crear modelos de visión por computadora para identificar números para los servicios postales.\n",
    "\n",
    "[FashionMNIST](https://github.com/zalandoresearch/fashion-mnist), creado por Zalando Research, es una configuración similar. \n",
    "\n",
    "Excepto que contiene imágenes en escala de grises de 10 tipos diferentes de ropa.\n",
    "\n",
    "![imagen de ejemplo de FashionMNIST](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png)\n",
    "*`torchvision.datasets` contiene muchos conjuntos de datos de ejemplo que puedes usar para practicar la escritura de código de visión por computadora. FashionMNIST es uno de esos conjuntos de datos. Y dado que tiene 10 clases de imágenes diferentes (diferentes tipos de ropa), es un problema de clasificación de clases múltiples.*\n",
    "\n",
    "Más adelante, construiremos una red neuronal de visión por computadora para identificar los diferentes estilos de ropa en estas imágenes.\n",
    "\n",
    "PyTorch tiene un montón de conjuntos de datos de visión por computadora comunes almacenados en \"torchvision.datasets\".\n",
    "\n",
    "Incluyendo FashionMNIST en [`torchvision.datasets.FashionMNIST()`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html).\n",
    "\n",
    "Para descargarlo, proporcionamos los siguientes parámetros:\n",
    "* `root: str` - ¿a qué carpeta desea descargar los datos?\n",
    "* `train: Bool` - ¿Quieres dividir el entrenamiento o la prueba?\n",
    "* `descargar: Bool` - ¿deben descargarse los datos?\n",
    "* `transform: torchvision.transforms`: ¿qué transformaciones le gustaría realizar en los datos?\n",
    "* `target_transform`: también puedes transformar los objetivos (etiquetas) si lo deseas.\n",
    "\n",
    "Muchos otros conjuntos de datos en \"torchvision\" tienen estas opciones de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar datos de entrenamiento\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Configurar datos de prueba\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e37074",
   "metadata": {},
   "source": [
    "Veamos la primera muestra de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el primer ejemplo de entrenamiento\n",
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7eb7c",
   "metadata": {},
   "source": [
    "### 1.1 Formas de entrada y salida de un modelo de visión por computadora\n",
    "\n",
    "Tenemos un gran tensor de valores (la imagen) que conduce a un valor único para el objetivo (la etiqueta).\n",
    "\n",
    "Veamos la forma de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cuál es la forma de la imagen?\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268e24d",
   "metadata": {},
   "source": [
    "La forma del tensor de imagen es `[1, 28, 28]` o más específicamente:\n",
    "\n",
    "```\n",
    "[color_channels=1, alto=28, ancho=28]\n",
    "```\n",
    "\n",
    "Tener `color_channels=1` significa que la imagen está en escala de grises.\n",
    "\n",
    "![ejemplo de formas de entrada y salida del problema fashionMNIST](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png )\n",
    "*Varios problemas tendrán diversas formas de entrada y salida. Pero la premisa sigue siendo: codificar datos en números, construir un modelo para encontrar patrones en esos números, convertir esos patrones en algo significativo.*\n",
    "\n",
    "Si `color_channels=3`, la imagen viene en valores de píxeles para rojo, verde y azul (esto también se conoce como [modelo de color RGB] (https://en.wikipedia.org/wiki/RGB_color_model)).\n",
    "\n",
    "El orden de nuestro tensor actual a menudo se denomina \"CHW\" (Canales de color, alto, ancho).\n",
    "\n",
    "Existe un debate sobre si las imágenes deben representarse como \"CHW\" (canales de color primero) o \"HWC\" (canales de color al final).\n",
    "\n",
    "> **Nota:** También verá los formatos `NCHW` y `NHWC` donde `N` significa *número de imágenes*. Por ejemplo, si tiene un `batch_size=32`, la forma de su tensor puede ser `[32, 1, 28, 28]`. Cubriremos los tamaños de lote más adelante.\n",
    "\n",
    "PyTorch generalmente acepta `NCHW` (canales primero) como valor predeterminado para muchos operadores.\n",
    "\n",
    "Sin embargo, PyTorch también explica que \"NHWC\" (los últimos canales) funcionan mejor y se [considera una mejor práctica] (https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice). \n",
    "\n",
    "Por ahora, dado que nuestro conjunto de datos y modelos son relativamente pequeños, esto no supondrá una gran diferencia.\n",
    "\n",
    "Pero téngalo en cuenta cuando trabaje en conjuntos de datos de imágenes más grandes y utilice redes neuronales convolucionales (las veremos más adelante).\n",
    "\n",
    "Veamos más formas de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cuántas muestras hay?\n",
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31edba47",
   "metadata": {},
   "source": [
    "Tenemos 60.000 muestras de entrenamiento y 10.000 muestras de prueba.\n",
    "\n",
    "¿Qué clases hay?\n",
    "\n",
    "Podemos encontrarlos a través del atributo `.classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver clases\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d87c25",
   "metadata": {},
   "source": [
    "¡Dulce! Parece que estamos ante 10 tipos diferentes de ropa.\n",
    "\n",
    "Debido a que estamos trabajando con 10 clases diferentes, significa que nuestro problema es **clasificación de clases múltiples**.\n",
    "\n",
    "Seamos visuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d49a6",
   "metadata": {},
   "source": [
    "### 1.2 Visualizando nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455fb397",
   "metadata": {},
   "source": [
    "Podemos convertir la imagen en escala de grises usando el parámetro `cmap` de `plt.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6a104",
   "metadata": {},
   "source": [
    "Hermoso, tan hermoso como lo puede ser un botín pixelado en escala de grises.\n",
    "\n",
    "Veamos algunos más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trazar más imágenes\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 4, 4\n",
    "for i in range(1, rows * cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f2127",
   "metadata": {},
   "source": [
    "Hmmm, este conjunto de datos no parece demasiado estético.\n",
    "\n",
    "Pero los principios que aprenderemos sobre cómo construir un modelo serán similares en una amplia gama de problemas de visión por computadora.\n",
    "\n",
    "En esencia, tomar valores de píxeles y construir un modelo para encontrar patrones en ellos para usarlos en valores de píxeles futuros.\n",
    "\n",
    "Además, incluso para este pequeño conjunto de datos (sí, incluso 60.000 imágenes en aprendizaje profundo se consideran bastante pequeñas), ¿podrías escribir un programa para clasificar cada una de ellas?\n",
    "\n",
    "Probablemente podrías.\n",
    "\n",
    "Pero creo que codificar un modelo en PyTorch sería más rápido.\n",
    "\n",
    "> **Pregunta:** ¿Crees que los datos anteriores se pueden modelar solo con líneas rectas (lineales)? ¿O crees que también necesitarías líneas no rectas (no lineales)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6471eb6",
   "metadata": {},
   "source": [
    "## 2. Preparar el cargador de datos\n",
    "\n",
    "Ahora tenemos un conjunto de datos listo para funcionar.\n",
    "\n",
    "El siguiente paso es prepararlo con un [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) o `DataLoader` para corto.\n",
    "\n",
    "El `DataLoader` hace lo que usted cree que podría hacer.\n",
    "\n",
    "Ayuda a cargar datos en un modelo.\n",
    "\n",
    "Para entrenamiento y para inferencia.\n",
    "\n",
    "Convierte un gran \"conjunto de datos\" en un Python iterable de fragmentos más pequeños.\n",
    "\n",
    "Estos fragmentos más pequeños se denominan **lotes** o **minilotes** y se pueden configurar mediante el parámetro `batch_size`.\n",
    "\n",
    "¿Por qué hacer esto?\n",
    "\n",
    "Porque es más eficiente computacionalmente.\n",
    "\n",
    "En un mundo ideal, podría realizar el pase hacia adelante y hacia atrás a través de todos sus datos a la vez.\n",
    "\n",
    "Pero una vez que empiezas a utilizar conjuntos de datos realmente grandes, a menos que tengas una potencia informática infinita, es más fácil dividirlos en lotes.\n",
    "\n",
    "También le brinda a su modelo más oportunidades de mejorar.\n",
    "\n",
    "Con **minilotes** (pequeñas porciones de datos), el descenso de gradiente se realiza con más frecuencia por época (una vez por minilote en lugar de una vez por época).\n",
    "\n",
    "¿Cuál es un buen tamaño de lote?\n",
    "\n",
    "[32 es un buen lugar para comenzar](https://twitter.com/ylectun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) para una buena cantidad de problemas.\n",
    "\n",
    "Pero dado que este es un valor que puede establecer (un **hiperparámetro**), puede probar todos los tipos diferentes de valores, aunque generalmente se usan potencias de 2 con mayor frecuencia (por ejemplo, 32, 64, 128, 256, 512).\n",
    "\n",
    "![un ejemplo de cómo se ve un conjunto de datos por lotes](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png)\n",
    "*Lote de FashionMNIST con un tamaño de lote de 32 y reproducción aleatoria activada. Se producirá un proceso de procesamiento por lotes similar para otros conjuntos de datos, pero diferirá según el tamaño del lote.*\n",
    "\n",
    "Creemos `DataLoader` para nuestros conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configurar el hiperparámetro de tamaño de lote\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Convierta conjuntos de datos en iterables (lotes)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Veamos lo que hemos creado.\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mira lo que hay dentro del cargador de datos de entrenamiento\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e3925",
   "metadata": {},
   "source": [
    "Y podemos ver que los datos permanecen sin cambios al verificar una sola muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar una muestra\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cc8be",
   "metadata": {},
   "source": [
    "## 3. Modelo 0: construir un modelo de referencia\n",
    "\n",
    "¡Datos cargados y preparados!\n",
    "\n",
    "Es hora de crear un **modelo de referencia** subclasificando `nn.Module`.\n",
    "\n",
    "Un **modelo de referencia** es uno de los modelos más simples que puedas imaginar.\n",
    "\n",
    "Utiliza la línea de base como punto de partida e intenta mejorarla con modelos posteriores más complicados.\n",
    "\n",
    "Nuestra línea base constará de dos capas [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "Hemos hecho esto en una sección anterior, pero habrá una pequeña diferencia.\n",
    "\n",
    "Debido a que estamos trabajando con datos de imágenes, usaremos una capa diferente para comenzar.\n",
    "\n",
    "Y esa es la capa [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html).\n",
    "\n",
    "`nn.Flatten()` comprime las dimensiones de un tensor en un solo vector.\n",
    "\n",
    "Esto es más fácil de entender cuando lo ves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una capa aplanada\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Obtenga una sola muestra\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Aplanar la muestra\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Imprime lo que pasó\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
    "\n",
    "# Intente descomentar a continuación y vea qué sucede\n",
    "# imprimir(x)\n",
    "# imprimir (salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da63f9",
   "metadata": {},
   "source": [
    "La capa `nn.Flatten()` tomó nuestra forma de `[color_channels, height, width]` a `[color_channels, height*width]`.\n",
    "\n",
    "¿Por qué hacer esto?\n",
    "\n",
    "Porque ahora hemos convertido nuestros datos de píxeles de las dimensiones de alto y ancho en un **vector de características** largo.\n",
    "\n",
    "Y a las capas `nn.Linear()` les gusta que sus entradas estén en forma de vectores de características.\n",
    "\n",
    "Creemos nuestro primer modelo usando `nn.Flatten()` como primera capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7a709",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Tenemos una clase de modelo de referencia que podemos usar, ahora creemos una instancia de un modelo.\n",
    "\n",
    "Necesitaremos establecer los siguientes parámetros:\n",
    "* `input_shape=784`: esta es la cantidad de funciones que tienes en el modelo; en nuestro caso, es una por cada píxel de la imagen de destino (28 píxeles de alto por 28 píxeles de ancho = 784 funciones).\n",
    "* `hidden_units=10` - número de unidades/neuronas en las capas ocultas, este número puede ser el que quieras, pero para mantener el modelo pequeño comenzaremos con `10`.\n",
    "* `output_shape=len(class_names)`: dado que estamos trabajando con un problema de clasificación de clases múltiples, necesitamos una neurona de salida por clase en nuestro conjunto de datos.\n",
    "\n",
    "Creemos una instancia de nuestro modelo y enviémosla a la CPU por ahora (pronto ejecutaremos una pequeña prueba para ejecutar `model_0` en la CPU frente a un modelo similar en la GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Necesidad de configurar el modelo con parámetros de entrada\n",
    "model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_names) # one for every class\n",
    ")\n",
    "model_0.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fda73",
   "metadata": {},
   "source": [
    "### 3.1 Pérdida de configuración, optimizador y métricas de evaluación\n",
    "\n",
    "Dado que estamos trabajando en un problema de clasificación, introduzcamos nuestro [script `helper_functions.py`] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) y posteriormente el `accuracy_fn()` lo definimos en [cuaderno 02] (https://www.learnpytorch.io/02_pytorch_classification/).\n",
    "\n",
    "> **Nota:** En lugar de importar y utilizar nuestra propia función de precisión o métrica(s) de evaluación, puede importar varias métricas de evaluación desde el [paquete TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Descargue funciones auxiliares del repositorio de Learn PyTorch (si aún no las ha descargado)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de precisión de importación\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Función de pérdida de configuración y optimizador.\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ba535",
   "metadata": {},
   "source": [
    "### 3.2 Creando una función para cronometrar nuestros experimentos\n",
    "\n",
    "¡Función de pérdida y optimizador listos!\n",
    "\n",
    "Es hora de empezar a entrenar un modelo.\n",
    "\n",
    "Pero ¿qué tal si hacemos un pequeño experimento mientras entrenamos?\n",
    "\n",
    "Quiero decir, creemos una función de sincronización para medir el tiempo que le toma a nuestro modelo entrenarse en la CPU en comparación con usar una GPU.\n",
    "\n",
    "Entrenaremos este modelo en la CPU pero el siguiente en la GPU y veremos qué sucede.\n",
    "\n",
    "Nuestra función de sincronización importará la función [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer) del [módulo `timeit`](https ://docs.python.org/3/library/timeit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588ccd8",
   "metadata": {},
   "source": [
    "### 3.3 Crear un bucle de entrenamiento y entrenar un modelo en lotes de datos\n",
    "\n",
    "¡Hermoso!\n",
    "\n",
    "Parece que tenemos todas las piezas del rompecabezas listas para funcionar: un temporizador, una función de pérdida, un optimizador, un modelo y, lo más importante, algunos datos.\n",
    "\n",
    "Ahora creemos un bucle de entrenamiento y un bucle de prueba para entrenar y evaluar nuestro modelo.\n",
    "\n",
    "Usaremos los mismos pasos que en los cuadernos anteriores, aunque como nuestros datos ahora están en forma de lotes, agregaremos otro bucle para recorrer nuestros lotes de datos.\n",
    "\n",
    "Nuestros lotes de datos están contenidos en nuestros `DataLoader`s, `train_dataloader` y `test_dataloader` para las divisiones de datos de entrenamiento y prueba respectivamente.\n",
    "\n",
    "Un lote son muestras `BATCH_SIZE` de `X` (características) e `y` (etiquetas), ya que estamos usando `BATCH_SIZE=32`, nuestros lotes tienen 32 muestras de imágenes y objetivos.\n",
    "\n",
    "Y dado que estamos calculando lotes de datos, nuestras métricas de pérdida y evaluación se calcularán **por lote** en lugar de hacerlo en todo el conjunto de datos.\n",
    "\n",
    "Esto significa que tendremos que dividir nuestros valores de pérdida y precisión por la cantidad de lotes en el cargador de datos respectivo de cada conjunto de datos. \n",
    "\n",
    "Repasémoslo: \n",
    "1. Recorre épocas.\n",
    "2. Recorra los lotes de entrenamiento, realice los pasos de entrenamiento, calcule la pérdida del tren *por lote*.\n",
    "3. Recorra los lotes de prueba, realice los pasos de prueba, calcule la pérdida de prueba *por lote*.\n",
    "4. Imprime lo que está pasando.\n",
    "5. Calcula el tiempo (por diversión).\n",
    "\n",
    "Unos cuantos pasos, pero...\n",
    "\n",
    "...en caso de duda, codifíquelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar tqdm para la barra de progreso\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Establecer la semilla y poner en marcha el cronómetro\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Establece el número de épocas (lo mantendremos pequeño para tiempos de entrenamiento más rápidos)\n",
    "epochs = 3\n",
    "\n",
    "# Crear un ciclo de entrenamiento y prueba\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calcular el tiempo de entrenamiento.\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ee4b9",
   "metadata": {},
   "source": [
    "¡Lindo! Parece que a nuestro modelo de referencia le fue bastante bien.\n",
    "\n",
    "Tampoco tomó mucho tiempo entrenar, incluso solo en la CPU. Me pregunto si se acelerará en la GPU.\n",
    "\n",
    "Escribamos un código para evaluar nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ec5df",
   "metadata": {},
   "source": [
    "## 4. Haga predicciones y obtenga resultados del Modelo 0\n",
    "\n",
    "Dado que vamos a construir algunos modelos, es una buena idea escribir código para evaluarlos todos de manera similar.\n",
    "\n",
    "Es decir, creemos una función que admita un modelo entrenado, un `DataLoader`, una función de pérdida y una función de precisión.\n",
    "\n",
    "La función utilizará el modelo para hacer predicciones sobre los datos en el `DataLoader` y luego podremos evaluar esas predicciones usando la función de pérdida y la función de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b34e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calcular los resultados del modelo 0 en el conjunto de datos de prueba\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de2ea7",
   "metadata": {},
   "source": [
    "¡Luciendo bien!\n",
    "\n",
    "Podemos utilizar este diccionario para comparar los resultados del modelo de referencia con otros modelos más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152092ae",
   "metadata": {},
   "source": [
    "## 5. Configurar código independiente del dispositivo (para usar una GPU, si la hay)\n",
    "Hemos visto cuánto tiempo lleva entrenar mi modelo PyTorch en 60.000 muestras en la CPU.\n",
    "\n",
    "> **Nota:** El tiempo de entrenamiento del modelo depende del hardware utilizado. Generalmente, más procesadores significan un entrenamiento más rápido y los modelos más pequeños en conjuntos de datos más pequeños a menudo se entrenarán más rápido que los modelos y conjuntos de datos grandes.\n",
    "\n",
    "Ahora configuremos algo de [código independiente del dispositivo](https://pytorch.org/docs/stable/notes/cuda.html#best-practices) para que nuestros modelos y datos se ejecuten en GPU si está disponible.\n",
    "\n",
    "Si está ejecutando esta computadora portátil en Google Colab y aún no tiene una GPU encendida, ahora es el momento de encender una a través de `Runtime -> Cambiar tipo de tiempo de ejecución -> Acelerador de hardware -> GPU`. Si hace esto, es probable que su tiempo de ejecución se reinicie y tendrá que ejecutar todas las celdas anteriores yendo a \"Tiempo de ejecución -> Ejecutar antes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar código independiente del dispositivo\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959363d1",
   "metadata": {},
   "source": [
    "¡Hermoso!\n",
    "\n",
    "Construyamos otro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503deedc",
   "metadata": {},
   "source": [
    "## 6. Modelo 1: construcción de un modelo mejor con no linealidad\n",
    "\n",
    "Aprendimos sobre [el poder de la no linealidad en el cuaderno 02] (https://www.learnpytorch.io/02_pytorch_classification/#6-the-missing-piece-non-linearity).\n",
    "\n",
    "Viendo los datos con los que hemos estado trabajando, ¿crees que necesitan funciones no lineales?\n",
    "\n",
    "Y recuerda, lineal significa recto y no lineal significa no recto.\n",
    "\n",
    "Vamos a averiguar.\n",
    "\n",
    "Lo haremos recreando un modelo similar al anterior, excepto que esta vez colocaremos funciones no lineales (`nn.ReLU()`) entre cada capa lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo con capas lineales y no lineales.\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # flatten inputs into single vector\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99725d59",
   "metadata": {},
   "source": [
    "Eso se ve bien.\n",
    "\n",
    "Ahora vamos a crear una instancia con la misma configuración que usamos antes.\n",
    "\n",
    "Necesitaremos `input_shape=784` (igual al número de características de nuestros datos de imagen), `hidden_units=10` (comenzando poco a poco y lo mismo que nuestro modelo de referencia) y `output_shape=len(class_names)` (una salida unidad por clase).\n",
    "\n",
    "> **Nota:** Observe cómo mantuvimos la mayoría de las configuraciones de nuestro modelo iguales excepto por un cambio: agregar capas no lineales. Esta es una práctica estándar para ejecutar una serie de experimentos de aprendizaje automático, cambiar una cosa y ver qué sucede, luego hacerlo una y otra vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names) # number of output classes desired\n",
    ").to(device) # send model to GPU if it's available\n",
    "next(model_1.parameters()).device # check model device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2d25e",
   "metadata": {},
   "source": [
    "### 6.1 Pérdida de configuración, optimizador y métricas de evaluación\n",
    "\n",
    "Como de costumbre, configuraremos una función de pérdida, un optimizador y una métrica de evaluación (podríamos hacer múltiples métricas de evaluación, pero por ahora nos limitaremos a la precisión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), \n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a308d3",
   "metadata": {},
   "source": [
    "### 6.2 Funcionalización de bucles de entrenamiento y prueba\n",
    "\n",
    "Hasta ahora hemos estado escribiendo bucles de entrenamiento y prueba una y otra vez. \n",
    "\n",
    "Escribámoslos nuevamente pero esta vez los pondremos en funciones para que puedan ser llamados una y otra vez.\n",
    "\n",
    "Y debido a que ahora estamos usando código independiente del dispositivo, nos aseguraremos de llamar a `.to(device)` en nuestros tensores de función (`X`) y objetivo (`y`).\n",
    "\n",
    "Para el ciclo de entrenamiento crearemos una función llamada `train_step()` que toma un modelo, un `DataLoader`, una función de pérdida y un optimizador.\n",
    "\n",
    "El ciclo de prueba será similar pero se llamará `test_step()` y aceptará un modelo, un `DataLoader`, una función de pérdida y una función de evaluación.\n",
    "\n",
    "> **Nota:** Dado que estas son funciones, puedes personalizarlas como quieras. Lo que estamos creando aquí pueden considerarse funciones básicas de entrenamiento y prueba para nuestro caso de uso de clasificación específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e118dbb",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Ahora que tenemos algunas funciones para entrenar y probar nuestro modelo, ejecutémoslas.\n",
    "\n",
    "Lo haremos dentro de otro bucle para cada época.\n",
    "\n",
    "De esa manera, para cada época vamos a realizar un paso de entrenamiento y de prueba.\n",
    "\n",
    "> **Nota:** Puede personalizar la frecuencia con la que realiza un paso de prueba. A veces la gente los hace cada cinco o diez épocas o, en nuestro caso, cada época.\n",
    "\n",
    "También cronometremos las cosas para ver cuánto tiempo tarda nuestro código en ejecutarse en la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Medir el tiempo\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_1, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_1,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241e2cc",
   "metadata": {},
   "source": [
    "¡Excelente!\n",
    "\n",
    "¿Nuestro modelo se entrenó pero el tiempo de entrenamiento tomó más tiempo?\n",
    "\n",
    "> **Nota:** El tiempo de entrenamiento en CUDA versus CPU dependerá en gran medida de la calidad de la CPU/GPU que estés usando. Siga leyendo para obtener una respuesta más explicada.\n",
    "\n",
    "> **Pregunta:** \"Usé una GPU pero mi modelo no se entrenó más rápido, ¿a qué se debe?\"\n",
    ">\n",
    "> **Respuesta:** Bueno, una razón podría ser que su conjunto de datos y su modelo son tan pequeños (como el conjunto de datos y el modelo con el que estamos trabajando) que los beneficios de usar una GPU se ven superados por el tiempo que realmente lleva la transferencia. los datos allí.\n",
    "> \n",
    "> Existe un pequeño cuello de botella entre la copia de datos de la memoria de la CPU (predeterminada) a la memoria de la GPU.\n",
    ">\n",
    "> Entonces, para modelos y conjuntos de datos más pequeños, la CPU podría ser el lugar óptimo para calcular.\n",
    ">\n",
    "> Pero para conjuntos de datos y modelos más grandes, la velocidad de computación que la GPU puede ofrecer generalmente supera con creces el costo de llevar los datos allí.\n",
    ">\n",
    "> Sin embargo, esto depende en gran medida del hardware que estás utilizando. Con la práctica, te acostumbrarás a cuál es el mejor lugar para entrenar a tus modelos. \n",
    "\n",
    "Evaluemos nuestro `model_1` entrenado usando nuestra función `eval_model()` y veamos cómo fue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Nota: Esto generará un error debido a que `eval_model()` no utiliza código independiente del dispositivo.\n",
    "model_1_results = eval_model(model=model_1, \n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, \n",
    "    accuracy_fn=accuracy_fn) \n",
    "model_1_results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf627f",
   "metadata": {},
   "source": [
    "¡Oh, no! \n",
    "\n",
    "Parece que nuestra función `eval_model()` falla con:\n",
    "\n",
    "> `RuntimeError: Se esperaba que todos los tensores estuvieran en el mismo dispositivo, pero encontré al menos dos dispositivos, cuda:0 y cpu. (al verificar el argumento mat1 en el método wrapper_addmm)`\n",
    "\n",
    "Es porque hemos configurado nuestros datos y modelo para usar código independiente del dispositivo, pero no nuestra función de evaluación.\n",
    "\n",
    "¿Qué tal si solucionamos eso pasando un parámetro de `dispositivo` de destino a nuestra función `eval_model()`?\n",
    "\n",
    "Luego intentaremos calcular los resultados nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5750d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mover valores al dispositivo\n",
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn, \n",
    "               device: torch.device = device):\n",
    "    \"\"\"Evaluates a given model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "        device (str, optional): Target device to compute on. Defaults to device.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "        \n",
    "        # Scale loss and acc\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calcule los resultados del modelo 1 con código independiente del dispositivo\n",
    "model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
    "    device=device\n",
    ")\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980aeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar resultados de referencia\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75352c4",
   "metadata": {},
   "source": [
    "Vaya, en este caso, parece que agregar no linealidades a nuestro modelo hizo que funcionara peor que la línea base.\n",
    "\n",
    "Eso es algo a tener en cuenta en el aprendizaje automático: a veces lo que pensaba que debería funcionar no funciona. \n",
    "\n",
    "Y luego lo que pensabas que podría no funcionar, funciona.\n",
    "\n",
    "Es en parte ciencia, en parte arte.\n",
    "\n",
    "Por lo que parece, parece que nuestro modelo se está **sobreajustando** en los datos de entrenamiento.\n",
    "\n",
    "El sobreajuste significa que nuestro modelo está aprendiendo bien los datos de entrenamiento, pero esos patrones no se generalizan a los datos de prueba.\n",
    "\n",
    "Dos de los principales para solucionar el sobreajuste incluyen:\n",
    "1. Usar un modelo más pequeño o diferente (algunos modelos se ajustan mejor a ciertos tipos de datos que otros).\n",
    "2. Usar un conjunto de datos más grande (cuantos más datos, más posibilidades tiene un modelo de aprender patrones generalizables).\n",
    "\n",
    "Hay más, pero lo dejaré como un desafío para que lo explores.\n",
    "\n",
    "Intente buscar en línea \"formas de evitar el sobreajuste en el aprendizaje automático\" y vea qué aparece.\n",
    "\n",
    "Mientras tanto, echemos un vistazo al número 1: usar un modelo diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f03d42",
   "metadata": {},
   "source": [
    "## 7. Modelo 2: Construcción de una red neuronal convolucional (CNN)\n",
    "\n",
    "Muy bien, es hora de dar un paso más.\n",
    "\n",
    "Es hora de crear una [red neuronal convolucional] (https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN o ConvNet).\n",
    "\n",
    "Las CNN son conocidas por sus capacidades para encontrar patrones en datos visuales.\n",
    "\n",
    "Y dado que estamos tratando con datos visuales, veamos si el uso de un modelo CNN puede mejorar nuestra línea de base.\n",
    "\n",
    "El modelo de CNN que vamos a utilizar se conoce como TinyVGG del sitio web [CNN Explicador] (https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "Sigue la estructura típica de una red neuronal convolucional:\n",
    "\n",
    "`Capa de entrada -> [Capa convolucional -> capa de activación -> capa de agrupación] -> Capa de salida`\n",
    "\n",
    "Donde el contenido de `[Capa convolucional -> capa de activación -> capa de agrupación]` se puede ampliar y repetir varias veces, según los requisitos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2083a29",
   "metadata": {},
   "source": [
    "### ¿Qué modelo debo usar?\n",
    "\n",
    "> **Pregunta:** Espera, dices que las CNN son buenas para imágenes, ¿hay algún otro tipo de modelo que deba tener en cuenta?\n",
    "\n",
    "Buena pregunta.\n",
    "\n",
    "Esta tabla es una buena guía general sobre qué modelo utilizar (aunque hay excepciones).\n",
    "\n",
    "| **Tipo de problema** | **Modelo a utilizar (generalmente)** | **Ejemplo de código** |\n",
    "| ----- | ----- | ----- |\n",
    "| Datos estructurados (hojas de cálculo Excel, datos de filas y columnas) | Modelos mejorados con gradiente, bosques aleatorios, XGBoost | [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble), [biblioteca XGBoost](https://xgboost.readthedocs.io/en/ estable/) |\n",
    "| Datos no estructurados (imágenes, audio, idioma) | Redes Neuronales Convolucionales, Transformadores | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [Transformadores HuggingFace](https://huggingface.co/docs/transformers/index) | \n",
    "\n",
    "> **Nota:** La tabla anterior es solo como referencia; el modelo que termine usando dependerá en gran medida del problema en el que esté trabajando y de las limitaciones que tenga (cantidad de datos, requisitos de latencia).\n",
    "\n",
    "Basta de hablar de modelos, ahora construyamos una CNN que replique el modelo en el [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "![Arquitectura TinyVGG, configurada por el sitio web explicativo de CNN](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-cnn-explainer-model.png)\n",
    "\n",
    "Para hacerlo, aprovecharemos [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) y [`nn.MaxPool2d()` ](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) capas de `torch.nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una red neuronal convolucional\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_2 = FashionMNISTModelV2(input_shape=1, \n",
    "    hidden_units=10, \n",
    "    output_shape=len(class_names)).to(device)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65733a55",
   "metadata": {},
   "source": [
    "¡Lindo!\n",
    "\n",
    "¡Nuestro modelo más grande hasta el momento!\n",
    "\n",
    "Lo que hemos hecho es una práctica común en el aprendizaje automático.\n",
    "\n",
    "Encuentre una arquitectura modelo en algún lugar y replíquela con código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7dfbc9",
   "metadata": {},
   "source": [
    "### 7.1 Paso a paso por `nn.Conv2d()`\n",
    "\n",
    "Podríamos comenzar a usar nuestro modelo anterior y ver qué sucede, pero primero veamos las dos nuevas capas que hemos agregado:\n",
    "* [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), también conocida como capa convolucional.\n",
    "* [`nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html), también conocida como capa de agrupación máxima.\n",
    "\n",
    "> **Pregunta:** ¿Qué significa \"2d\" en `nn.Conv2d()`?\n",
    ">\n",
    "> El 2d es para datos bidimensionales. Como en, nuestras imágenes tienen dos dimensiones: alto y ancho. Sí, hay una dimensión del canal de color, pero cada una de las dimensiones del canal de color también tiene dos dimensiones: alto y ancho.\n",
    ">\n",
    "> Para otros datos dimensionales (como 1D para texto o 3D para objetos 3D) también están `nn.Conv1d()` y `nn.Conv3d()`. \n",
    "\n",
    "Para probar las capas, creemos algunos datos de juguetes similares a los datos utilizados en CNN Explicador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Cree un lote de muestra de números aleatorios con el mismo tamaño que el lote de imágenes\n",
    "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
    "test_image = images[0] # get a single image for testing\n",
    "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\") \n",
    "print(f\"Single image pixel values:\\n{test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944f7ae",
   "metadata": {},
   "source": [
    "Creemos un ejemplo `nn.Conv2d()` con varios parámetros:\n",
    "* `in_channels` (int) - Número de canales en la imagen de entrada.\n",
    "* `out_channels` (int) - Número de canales producidos por la convolución.\n",
    "* `kernel_size` (int o tupla): tamaño del kernel/filtro convolutivo.\n",
    "* `stride` (int o tuple, opcional): qué tan grande es el paso que da el núcleo convolutivo a la vez. Predeterminado: 1.\n",
    "* `padding` (int, tuple, str): relleno agregado a los cuatro lados de la entrada. Predeterminado: 0.\n",
    "\n",
    "![ejemplo de cómo revisar los diferentes parámetros de una capa Conv2d](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif)\n",
    "\n",
    "*Ejemplo de lo que sucede cuando cambias los hiperparámetros de una capa `nn.Conv2d()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Crea una capa convolucional con las mismas dimensiones que TinyVGG\n",
    "# (intente cambiar cualquiera de los parámetros y vea qué sucede)\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=10,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0) # also try using \"valid\" or \"same\" here \n",
    "\n",
    "# Pasar los datos a través de la capa convolucional.\n",
    "conv_layer(test_image) # Note: If running PyTorch <1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b5b74",
   "metadata": {},
   "source": [
    "Si intentamos pasar una sola imagen, obtenemos un error de falta de coincidencia de forma:\n",
    "\n",
    "> `RuntimeError: Se esperaba una entrada de 4 dimensiones para un peso de 4 dimensiones [10, 3, 3, 3], pero en su lugar obtuve una entrada de 3 dimensiones de tamaño [3, 64, 64]`\n",
    ">\n",
    "> **Nota:** Si está ejecutando PyTorch 1.11.0+, este error no ocurrirá.\n",
    "\n",
    "Esto se debe a que nuestra capa `nn.Conv2d()` espera un tensor de 4 dimensiones como entrada con tamaño `(N, C, H, W)` o `[batch_size, color_channels, height, width]`.\n",
    "\n",
    "En este momento, nuestra imagen única `test_image` solo tiene la forma `[color_channels, height, width]` o `[3, 64, 64]`.\n",
    "\n",
    "Podemos solucionar este problema para una sola imagen usando `test_image.unsqueeze(dim=0)` para agregar una dimensión adicional para `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43885543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregue una dimensión adicional a la imagen de prueba\n",
    "test_image.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab821f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pase la imagen de prueba con dimensión adicional a través de conv_layer\n",
    "conv_layer(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527bd50",
   "metadata": {},
   "source": [
    "Hmm, observe lo que sucede con nuestra forma (la misma forma que la primera capa de TinyVGG en [CNN Explicador] (https://poloclub.github.io/cnn-explainer/)), obtenemos diferentes tamaños de canal, así como diferentes tamaños de píxeles.\n",
    "\n",
    "¿Qué pasa si cambiamos los valores de `conv_layer`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db72425",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Crea una nueva conv_layer con diferentes valores (intenta configurarlos como quieras)\n",
    "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
    "                         out_channels=10,\n",
    "                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "\n",
    "# Pase una sola imagen a través del nuevo conv_layer_2 (esto llama al método forward() de nn.Conv2d() en la entrada)\n",
    "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e8611",
   "metadata": {},
   "source": [
    "Vaya, tenemos otro cambio de forma.\n",
    "\n",
    "Ahora nuestra imagen tiene la forma `[1, 10, 30, 30]` (será diferente si usa valores diferentes) o `[batch_size=1, color_channels=10, height=30, width=30]`.\n",
    "\n",
    "¿Que está pasando aqui?\n",
    "\n",
    "Detrás de escena, nuestro `nn.Conv2d()` está comprimiendo la información almacenada en la imagen.\n",
    "\n",
    "Para ello, realiza operaciones en la entrada (nuestra imagen de prueba) con sus parámetros internos.\n",
    "\n",
    "El objetivo de esto es similar al de todas las demás redes neuronales que hemos estado construyendo.\n",
    "\n",
    "Los datos entran y las capas intentan actualizar sus parámetros internos (patrones) para reducir la función de pérdida gracias a la ayuda del optimizador.\n",
    "\n",
    "La única diferencia es *cómo* las diferentes capas calculan sus actualizaciones de parámetros o, en términos de PyTorch, la operación presente en el método `forward()` de la capa.\n",
    "\n",
    "Si revisamos nuestro `conv_layer_2.state_dict()` encontraremos una configuración de peso y sesgo similar a la que hemos visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252bb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulte los parámetros internos de conv_layer_2\n",
    "print(conv_layer_2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d98f60",
   "metadata": {},
   "source": [
    "¡Mira eso! Un montón de números aleatorios para un tensor de peso y sesgo.\n",
    "\n",
    "Las formas de estos son manipuladas por las entradas que le pasamos a `nn.Conv2d()` cuando lo configuramos.\n",
    "\n",
    "Echemos un vistazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenga formas de tensores de peso y sesgo dentro de conv_layer_2\n",
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
    "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ce209",
   "metadata": {},
   "source": [
    "> **Pregunta:** ¿Qué debemos configurar los parámetros de nuestras capas `nn.Conv2d()`?\n",
    ">\n",
    "> Esa es buena. Pero al igual que muchas otras cosas en el aprendizaje automático, los valores de estos no están escritos en piedra (y recuerden, debido a que estos valores son los que podemos establecer nosotros mismos, se los conoce como \"**hiperparámetros**\"). \n",
    ">\n",
    "> La mejor manera de averiguarlo es probar diferentes valores y ver cómo afectan el rendimiento de su modelo.\n",
    ">\n",
    "> O mejor aún, busque un ejemplo funcional sobre un problema similar al suyo (como lo hemos hecho con TinyVGG) y cópielo. \n",
    "\n",
    "Estamos trabajando con una capa diferente a la que hemos visto antes.\n",
    "\n",
    "Pero la premisa sigue siendo la misma: empezar con números aleatorios y actualizarlos para representar mejor los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12433d96",
   "metadata": {},
   "source": [
    "### 7.2 Paso a paso por `nn.MaxPool2d()`\n",
    "Ahora veamos qué sucede cuando movemos datos a través de `nn.MaxPool2d()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprima la forma de la imagen original sin y con dimensión sin comprimir\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
    "\n",
    "# Cree una capa de muestra nn.MaxPoo2d()\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "# Pasar datos solo a través de conv_layer\n",
    "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
    "\n",
    "# Pasar datos a través de la capa de grupo máximo\n",
    "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a7232",
   "metadata": {},
   "source": [
    "Observe el cambio en las formas de lo que sucede dentro y fuera de una capa `nn.MaxPool2d()`.\n",
    "\n",
    "El `kernel_size` de la capa `nn.MaxPool2d()` afectará el tamaño de la forma de salida.\n",
    "\n",
    "En nuestro caso, la forma se reduce a la mitad de una imagen de \"62x62\" a una imagen de \"31x31\".\n",
    "\n",
    "Veamos cómo funciona con un tensor más pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Crea un tensor aleatorio con un número de dimensiones similar a nuestras imágenes.\n",
    "random_tensor = torch.randn(size=(1, 1, 2, 2))\n",
    "print(f\"Random tensor:\\n{random_tensor}\")\n",
    "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
    "\n",
    "# Crear una capa de grupo máxima\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n",
    "\n",
    "# Pase el tensor aleatorio a través de la capa de grupo máxima\n",
    "max_pool_tensor = max_pool_layer(random_tensor)\n",
    "print(f\"\\nMax pool tensor:\\n{max_pool_tensor} <- this is the maximum value from random_tensor\")\n",
    "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96051df2",
   "metadata": {},
   "source": [
    "Observe las dos dimensiones finales entre `random_tensor` y `max_pool_tensor`, van de `[2, 2]` a `[1, 1]`.\n",
    "\n",
    "En esencia, se reducen a la mitad.\n",
    "\n",
    "Y el cambio sería diferente para diferentes valores de `kernel_size` para `nn.MaxPool2d()`.\n",
    "\n",
    "Observe también que el valor sobrante en `max_pool_tensor` es el valor **máximo** de `random_tensor`.\n",
    "\n",
    "¿Que esta pasando aqui?\n",
    "\n",
    "Ésta es otra pieza importante del rompecabezas de las redes neuronales.\n",
    "\n",
    "Esencialmente, **cada capa de una red neuronal intenta comprimir datos desde un espacio de dimensiones superiores a un espacio de dimensiones inferiores**. \n",
    "\n",
    "En otras palabras, tome muchos números (datos sin procesar) y aprenda patrones en esos números, patrones que sean predictivos y al mismo tiempo sean *más pequeños* en tamaño que los valores originales.\n",
    "\n",
    "Desde una perspectiva de inteligencia artificial, se podría considerar el objetivo completo de una red neuronal de *comprimir* información.\n",
    "\n",
    "![cada capa de una red neuronal comprime los datos de entrada originales en una representación más pequeña que (con suerte) es capaz de hacer predicciones sobre datos de entrada futuros](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/ principal/images/03-conv-net-as-compression.png)\n",
    "\n",
    "Esto significa que, desde el punto de vista de una red neuronal, la inteligencia es compresión.\n",
    "\n",
    "Esta es la idea del uso de una capa `nn.MaxPool2d()`: tomar el valor máximo de una parte de un tensor y ignorar el resto.\n",
    "\n",
    "En esencia, reducir la dimensionalidad de un tensor y al mismo tiempo conservar una (con suerte) parte significativa de la información.\n",
    "\n",
    "Es la misma historia para una capa `nn.Conv2d()`.\n",
    "\n",
    "Excepto que en lugar de simplemente tomar el máximo, `nn.Conv2d()` realiza una operación convolucional en los datos (vea esto en acción en la [página web de CNN Explicador](https://poloclub.github.io/cnn-explainer/ )).\n",
    "\n",
    "> **Ejercicio:** ¿Qué crees que hace la capa [`nn.AvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html)? Intente hacer un tensor aleatorio como hicimos arriba y páselo. Verifique las formas de entrada y salida, así como los valores de entrada y salida.\n",
    "\n",
    "> **Extracurricular:** Busque \"redes neuronales convolucionales más comunes\", ¿qué arquitecturas encuentra? ¿Alguno de ellos está contenido en la biblioteca [`torchvision.models`](https://pytorch.org/vision/stable/models.html)? ¿Qué crees que podrías hacer con estos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15055300",
   "metadata": {},
   "source": [
    "### 7.3 Configurar una función de pérdida y un optimizador para `model_2`\n",
    "\n",
    "Hemos recorrido suficientes capas en nuestra primera CNN.\n",
    "\n",
    "Pero recuerde, si algo aún no está claro, intente empezar poco a poco.\n",
    "\n",
    "Elija una sola capa de un modelo, pase algunos datos a través de ella y vea qué sucede.\n",
    "\n",
    "¡Ahora es el momento de seguir adelante y ponerse a entrenar!\n",
    "\n",
    "Configuremos una función de pérdida y un optimizador.\n",
    "\n",
    "Usaremos las funciones como antes, `nn.CrossEntropyLoss()` como función de pérdida (ya que estamos trabajando con datos de clasificación de múltiples clases).\n",
    "\n",
    "Y `torch.optim.SGD()` como optimizador para optimizar `model_2.parameters()` con una tasa de aprendizaje de `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a520041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pérdida de configuración y optimizador.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), \n",
    "                             lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa9df5",
   "metadata": {},
   "source": [
    "### 7.4 Entrenamiento y prueba `model_2` usando nuestras funciones de entrenamiento y prueba\n",
    "\n",
    "¡Pérdida y optimizador listos!\n",
    "\n",
    "Es hora de entrenar y probar.\n",
    "\n",
    "Usaremos nuestras funciones `train_step()` y `test_step()` que creamos antes.\n",
    "\n",
    "También mediremos el tiempo para compararlo con nuestros otros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c46073",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Medir el tiempo\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Modelo de entrenamiento y prueba.\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_2, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=model_2,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb056a7",
   "metadata": {},
   "source": [
    "¡Guau! Parece que las capas convolucional y de agrupación máxima ayudaron a mejorar un poco el rendimiento.\n",
    "\n",
    "Evaluemos los resultados de `model_2` con nuestra función `eval_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53517f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener resultados del modelo_2\n",
    "model_2_results = eval_model(\n",
    "    model=model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c90a3",
   "metadata": {},
   "source": [
    "## 8. Compare los resultados del modelo y el tiempo de entrenamiento\n",
    "\n",
    "Hemos entrenado tres modelos diferentes.\n",
    "\n",
    "1. `model_0`: nuestro modelo de referencia con dos capas `nn.Linear()`.\n",
    "2. `model_1`: la misma configuración que nuestro modelo de referencia, excepto con capas `nn.ReLU()` entre las capas `nn.Linear()`.\n",
    "3. `model_2`: nuestro primer modelo de CNN que imita la arquitectura TinyVGG en el sitio web CNN Explicador.\n",
    "\n",
    "Esta es una práctica habitual en el aprendizaje automático.\n",
    "\n",
    "Construya múltiples modelos y realice múltiples experimentos de entrenamiento para ver cuál funciona mejor.\n",
    "\n",
    "Combinemos los diccionarios de resultados de nuestros modelos en un DataFrame y averigüémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
    "compare_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91d60e",
   "metadata": {},
   "source": [
    "¡Lindo!\n",
    "\n",
    "También podemos agregar los valores del tiempo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f73805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir tiempos de entrenamiento a la comparación de resultados\n",
    "compare_results[\"training_time\"] = [total_train_time_model_0,\n",
    "                                    total_train_time_model_1,\n",
    "                                    total_train_time_model_2]\n",
    "compare_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d5e77",
   "metadata": {},
   "source": [
    "Parece que nuestro modelo CNN (`FashionMNISTModelV2`) tuvo el mejor rendimiento (pérdida más baja, mayor precisión) pero tuvo el tiempo de entrenamiento más largo.\n",
    "\n",
    "Y nuestro modelo de referencia (`FashionMNISTModelV0`) funcionó mejor que `model_1` (`FashionMNISTModelV1`).\n",
    "\n",
    "### Compensación rendimiento-velocidad\n",
    "\n",
    "Algo a tener en cuenta en el aprendizaje automático es la compensación **rendimiento-velocidad**.\n",
    "\n",
    "Generalmente, se obtiene un mejor rendimiento con un modelo más grande y complejo (como hicimos con `model_2`).\n",
    "\n",
    "Sin embargo, este aumento del rendimiento a menudo se produce sacrificando la velocidad de entrenamiento y la velocidad de inferencia.\n",
    "\n",
    "> **Nota:** Los tiempos de capacitación que obtenga dependerán en gran medida del hardware que utilice. \n",
    ">\n",
    "> Generalmente, cuantos más núcleos de CPU tenga, más rápido se entrenarán sus modelos en la CPU. Y similar para las GPU.\n",
    "> \n",
    "> El hardware más nuevo (en términos de antigüedad) también suele entrenar modelos más rápido debido a la incorporación de avances tecnológicos.\n",
    "\n",
    "¿Qué tal si nos volvemos visuales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualice los resultados de nuestro modelo.\n",
    "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"accuracy (%)\")\n",
    "plt.ylabel(\"model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3aea0f",
   "metadata": {},
   "source": [
    "## 9. Realice y evalúe predicciones aleatorias con el mejor modelo.\n",
    "\n",
    "Muy bien, hemos comparado nuestros modelos entre sí, evaluemos más a fondo nuestro modelo de mejor rendimiento, \"model_2\".\n",
    "\n",
    "Para hacerlo, creemos una función `make_predictions()` donde podemos pasar el modelo y algunos datos para que prediga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ebfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
    "    pred_probs = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for sample in data:\n",
    "            # Prepare sample\n",
    "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
    "\n",
    "            # Forward pass (model outputs raw logit)\n",
    "            pred_logit = model(sample)\n",
    "\n",
    "            # Get prediction probability (logit -> prediction probability)\n",
    "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n",
    "\n",
    "            # Get pred_prob off GPU for further calculations\n",
    "            pred_probs.append(pred_prob.cpu())\n",
    "            \n",
    "    # Stack the pred_probs to turn list into a tensor\n",
    "    return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "for sample, label in random.sample(list(test_data), k=9):\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Ver la forma y la etiqueta de la primera muestra de prueba\n",
    "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7cb94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga predicciones sobre muestras de prueba con el modelo 2\n",
    "pred_probs= make_predictions(model=model_2, \n",
    "                             data=test_samples)\n",
    "\n",
    "# Ver la lista de las dos primeras probabilidades de predicción\n",
    "pred_probs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db1339",
   "metadata": {},
   "source": [
    "Y ahora podemos usar nuestra función `make_predictions()` para predecir en `test_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a30984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga predicciones sobre muestras de prueba con el modelo 2\n",
    "pred_probs= make_predictions(model=model_2, \n",
    "                             data=test_samples)\n",
    "\n",
    "# Ver la lista de las dos primeras probabilidades de predicción\n",
    "pred_probs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe1d72",
   "metadata": {},
   "source": [
    "¡Excelente!\n",
    "\n",
    "Y ahora podemos pasar de probabilidades de predicción a etiquetas de predicción tomando el `torch.argmax()` de la salida de la función de activación `torch.softmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b038edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierta las probabilidades de predicción en etiquetas de predicción tomando argmax()\n",
    "pred_classes = pred_probs.argmax(dim=1)\n",
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Nuestras predicciones tienen la misma forma que nuestras etiquetas de prueba?\n",
    "test_labels, pred_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a85434",
   "metadata": {},
   "source": [
    "Ahora nuestras clases previstas tienen el mismo formato que nuestras etiquetas de prueba, podemos comparar.\n",
    "\n",
    "Dado que estamos tratando con datos de imágenes, seamos fieles al lema del explorador de datos. \n",
    "\n",
    "\"¡Visualiza, visualiza, visualiza!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones de la trama\n",
    "plt.figure(figsize=(9, 9))\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "for i, sample in enumerate(test_samples):\n",
    "  # Create a subplot\n",
    "  plt.subplot(nrows, ncols, i+1)\n",
    "\n",
    "  # Plot the target image\n",
    "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
    "\n",
    "  # Find the prediction label (in text form, e.g. \"Sandal\")\n",
    "  pred_label = class_names[pred_classes[i]]\n",
    "\n",
    "  # Get the truth label (in text form, e.g. \"T-shirt\")\n",
    "  truth_label = class_names[test_labels[i]] \n",
    "\n",
    "  # Create the title text of the plot\n",
    "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
    "  \n",
    "  # Check for equality and change title colour accordingly\n",
    "  if pred_label == truth_label:\n",
    "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
    "  else:\n",
    "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
    "  plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b76289",
   "metadata": {},
   "source": [
    "Bueno, bueno, bueno, ¿no se ve bien?\n",
    "\n",
    "¡Nada mal para un par de docenas de líneas de código PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a7a80",
   "metadata": {},
   "source": [
    "## 10. Hacer una matriz de confusión para una evaluación de predicción adicional\n",
    "\n",
    "Hay muchas [métricas de evaluación diferentes](https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluación-metrics) que podemos usar para problemas de clasificación. \n",
    "\n",
    "Uno de los más visuales es una [matriz de confusión](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/).\n",
    "\n",
    "Una matriz de confusión le muestra dónde se confundió su modelo de clasificación entre predicciones y etiquetas verdaderas.\n",
    "\n",
    "Para crear una matriz de confusión, seguiremos tres pasos:\n",
    "1. Haga predicciones con nuestro modelo entrenado, `model_2` (una matriz de confusión compara las predicciones con etiquetas verdaderas).\n",
    "2. Haga una matriz de confusión usando [`torchmetrics.ConfusionMatrix`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix).\n",
    "3. Trace la matriz de confusión usando [`mlxtend.plotting.plot_confusion_matrix()`](http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/).\n",
    "\n",
    "Comencemos haciendo predicciones con nuestro modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar tqdm para la barra de progreso\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1.Hacer predicciones con un modelo entrenado.\n",
    "y_preds = []\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
    "    # Send data and targets to target device\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    # Do the forward pass\n",
    "    y_logit = model_2(X)\n",
    "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
    "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
    "    # Put predictions on CPU for evaluation\n",
    "    y_preds.append(y_pred.cpu())\n",
    "# Concatenar lista de predicciones en un tensor\n",
    "y_pred_tensor = torch.cat(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b4cb1",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Ahora que tenemos predicciones, veamos los pasos 2 y 3:\n",
    "2. Haga una matriz de confusión usando [`torchmetrics.ConfusionMatrix`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix).\n",
    "3. Trace la matriz de confusión usando [`mlxtend.plotting.plot_confusion_matrix()`](http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/).\n",
    "\n",
    "Primero necesitaremos asegurarnos de tener instalados `torchmetrics` y `mlxtend` (estas dos bibliotecas nos ayudarán a crear y visualizar una matriz de confusión).\n",
    "\n",
    "> **Nota:** Si está utilizando Google Colab, la versión predeterminada de `mlxtend` instalada es 0.14.0 (a partir de marzo de 2022); sin embargo, para los parámetros de la función `plot_confusion_matrix()` Como uso, necesitamos 0.19.0 o superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vea si existe torchmetrics, si no, instálelo\n",
    "try:\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
    "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
    "except:\n",
    "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
    "    import torchmetrics, mlxtend\n",
    "    print(f\"mlxtend version: {mlxtend.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459ecb4",
   "metadata": {},
   "source": [
    "Para trazar la matriz de confusión, debemos asegurarnos de tener una versión [`mlxtend`](http://rasbt.github.io/mlxtend/) de 0.19.0 o superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar versión actualizada de mlxtend\n",
    "import mlxtend \n",
    "print(mlxtend.__version__)\n",
    "assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ecc89",
   "metadata": {},
   "source": [
    "`torchmetrics` y `mlxtend` instalados, ¡hagamos una matriz de confusión!\n",
    "\n",
    "Primero crearemos una instancia `torchmetrics.ConfusionMatrix` diciéndole con cuántas clases estamos tratando configurando `num_classes=len(class_names)`.\n",
    "\n",
    "Luego crearemos una matriz de confusión (en formato tensorial) pasando a nuestra instancia las predicciones de nuestro modelo (`preds=y_pred_tensor`) y los objetivos (`target=test_data.targets`).\n",
    "\n",
    "Finalmente podemos trazar nuestra matriz de configuración usando la función `plot_confusion_matrix()` de `mlxtend.plotting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94112ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# 2. Configure una instancia de matriz de confusión y compare las predicciones con los objetivos.\n",
    "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=test_data.targets)\n",
    "\n",
    "# 3. Traza la matriz de confusión\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n",
    "    class_names=class_names, # turn the row and column labels into class names\n",
    "    figsize=(10, 7)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52f6cf",
   "metadata": {},
   "source": [
    "¡Guau! ¿No se ve bien?\n",
    "\n",
    "Podemos ver que nuestro modelo funciona bastante bien ya que la mayoría de los cuadrados oscuros están en la diagonal desde la parte superior izquierda hasta la inferior derecha (y el modelo ideal solo tendrá valores en estos cuadrados y 0 en el resto).\n",
    "\n",
    "El modelo se \"confunde\" más en clases que son similares, por ejemplo, prediciendo \"Pullover\" para imágenes que en realidad están etiquetadas como \"Camisa\".\n",
    "\n",
    "Y lo mismo para predecir \"Camisa\" para clases que en realidad están etiquetadas como \"Camiseta/top\".\n",
    "\n",
    "Este tipo de información suele ser más útil que una única métrica de precisión porque indica al usuario *dónde* un modelo está haciendo las cosas mal.\n",
    "\n",
    "También da pistas de *por qué* el modelo puede estar haciendo ciertas cosas mal.\n",
    "\n",
    "Es comprensible que el modelo a veces prediga \"Camisa\" para imágenes etiquetadas como \"Camiseta/top\".\n",
    "\n",
    "Podemos utilizar este tipo de información para inspeccionar más a fondo nuestros modelos y datos y ver cómo podrían mejorarse.\n",
    "\n",
    "> **Ejercicio:** Utilice el `model_2` entrenado para hacer predicciones en el conjunto de datos de prueba FashionMNIST. Luego, traza algunas predicciones en las que el modelo se equivocó junto con cuál debería haber sido la etiqueta de la imagen. Después de visualizar estas predicciones, ¿crees que se trata más de un error de modelado o de un error de datos? Por ejemplo, ¿podría funcionar mejor el modelo o las etiquetas de los datos están demasiado cerca entre sí (por ejemplo, una etiqueta de \"Camisa\" está demasiado cerca de \"Camiseta/top\")?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b14c0",
   "metadata": {},
   "source": [
    "## 11. Guarde y cargue el modelo con mejor rendimiento\n",
    "\n",
    "Terminemos esta sección guardando y cargando en nuestro modelo de mejor rendimiento.\n",
    "\n",
    "Recuerde del [cuaderno 01] (https://www.learnpytorch.io/01_pytorch_workflow/#5-served-and-loading-a-pytorch-model) que podemos guardar y cargar un modelo de PyTorch usando una combinación de:\n",
    "* `torch.save`: una función para guardar un modelo PyTorch completo o el `state_dict()` de un modelo. \n",
    "* `torch.load`: una función para cargar en un objeto PyTorch guardado.\n",
    "* `torch.nn.Module.load_state_dict()` - una función para cargar un `state_dict()` guardado en una instancia de modelo existente.\n",
    "\n",
    "Puede ver más de estos tres en la [documentación de modelos de carga y guardado de PyTorch] (https://pytorch.org/tutorials/beginner/ Saving_loading_models.html).\n",
    "\n",
    "Por ahora, guardemos el `state_dict()` de nuestro `model_2`, luego volvamos a cargarlo y evaluémoslo para asegurarnos de que el guardado y la carga se realizaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Cree el directorio de modelos (si aún no existe), consulte: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
    "                 exist_ok=True # if models directory already exists, don't error\n",
    ")\n",
    "\n",
    "# Crear ruta para guardar el modelo\n",
    "MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Guarde el dictado del estado del modelo\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968b651",
   "metadata": {},
   "source": [
    "Ahora que tenemos un modelo guardado `state_dict()` podemos volver a cargarlo usando una combinación de `load_state_dict()` y `torch.load()`.\n",
    "\n",
    "Como estamos usando `load_state_dict()`, necesitaremos crear una nueva instancia de `FashionMNISTModelV2()` con los mismos parámetros de entrada que nuestro modelo guardado `state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fdd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree una nueva instancia de FashionMNISTModelV2 (la misma clase que nuestro state_dict() guardado)\n",
    "# Nota: al cargar el modelo se producirá un error si las formas aquí no son las mismas que las de la versión guardada.\n",
    "loaded_model_2 = FashionMNISTModelV2(input_shape=1, \n",
    "                                    hidden_units=10, # try changing this to 128 and seeing what happens \n",
    "                                    output_shape=10) \n",
    "\n",
    "# Cargar en el state_dict() guardado\n",
    "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "\n",
    "# Enviar modelo a GPU\n",
    "loaded_model_2 = loaded_model_2.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caa859",
   "metadata": {},
   "source": [
    "Y ahora que tenemos un modelo cargado, podemos evaluarlo con `eval_model()` para asegurarnos de que sus parámetros funcionen de manera similar a `model_2` antes de guardarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5502b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo cargado\n",
    "torch.manual_seed(42)\n",
    "\n",
    "loaded_model_2_results = eval_model(\n",
    "    model=loaded_model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, \n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "\n",
    "loaded_model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac830ef",
   "metadata": {},
   "source": [
    "¿Estos resultados tienen el mismo aspecto que `model_2_results`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a3332",
   "metadata": {},
   "source": [
    "Podemos averiguar si dos tensores están cerca uno del otro usando `torch.isclose()` y pasando un nivel de tolerancia de cercanía a través de los parámetros `atol` (tolerancia absoluta) y `rtol` (tolerancia relativa).\n",
    "\n",
    "Si los resultados de nuestro modelo son similares, la salida de `torch.isclose()` debería ser verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba si los resultados están cerca uno del otro (si están muy lejos puede haber un error)\n",
    "torch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n",
    "              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n",
    "              atol=1e-08, # absolute tolerance\n",
    "              rtol=0.0001) # relative tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a05c28",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Todos los ejercicios se centran en practicar el código de las secciones anteriores.\n",
    "\n",
    "Debería poder completarlos haciendo referencia a cada sección o siguiendo los recursos vinculados.\n",
    "\n",
    "Todos los ejercicios deben completarse utilizando [código independiente del dispositivo](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n",
    "\n",
    "**Recursos:**\n",
    "* [Cuaderno de plantilla de ejercicios para 03](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb)\n",
    "* [Cuaderno de soluciones de ejemplo para 03](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/03_pytorch_computer_vision_exercise_solutions.ipynb) (pruebe los ejercicios *antes* de mirar esto)\n",
    "\n",
    "1. ¿Cuáles son las 3 áreas de la industria donde se utiliza actualmente la visión por computadora?\n",
    "2. Busque \"qué es el sobreajuste en el aprendizaje automático\" y escriba una oración sobre lo que encuentre. \n",
    "3. Busca \"formas de prevenir el sobreajuste en el aprendizaje automático\", escribe 3 de las cosas que encuentres y una oración sobre cada una. **Nota:** hay muchos de estos, así que no te preocupes demasiado por todos, simplemente elige 3 y comienza con ellos.\n",
    "4. Dedique 20 minutos a leer y hacer clic en el [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/).\n",
    "    * Cargue su propia imagen de ejemplo usando el botón \"cargar\" y vea qué sucede en cada capa de una CNN a medida que su imagen la atraviesa.\n",
    "5. Cargue el tren [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) y pruebe los conjuntos de datos.\n",
    "6. Visualice al menos 5 muestras diferentes del conjunto de datos de entrenamiento MNIST.\n",
    "7. Convierta el tren MNIST y los conjuntos de datos de prueba en cargadores de datos usando `torch.utils.data.DataLoader`, establezca `batch_size=32`.\n",
    "8. Recrea el `model_2` usado en este cuaderno (el mismo modelo del [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/), también conocido como TinyVGG) capaz de ajustarse al conjunto de datos MNIST. .\n",
    "9. Entrene el modelo que creó en el ejercicio 8. en CPU y GPU y vea cuánto tiempo lleva cada uno.\n",
    "10. Haga predicciones utilizando su modelo entrenado y visualice al menos 5 de ellas comparando la predicción con la etiqueta objetivo.\n",
    "11. Traza una matriz de confusión comparando las predicciones de tu modelo con las etiquetas de verdad.\n",
    "12. Cree un tensor aleatorio de forma `[1, 3, 64, 64]` y páselo a través de una capa `nn.Conv2d()` con varias configuraciones de hiperparámetros (pueden ser cualquier configuración que elija), ¿qué nota? ¿Si el parámetro `kernel_size` sube y baja?\n",
    "13. Utilice un modelo similar al `model_2` entrenado de este cuaderno para hacer predicciones en la prueba [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST .html) conjunto de datos. \n",
    "    * Luego, traza algunas predicciones en las que el modelo se equivocó junto con cuál debería haber sido la etiqueta de la imagen. \n",
    "    * Después de visualizar estas predicciones, ¿crees que se trata más de un error de modelado o de un error de datos? \n",
    "    * Como en, ¿podría funcionar mejor el modelo o las etiquetas de los datos están demasiado cerca entre sí (por ejemplo, una etiqueta de \"Camisa\" está demasiado cerca de \"Camiseta/top\")?\n",
    "\n",
    "## Extracurricular\n",
    "* **Ver:** conferencia [Introducción del MIT a la visión informática profunda](https://www.youtube.com/watch?v=iaSUYvmCekI&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3). Esto le dará una gran intuición detrás de las redes neuronales convolucionales.\n",
    "* Dedique 10 minutos a hacer clic en las diferentes opciones de la [biblioteca de visión de PyTorch] (https://pytorch.org/vision/stable/index.html), ¿qué diferentes módulos están disponibles?\n",
    "* Busque \"redes neuronales convolucionales más comunes\", ¿qué arquitecturas encuentra? ¿Alguno de ellos está contenido en la biblioteca [`torchvision.models`](https://pytorch.org/vision/stable/models.html)? ¿Qué crees que podrías hacer con estos?\n",
    "* Para obtener una gran cantidad de modelos de visión por computadora de PyTorch previamente entrenados, así como muchas extensiones diferentes de las funcionalidades de visión por computadora de PyTorch, consulte la [biblioteca de modelos de imágenes de PyTorch `timm`] (https://github.com/rwightman/pytorch-image-models /) (Modelos de imágenes de antorcha) de Ross Wightman."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
