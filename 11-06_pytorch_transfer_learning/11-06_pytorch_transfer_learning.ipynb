{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550b97ba",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/06_pytorch_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https:// colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/></a>\n",
    "\n",
    "[Ver código fuente](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/06_pytorch_transfer_learning.ipynb) | [Ver diapositivas] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/06_pytorch_transfer_learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa479d",
   "metadata": {},
   "source": [
    "# 06. Aprendizaje por transferencia de PyTorch\n",
    "\n",
    "> **Nota:** Este cuaderno utiliza la nueva [API de soporte multipeso de `torchvision` (disponible en `torchvision` v0.13+)](https://pytorch.org/blog/introtaining-torchvision-new -api-soporte-multi-peso/).\n",
    "\n",
    "Hasta ahora hemos construido algunos modelos a mano.\n",
    "\n",
    "Pero su desempeño ha sido pobre.\n",
    "\n",
    "Quizás esté pensando: **¿Existe ya un modelo de buen rendimiento para nuestro problema?**\n",
    "\n",
    "Y en el mundo del aprendizaje profundo, la respuesta suele ser *sí*.\n",
    "\n",
    "Veremos cómo utilizar una poderosa técnica llamada [**transferir aprendizaje**](https://developers.google.com/machine-learning/glossary#transfer-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae4d58",
   "metadata": {},
   "source": [
    "## ¿Qué es el aprendizaje por transferencia?\n",
    "\n",
    "**El aprendizaje por transferencia** nos permite tomar los patrones (también llamados pesos) que otro modelo ha aprendido de otro problema y usarlos para nuestro propio problema.\n",
    "\n",
    "Por ejemplo, podemos tomar los patrones que un modelo de visión por computadora ha aprendido de conjuntos de datos como [ImageNet](https://www.image-net.org/) (millones de imágenes de diferentes objetos) y usarlos para impulsar nuestro FoodVision. Modelo mini.\n",
    "\n",
    "O podríamos tomar los patrones de un [modelo de lenguaje](https://developers.google.com/machine-learning/glossary#masked-language-model) (un modelo que ha analizado grandes cantidades de texto para aprender una representación de lenguaje) y utilizarlos como base de un modelo para clasificar diferentes muestras de texto.\n",
    "\n",
    "La premisa sigue siendo: encuentre un modelo existente que funcione bien y aplíquelo a su propio problema.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-example-overview.png\" alt=\"transferencia general del aprendizaje sobre diferentes problemas\" ancho =900/>\n",
    "\n",
    "*Ejemplo de aprendizaje por transferencia aplicado a la visión por computadora y al procesamiento del lenguaje natural (PLN). En el caso de la visión por computadora, un modelo de visión por computadora podría aprender patrones en millones de imágenes en ImageNet y luego usar esos patrones para inferir otro problema. Y para la PNL, un modelo de lenguaje puede aprender la estructura del lenguaje leyendo toda Wikipedia (y quizás más) y luego aplicar ese conocimiento a un problema diferente.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961380e",
   "metadata": {},
   "source": [
    "## ¿Por qué utilizar el aprendizaje por transferencia?\n",
    "\n",
    "Hay dos beneficios principales al utilizar el aprendizaje por transferencia:\n",
    "\n",
    "1. Puede aprovechar un modelo existente (generalmente una arquitectura de red neuronal) que ha demostrado funcionar en problemas similares al nuestro.\n",
    "2. Puede aprovechar un modelo funcional que **ya ha aprendido** patrones sobre datos similares a los nuestros. Esto a menudo da como resultado **excelentes resultados con menos datos personalizados**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-for-foodvision-mini%20.png\" alt=\"transferir aprendizaje aplicado a FoodVision Mini\" ancho=900/>\n",
    "\n",
    "*Los pondremos a prueba para nuestro problema FoodVision Mini, tomaremos un modelo de visión por computadora previamente entrenado en ImageNet e intentaremos aprovechar sus representaciones aprendidas subyacentes para clasificar imágenes de pizza, bistec y sushi.*\n",
    "\n",
    "Tanto la investigación como la práctica también respaldan el uso del aprendizaje por transferencia.\n",
    "\n",
    "Un hallazgo de un artículo de investigación reciente sobre aprendizaje automático recomendó que los profesionales utilicen el aprendizaje por transferencia siempre que sea posible.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-how-to-train-your-vit-section-6-transfer-learning-highlight.png \" width=900 alt=\"cómo entrenar tu visión, sección 6 del documento transformador, recomendamos utilizar el aprendizaje por transferencia si puedes\"/>\n",
    "\n",
    "*Un estudio sobre los efectos de si entrenar desde cero o utilizar el aprendizaje por transferencia era mejor desde el punto de vista de un profesional, encontró que el aprendizaje por transferencia era mucho más beneficioso en términos de costo y tiempo. **Fuente:** [¿Cómo entrenar tu ViT? Datos, aumento y regularización en Vision Transformers](https://arxiv.org/abs/2106.10270) sección 6 del artículo (conclusión).*\n",
    "\n",
    "Y Jeremy Howard (fundador de [fastai](https://www.fast.ai/)) es un gran defensor del aprendizaje por transferencia.\n",
    "\n",
    "> Las cosas que realmente marcan la diferencia (aprendizaje por transferencia), si podemos hacerlo mejor en el aprendizaje por transferencia, es algo que cambiará el mundo. De repente, mucha más gente puede realizar un trabajo de primer nivel con menos recursos y menos datos. — [Jeremy Howard en el podcast de Lex Fridman] (https://youtu.be/Bi7f1JSSlh8?t=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a9f72",
   "metadata": {},
   "source": [
    "## Dónde encontrar modelos previamente entrenados\n",
    "\n",
    "El mundo del aprendizaje profundo es un lugar asombroso.\n",
    "\n",
    "Es tan sorprendente que muchas personas alrededor del mundo comparten su trabajo.\n",
    "\n",
    "A menudo, el código y los modelos previamente entrenados para las últimas investigaciones de vanguardia se publican a los pocos días de su publicación.\n",
    "\n",
    "Y hay varios lugares donde puede encontrar modelos previamente entrenados para utilizarlos en sus propios problemas.\n",
    "\n",
    "| **Ubicación** | **¿Qué hay ahí?** | **Enlace(s)** | \n",
    "| ----- | ----- | ----- |\n",
    "| **Bibliotecas de dominio PyTorch** | Cada una de las bibliotecas de dominio de PyTorch (`torchvision`, `torchtext`) viene con modelos previamente entrenados de algún tipo. Los modelos allí funcionan directamente dentro de PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n",
    "| **HuggingFace Hub** | Una serie de modelos previamente entrenados en muchos dominios diferentes (visión, texto, audio y más) de organizaciones de todo el mundo. También hay muchos conjuntos de datos diferentes. | https://huggingface.co/models, https://huggingface.co/datasets | \n",
    "| **Biblioteca `timm` (modelos de imágenes PyTorch)** | Casi todos los modelos de visión por computadora más recientes y mejores en código PyTorch, así como muchas otras funciones útiles de visión por computadora. | https://github.com/rwightman/pytorch-image-models|\n",
    "| **Papelesconcódigo** | Una colección de los últimos artículos sobre aprendizaje automático con implementaciones de código adjuntas. También puede encontrar aquí puntos de referencia del rendimiento del modelo en diferentes tareas. | https://paperswithcode.com/ | \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-where-to-find-pretrained-models.png\" alt=\"diferentes ubicaciones para encontrar modelos de redes neuronales previamente entrenados\" width=900/>\n",
    "\n",
    "*Con acceso a recursos de alta calidad como los anteriores, debería ser una práctica común al comienzo de cada problema de aprendizaje profundo que asuma preguntar: \"¿Existe un modelo previamente entrenado para mi problema?\"*\n",
    "\n",
    "> **Ejercicio:** Dedique 5 minutos a revisar [`torchvision.models`](https://pytorch.org/vision/stable/models.html), así como a la [página de modelos de HuggingFace Hub](https: //huggingface.co/models), ¿qué encuentras? (aquí no hay respuestas correctas, es solo para practicar la exploración)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d5186",
   "metadata": {},
   "source": [
    "## Qué vamos a cubrir\n",
    "\n",
    "Tomaremos un modelo previamente entrenado de `torchvision.models` y lo personalizaremos para que funcione (y con suerte mejore) nuestro problema FoodVision Mini.\n",
    "\n",
    "| **Tema** | **Contenido** |\n",
    "| ----- | ----- |\n",
    "| **0. Obteniendo configuración** | Hemos escrito bastante código útil en las últimas secciones, descarguémoslo y asegurémonos de poder usarlo nuevamente. |\n",
    "| **1. Obtener datos** | Obtengamos el conjunto de datos de clasificación de imágenes de pizza, bistec y sushi que hemos estado usando para intentar mejorar los resultados de nuestro modelo. |\n",
    "| **2. Crear conjuntos de datos y cargadores de datos** | Usaremos el script `data_setup.py` que escribimos en el capítulo 05. PyTorch se vuelve modular para configurar nuestros DataLoaders. |\n",
    "| **3. Obtenga y personalice un modelo previamente entrenado** | Aquí descargaremos un modelo previamente entrenado desde `torchvision.models` y lo personalizaremos según nuestro propio problema. | \n",
    "| **4. Modelo de tren** | Veamos cómo funciona el nuevo modelo previamente entrenado en nuestro conjunto de datos de pizza, bistec y sushi. Usaremos las funciones de entrenamiento que creamos en el capítulo anterior. |\n",
    "| **5. Evalúe el modelo trazando curvas de pérdidas** | ¿Cómo fue nuestro primer modelo de aprendizaje por transferencia? ¿Se ajustaba demasiado o no?  |\n",
    "| **6. Haga predicciones sobre imágenes del conjunto de prueba** | Una cosa es comprobar las métricas de evaluación de un modelo, pero otra cosa es ver sus predicciones en muestras de prueba. ¡*visualicemos, visualicemos, visualicemos*! |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8eabbe",
   "metadata": {},
   "source": [
    "## ¿Dónde puedes obtener ayuda?\n",
    "\n",
    "Todos los materiales de este curso [están disponibles en GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
    "\n",
    "Si tiene problemas, puede hacer una pregunta en el curso [página de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
    "\n",
    "Y, por supuesto, está la [documentación de PyTorch](https://pytorch.org/docs/stable/index.html) y los [foros de desarrolladores de PyTorch](https://discuss.pytorch.org/), un lugar muy útil para todo lo relacionado con PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532c8c0",
   "metadata": {},
   "source": [
    "## 0. Configuración\n",
    "\n",
    "Comencemos importando/descargando los módulos necesarios para esta sección.\n",
    "\n",
    "Para ahorrarnos escribir código adicional, aprovecharemos algunos de los scripts de Python (como `data_setup.py` y `engine.py`) que creamos en la sección anterior, [05. PyTorch se vuelve modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
    "\n",
    "Específicamente, vamos a descargar el directorio [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) del repositorio `pytorch-deep-learning` (si aún no lo tenemos).\n",
    "\n",
    "También obtendremos el paquete [`torchinfo`](https://github.com/TylerYep/torchinfo) si no está disponible. \n",
    "\n",
    "`torchinfo` nos ayudará más adelante a darnos una representación visual de nuestro modelo.\n",
    "\n",
    "> **Nota:** A partir de junio de 2022, este cuaderno utiliza las versiones nocturnas de `torch` y `torchvision`, ya que se requiere `torchvision` v0.13+ para usar la API de pesos múltiples actualizada. Puede instalarlos usando el siguiente comando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85911ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que este portátil se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f157eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuar con las importaciones regulares\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Intente obtener torchinfo, instálelo si no funciona\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Intente importar el directorio going_modular, descárguelo de GitHub si no funciona\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721ebc4",
   "metadata": {},
   "source": [
    "Ahora configuremos el código independiente del dispositivo.\n",
    "\n",
    "> **Nota:** Si estás usando Google Colab y aún no tienes una GPU activada, ahora es el momento de activar una a través de `Runtime -> Cambiar tipo de tiempo de ejecución -> Acelerador de hardware -> GPU` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar código independiente del dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d580bae",
   "metadata": {},
   "source": [
    "## 1. Obtener datos\n",
    "\n",
    "Antes de que podamos comenzar a utilizar **transferencia de aprendizaje**, necesitaremos un conjunto de datos.\n",
    "\n",
    "Para ver cómo se compara el aprendizaje por transferencia con nuestros intentos anteriores de creación de modelos, descargaremos el mismo conjunto de datos que hemos estado usando para FoodVision Mini.\n",
    "\n",
    "Escribamos un código para descargar el conjunto de datos [`pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) del curso GitHub y luego descomprímalo. .\n",
    "\n",
    "También podemos asegurarnos de que si ya tenemos los datos, no se vuelvan a descargar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Ruta de configuración a la carpeta de datos\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# Si la carpeta de imágenes no existe, descárgala y prepárala...\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\") \n",
    "        zip_ref.extractall(image_path)\n",
    "\n",
    "    # Remove .zip file\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085cbd5",
   "metadata": {},
   "source": [
    "¡Excelente!\n",
    "\n",
    "Ahora tenemos el mismo conjunto de datos que hemos estado usando anteriormente, una serie de imágenes de pizza, bistec y sushi en formato de clasificación de imágenes estándar.\n",
    "\n",
    "Ahora creemos rutas a nuestros directorios de capacitación y pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de configuración\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4835c",
   "metadata": {},
   "source": [
    "## 2. Crear conjuntos de datos y cargadores de datos\n",
    "\n",
    "Como hemos descargado el directorio `going_modular`, podemos usar [`data_setup.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py ) script que creamos en la sección [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy) para preparar y configurar nuestros DataLoaders.\n",
    "\n",
    "Pero como usaremos un modelo previamente entrenado de [`torchvision.models`](https://pytorch.org/vision/stable/models.html), hay una transformación específica que necesitamos para preparar nuestras imágenes primero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57fdc56",
   "metadata": {},
   "source": [
    "### 2.1 Creando una transformación para `torchvision.models` (creación manual)\n",
    "\n",
    "> **Nota:** A partir de `torchvision` v0.13+, hay una actualización sobre cómo se pueden crear transformaciones de datos usando `torchvision.models`. Llamé al método anterior \"creación manual\" y al nuevo método \"creación automática\". Este cuaderno muestra ambos.\n",
    "\n",
    "Cuando se utiliza un modelo previamente entrenado, es importante que **los datos personalizados que se incluyen en el modelo se preparen de la misma manera que los datos de entrenamiento originales que se incluyeron en el modelo**.\n",
    "\n",
    "Antes de `torchvision` v0.13+, para crear una transformación para un modelo previamente entrenado en `torchvision.models`, la documentación decía:\n",
    "\n",
    "> Todos los modelos previamente entrenados esperan imágenes de entrada normalizadas de la misma manera, es decir, minilotes de imágenes de forma RGB de 3 canales (3 x H x W), donde se espera que H y W sean al menos 224. \n",
    ">\n",
    "> Las imágenes deben cargarse en un rango de `[0, 1]` y luego normalizarse usando `mean = [0.485, 0.456, 0.406]` y `std = [0.229, 0.224, 0.225]`. \n",
    ">\n",
    "> Puedes usar la siguiente transformación para normalizar:\n",
    ">\n",
    "> ```\n",
    "> normalizar = transforma.Normalizar(media=[0.485, 0.456, 0.406],\n",
    "> estándar=[0,229, 0,224, 0,225])\n",
    "> ```\n",
    "\n",
    "La buena noticia es que podemos lograr las transformaciones anteriores con una combinación de: \n",
    "\n",
    "| **Número de transformación** | **Se requiere transformación** | **Código para realizar la transformación** | \n",
    "| ----- | ----- | ----- |\n",
    "| 1 | Minilotes de tamaño `[batch_size, 3, height, width]` donde la altura y el ancho son al menos 224x224^. | `torchvision.transforms.Resize()` para cambiar el tamaño de las imágenes a `[3, 224, 224]`^ y `torch.utils.data.DataLoader()` para crear lotes de imágenes. |\n",
    "| 2 | Valores entre 0 y 1. | `torchvision.transforms.ToTensor()` |\n",
    "| 3 | Una media de `[0,485, 0,456, 0,406]` (valores en cada canal de color). | `torchvision.transforms.Normalize(mean=...)` para ajustar la media de nuestras imágenes.  |\n",
    "| 4 | Una desviación estándar de \"[0,229, 0,224, 0,225]\" (valores en cada canal de color). | `torchvision.transforms.Normalize(std=...)` para ajustar la desviación estándar de nuestras imágenes.  | \n",
    "\n",
    "> **Nota:** ^algunos modelos previamente entrenados desde `torchvision.models` en diferentes tamaños hasta `[3, 224, 224]`, por ejemplo, algunos podrían tomarlos en `[3, 240, 240]`. Para tamaños de imagen de entrada específicos, consulte la documentación.\n",
    "\n",
    "> **Pregunta:** *¿De dónde provienen los valores de media y desviación estándar? ¿Por qué necesitamos hacer esto?*\n",
    ">\n",
    "> Estos fueron calculados a partir de los datos. Específicamente, el conjunto de datos ImageNet toma las medias y las desviaciones estándar de un subconjunto de imágenes.\n",
    ">\n",
    "> Tampoco *necesitamos* hacer esto. Las redes neuronales suelen ser bastante capaces de determinar distribuciones de datos apropiadas (calcularán por sí mismas dónde deben estar la media y las desviaciones estándar), pero establecerlas desde el principio puede ayudar a nuestras redes a lograr un mejor rendimiento más rápido.\n",
    "\n",
    "Compongamos una serie de `torchvision.transforms` para realizar los pasos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree una canalización de transformaciones manualmente (requerido para torchvision <0.13)\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b88a08",
   "metadata": {},
   "source": [
    "¡Maravilloso!\n",
    "\n",
    "Ahora que tenemos una **serie de transformaciones creadas manualmente** lista para preparar nuestras imágenes, creemos DataLoaders de entrenamiento y prueba.\n",
    "\n",
    "Podemos crearlos usando la función `create_dataloaders` desde el script [`data_setup.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) creado en [05. PyTorch se vuelve modular, parte 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
    "\n",
    "Estableceremos `batch_size=32` para que nuestro modelo vea minilotes de 32 muestras a la vez.\n",
    "\n",
    "Y podemos transformar nuestras imágenes usando el canal de transformación que creamos anteriormente configurando `transform=manual_transforms`.\n",
    "\n",
    "> **Nota:** He incluido esta creación manual de transformaciones en este cuaderno porque es posible que encuentres recursos que utilicen este estilo. También es importante tener en cuenta que debido a que estas transformaciones se crean manualmente, también son infinitamente personalizables. Entonces, si quisiera incluir técnicas de aumento de datos en su proceso de transformación, podría hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree cargadores de datos de entrenamiento y prueba y obtenga una lista de nombres de clases\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
    "                                                                               batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db6d29",
   "metadata": {},
   "source": [
    "### 2.2 Creando una transformación para `torchvision.models` (creación automática)\n",
    "\n",
    "Como se indicó anteriormente, cuando se utiliza un modelo previamente entrenado, es importante que **los datos personalizados que se ingresan en el modelo se preparen de la misma manera que los datos de entrenamiento originales que se ingresaron en el modelo**.\n",
    "\n",
    "Arriba vimos cómo crear manualmente una transformación para un modelo previamente entrenado.\n",
    "\n",
    "Pero a partir de `torchvision` v0.13+, se agregó una función de creación de transformación automática.\n",
    "\n",
    "Cuando configura un modelo desde `torchvision.models` y selecciona los pesos del modelo previamente entrenado que le gustaría usar, por ejemplo, digamos que nos gustaría usar:\n",
    "    \n",
    "```pitón\n",
    "pesos = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "```\n",
    "\n",
    "Dónde,\n",
    "* `EfficientNet_B0_Weights` son los pesos de la arquitectura del modelo que nos gustaría usar (hay muchas opciones diferentes de arquitectura de modelo en `torchvision.models`).\n",
    "* `DEFAULT` significa los *mejores pesos disponibles* (el mejor rendimiento en ImageNet).\n",
    "    * **Nota:** Dependiendo de la arquitectura del modelo que elija, también puede ver otras opciones como `IMAGENET_V1` e `IMAGENET_V2`, donde generalmente cuanto mayor sea el número de versión, mejor. Aunque si desea lo mejor disponible, \"DEFAULT\" es la opción más sencilla. Consulte la [documentación `torchvision.models`](https://pytorch.org/vision/main/models.html) para obtener más información.\n",
    "    \n",
    "Probémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenga un conjunto de pesos de modelo previamente entrenados\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3f344",
   "metadata": {},
   "source": [
    "Y ahora para acceder a las transformaciones asociadas con nuestros `pesos`, podemos usar el método `transforms()`.\n",
    "\n",
    "Básicamente, esto significa \"obtener las transformaciones de datos que se utilizaron para entrenar `EfficientNet_B0_Weights` en ImageNet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenga las transformaciones utilizadas para crear nuestros pesos previamente entrenados.\n",
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19227a6",
   "metadata": {},
   "source": [
    "Observe cómo `auto_transforms` es muy similar a `manual_transforms`, la única diferencia es que `auto_transforms` vino con la arquitectura del modelo que elegimos, mientras que tuvimos que crear `manual_transforms` a mano.\n",
    "\n",
    "El beneficio de crear automáticamente una transformación a través de `weights.transforms()` es que garantiza que está utilizando la misma transformación de datos que el modelo previamente entrenado que se usó cuando se entrenó.\n",
    "\n",
    "Sin embargo, la desventaja de utilizar transformaciones creadas automáticamente es la falta de personalización.\n",
    "\n",
    "Podemos usar `auto_transforms` para crear DataLoaders con `create_dataloaders()` tal como antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree cargadores de datos de entrenamiento y prueba y obtenga una lista de nombres de clases\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n",
    "                                                                               batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5630a5",
   "metadata": {},
   "source": [
    "## 3. Obtener un modelo previamente entrenado\n",
    "\n",
    "Muy bien, ¡aquí viene la parte divertida!\n",
    "\n",
    "En los últimos cuadernos hemos estado construyendo redes neuronales PyTorch desde cero.\n",
    "\n",
    "Y si bien es una buena habilidad, nuestros modelos no han funcionado tan bien como nos gustaría. \n",
    "\n",
    "Ahí es donde entra en juego la **transferencia de aprendizaje**.\n",
    "\n",
    "La idea general del aprendizaje por transferencia es **tomar un modelo que ya funciona bien en un espacio de problemas similar al suyo y luego personalizarlo según su caso de uso**.\n",
    "\n",
    "Dado que estamos trabajando en un problema de visión por computadora (clasificación de imágenes con FoodVision Mini), podemos encontrar modelos de clasificación previamente entrenados en [`torchvision.models`](https://pytorch.org/vision/stable/models.html#classification ).\n",
    "\n",
    "Al explorar la documentación, encontrará muchos pilares de arquitectura de visión por computadora comunes, como:\n",
    "\n",
    "| **La columna vertebral de la arquitectura** | **Código** |\n",
    "| ----- | ----- |\n",
    "| [ResNet](https://arxiv.org/abs/1512.03385)'s | `torchvision.models.resnet18()`, `torchvision.models.resnet50()`... | \n",
    "| [VGG](https://arxiv.org/abs/1409.1556) (similar a lo que usamos para TinyVGG) | `torchvision.models.vgg16()` | \n",
    "| [EfficientNet](https://arxiv.org/abs/1905.11946)'s | `torchvision.models.ficientnet_b0()`, `torchvision.models.ficientnet_b1()`... | \n",
    "| [VisionTransformer](https://arxiv.org/abs/2010.11929) (ViT)| `torchvision.models.vit_b_16()`, `torchvision.models.vit_b_32()`... | \n",
    "| [ConvNeXt](https://arxiv.org/abs/2201.03545) | `torchvision.models.convnext_tiny()`, `torchvision.models.convnext_small()`... |\n",
    "| Más disponible en `torchvision.models` | `modelos.torchvision...` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c5b65",
   "metadata": {},
   "source": [
    "### 3.1 ¿Qué modelo previamente entrenado debería utilizar?\n",
    "\n",
    "Depende de su problema/del dispositivo con el que esté trabajando.\n",
    "\n",
    "Generalmente, el número más alto en el nombre del modelo (por ejemplo, `ficientnet_b0()` -> `ficientnet_b1()` -> `ficientnet_b7()`) significa *mejor rendimiento* pero un modelo *más grande*.\n",
    "\n",
    "Se podría pensar que un mejor rendimiento es *siempre mejor*, ¿verdad?\n",
    "\n",
    "Eso es cierto, pero **algunos modelos de mejor rendimiento son demasiado grandes para algunos dispositivos**.\n",
    "\n",
    "Por ejemplo, supongamos que desea ejecutar su modelo en un dispositivo móvil, tendrá que tener en cuenta los recursos informáticos limitados del dispositivo, por lo que buscará un modelo más pequeño.\n",
    "\n",
    "Pero si tienes un poder de cómputo ilimitado, como afirma [*The Bitter Lesson*](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), probablemente elegirás el modelo más grande y con mayor necesidad de cómputo que puedas. poder.\n",
    "\n",
    "Comprender esta **compensación entre rendimiento, velocidad y tamaño** llegará con el tiempo y la práctica.\n",
    "\n",
    "Para mí, he encontrado un buen equilibrio en los modelos `ficientnet_bX`. \n",
    "\n",
    "A partir de mayo de 2022, [Nutrify](https://nutrify.app) (la aplicación basada en aprendizaje automático en la que estoy trabajando) funciona con un `ficientnet_b0`.\n",
    "\n",
    "[Comma.ai](https://comma.ai/) (una empresa que fabrica software de código abierto para vehículos autónomos) [utiliza un `ficientnet_b2`](https://geohot.github.io/blog/jekyll/ update/2021/10/29/an-architecture-for-life.html) para conocer una representación de la carretera.\n",
    "\n",
    "> **Nota:** Aunque estamos usando `ficientnet_bX`, es importante no apegarse demasiado a ninguna arquitectura en particular, ya que siempre cambian a medida que se publican nuevas investigaciones. Lo mejor es experimentar, experimentar, experimentar y ver qué funciona para su problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600b5f5",
   "metadata": {},
   "source": [
    "### 3.2 Configurar un modelo previamente entrenado\n",
    "\n",
    "El modelo previamente entrenado que usaremos es [`torchvision.models.ficientnet_b0()`](https://pytorch.org/vision/main/models/generated/torchvision.models.ficientnet_b0.html).\n",
    "\n",
    "La arquitectura es del artículo *[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)*.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-effnet-b0-feature-extractor.png\" alt=\"efficienet_b0 del modelo de extracción de características de PyTorch torchvision\" ancho=900/>\n",
    "\n",
    "*Ejemplo de lo que vamos a crear, un [modelo `EfficientNet_B0`](https://ai.googleblog.com/2019/05/ficientnet-improving-accuracy-and.html) previamente entrenado de `torchvision.models` con la capa de salida ajustada para nuestro caso de uso de clasificación de imágenes de pizza, bistec y sushi.*\n",
    "\n",
    "Podemos configurar los pesos de ImageNet previamente entrenados en `EfficientNet_B0` usando el mismo código que usamos para crear las transformaciones.\n",
    "\n",
    "\n",
    "```pitón\n",
    "pesos = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = mejores pesos disponibles para ImageNet\n",
    "```\n",
    "\n",
    "Esto significa que el modelo ya ha sido entrenado en millones de imágenes y tiene una buena representación base de los datos de las imágenes.\n",
    "\n",
    "La versión PyTorch de este modelo previamente entrenado es capaz de lograr una precisión de ~77,7% en las 1000 clases de ImageNet.\n",
    "\n",
    "También lo enviaremos al dispositivo de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75159270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTIGUO: configure el modelo con pesos previamente entrenados y envíelo al dispositivo de destino (esto era antes de torchvision v0.13)\n",
    "# model = torchvision.models.ficientnet_b0(pretrained=True).to(device) # Método ANTIGUO (con pretrained=True)\n",
    "\n",
    "# NUEVO: Configure el modelo con pesas previamente entrenadas y envíelo al dispositivo de destino (torchvision v0.13+)\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# modelo # descomentar en la salida (es muy largo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554860b4",
   "metadata": {},
   "source": [
    "> **Nota:** En versiones anteriores de `torchvision`, se creaba un modelo previamente entrenado con código como:\n",
    ">\n",
    "> `modelo = torchvision.models.ficientnet_b0(preentrenado=True).to(dispositivo)`\n",
    ">\n",
    "> Sin embargo, ejecutar esto usando `torchvision` v0.13+ resultará en errores como los siguientes:\n",
    "> \n",
    "> `Advertencia de usuario: el parámetro 'preentrenado' está obsoleto desde 0.13 y se eliminará en 0.15; utilice 'pesos' en su lugar.`\n",
    ">\n",
    "> Y...\n",
    "> \n",
    "> `Advertencia de usuario: Los argumentos distintos de una enumeración de peso o Ninguno para los pesos están obsoletos desde 0.13 y se eliminarán en 0.15. El comportamiento actual es equivalente a pasar pesos=EfficientNet_B0_Weights.IMAGENET1K_V1. También puede utilizar Weights=EfficientNet_B0_Weights.DEFAULT para obtener los pesos más actualizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431a2e9",
   "metadata": {},
   "source": [
    "Si imprimimos el modelo, obtenemos algo similar a lo siguiente:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-v2-effnetb0-model-print-out.png\" alt=\"resultado de la impresión del modelo eficientenet_b0 de torchvision.models\" ancho=900/>\n",
    "\n",
    "Montones, montones, montones de capas.\n",
    "\n",
    "Este es uno de los beneficios del aprendizaje por transferencia: tomar un modelo existente, que ha sido elaborado por algunos de los mejores ingenieros del mundo y aplicarlo a su propio problema.\n",
    "\n",
    "Nuestro `ficientnet_b0` se compone de tres partes principales:\n",
    "1. `características`: una colección de capas convolucionales y otras capas de activación para aprender una representación base de los datos de visión (esta representación/colección base de capas a menudo se denomina **características** o **extractor de características**, \"las capas base del modelo aprenden las diferentes **características** de las imágenes\").\n",
    "2. `avgpool`: toma el promedio de la salida de las capas de `características` y lo convierte en un **vector de características**.\n",
    "3. `classifier`: convierte el **vector de características** en un vector con la misma dimensionalidad que el número de clases de salida requeridas (ya que `ficientnet_b0` está preentrenado en ImageNet y debido a que ImageNet tiene 1000 clases, `out_features=1000` es el valor por defecto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2e98f",
   "metadata": {},
   "source": [
    "### 3.3 Obteniendo un resumen de nuestro modelo con `torchinfo.summary()`\n",
    "\n",
    "Para obtener más información sobre nuestro modelo, usemos el método [`summary()`] de `torchinfo` (https://github.com/TylerYep/torchinfo#documentation).\n",
    "\n",
    "Para hacerlo, pasaremos:\n",
    " * `model`: el modelo del que nos gustaría obtener un resumen.\n",
    " * `input_size` - la forma de los datos que nos gustaría pasar a nuestro modelo, para el caso de `ficientnet_b0`, el tamaño de entrada es `(batch_size, 3, 224, 224)`, aunque [otras variantes de ` eficientenet_bX` tiene diferentes tamaños de entrada](https://github.com/pytorch/vision/blob/d2bfd639e46e1c5dc3c177f889dc7750c8d137c7/references/classification/train.py#L92-L93).\n",
    "    * **Nota:** Muchos modelos modernos pueden manejar imágenes de entrada de diferentes tamaños gracias a [`torch.nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d .html), esta capa ajusta de forma adaptativa el `output_size` de una entrada determinada según sea necesario. Puede probar esto pasando imágenes de entrada de diferentes tamaños a `summary()` o a sus modelos.\n",
    " * `col_names`: las diversas columnas de información que nos gustaría ver sobre nuestro modelo. \n",
    " * `col_width`: qué ancho deben tener las columnas para el resumen.\n",
    " * `row_settings`: qué funciones mostrar en una fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f10888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprima un resumen usando torchinfo (descomente el resultado real)\n",
    "summary(model=model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065da521",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-torchinfo-summary-unfrozen-layers.png\" alt=\"salida de torchinfo.summary() cuando pasó nuestro modelo con todas las capas como entrenables\" width=900/>\n",
    "\n",
    "¡Guau!\n",
    "\n",
    "¡Ese sí que es un gran modelo!\n",
    "\n",
    "Desde el resultado del resumen, podemos ver todos los diversos cambios de forma de entrada y salida a medida que los datos de nuestra imagen pasan por el modelo.\n",
    "\n",
    "Y hay muchos más parámetros totales (pesos previamente entrenados) para reconocer diferentes patrones en nuestros datos.\n",
    "\n",
    "Como referencia, nuestro modelo de secciones anteriores, **TinyVGG, tenía 8.083 parámetros frente a 5.288.548 parámetros para `ficientnet_b0`, ¡un aumento de ~654x**!\n",
    "\n",
    "¿Qué opinas? ¿Esto significará un mejor rendimiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9b8fb",
   "metadata": {},
   "source": [
    "### 3.4 Congelar el modelo base y cambiar la capa de salida para adaptarla a nuestras necesidades\n",
    "\n",
    "El proceso de aprendizaje por transferencia suele ser el siguiente: congelar algunas capas base de un modelo previamente entrenado (normalmente la sección \"características\") y luego ajustar las capas de salida (también llamadas capas principales/clasificadoras) para satisfacer sus necesidades.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-v2-effnet-changing-the-classifier-head.png\" alt=\"cambiando el clasificador eficientenet dirígete a un número personalizado de salidas\" width=900/>\n",
    "\n",
    "*Puede personalizar las salidas de un modelo previamente entrenado cambiando las capas de salida para adaptarlas a su problema. El `torchvision.models.ficientnet_b0()` original viene con `out_features=1000` porque hay 1000 clases en ImageNet, el conjunto de datos en el que se entrenó. Sin embargo, para nuestro problema de clasificar imágenes de pizza, bistec y sushi solo necesitamos `out_features=3`.*\n",
    "\n",
    "Congelemos todas las capas/parámetros en la sección \"características\" de nuestro modelo \"ficientnet_b0\".\n",
    "\n",
    "> **Nota:** *Congelar* capas significa mantenerlas como están durante el entrenamiento. Por ejemplo, si su modelo tiene capas previamente entrenadas, *congelarlas* sería decir: \"no cambie ninguno de los patrones en estas capas durante el entrenamiento, manténgalos como están\". En esencia, nos gustaría mantener los pesos/patrones previamente entrenados que nuestro modelo ha aprendido de ImageNet como columna vertebral y luego solo cambiar las capas de salida.\n",
    "\n",
    "Podemos congelar todas las capas/parámetros en la sección \"características\" configurando el atributo \"requires_grad=False\".\n",
    "\n",
    "Para los parámetros con `requires_grad=False`, PyTorch no realiza un seguimiento de las actualizaciones de gradiente y, a su vez, nuestro optimizador no cambiará estos parámetros durante el entrenamiento.\n",
    "\n",
    "En esencia, un parámetro con `requires_grad=False` es \"no entrenable\" o \"congelado\" en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congele todas las capas base en la sección \"características\" del modelo (el extractor de características) configurando require_grad=False\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc7bce",
   "metadata": {},
   "source": [
    "¡Características capas extractoras congeladas!\n",
    "\n",
    "Ahora ajustemos la capa de salida o la parte del \"clasificador\" de nuestro modelo previamente entrenado a nuestras necesidades.\n",
    "\n",
    "En este momento, nuestro modelo previamente entrenado tiene `out_features=1000` porque hay 1000 clases en ImageNet. \n",
    "\n",
    "Sin embargo, no tenemos 1000 clases, solo tenemos tres: pizza, bistec y sushi.\n",
    "\n",
    "Podemos cambiar la parte \"clasificador\" de nuestro modelo creando una nueva serie de capas.\n",
    "\n",
    "El \"clasificador\" actual consta de:\n",
    "\n",
    "```\n",
    "(clasificador): Secuencial(\n",
    "    (0): Abandono(p=0,2, in situ=Verdadero)\n",
    "    (1): Lineal (in_features=1280, out_features=1000, sesgo=Verdadero)\n",
    "```\n",
    "\n",
    "Mantendremos la capa `Dropout` igual usando [`torch.nn.Dropout(p=0.2, inplace=True)`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout .html).\n",
    "\n",
    "> **Nota:** [Capas de abandono](https://developers.google.com/machine-learning/glossary#dropout_regularization) elimina aleatoriamente conexiones entre dos capas de redes neuronales con una probabilidad de \"p\". Por ejemplo, si `p=0.2`, el 20% de las conexiones entre capas de la red neuronal se eliminarán aleatoriamente en cada pasada. Esta práctica está destinada a ayudar a regularizar (evitar el sobreajuste) un modelo asegurándose de que las conexiones que quedan aprendan características para compensar la eliminación de las otras conexiones (con suerte, estas características restantes son *más generales*). \n",
    "\n",
    "Y mantendremos `in_features=1280` para nuestra capa de salida `Lineal` pero cambiaremos el valor `out_features` a la longitud de nuestros `class_names` (`len(['pizza', 'steak', 'sushi ']) = 3`).\n",
    "\n",
    "Nuestra nueva capa \"clasificador\" debería estar en el mismo dispositivo que nuestro \"modelo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocar las semillas manuales.\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Obtenga la longitud de class_names (una unidad de salida para cada clase)\n",
    "output_shape = len(class_names)\n",
    "\n",
    "# Vuelva a crear la capa del clasificador y siémbrela en el dispositivo de destino.\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=1280, \n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4466a",
   "metadata": {},
   "source": [
    "¡Lindo!\n",
    "\n",
    "Capa de salida actualizada, obtengamos otro resumen de nuestro modelo y veamos qué ha cambiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ec1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hacer un resumen *después* de congelar las características y cambiar la capa del clasificador de salida (descomentar para la salida real)\n",
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5831881",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-torchinfo-summary-frozen-layers.png\" alt=\"salida de torchinfo.summary() después congelar múltiples capas en nuestro modelo y cambiar el cabezal del clasificador\" width=900/>\n",
    "\n",
    "¡Ho, ho! ¡Hay algunos cambios aquí!\n",
    "\n",
    "Repasémoslos:\n",
    "* **Columna entrenable**: verá que muchas de las capas base (las que están en la parte \"características\") tienen su valor entrenable como \"Falso\". Esto se debe a que configuramos su atributo `requires_grad=False`. A menos que cambiemos esto, estas capas no se actualizarán durante el entrenamiento futuro.\n",
    "* **Forma de salida del `clasificador`**: la parte del `clasificador` del modelo ahora tiene un valor de Forma de salida de `[32, 3]` en lugar de `[32, 1000]`. Su valor entrenable también es \"Verdadero\". Esto significa que sus parámetros se actualizarán durante el entrenamiento. En esencia, estamos usando la parte de \"características\" para alimentar a nuestra parte de \"clasificador\" con una representación base de una imagen y luego nuestra capa de \"clasificador\" aprenderá cómo alinear la representación base con nuestro problema.\n",
    "* **Menos parámetros entrenables**: anteriormente había 5.288.548 parámetros entrenables. Pero como congelamos muchas de las capas del modelo y solo dejamos el \"clasificador\" como entrenable, ahora solo hay 3843 parámetros entrenables (incluso menos que nuestro modelo TinyVGG). Aunque también hay 4.007.548 parámetros no entrenables, estos crearán una representación base de nuestras imágenes de entrada para alimentar nuestra capa \"clasificadora\".\n",
    "\n",
    "> **Nota:** Cuantos más parámetros entrenables tenga un modelo, más potencia de cálculo y más tiempo llevará entrenar. Congelar las capas base de nuestro modelo y dejarlo con parámetros menos entrenables significa que nuestro modelo debería entrenarse con bastante rapidez. Este es un gran beneficio del aprendizaje por transferencia: tomar los parámetros ya aprendidos de un modelo entrenado en un problema similar al suyo y ajustar solo ligeramente los resultados para adaptarlos a su problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ffdaf",
   "metadata": {},
   "source": [
    "## 4. Modelo de tren\n",
    "\n",
    "Ahora que tenemos un modelo previamente entrenado que está semicongelado y tiene un \"clasificador\" personalizado, ¿qué tal si vemos el aprendizaje por transferencia en acción?\n",
    "\n",
    "Para comenzar a entrenar, creemos una función de pérdida y un optimizador.\n",
    "\n",
    "Como todavía estamos trabajando con clasificación de clases múltiples, usaremos `nn.CrossEntropyLoss()` para la función de pérdida.\n",
    "\n",
    "Y nos quedaremos con `torch.optim.Adam()` como nuestro optimizador con `lr=0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir pérdida y optimizador\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2986a7",
   "metadata": {},
   "source": [
    "¡Maravilloso! \n",
    "\n",
    "Para entrenar nuestro modelo, podemos usar la función `train()` que definimos en [05. PyTorch Going Modular sección 04](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them).\n",
    "\n",
    "La función `train()` está en el script [`engine.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py) dentro del [ Directorio `going_modular`] (https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular/going_modular). \n",
    "\n",
    "Veamos cuánto tiempo lleva entrenar nuestro modelo durante 5 épocas.\n",
    "\n",
    "> **Nota:** Aquí solo entrenaremos los parámetros `clasificador` ya que todos los demás parámetros de nuestro modelo se han congelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e923c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer las semillas aleatorias\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# iniciar el cronómetro\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Configurar el entrenamiento y guardar los resultados.\n",
    "results = engine.train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=5,\n",
    "                       device=device)\n",
    "\n",
    "# Finalice el cronómetro e imprima cuánto tiempo tardó\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc4faa",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Nuestro modelo se entrenó bastante rápido (~5 segundos en mi máquina local con una [GPU NVIDIA TITAN RTX](https://www.nvidia.com/en-au/deep-learning-ai/products/titan-rtx/)/ unos 15 segundos en Google Colab con una [GPU NVIDIA P100](https://www.nvidia.com/en-au/data-center/tesla-p100/)).\n",
    "\n",
    "¡Y parece que arrasó con los resultados de nuestro modelo anterior!\n",
    "\n",
    "Con una columna vertebral `ficientnet_b0`, nuestro modelo logra una precisión de casi el 85%+ en el conjunto de datos de prueba, casi *el doble* de lo que pudimos lograr con TinyVGG.\n",
    "\n",
    "Nada mal para un modelo que descargamos con unas pocas líneas de código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90abc62",
   "metadata": {},
   "source": [
    "## 5. Evaluar el modelo trazando curvas de pérdida\n",
    "\n",
    "Nuestro modelo parece estar funcionando bastante bien.\n",
    "\n",
    "Tracemos sus curvas de pérdida para ver cómo se ve el entrenamiento a lo largo del tiempo. \n",
    "\n",
    "Podemos trazar las curvas de pérdida usando la función `plot_loss_curves()` que creamos en [04. Sección 7.8 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0).\n",
    "\n",
    "La función está almacenada en el script [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py), por lo que intentaremos importarla y descargarla. script si no lo tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenga la función plot_loss_curves() de helper_functions.py, descargue el archivo si no lo tenemos\n",
    "try:\n",
    "    from helper_functions import plot_loss_curves\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        import requests\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "        f.write(request.content)\n",
    "    from helper_functions import plot_loss_curves\n",
    "\n",
    "# Trazar las curvas de pérdidas de nuestro modelo.\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873cce7",
   "metadata": {},
   "source": [
    "¡Esas son algunas curvas de pérdidas de excelente apariencia! \n",
    "\n",
    "Parece que la pérdida de ambos conjuntos de datos (entrenamiento y prueba) va en la dirección correcta.\n",
    "\n",
    "Lo mismo ocurre con los valores de precisión, con tendencia al alza.\n",
    "\n",
    "Esto demuestra el poder de la **transferencia de aprendizaje**. El uso de un modelo previamente entrenado a menudo genera resultados bastante buenos con una pequeña cantidad de datos en menos tiempo.\n",
    "\n",
    "Me pregunto qué pasaría si intentaras entrenar al modelo por más tiempo. ¿O si agregamos más datos?\n",
    "\n",
    "> **Pregunta:** Al observar las curvas de pérdida, ¿nuestro modelo parece estar sobreajustado o insuficientemente ajustado? ¿O tal vez ninguno de los dos? Pista: consulte el cuaderno [04. Conjuntos de datos personalizados de PyTorch, parte 8. ¿Cómo debería ser una curva de pérdida ideal?](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like) para obtener ideas ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ed7fa",
   "metadata": {},
   "source": [
    "## 6. Haga predicciones sobre imágenes del conjunto de prueba.\n",
    "\n",
    "Parece que nuestro modelo funciona bien cuantitativamente pero ¿qué tal cualitativamente?\n",
    "\n",
    "Averigüemos haciendo algunas predicciones con nuestro modelo en imágenes del conjunto de prueba (éstas no se ven durante el entrenamiento) y grafiquémoslas.\n",
    "\n",
    "*¡Visualiza, visualiza, visualiza!*\n",
    "\n",
    "Una cosa que tendremos que recordar es que para que nuestro modelo haga predicciones sobre una imagen, la imagen debe tener el *mismo* formato que las imágenes en las que se entrenó nuestro modelo.\n",
    "\n",
    "Esto significa que necesitaremos asegurarnos de que nuestras imágenes tengan:\n",
    "* **Misma forma**: si nuestras imágenes tienen formas diferentes a las que se entrenó nuestro modelo, obtendremos errores de forma.\n",
    "* **Mismo tipo de datos**: si nuestras imágenes tienen un tipo de datos diferente (por ejemplo, `torch.int8` frente a `torch.float32`), obtendremos errores de tipo de datos.\n",
    "* **Mismo dispositivo**: si nuestras imágenes están en un dispositivo diferente a nuestro modelo, obtendremos errores de dispositivo.\n",
    "* **Mismas transformaciones**: si nuestro modelo se entrena con imágenes que se han transformado de cierta manera (por ejemplo, normalizadas con una media y una desviación estándar específicas) e intentamos hacer predicciones sobre imágenes transformadas de una manera diferente, estas predicciones pueden me voy.\n",
    "\n",
    "> **Nota:** Estos requisitos se aplican a todo tipo de datos si intentas hacer predicciones con un modelo entrenado. Los datos que desea predecir deben estar en el mismo formato en el que se entrenó su modelo.\n",
    "\n",
    "Para hacer todo esto, crearemos una función `pred_and_plot_image()` para:\n",
    "\n",
    "1. Tome un modelo entrenado, una lista de nombres de clases, una ruta de archivo a una imagen de destino, un tamaño de imagen, una transformación y un dispositivo de destino.\n",
    "2. Abra una imagen con [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open).\n",
    "3. Cree una transformación para la imagen (por defecto será `manual_transforms` que creamos anteriormente o podría usar una transformación generada a partir de `weights.transforms()`).\n",
    "4. Asegúrese de que el modelo esté en el dispositivo de destino.\n",
    "5. Active el modo de evaluación del modelo con `model.eval()` (esto desactiva capas como `nn.Dropout()`, por lo que no se usan para la inferencia) y el administrador de contexto del modo de inferencia.\n",
    "6. Transforme la imagen de destino con la transformación realizada en el paso 3 y agregue una dimensión de lote adicional con `torch.unsqueeze(dim=0)` para que nuestra imagen de entrada tenga la forma `[batch_size, color_channels, height, width]`.\n",
    "7. Haga una predicción sobre la imagen pasándola al modelo asegurándose de que esté en el dispositivo de destino.\n",
    "8. Convierta los logits de salida del modelo en probabilidades de predicción con `torch.softmax()`.\n",
    "9. Convierta las probabilidades de predicción del modelo en etiquetas de predicción con `torch.argmax()`.\n",
    "10. Trace la imagen con `matplotlib` y establezca el título en la etiqueta de predicción del paso 9 y la probabilidad de predicción del paso 8.\n",
    "\n",
    "> **Nota:** Esta es una función similar a [04. Sección 11.3 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function) `pred_and_plot_image()` con algunos pasos modificados ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Tome un modelo entrenado, nombres de clases, ruta de imagen, tamaño de imagen, una transformación y un dispositivo de destino.\n",
    "def pred_and_plot_image(model: torch.nn.Module,\n",
    "                        image_path: str, \n",
    "                        class_names: List[str],\n",
    "                        image_size: Tuple[int, int] = (224, 224),\n",
    "                        transform: torchvision.transforms = None,\n",
    "                        device: torch.device=device):\n",
    "    \n",
    "    \n",
    "    # 2. Open image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # 3. Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    ### Predict on image ### \n",
    "\n",
    "    # 4. Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "      transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "      # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
    "      target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # 9. Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # 10. Plot image with predicted label and probability \n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2de291",
   "metadata": {},
   "source": [
    "¡Qué función tan atractiva!\n",
    "\n",
    "Probémoslo haciendo predicciones sobre algunas imágenes aleatorias del conjunto de prueba.\n",
    "\n",
    "Podemos obtener una lista de todas las rutas de las imágenes de prueba usando `list(Path(test_dir).glob(\"*/*.jpg\"))`, las estrellas en el método `glob()` dicen \"cualquier archivo que coincida con este patrón \", es decir, cualquier archivo que termine en `.jpg` (todas nuestras imágenes).\n",
    "\n",
    "Y luego podemos muestrear aleatoriamente varios de estos usando [`random.sample(populuation, k)`](https://docs.python.org/3/library/random.html#random.sample) de Python donde `población ` es la secuencia a muestrear y `k` es el número de muestras a recuperar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2609f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenga una lista aleatoria de rutas de imágenes del conjunto de prueba\n",
    "import random\n",
    "num_images_to_plot = 3\n",
    "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \n",
    "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
    "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
    "\n",
    "# Hacer predicciones y trazar las imágenes.\n",
    "for image_path in test_image_path_sample:\n",
    "    pred_and_plot_image(model=model, \n",
    "                        image_path=image_path,\n",
    "                        class_names=class_names,\n",
    "                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
    "                        image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b9c6e",
   "metadata": {},
   "source": [
    "¡Guau!\n",
    "\n",
    "Esas predicciones parecen mucho mejores que las que nuestro modelo TinyVGG hacía anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1530a",
   "metadata": {},
   "source": [
    "### 6.1 Hacer predicciones sobre una imagen personalizada\n",
    "\n",
    "Parece que nuestro modelo obtiene buenos resultados cualitativos con los datos del conjunto de prueba.\n",
    "\n",
    "Pero ¿qué tal nuestra propia imagen personalizada?\n",
    "\n",
    "¡Ahí es donde está la verdadera diversión del aprendizaje automático!\n",
    "\n",
    "Predecir sobre sus propios datos personalizados, fuera de cualquier conjunto de entrenamiento o prueba.\n",
    "\n",
    "Para probar nuestro modelo en una imagen personalizada, importemos la antigua y fiel imagen `pizza-dad.jpeg` (una imagen de mi papá comiendo pizza).\n",
    "\n",
    "Luego lo pasaremos a la función `pred_and_plot_image()` que creamos anteriormente y veremos qué sucede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbec0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar imagen personalizada\n",
    "import requests\n",
    "\n",
    "# Configurar ruta de imagen personalizada\n",
    "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
    "\n",
    "# Descarga la imagen si aún no existe\n",
    "if not custom_image_path.is_file():\n",
    "    with open(custom_image_path, \"wb\") as f:\n",
    "        # When downloading from GitHub, need to use the \"raw\" file link\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "        print(f\"Downloading {custom_image_path}...\")\n",
    "        f.write(request.content)\n",
    "else:\n",
    "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
    "\n",
    "# Predecir en imagen personalizada\n",
    "pred_and_plot_image(model=model,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407b02b",
   "metadata": {},
   "source": [
    "¡Dos pulgares arriba!\n",
    "\n",
    "¡Parece que nuestro modelo volvió a acertar!\n",
    "\n",
    "Pero esta vez la probabilidad de predicción es mayor que la de TinyVGG (`0.373`) en [04. Sección 11.3 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function).\n",
    "\n",
    "Esto indica que nuestro modelo `ficientnet_b0` tiene *más* confianza en su predicción, mientras que nuestro modelo TinyVGG era equivalente a solo adivinar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e566ad01",
   "metadata": {},
   "source": [
    "## Principales conclusiones\n",
    "* **El aprendizaje por transferencia** a menudo le permite obtener buenos resultados con una cantidad relativamente pequeña de datos personalizados.\n",
    "* Conociendo el poder del aprendizaje por transferencia, es una buena idea preguntar al comienzo de cada problema: \"¿Existe un modelo de buen rendimiento para mi problema?\"\n",
    "* Cuando utilice un modelo previamente entrenado, es importante que sus datos personalizados estén formateados/preprocesados ​​de la misma manera que se entrenó el modelo original; de lo contrario, es posible que se degrade el rendimiento.\n",
    "* Lo mismo ocurre con la predicción de datos personalizados; asegúrese de que sus datos personalizados estén en el mismo formato que los datos con los que se entrenó su modelo.\n",
    "* Hay [varios lugares diferentes para encontrar modelos previamente entrenados](https://www.learnpytorch.io/06_pytorch_transfer_learning/#where-to-find-pretrained-models) de las bibliotecas del dominio PyTorch, HuggingFace Hub y bibliotecas como `timm ` (Modelos de imagen de PyTorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afb46b",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Todos los ejercicios se centran en practicar el código anterior.\n",
    "\n",
    "Debería poder completarlos haciendo referencia a cada sección o siguiendo los recursos vinculados.\n",
    "\n",
    "Todos los ejercicios deben completarse utilizando [código independiente del dispositivo](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n",
    "\n",
    "**Recursos:**\n",
    "* [Cuaderno de plantilla de ejercicios para 06](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/06_pytorch_transfer_learning_exercises.ipynb)\n",
    "* [Cuaderno de soluciones de ejemplo para 06](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/06_pytorch_transfer_learning_exercise_solutions.ipynb) (pruebe los ejercicios *antes* de mirar esto)\n",
    "    * Vea un [video tutorial de las soluciones en vivo en YouTube](https://youtu.be/ueLolShyFqs) (errores y todo)\n",
    "\n",
    "1. Haga predicciones sobre todo el conjunto de datos de prueba y trace una matriz de confusión para los resultados de nuestro modelo en comparación con las etiquetas de verdad. Consulte [03. Sección 10 de PyTorch Computer Vision](https://www.learnpytorch.io/03_pytorch_computer_vision/#10-making-a-confusion-matrix-for-further-prediction-evaluación) para obtener ideas.\n",
    "2. Obtenga las predicciones \"más incorrectas\" en el conjunto de datos de prueba y trace las 5 imágenes \"más incorrectas\". Puedes hacer esto mediante:\n",
    "    * Predecir en todo el conjunto de datos de prueba, almacenar las etiquetas y las probabilidades predichas.\n",
    "    * Ordene las predicciones por *predicción incorrecta* y luego *probabilidades predichas descendentes*, esto le dará las predicciones incorrectas con las probabilidades de predicción *más altas*, en otras palabras, las \"más incorrectas\".\n",
    "    * Traza las 5 imágenes \"más incorrectas\", ¿por qué crees que el modelo se equivocó? \n",
    "3. Predice tu propia imagen de pizza/filete/sushi: ¿cómo va el modelo? ¿Qué sucede si predices en una imagen que no es pizza/filete/sushi?\n",
    "4. Entrene el modelo de la sección 4 anterior por más tiempo (10 épocas deberían ser suficientes), ¿qué sucede con el rendimiento?\n",
    "5. Entrene el modelo de la sección 4 anterior con más datos, digamos el 20% de las imágenes de Food101 de pizza, bistec y sushi.\n",
    "    * Puede encontrar el [conjunto de datos 20% de pizza, bistec y sushi](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) en el curso GitHub. Fue creado con el cuaderno [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb). \n",
    "6. Pruebe un modelo diferente de [`torchvision.models`](https://pytorch.org/vision/stable/models.html) en los datos de pizza, bistec y sushi. ¿Cómo funciona este modelo?\n",
    "    * Tendrás que cambiar el tamaño de la capa clasificadora para adaptarla a nuestro problema.\n",
    "    * Es posible que desee probar un EfficientNet con un número mayor que nuestro B0, ¿tal vez `torchvision.models.ficientnet_b2()`?\n",
    "  \n",
    "## Extracurricular\n",
    "* Busque qué es el \"ajuste de modelo\" y dedique 30 minutos a investigar diferentes métodos para realizarlo con PyTorch. ¿Cómo cambiaríamos nuestro código para perfeccionarlo? Consejo: el ajuste fino generalmente funciona mejor si tiene *muchos* datos personalizados, mientras que la extracción de características suele ser mejor si tiene menos datos personalizados.\n",
    "* Consulte la nueva/próxima [API de pesos múltiples de PyTorch] (https://pytorch.org/blog/introtaining-torchvision-new-multi-weight-support-api/) (aún en versión beta al momento de escribir este artículo, mayo 2022), es una nueva forma de realizar aprendizaje por transferencia en PyTorch. ¿Qué cambios sería necesario realizar en nuestro código para utilizar la nueva API?\n",
    "* Intente crear su propio clasificador en dos clases de imágenes; por ejemplo, podría recopilar 10 fotos de su perro y el perro de sus amigos y entrenar un modelo para clasificar a los dos perros. Esta sería una buena manera de practicar la creación de un conjunto de datos y la construcción de un modelo a partir de ese conjunto de datos."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
