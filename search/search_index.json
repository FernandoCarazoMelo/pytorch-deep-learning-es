{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apuntes de PyTorch y Redes Neuronales","text":"<p>PyTorch es una librer\u00eda de c\u00f3digo abierto para el desarrollo de modelos de inteligencia artificial con redes neuronales (Deep Learning). Aplicaciones como ChatGPT se han desarrollado con esta librer\u00eda, por lo que PyTorch es una herramienta imprescindible para cualquier persona interesada en desarrollar aplicaciones de AI. </p> <p>En esta web te comparto mis apuntes personales sobre PyTorch. Si tienes alguna sugerencia o correcci\u00f3n, no dudes en contactarme a trav\u00e9s de mi perfil de LinkedIn o hacer un pull request en el repositorio de GitHub.</p> <p>Para prearar este material me he basado principalmente en los siguienes recursos:</p> <ul> <li>PyTorch documentaci\u00f3n oficial</li> <li>3Blue1Brown canal del YouTube divulgativo de matem\u00e1ticas y ciencia</li> <li>Learn PyTorch for Deep Learning curso de PyTorch de Daniel Bourke</li> <li>Jovian curso de PyTorch de Jovian</li> <li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow libro de Aur\u00e9lien G\u00e9ron</li> <li>Short-courses de DeepLearning.ai Cursos sobre procesamiento del lenguaje natural (NLP) de Andrew Ng y colaboradores</li> </ul>"},{"location":"#temario-del-curso-de-pytorch-y-redes-neuronales","title":"Temario del Curso de PyTorch y Redes Neuronales","text":"<ul> <li> <p> Lecci\u00f3n 1: Fundamentos de PyTorch y Descenso de Gradiente</p> <ul> <li>Conceptos b\u00e1sicos de PyTorch: tensores, gradientes y autograd</li> <li>Regresi\u00f3n lineal y descenso de gradiente desde cero</li> </ul> </li> <li> <p> Lecci\u00f3n 2: Introducci\u00f3n a Redes Neuronales con PyTorch</p> <ul> <li>Conceptos b\u00e1sicos: neurona, funcion de activaci\u00f3n, funci\u00f3n de p\u00e9rdida</li> <li>Flujo est\u00e1ndard de entrenamiento de una red neuronal con descenso de gradiente</li> <li>Uso de m\u00f3dulos de PyTorch: <code>nn.Linear</code> y <code>nn.functional</code></li> </ul> </li> <li> <p> Lecci\u00f3n 3: Procesamiento de Im\u00e1genes y Regresi\u00f3n Log\u00edstica</p> <ul> <li>Divisi\u00f3n de entrenamiento y validaci\u00f3n en el conjunto de datos MNIST</li> <li>Regresi\u00f3n log\u00edstica, softmax y entrop\u00eda cruzada (cross-entropy)</li> <li>Entrenamiento del modelo, evaluaci\u00f3n y predicci\u00f3n</li> </ul> </li> <li> <p> Lecci\u00f3n 4: Entrenamiento de Redes Neuronales Profundas en GPU</p> <ul> <li>Redes neuronales multicapa usando <code>nn.Module</code></li> <li>Funciones de activaci\u00f3n, no linealidad y retropropagaci\u00f3n</li> <li>Entrenamiento de modelos m\u00e1s r\u00e1pido usando GPUs en la nube</li> </ul> </li> <li> <p> Lecci\u00f3n 5: Clasificaci\u00f3n de Im\u00e1genes con Redes Neuronales Convolucionales</p> <ul> <li>Trabajo con im\u00e1genes RGB de 3 canales</li> <li>Convoluciones, kernels y mapas de caracter\u00edsticas</li> <li>Curva de entrenamiento, sobreajuste y subajuste</li> </ul> </li> <li> <p> Lecci\u00f3n 6: Introducci\u00f3n a Transformers y HuggingFace</p> <ul> <li>Modelos de lenguaje preentrenados y transferencia de aprendizaje</li> <li>Clasificaci\u00f3n de texto con BERT y GPT-2</li> <li>Generaci\u00f3n de texto con GPT-2 y GPT-3</li> </ul> </li> <li> <p> Lecci\u00f3n 7: Procesamiento de Lenguaje Natural Avanzado con PyTorch</p> <ul> <li>Tokenizaci\u00f3n, incrustaci\u00f3n de palabras y modelos de lenguaje</li> <li>Clasificaci\u00f3n de sentimientos con una red neuronal recurrente</li> <li>Entrenamiento de modelos de lenguaje desde cero</li> </ul> </li> </ul>"},{"location":"01_pytorch_fundamentals_es/","title":"01. Fundamentos de PyTorch y Descenso de Gradiente","text":"<p>Este primer tutorial cubre los siguientes temas:</p> <ul> <li>Introducciones a los tensores PyTorch</li> <li>Operaciones con tensores</li> <li>Introducci\u00f3n a los gradientes tensoriales</li> <li>Interoperabilidad entre PyTorch y Numpy</li> </ul> <p>Como alternativa, tambi\u00e9n se puede instalar directamente el requirments.txt localizado el repositorio de este notebook.</p> <p>Una vez instalado, se importa PyTorch y se comprueba la versi\u00f3n utilizada.</p> In\u00a0[46]: Copied! <pre>import torch\nimport numpy as np\n</pre> import torch import numpy as np In\u00a0[47]: Copied! <pre>torch.__version__\n</pre> torch.__version__ Out[47]: <pre>'2.0.1'</pre> In\u00a0[48]: Copied! <pre># Escalares en PyTorch\n# ======================================================================================\nt1 = torch.tensor(4.)\nt1\n</pre> # Escalares en PyTorch # ====================================================================================== t1 = torch.tensor(4.) t1 Out[48]: <pre>tensor(4.)</pre> In\u00a0[49]: Copied! <pre>t1.ndim\n</pre> t1.ndim Out[49]: <pre>0</pre> <p><code>4.</code> es una abreviatura de <code>4.0</code>. Se utiliza para indicar a Python (y PyTorch) que desea crear un n\u00famero de coma flotante. Se puede verificar esto comprobando el atributo <code>dtype</code> de nuestro tensor.</p> In\u00a0[50]: Copied! <pre># Tipo de dato de un tensor\n# ======================================================================================\nt1.dtype\n</pre> # Tipo de dato de un tensor # ====================================================================================== t1.dtype Out[50]: <pre>torch.float32</pre> <p>Creaci\u00f3n de vectores</p> In\u00a0[51]: Copied! <pre># Vector de 1 dimensi\u00f3n\n# ======================================================================================\nt2 = torch.tensor([1., 2, 3, 4])\nt2_int = torch.tensor([1, 2, 3, 4])\nprint(t2)\nprint(t2.dtype)\nprint(t2_int)\nprint(t2_int.dtype)\n</pre> # Vector de 1 dimensi\u00f3n # ====================================================================================== t2 = torch.tensor([1., 2, 3, 4]) t2_int = torch.tensor([1, 2, 3, 4]) print(t2) print(t2.dtype) print(t2_int) print(t2_int.dtype) <pre>tensor([1., 2., 3., 4.])\ntorch.float32\ntensor([1, 2, 3, 4])\ntorch.int64\n</pre> <p>Como se puede ver, todos los n\u00fameros del tensor tienen el mismo tipo.</p> <p>La dimensi\u00f3n ahora es 1.</p> In\u00a0[52]: Copied! <pre>t2.ndim\n</pre> t2.ndim Out[52]: <pre>1</pre> In\u00a0[53]: Copied! <pre># Matrices \n# ======================================================================================\nt3 = torch.tensor([[5., 6], \n                   [7, 8], \n                   [9, 10]])\nt3\n</pre> # Matrices  # ====================================================================================== t3 = torch.tensor([[5., 6],                     [7, 8],                     [9, 10]]) t3 Out[53]: <pre>tensor([[ 5.,  6.],\n        [ 7.,  8.],\n        [ 9., 10.]])</pre> In\u00a0[54]: Copied! <pre>t3.ndim\n</pre> t3.ndim Out[54]: <pre>2</pre> <p>Los tensores propiamente dichos son aquellos arrays de 3 dimensiones o m\u00e1s, aunque se suele hablar de tensores siempre que son de objeto tensor. Los tensores son la estructura de datos b\u00e1sica en PyTorch.</p> <p>Los tensores son similares a los arrays de NumPy, pero pueden ser usados en GPUs para acelerar los c\u00e1lculos, como se ver\u00e1 m\u00e1s adelante.</p> In\u00a0[55]: Copied! <pre># Tensor tridimensional\n# ======================================================================================\nt4 = torch.tensor([\n    [[11, 12, 13, 10],\n     [11, 12, 13, 10], \n     [13, 14, 15, 10]], \n    [[15, 16, 17, 10], \n     [11, 12, 13, 10],\n     [17, 18, 19., 10]]])\nt4\n</pre> # Tensor tridimensional # ====================================================================================== t4 = torch.tensor([     [[11, 12, 13, 10],      [11, 12, 13, 10],       [13, 14, 15, 10]],      [[15, 16, 17, 10],       [11, 12, 13, 10],      [17, 18, 19., 10]]]) t4 Out[55]: <pre>tensor([[[11., 12., 13., 10.],\n         [11., 12., 13., 10.],\n         [13., 14., 15., 10.]],\n\n        [[15., 16., 17., 10.],\n         [11., 12., 13., 10.],\n         [17., 18., 19., 10.]]])</pre> <p>Los tensores pueden tener cualquier n\u00famero de dimensiones y diferentes longitudes a lo largo de cada dimensi\u00f3n. Se puede inspeccionar la longitud a lo largo de cada dimensi\u00f3n usando la propiedad <code>.shape</code> de un tensor.</p> <p>Al igual que pasa con numpy, no es posible crear tensores con una dimensionalidad incompatible.</p> In\u00a0[56]: Copied! <pre># Tensor con dimensiones imcompatibles\n# ======================================================================================\n# t5 = torch.tensor([[5., 6, 11], \n#                    [7, 8], \n#                    [9, 10]])\n</pre> # Tensor con dimensiones imcompatibles # ====================================================================================== # t5 = torch.tensor([[5., 6, 11],  #                    [7, 8],  #                    [9, 10]]) <p>El <code>ValueError</code> se debe a que las longitudes de las filas <code>[5., 6, 11]</code> y <code>[7, 8]</code> no coinciden.</p> In\u00a0[57]: Copied! <pre># Tensor con valores aleatorios de dimensiones (3, 4)\n# ======================================================================================\ntensor_aleatorio = torch.rand(size=(3, 4))\n# Se imprime por pantalla\ntensor_aleatorio, tensor_aleatorio.dtype, tensor_aleatorio.shape, tensor_aleatorio.ndim\n</pre> # Tensor con valores aleatorios de dimensiones (3, 4) # ====================================================================================== tensor_aleatorio = torch.rand(size=(3, 4)) # Se imprime por pantalla tensor_aleatorio, tensor_aleatorio.dtype, tensor_aleatorio.shape, tensor_aleatorio.ndim Out[57]: <pre>(tensor([[0.8955, 0.7819, 0.9867, 0.6980],\n         [0.8123, 0.1524, 0.3739, 0.8194],\n         [0.0155, 0.1711, 0.2164, 0.8552]]),\n torch.float32,\n torch.Size([3, 4]),\n 2)</pre> <p>Comencemos con algunas de las operaciones fundamentales, suma (<code>+</code>), resta (<code>-</code>), multiplicaci\u00f3n (<code>*</code>).</p> <p>Funcionan tal como piensas que lo har\u00edan, como en numpy.</p> In\u00a0[58]: Copied! <pre># Suma\n# ======================================================================================\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n</pre> # Suma # ====================================================================================== tensor = torch.tensor([1, 2, 3]) tensor + 10 Out[58]: <pre>tensor([11, 12, 13])</pre> In\u00a0[59]: Copied! <pre># Multiplicaci\u00f3n por un escalar\n# ======================================================================================\ntensor * 10\n</pre> # Multiplicaci\u00f3n por un escalar # ====================================================================================== tensor * 10 Out[59]: <pre>tensor([10, 20, 30])</pre> In\u00a0[60]: Copied! <pre># Resta\n# ======================================================================================\ntensor = tensor - 10\ntensor\n</pre> # Resta # ====================================================================================== tensor = tensor - 10 tensor Out[60]: <pre>tensor([-9, -8, -7])</pre> In\u00a0[61]: Copied! <pre>tensor = torch.tensor([1, 2, 3])\ntensor.shape\n</pre> tensor = torch.tensor([1, 2, 3]) tensor.shape Out[61]: <pre>torch.Size([3])</pre> <p>La diferencia entre la multiplicaci\u00f3n elemento a elemento y la multiplicaci\u00f3n de matrices es la adici\u00f3n de valores.</p> <p>Para nuestra variable <code>tensor</code> con valores <code>[1, 2, 3]</code>:</p> Operaci\u00f3n C\u00e1lculo C\u00f3digo Multiplicaci\u00f3n elemento a elemento <code>[1*1, 2*2, 3*3]</code> = <code>[1, 4, 9]</code> <code>tensor * tensor</code> Multiplicaci\u00f3n de matrices <code>[1*1 + 2*2 + 3*3]</code> = <code>[14]</code> <code>tensor.matmul(tensor)</code> In\u00a0[62]: Copied! <pre># Multiplicaci\u00f3n elemento a elemento de tensores\n# ======================================================================================\ntensor * tensor\n</pre> # Multiplicaci\u00f3n elemento a elemento de tensores # ====================================================================================== tensor * tensor Out[62]: <pre>tensor([1, 4, 9])</pre> In\u00a0[63]: Copied! <pre># Multiplicaci\u00f3n matricial con el operador @\n# ======================================================================================\ntensor @ tensor\n</pre> # Multiplicaci\u00f3n matricial con el operador @ # ====================================================================================== tensor @ tensor Out[63]: <pre>tensor(14)</pre> In\u00a0[64]: Copied! <pre># Multiplicaci\u00f3n matricial con el m\u00e9todo matmul\n# ======================================================================================\ntorch.matmul(tensor, tensor)\n</pre> # Multiplicaci\u00f3n matricial con el m\u00e9todo matmul # ====================================================================================== torch.matmul(tensor, tensor) Out[64]: <pre>tensor(14)</pre> In\u00a0[65]: Copied! <pre># Se crea un tensor\ntensor = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ntensor.dtype\n</pre> # Se crea un tensor tensor = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) tensor.dtype Out[65]: <pre>torch.int64</pre> In\u00a0[66]: Copied! <pre># C\u00e1lculo de los valores maximo, minimo, media y suma de un tensor\n# ======================================================================================\nx = torch.tensor([1,2,1,3,1,2], dtype=torch.float32)  # para calcular la media hay que convertir a float\nprint(f\"Min: {x.min()}\")\nprint(f\"Max: {x.max()}\")\nprint(f\"Media: {x.mean()}\")\nprint(f\"Suma: {x.sum()}\")\n</pre> # C\u00e1lculo de los valores maximo, minimo, media y suma de un tensor # ====================================================================================== x = torch.tensor([1,2,1,3,1,2], dtype=torch.float32)  # para calcular la media hay que convertir a float print(f\"Min: {x.min()}\") print(f\"Max: {x.max()}\") print(f\"Media: {x.mean()}\") print(f\"Suma: {x.sum()}\") <pre>Min: 1.0\nMax: 3.0\nMedia: 1.6666666269302368\nSuma: 10.0\n</pre> In\u00a0[67]: Copied! <pre># Obtenci\u00f3n del \u00edndice del valor m\u00e1ximo y m\u00ednimo de un tensor\n# ======================================================================================\n# Se crea un tensor secuencial\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Se devuelve el \u00edndice del valor m\u00e1ximo y m\u00ednimo\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n</pre> # Obtenci\u00f3n del \u00edndice del valor m\u00e1ximo y m\u00ednimo de un tensor # ====================================================================================== # Se crea un tensor secuencial tensor = torch.arange(10, 100, 10) print(f\"Tensor: {tensor}\")  # Se devuelve el \u00edndice del valor m\u00e1ximo y m\u00ednimo print(f\"Index where max value occurs: {tensor.argmax()}\") print(f\"Index where min value occurs: {tensor.argmin()}\") <pre>Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n</pre> <p>Ejercicio Hasta ahora se han cubierto algunos m\u00e9todos de tensor, pero hay muchos m\u00e1s en la documentaci\u00f3n de <code>torch.Tensor</code>, se recomienda revisar la web y repasar  cualquier funci\u00f3n o m\u00e9todo que llame la atenci\u00f3n.</p> In\u00a0[68]: Copied! <pre>#  Crear un tensor con un valor fijo para cada elemento\n# ======================================================================================\ntensor = torch.full((3, 2), 42)\ntensor\n</pre> #  Crear un tensor con un valor fijo para cada elemento # ====================================================================================== tensor = torch.full((3, 2), 42) tensor Out[68]: <pre>tensor([[42, 42],\n        [42, 42],\n        [42, 42]])</pre> <p>Para aplicar funciones matem\u00e1ticas a un tensor, hay que realizarlo a trav\u00e9s de una funci\u00f3n de torch. En la documentaci\u00f3n de PyTorch, se pueden encontrar muchas otras funciones matematicas. Se recomienda dedicar un tiempo a revisar la documentaci\u00f3n.</p> In\u00a0[69]: Copied! <pre># Seno de un tensor\n# ======================================================================================\ntensor_sin = torch.sin(tensor)\ntensor_sin\n</pre> # Seno de un tensor # ====================================================================================== tensor_sin = torch.sin(tensor) tensor_sin Out[69]: <pre>tensor([[-0.9165, -0.9165],\n        [-0.9165, -0.9165],\n        [-0.9165, -0.9165]])</pre> In\u00a0[70]: Copied! <pre># Se crea un tensor de 1 dimensi\u00f3n con los valores del 1 al 7\n# ======================================================================================\nx = torch.arange(1., 8.)\nx, x.shape\n</pre> # Se crea un tensor de 1 dimensi\u00f3n con los valores del 1 al 7 # ====================================================================================== x = torch.arange(1., 8.) x, x.shape Out[70]: <pre>(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))</pre> <p>A\u00f1adimos una dimensi\u00f3n extra con <code>torch.reshape()</code>.</p> In\u00a0[71]: Copied! <pre># Se a\u00f1ade una dimension extra al tensor x\n# ======================================================================================\nx_reshaped = x.reshape(1, 7)\n\nx_reshaped, x_reshaped.shape\n</pre> # Se a\u00f1ade una dimension extra al tensor x # ====================================================================================== x_reshaped = x.reshape(1, 7)  x_reshaped, x_reshaped.shape Out[71]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>Con view se puede hacer lo mismo, pero sin crear una copia del tensor original.</p> In\u00a0[72]: Copied! <pre># Con el m\u00e9todo view\n# ======================================================================================\nz = x.view(1, 7)\nz, z.shape\n</pre> # Con el m\u00e9todo view # ====================================================================================== z = x.view(1, 7) z, z.shape Out[72]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> In\u00a0[73]: Copied! <pre># Cambiamos un elemento y se cambia en los dos tensores\n# ======================================================================================\nz[:, 0] = 5\nz, x\n</pre> # Cambiamos un elemento y se cambia en los dos tensores # ====================================================================================== z[:, 0] = 5 z, x Out[73]: <pre>(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))</pre> <p>Los tensores pueden ser cambiados de dimensiones, pero la cantidad de elementos debe ser la misma. Y, por lo tanto, las dimensiones deben de ser compatibles.</p> <p>Por ejemplo, un tensor de dimensi\u00f3n (10, 10, 3) tiene 300 elementos. Lo podr\u00edamos cambi\u00e1r a la diemensi\u00f3n (30, 10), pero no a otra incompatible como (4, 10, 10)</p> In\u00a0[74]: Copied! <pre># Se crea un tensor\n# ======================================================================================\nx = torch.randn(10, 10, 3)\nx.shape\n</pre> # Se crea un tensor # ====================================================================================== x = torch.randn(10, 10, 3) x.shape Out[74]: <pre>torch.Size([10, 10, 3])</pre> In\u00a0[75]: Copied! <pre># Cambiamos la dimesi\u00f3n de un tensor\n# ======================================================================================\ny = x.reshape(30,10)\nprint(y.shape)\ny = x.reshape(3,10,10)\nprint(y.shape)\n# Dimensiones incompatibles\n# y = x.reshape(4,10,10)\n# print(y.shape)\n</pre> # Cambiamos la dimesi\u00f3n de un tensor # ====================================================================================== y = x.reshape(30,10) print(y.shape) y = x.reshape(3,10,10) print(y.shape) # Dimensiones incompatibles # y = x.reshape(4,10,10) # print(y.shape) <pre>torch.Size([30, 10])\ntorch.Size([3, 10, 10])\n</pre> <p>Si se utiliza el -1 como valor de una dimensi\u00f3n, esta se calcular\u00e1 autom\u00e1ticamente para que la cantidad de elementos sea la misma.</p> In\u00a0[76]: Copied! <pre># Equivale a poner -1 en la dimensi\u00f3n que se quiere calcular autom\u00e1ticamente\ny = x.reshape(3,-1,10)\nprint(y.shape)\n</pre>   # Equivale a poner -1 en la dimensi\u00f3n que se quiere calcular autom\u00e1ticamente y = x.reshape(3,-1,10) print(y.shape)  <pre>torch.Size([3, 10, 10])\n</pre> <p>Se pueede obtener m\u00e1s informaci\u00f3n sobre las operaciones de tensor aqu\u00ed: https://pytorch.org/docs/stable/torch.html. Se recomienda experimentar 5-10 funciones y operaciones de tensor para familiarizarse con la librer\u00eda.</p> <p>Para apilar tensores se utiliza la funci\u00f3n <code>torch.stack()</code>.</p> In\u00a0[77]: Copied! <pre># Apilar tensores - horizontalmente\n# ======================================================================================\nx = torch.tensor([1, 2, 3, 4])\nx_stacked = torch.stack([x, x, x, x], dim=0) \nx_stacked\n</pre> # Apilar tensores - horizontalmente # ====================================================================================== x = torch.tensor([1, 2, 3, 4]) x_stacked = torch.stack([x, x, x, x], dim=0)  x_stacked Out[77]: <pre>tensor([[1, 2, 3, 4],\n        [1, 2, 3, 4],\n        [1, 2, 3, 4],\n        [1, 2, 3, 4]])</pre> In\u00a0[78]: Copied! <pre># Apilar tensores - verticalmente\n# ======================================================================================\nx = torch.tensor([1, 2, 3, 4])\nx_stacked = torch.stack([x, x, x, x], dim=1)\nx_stacked\n</pre> # Apilar tensores - verticalmente # ====================================================================================== x = torch.tensor([1, 2, 3, 4]) x_stacked = torch.stack([x, x, x, x], dim=1) x_stacked Out[78]: <pre>tensor([[1, 1, 1, 1],\n        [2, 2, 2, 2],\n        [3, 3, 3, 3],\n        [4, 4, 4, 4]])</pre> <p>Tambi\u00e9n se puede reordenar el orden de los valores de los ejes con <code>torch.permute(input, dims)</code>, donde el <code>input</code> se convierte en una vista con nuevos <code>dims</code>.</p> In\u00a0[79]: Copied! <pre># Permutar tensor\n\n# Se crea un tensor con una forma espec\u00edfica\nx_original = torch.rand(size=(224, 224, 3))\n\n# Se permuta el tensor\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Inicial: {x_original.shape}\")\nprint(f\"Final: {x_permuted.shape}\")\n</pre> # Permutar tensor  # Se crea un tensor con una forma espec\u00edfica x_original = torch.rand(size=(224, 224, 3))  # Se permuta el tensor x_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0  print(f\"Inicial: {x_original.shape}\") print(f\"Final: {x_permuted.shape}\") <pre>Inicial: torch.Size([224, 224, 3])\nFinal: torch.Size([3, 224, 224])\n</pre> <p>Nota: Debido a que permutar devuelve una vista (comparte los mismos datos que el original), los valores en el tensor permutado ser\u00e1n los mismos que el tensor original y si cambias los valores en la vista, cambiar\u00e1 los valores del original.</p> In\u00a0[80]: Copied! <pre># Creaci\u00f3n de tensores\n# ======================================================================================\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad=True)\nb = torch.tensor(5., requires_grad=True)\nx, w, b\n</pre> # Creaci\u00f3n de tensores # ====================================================================================== x = torch.tensor(3.) w = torch.tensor(4., requires_grad=True) b = torch.tensor(5., requires_grad=True) x, w, b Out[80]: <pre>(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))</pre> <p>Se han creado tres tensores: <code>x</code>, <code>w</code> y <code>b</code>, todos son simplemente n\u00fameros. <code>w</code> y <code>b</code> tienen un par\u00e1metro adicional <code>requires_grad</code> establecido en <code>True</code>. Ahora se ver\u00e1 su importante funci\u00f3n.</p> <p>Ahora se crear\u00e1 un nuevo tensor <code>y</code> combinando estos tensores.</p> In\u00a0[81]: Copied! <pre># Operaci\u00f3n aritmetica\n# ======================================================================================\ny = w * x**2 + b\ny\n</pre> # Operaci\u00f3n aritmetica # ====================================================================================== y = w * x**2 + b y Out[81]: <pre>tensor(41., grad_fn=&lt;AddBackward0&gt;)</pre> <p>Como era de esperar, <code>y</code> es un tensor con el valor $4 * 3^2 + 5 = 41$.</p> <p>Lo que hace \u00fanico a PyTorch es que podemos calcular autom\u00e1ticamente la derivada de <code>y</code>!</p> <p>Los tensores que tienen <code>requires_grad</code> establecido en <code>True</code>, es decir, w y b. Esta caracter\u00edstica de PyTorch se llama_autograd_ (gradientes autom\u00e1ticos).</p> <p>Para calcular las derivadas, podemos invocar el m\u00e9todo <code>.backward</code> en nuestro resultado <code>y</code>.</p> In\u00a0[82]: Copied! <pre># Calculamos las derivadas\n# ======================================================================================\ny.backward()\n</pre> # Calculamos las derivadas # ====================================================================================== y.backward() <p>Las derivadas de <code>y</code> con respecto a los tensores de entrada se almacenan en la propiedad <code>.grad</code> de los respectivos tensores. Se puede observar que <code>x</code> tiene una derivada igual a <code>None</code> porque no se ha incluido el par\u00e1metro <code>requires_grad</code> en su definici\u00f3n.</p> In\u00a0[83]: Copied! <pre># Obtenci\u00f3n de las derivadas\n# ======================================================================================\nprint('dy/dx:', x.grad)\nprint('dy/dw:', w.grad)\nprint('dy/db:', b.grad)\n</pre> # Obtenci\u00f3n de las derivadas # ====================================================================================== print('dy/dx:', x.grad) print('dy/dw:', w.grad) print('dy/db:', b.grad) <pre>dy/dx: None\ndy/dw: tensor(9.)\ndy/db: tensor(1.)\n</pre> <p>Como era de esperar, <code>dy/dw</code> tiene el mismo valor que <code>x</code>, es decir, <code>3</code>, y <code>dy/db</code> tiene el valor <code>1</code>.</p> <p>Se presenta a continuaci\u00f3n un segundo ejemplo:</p> <p>$y=2*x^2$</p> <p>Donde,</p> <ul> <li>dy/dx = 4x</li> <li>como x=3, dy/dx = 4*3 = 12</li> </ul> In\u00a0[84]: Copied! <pre># Otro ejemplo\n# ======================================================================================\nx = torch.tensor(3., requires_grad=True)\ny = 2*x**2\ny.backward()\nx.grad\n</pre> # Otro ejemplo # ====================================================================================== x = torch.tensor(3., requires_grad=True) y = 2*x**2 y.backward() x.grad Out[84]: <pre>tensor(12.)</pre> <p>Esta propiedad de los tensores es muy \u00fatil para la implementaci\u00f3n de redes neuronales, ya que permite definir el tama\u00f1o de las capas de forma din\u00e1mica, en funci\u00f3n de los datos de entrada.</p> <p>As\u00ed es como se crea una matriz en Numpy:</p> In\u00a0[85]: Copied! <pre># Creaci\u00f3n de un array de numpy\n# ======================================================================================\nx = np.array([[1, 2], [3, 4.]])\nx\n</pre> # Creaci\u00f3n de un array de numpy # ====================================================================================== x = np.array([[1, 2], [3, 4.]]) x Out[85]: <pre>array([[1., 2.],\n       [3., 4.]])</pre> <p>Se puede convertir una matriz Numpy en un tensor PyTorch de forma muy sencilla usando <code>torch.from_numpy</code>.</p> In\u00a0[86]: Copied! <pre># Cambio de numpy a tensor\n# ======================================================================================\ny = torch.from_numpy(x)\ny\n</pre> # Cambio de numpy a tensor # ====================================================================================== y = torch.from_numpy(x) y Out[86]: <pre>tensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)</pre> <p>Se verifica que la matriz numpy y el tensor de antorcha tengan tipos de datos similares.</p> In\u00a0[87]: Copied! <pre>x.dtype, y.dtype\n</pre> x.dtype, y.dtype Out[87]: <pre>(dtype('float64'), torch.float64)</pre> <p>Tambi\u00e9n se pueda hacer el paso contrario: convertir un tensor PyTorch en una matriz Numpy. Para ello se utiliza el m\u00e9todo <code>.numpy</code> de un tensor.</p> In\u00a0[88]: Copied! <pre># Convertir un tensor de torch a un array de numpy\n# ======================================================================================\nz = y.numpy()\nz\n</pre> # Convertir un tensor de torch a un array de numpy # ====================================================================================== z = y.numpy() z Out[88]: <pre>array([[1., 2.],\n       [3., 4.]])</pre> <p>La interoperabilidad entre PyTorch y Numpy es esencial porque la mayor\u00eda de los conjuntos de datos con los que trabajar\u00e1 probablemente se leer\u00e1n y preprocesar\u00e1n como matrices de Numpy.</p> In\u00a0[89]: Copied! <pre>import session_info\nsession_info.show(html=False)\n</pre> import session_info session_info.show(html=False) <pre>-----\nnumpy               1.25.2\nsession_info        1.0.0\ntorch               2.0.1\n-----\nIPython             8.14.0\njupyter_client      8.3.0\njupyter_core        5.3.1\njupyterlab          4.0.5\nnotebook            7.0.3\n-----\nPython 3.10.12 (main, Jul  5 2023, 15:34:07) [Clang 14.0.6 ]\nmacOS-10.16-x86_64-i386-64bit\n-----\nSession information updated at 2023-09-02 11:33\n</pre>"},{"location":"01_pytorch_fundamentals_es/#01-fundamentos-de-pytorch-y-descenso-de-gradiente","title":"01. Fundamentos de PyTorch y Descenso de Gradiente\u00b6","text":""},{"location":"01_pytorch_fundamentals_es/#instalacion-de-pytorch","title":"Instalaci\u00f3n de PyTorch\u00b6","text":"<p>Nota: Antes de ejecutar cualquier c\u00f3digo en este cuaderno, deber\u00edas haber pasado por los pasos de instalaci\u00f3n de PyTorch.</p> <p>Si est\u00e1s ejecutando en Google Colab, no es necesario instalar ninguna librer\u00eda (Google Colab tiene PyTorch y otras bibliotecas preinstaladas).</p>"},{"location":"01_pytorch_fundamentals_es/#introduccion-a-los-tensores-con-pytorch","title":"Introducci\u00f3n a los tensores con Pytorch\u00b6","text":""},{"location":"01_pytorch_fundamentals_es/#tensores","title":"Tensores\u00b6","text":"En esencia, PyTorch es una biblioteca para procesar tensores. Un tensor es un n\u00famero, vector, matriz o cualquier array de n dimensiones (tambi\u00e9n denominados simplemente tensores).   <p>Para empezar de forma sencilla, se crear\u00e1 un tensor con un solo n\u00famero.</p>"},{"location":"01_pytorch_fundamentals_es/#tensores-aleatorios","title":"Tensores aleatorios\u00b6","text":"Los modelos de aprendizaje autom\u00e1tico como las redes neuronales manipulan y buscan patrones dentro de los tensores. Cuando se construyen modelos de aprendizaje autom\u00e1tico con PyTorch, es raro que se creen tensores a mano.  <p>Sin embargo, un modelo de aprendizaje autom\u00e1tico a menudo comienza con grandes tensores de n\u00fameros aleatorios (weights and biases) que posteriormente se ajustan estos n\u00fameros aleatorios a medida que trabaja a trav\u00e9s de los datos para representarlos mejor.</p> <p>Para crear tensores con n\u00fameros aleatorios entre [0,1] se utiliza la funic\u00f3n <code>torch.rand ()</code> pasando el par\u00e1metro <code>size</code>.</p>"},{"location":"01_pytorch_fundamentals_es/#operaciones-con-tensores","title":"Operaciones con tensores\u00b6","text":"En el aprendizaje profundo, los datos (im\u00e1genes, texto, video, audio, estructuras de prote\u00ednas, etc.) se representan como tensores.  <p>Para codificar una red neuronal, es necesario realizar operaciones b\u00e1sicas entre tensores:</p> <ul> <li>Suma</li> <li>Resta</li> <li>Multiplicaci\u00f3n (elemento a elemento)</li> <li>Divisi\u00f3n</li> <li>Multiplicaci\u00f3n de matrices</li> </ul>"},{"location":"01_pytorch_fundamentals_es/#multiplicacion-de-matrices","title":"Multiplicaci\u00f3n de matrices\u00b6","text":"Una de las operaciones m\u00e1s comunes en los algoritmos de aprendizaje autom\u00e1tico y aprendizaje profundo (como las redes neuronales) es la [multiplicaci\u00f3n de matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html).  <p>PyTorch implementa la funcionalidad de multiplicaci\u00f3n de matrices en el m\u00e9todo <code>torch.matmul()</code>.</p> <p>Las dos reglas principales para la multiplicaci\u00f3n de matrices a recordar son:</p> <ol> <li>Las dimensiones internas deben coincidir:</li> </ol> <ul> <li><code>(3, 2) @ (3, 2)</code> no funcionar\u00e1</li> <li><code>(2, 3) @ (3, 2)</code> funcionar\u00e1</li> <li><code>(3, 2) @ (2, 3)</code> funcionar\u00e1</li> </ul> <ol> <li>La matriz resultante tiene la forma de las dimensiones externas:</li> </ol> <ul> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code></li> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>Nota: \"<code>@</code>\" en Python es el s\u00edmbolo para la multiplicaci\u00f3n de matrices.</p> <p>Recurso: Puede ver todas las reglas para la multiplicaci\u00f3n de matrices usando <code>torch.matmul()</code> en la documentaci\u00f3n de PyTorch.</p>"},{"location":"01_pytorch_fundamentals_es/#calculo-de-los-valores-maximo-minimo-media-y-suma-de-un-tensor","title":"C\u00e1lculo de los valores maximo, minimo, media y suma de un tensor\u00b6","text":""},{"location":"01_pytorch_fundamentals_es/#minmax-posicional","title":"Min/Max posicional\u00b6","text":"Tambi\u00e9n se puede encontrar el \u00edndice de un tensor donde ocurre el m\u00e1ximo o el m\u00ednimo con [`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) y [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html) respectivamente.  <p>Esto es \u00fatil en caso de que s\u00f3lo se quiera la posici\u00f3n donde est\u00e1 el valor m\u00e1s alto (o m\u00e1s bajo) y no el valor en s\u00ed (lo veremos en una secci\u00f3n posterior cuando usemos la funci\u00f3n de activaci\u00f3n softmax).</p>"},{"location":"01_pytorch_fundamentals_es/#otras-funciones-tensoriales","title":"Otras funciones tensoriales\u00b6","text":"El m\u00f3dulo `torch` tambi\u00e9n contiene muchas funciones para crear y manipular tensores."},{"location":"01_pytorch_fundamentals_es/#reorganizacion-apilamiento-y-permutacion","title":"Reorganizaci\u00f3n, apilamiento y permutaci\u00f3n\u00b6","text":"Frecuentemente se quiere reorganizar o cambiar las dimensiones de los tensores sin cambiar los valores que contienen.  <p>Para hacerlo, algunos m\u00e9todos populares son:</p> M\u00e9todo Descripci\u00f3n <code>torch.reshape(input, shape)</code> Reorganiza <code>input</code> a <code>shape</code> (si es compatible), tambi\u00e9n se puede usar <code>torch.Tensor.reshape()</code>. <code>torch.Tensor.view(shape)</code> Devuelve una vista del tensor original en una <code>shape</code> diferente pero comparte los mismos datos que el tensor original. <code>torch.stack(tensors, dim=0)</code> Concatena una secuencia de <code>tensors</code> a lo largo de una nueva dimensi\u00f3n (<code>dim</code>), todos los <code>tensors</code> deben tener el mismo tama\u00f1o. <code>torch.permute(input, dims)</code> Devuelve una vista del <code>input</code> original con sus dimensiones permutadas (reorganizadas) a <code>dims</code>. <p>redes neuronales) se tratan de manipular tensores de alguna manera. Y debido a las reglas de la multiplicaci\u00f3n de matrices, si hay incompatibilidades de forma, se producir\u00e1n errores. Estos m\u00e9todos te ayudan a asegurarte de que los elementos correctos de tus tensores se mezclan con los elementos correctos de otros tensores.</p>"},{"location":"01_pytorch_fundamentals_es/#tensores-y-gradientes","title":"Tensores y Gradientes\u00b6","text":"<p>Una de las propiedades m\u00e1s importantes de PyTorch es que se pueden calcular los gradientes, o derivadas, de los tensores. Esto es muy \u00fatil para el entrenamiento de redes neuronales, ya que se puede calcular el error de la red y ajustar los pesos para minimizar el error con el algoritmo del descenso del gradiente.</p> <p>Como se ver\u00e1 m\u00e1s adelante, el algoritmo del descenso del gradiente es el que se utiliza para ajustar los pesos de la red. Este algoritmo consiste en calcular el gradiente de la funci\u00f3n de error con respecto a los pesos, y actualizar los pesos en la direcci\u00f3n opuesta al gradiente, ya que en esa direcci\u00f3n la funci\u00f3n de error decrece m\u00e1s r\u00e1pidamente.</p> <p>Por este motivo, es muy importante que la librer\u00eda utilizada permita calcular de forma eficiente los gradientes de un tensor. A continuaci\u00f3n mostramos un ejemplo.</p>"},{"location":"01_pytorch_fundamentals_es/#interoperabilidad-con-numpy","title":"Interoperabilidad con Numpy\u00b6","text":"<p>Numpy es una popular biblioteca de c\u00f3digo abierto que se utiliza para la computaci\u00f3n matem\u00e1tica y cient\u00edfica en Python. Permite operaciones eficientes en grandes arrays multidimensionales y tiene un vasto ecosistema de bibliotecas de soporte, que incluyen:</p> <p>Pandas para E/S de archivos y an\u00e1lisis de datos Matplotlib para trazado y visualizaci\u00f3n</p> <ul> <li>OpenCV para procesamiento de im\u00e1genes y videos</li> </ul> <p>Una pregunta que surge a menudo es por qu\u00e9 necesitamos una biblioteca como PyTorch, ya que Numpy ya proporciona estructuras de datos y utilidades para trabajar con datos num\u00e9ricos multidimensionales.</p> <p>Hay dos razones principales:</p> <ol> <li>Autograd: la capacidad de calcular gradientes autom\u00e1ticamente para operaciones de tensor es esencial para entrenar modelos de aprendizaje profundo.</li> <li>Compatibilidad con GPU: al trabajar con conjuntos de datos masivos y modelos grandes, las operaciones de tensor de PyTorch se pueden realizar de manera eficiente utilizando una Unidad de procesamiento de gr\u00e1ficos (GPU). Los c\u00e1lculos que normalmente pueden llevar horas se pueden completar en minutos usando GPU.</li> </ol> <p>Aprovecharemos ampliamente estas dos funciones de PyTorch en esta serie de tutoriales.</p> <p>En lugar de reinventar la rueda, PyTorch interact\u00faa bien con Numpy para aprovechar su ecosistema existente de herramientas y bibliotecas.</p>"},{"location":"01_pytorch_fundamentals_es/#informacion-de-sesion","title":"Informaci\u00f3n de sesi\u00f3n\u00b6","text":""},{"location":"01_pytorch_fundamentals_es/#bibliografia-y-recursos","title":"Bibliograf\u00eda y recursos\u00b6","text":"<p>Recursos y bibliografia de Deep Learning con PyToch</p> <ul> <li>PyTorch Tutorials</li> <li>PyTorch API</li> <li>Redes Neuronales y Deep Learning</li> <li>Jovian.ai</li> <li>Daniel Bourke</li> </ul>"},{"location":"02_pytorch_linear_regression_es/","title":"02. Introducci\u00f3n a Redes Neuronales con PyTorch","text":"<p>El problema que se va a resolver es el siguiente: se tiene informaci\u00f3n sobre el clima en ciertas localidades y se desea predecir la producci\u00f3n de manzanas y naranjas en esas localidades en base a los datos clim\u00e1ticos.</p> <p>Es interesante recalcar que se dispone de dos columnas que se quieren predecir, por lo que este ejemplo consiste en de dos modelos de regresi\u00f3n lineal. Cada modelo de regresi\u00f3n lineal predice una columna distinta.</p> Region Temperatura Lluvia Humedad Manzanas (target 1) Naranjas (target 2) Espa\u00f1a 73 67 43 56 70 Italia 91 88 64 81 101 Alemania 87 134 58 119 133 Portugal 102 43 37 22 37 Francia 69 96 70 103 119 <p>En un modelo de regresi\u00f3n lineal, cada variable objetivo se estima como una suma ponderada (tambi\u00e9n llamados weights) de las variables de entrada, sumando una constante o bias:</p> <pre><code>produccion_manzana = w11 * temperatura + w12 * lluvia + w13 * humedad + b1\nproduccion_naranja = w21 * temperatura + w22 * lluvia + w23 * humedad + b2\n</code></pre> <p>Ahora implementaremos un modelo de regresi\u00f3n lineal para predecir con PyTorch.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport torch\n</pre> import numpy as np import torch In\u00a0[2]: Copied! <pre># Entrada (tempertura, precipitaci\u00f3n, humedad)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')\n</pre> # Entrada (tempertura, precipitaci\u00f3n, humedad) inputs = np.array([[73, 67, 43],                     [91, 88, 64],                     [87, 134, 58],                     [102, 43, 37],                     [69, 96, 70]], dtype='float32') In\u00a0[3]: Copied! <pre># Salida (manzanas, naranjas)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')\n</pre> # Salida (manzanas, naranjas) targets = np.array([[56, 70],                      [81, 101],                      [119, 133],                      [22, 37],                      [103, 119]], dtype='float32') <p>NOTA:</p> <p>Se convierten las matrices en tensores PyTorch. Si quieres saber m\u00e1s sobre tensores y operaciones con ellos, puedes consultar el post de Introducci\u00f3n a PyTorch.</p> In\u00a0[4]: Copied! <pre># Se transforman las matrices a tensores\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(inputs)\nprint(targets)\n</pre> # Se transforman las matrices a tensores inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) print(inputs) print(targets) <pre>tensor([[ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.]])\ntensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])\n</pre> In\u00a0[5]: Copied! <pre># Pesos y sesgos\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2, requires_grad=True)\nprint(w)\nprint(b)\n</pre> # Pesos y sesgos w = torch.randn(2, 3, requires_grad=True) b = torch.randn(2, requires_grad=True) print(w) print(b) <pre>tensor([[-1.7679,  1.1114, -0.2267],\n        [ 0.0614, -1.4329,  2.0267]], requires_grad=True)\ntensor([ 1.7997, -2.2624], requires_grad=True)\n</pre> <p><code>torch.randn</code> crea un tensor con la forma dada, con elementos elegidos aleatoriamente de una distribuci\u00f3n normal con media 0 y desviaci\u00f3n est\u00e1ndar 1.</p> <p>El modelo que se va a crear es simplemente una funci\u00f3n que realiza una multiplicaci\u00f3n matricial de las <code>entradas</code> y los pesos <code>w</code> (transpuestos) y agrega el sesgo <code>b</code> (replicado para cada observaci\u00f3n).</p> <p></p> <p>Podemos definir el modelo de la siguiente manera:</p> In\u00a0[6]: Copied! <pre>def model(x):\n    return x @ w.t() + b\n</pre> def model(x):     return x @ w.t() + b <p><code>@</code> representa la multiplicaci\u00f3n de matrices en PyTorch, y el m\u00e9todo\u00a0<code>.t</code> devuelve la transposici\u00f3n de un tensor.</p> <p>La matriz obtenida al pasar los datos de entrada al modelo es un conjunto de predicciones para las variables objetivo.</p> In\u00a0[7]: Copied! <pre># Generar predicciones\npreds = model(inputs)\nprint(preds)\n</pre> # Generar predicciones preds = model(inputs) print(preds) <pre>tensor([[ -62.5404,   -6.6384],\n        [ -75.7836,    6.9362],\n        [ -16.2256,  -71.3851],\n        [-139.1242,   17.3723],\n        [ -29.3582,    6.2823]], grad_fn=&lt;AddBackward0&gt;)\n</pre> <p>Comparemos las predicciones de nuestro modelo con los objetivos reales.</p> In\u00a0[8]: Copied! <pre># Comparar con los targets\nprint(targets)\n</pre> # Comparar con los targets print(targets) <pre>tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])\n</pre> <p>Como se pod\u00eda esperar, existe una gran diferencia entre las predicciones de nuestro modelo y los valores reales de las variables a predecir. Como todav\u00eda no hemos entrenado el modelo, los pesos y sesgos son n\u00fameros aleatorios.</p> In\u00a0[\u00a0]: Copied! <pre># MSE loss\ndef mse(t1, t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) / diff.numel()\n</pre> # MSE loss def mse(t1, t2):     diff = t1 - t2     return torch.sum(diff * diff) / diff.numel() <p><code>torch.sum</code> devuelve la suma de todos los elementos en un tensor. El m\u00e9todo\u00a0<code>.numel</code> de un tensor devuelve el n\u00famero de elementos en un tensor. Calculemos el error cuadr\u00e1tico medio de las predicciones actuales de nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre># Computar loss\nloss = mse(preds, targets)\nprint(loss)\n</pre> # Computar loss loss = mse(preds, targets) print(loss) <p>As\u00ed es como podemos interpretar el resultado: En promedio, cada elemento en la predicci\u00f3n difiere del objetivo real por la ra\u00edz cuadrada de la p\u00e9rdida. Y eso es bastante malo, considerando que los n\u00fameros que estamos tratando de predecir est\u00e1n en el rango de 50 a 200. El resultado se llama p\u00e9rdida porque indica qu\u00e9 tan malo es el modelo para predecir las variables de destino. Representa la p\u00e9rdida de informaci\u00f3n en el modelo: cuanto menor es la p\u00e9rdida, mejor es el modelo.</p> In\u00a0[\u00a0]: Copied! <pre># Computar gradients\nloss.backward()\n</pre> # Computar gradients loss.backward() <p>Los gradientes se almacenan en la propiedad\u00a0<code>.grad</code> de los respectivos tensores. Tenga en cuenta que la derivada de la p\u00e9rdida w.r.t. la matriz de pesos es en s\u00ed misma una matriz con las mismas dimensiones.</p> In\u00a0[\u00a0]: Copied! <pre>w\nw.grad\n</pre> w w.grad In\u00a0[\u00a0]: Copied! <pre>with torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n</pre> with torch.no_grad():     w -= w.grad * 1e-5     b -= b.grad * 1e-5 <p>Multiplicamos los gradientes con un n\u00famero muy peque\u00f1o (<code>10^-5</code> en este caso) para asegurarnos de no modificar los pesos en una cantidad muy grande. Queremos dar un peque\u00f1o paso en la direcci\u00f3n cuesta abajo de la pendiente, no un salto gigante. Este n\u00famero se denomina tasa de aprendizaje del algoritmo.</p> <p>Usamos <code>torch.no_grad</code> para indicarle a PyTorch que no debemos rastrear, calcular o modificar gradientes mientras actualizamos los pesos y sesgos.</p> In\u00a0[\u00a0]: Copied! <pre># Let's verify that the loss is actually lower\nloss = mse(preds, targets)\nprint(loss)\n</pre> # Let's verify that the loss is actually lower loss = mse(preds, targets) print(loss) <p>Antes de continuar, restablecemos los gradientes a cero invocando el m\u00e9todo\u00a0<code>.zero_()</code>. Necesitamos hacer esto porque PyTorch acumula gradientes. De lo contrario, la pr\u00f3xima vez que invocamos <code>.backward</code> en la p\u00e9rdida, los nuevos valores de gradiente se agregan a los gradientes existentes, lo que puede generar resultados inesperados.</p> In\u00a0[\u00a0]: Copied! <pre>w.grad.zero_()\nb.grad.zero_()\nprint(w.grad)\nprint(b.grad)\n</pre> w.grad.zero_() b.grad.zero_() print(w.grad) print(b.grad) In\u00a0[\u00a0]: Copied! <pre>p = model(inputs)\nloss = mse(p, targets)\nloss.backward()\nwith torch.no_grad():\n  w -= w.grad * 1e-4\n  b -= b.grad * 1e-4\n  w.grad.zero_()\n  b.grad.zero_()\n\nprint(loss)\n</pre> p = model(inputs) loss = mse(p, targets) loss.backward() with torch.no_grad():   w -= w.grad * 1e-4   b -= b.grad * 1e-4   w.grad.zero_()   b.grad.zero_()  print(loss) In\u00a0[\u00a0]: Copied! <pre># Generate predictions\npreds = model(inputs)\nprint(preds)\n</pre> # Generate predictions preds = model(inputs) print(preds) In\u00a0[\u00a0]: Copied! <pre># Calculate the loss\nloss = mse(preds, targets)\nprint(loss)\n</pre> # Calculate the loss loss = mse(preds, targets) print(loss) In\u00a0[\u00a0]: Copied! <pre># Compute gradients\nloss.backward()\nprint(w.grad)\nprint(b.grad)\n</pre> # Compute gradients loss.backward() print(w.grad) print(b.grad) <p>Actualicemos los pesos y sesgos usando los gradientes calculados arriba.</p> In\u00a0[\u00a0]: Copied! <pre># Adjust weights &amp; reset gradients\nwith torch.no_grad():\n    w -= w.grad * 1e-4\n    b -= b.grad * 1e-4\n    w.grad.zero_()\n    b.grad.zero_()\n</pre> # Adjust weights &amp; reset gradients with torch.no_grad():     w -= w.grad * 1e-4     b -= b.grad * 1e-4     w.grad.zero_()     b.grad.zero_() <p>Echemos un vistazo a los nuevos pesos y sesgos.</p> In\u00a0[\u00a0]: Copied! <pre>print(w)\nprint(b)\n</pre> print(w) print(b) <p>Con los nuevos pesos y sesgos, el modelo deber\u00eda tener una p\u00e9rdida menor.</p> In\u00a0[\u00a0]: Copied! <pre># Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)\n</pre> # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) In\u00a0[\u00a0]: Copied! <pre>with torch.no_grad():\n    w -= w.grad * 1e-4\n    b -= b.grad * 1e-4\n    w.grad.zero_()\n    b.grad.zero_()\n \n# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)\nloss.backward()\nprint(targets)\nprint(preds)\n</pre> with torch.no_grad():     w -= w.grad * 1e-4     b -= b.grad * 1e-4     w.grad.zero_()     b.grad.zero_()   # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) loss.backward() print(targets) print(preds) <p>Ya hemos logrado una reducci\u00f3n significativa en la p\u00e9rdida simplemente ajustando los pesos y sesgos ligeramente mediante el descenso de gradiente.</p> In\u00a0[\u00a0]: Copied! <pre># Train for 100 epochs\nfor i in range(100):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    print(loss)\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()\n</pre> # Train for 100 epochs for i in range(100):     preds = model(inputs)     loss = mse(preds, targets)     loss.backward()     print(loss)     with torch.no_grad():         w -= w.grad * 1e-5         b -= b.grad * 1e-5         w.grad.zero_()         b.grad.zero_() <p>Una vez m\u00e1s, comprobemos que la p\u00e9rdida ahora es menor:</p> In\u00a0[\u00a0]: Copied! <pre># Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)\n</pre> # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) <p>La p\u00e9rdida es ahora mucho menor que su valor inicial. Veamos las predicciones del modelo y compar\u00e9moslas con los objetivos.</p> In\u00a0[\u00a0]: Copied! <pre># Predictions\npreds\n</pre> # Predictions preds In\u00a0[\u00a0]: Copied! <pre># Targets\ntargets\n</pre> # Targets targets <p>Las predicciones ahora est\u00e1n bastante cerca de las variables objetivo. Podemos obtener resultados a\u00fan mejores si entrenamos durante algunas \u00e9pocas m\u00e1s.</p> In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\n</pre> import torch.nn as nn <p>Como antes, representamos las entradas, los objetivos y las matrices.</p> In\u00a0[\u00a0]: Copied! <pre># Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70], \n                   [74, 66, 43], \n                   [91, 87, 65], \n                   [88, 134, 59], \n                   [101, 44, 37], \n                   [68, 96, 71], \n                   [73, 66, 44], \n                   [92, 87, 64], \n                   [87, 135, 57], \n                   [103, 43, 36], \n                   [68, 97, 70]], \n                  dtype='float32')\n \n# Targets (apples, oranges)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119],\n                    [57, 69], \n                    [80, 102], \n                    [118, 132], \n                    [21, 38], \n                    [104, 118], \n                    [57, 69], \n                    [82, 100], \n                    [118, 134], \n                    [20, 38], \n                    [102, 120]], \n                   dtype='float32')\n \ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\n</pre> # Input (temp, rainfall, humidity) inputs = np.array([[73, 67, 43],                     [91, 88, 64],                     [87, 134, 58],                     [102, 43, 37],                     [69, 96, 70],                     [74, 66, 43],                     [91, 87, 65],                     [88, 134, 59],                     [101, 44, 37],                     [68, 96, 71],                     [73, 66, 44],                     [92, 87, 64],                     [87, 135, 57],                     [103, 43, 36],                     [68, 97, 70]],                    dtype='float32')   # Targets (apples, oranges) targets = np.array([[56, 70],                      [81, 101],                      [119, 133],                      [22, 37],                      [103, 119],                     [57, 69],                      [80, 102],                      [118, 132],                      [21, 38],                      [104, 118],                      [57, 69],                      [82, 100],                      [118, 134],                      [20, 38],                      [102, 120]],                     dtype='float32')   inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) In\u00a0[\u00a0]: Copied! <pre>inputs\n</pre> inputs <p>Usamos 15 ejemplos de capacitaci\u00f3n para ilustrar c\u00f3mo trabajar con grandes conjuntos de datos en lotes peque\u00f1os.</p> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import TensorDataset\n</pre> from torch.utils.data import TensorDataset In\u00a0[\u00a0]: Copied! <pre>TensorDataset?\n</pre> TensorDataset?  In\u00a0[\u00a0]: Copied! <pre># Define dataset\ntrain_ds = TensorDataset(inputs, targets)\ntrain_ds[:3]\n</pre> # Define dataset train_ds = TensorDataset(inputs, targets) train_ds[:3] In\u00a0[\u00a0]: Copied! <pre>train_ds[0:3]\n</pre> train_ds[0:3] <p>El <code>TensorDataset</code> nos permite acceder a una peque\u00f1a secci\u00f3n de los datos de entrenamiento utilizando la notaci\u00f3n de indexaci\u00f3n de matriz (<code>[0:3]</code> en el c\u00f3digo anterior). Devuelve una tupla con dos elementos. El primer elemento contiene las variables de entrada para las filas seleccionadas y el segundo contiene los objetivos.</p> <p>Tambi\u00e9n crearemos un <code>DataLoader</code>, que puede dividir los datos en lotes de un tama\u00f1o predefinido durante el entrenamiento. Tambi\u00e9n proporciona otras utilidades como la reproducci\u00f3n aleatoria y el muestreo aleatorio de los datos.</p> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import DataLoader\n</pre> from torch.utils.data import DataLoader In\u00a0[\u00a0]: Copied! <pre>DataLoader?\n</pre> DataLoader? In\u00a0[\u00a0]: Copied! <pre># Define data loader\nbatch_size = 5\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\n</pre> # Define data loader batch_size = 5 train_dl = DataLoader(train_ds, batch_size, shuffle=True) In\u00a0[\u00a0]: Copied! <pre>print(train_dl)\n</pre> print(train_dl) <p>Podemos usar el cargador de datos en un bucle <code>for</code>. Veamos un ejemplo.</p> In\u00a0[\u00a0]: Copied! <pre>for xb, yb in train_dl:\n    print(xb)\n    print(yb)\n    # break\n</pre> for xb, yb in train_dl:     print(xb)     print(yb)     # break <p>En cada iteraci\u00f3n, el cargador de datos devuelve un lote de datos con el tama\u00f1o de lote dado. Si <code>shuffle</code> se establece en <code>True</code>, mezcla los datos de entrenamiento antes de crear lotes. El barajado ayuda a aleatorizar la entrada al algoritmo de optimizaci\u00f3n, lo que conduce a una reducci\u00f3n m\u00e1s r\u00e1pida de la p\u00e9rdida.</p> In\u00a0[\u00a0]: Copied! <pre>mod = nn.Linear(10, 20)\nx = torch.randn(120, 10)\nmod(x).shape\nmod.weight.shape\n</pre> mod = nn.Linear(10, 20) x = torch.randn(120, 10) mod(x).shape mod.weight.shape In\u00a0[\u00a0]: Copied! <pre># Define model\nmodel = nn.Linear(3, 2)\nprint(model.weight)\nprint(model.bias)\n</pre> # Define model model = nn.Linear(3, 2) print(model.weight) print(model.bias) <p>Los modelos de PyTorch tambi\u00e9n tienen un \u00fatil m\u00e9todo <code>.parameters</code>, que devuelve una lista que contiene todas las matrices de ponderaci\u00f3n y sesgo presentes en el modelo. Para nuestro modelo de regresi\u00f3n lineal, tenemos una matriz de ponderaci\u00f3n y una matriz de sesgo.</p> In\u00a0[\u00a0]: Copied! <pre># Parameters\nlist(model.parameters())\n</pre> # Parameters list(model.parameters()) In\u00a0[\u00a0]: Copied! <pre>print(model)\n</pre> print(model) <p>Podemos usar el modelo para generar predicciones de la misma manera que antes.</p> In\u00a0[\u00a0]: Copied! <pre># Generate predictions\npreds = model(inputs)\npreds\n</pre> # Generate predictions preds = model(inputs) preds In\u00a0[\u00a0]: Copied! <pre># Import nn.functional\nimport torch.nn.functional as F\n</pre> # Import nn.functional import torch.nn.functional as F <p>El paquete <code>nn.function</code> contiene muchas funciones de p\u00e9rdida \u00fatiles y varias otras utilidades.</p> In\u00a0[\u00a0]: Copied! <pre># Define loss function\nloss_fn = F.mse_loss\n</pre> # Define loss function loss_fn = F.mse_loss <p>Calculemos la p\u00e9rdida para las predicciones actuales de nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre>loss = loss_fn(model(inputs), targets)\nprint(loss)\n</pre> loss = loss_fn(model(inputs), targets) print(loss) In\u00a0[\u00a0]: Copied! <pre>torch.optim.SGD?\n</pre> torch.optim.SGD? In\u00a0[\u00a0]: Copied! <pre># Define optimizer\nopt = torch.optim.SGD(model.parameters(), lr=1e-5)\n</pre> # Define optimizer opt = torch.optim.SGD(model.parameters(), lr=1e-5) <p>Tenga en cuenta que <code>model.parameters()</code> se pasa como argumento a <code>optim.SGD</code> para que el optimizador sepa qu\u00e9 matrices deben modificarse durante el paso de actualizaci\u00f3n. Adem\u00e1s, podemos especificar una tasa de aprendizaje que controle la cantidad en la que se modifican los par\u00e1metros.</p> In\u00a0[\u00a0]: Copied! <pre># Utility function to train the model\ndef fit(num_epochs, model, loss_fn, opt, train_dl):\n    \n    # Repeat for given number of epochs\n    for epoch in range(num_epochs):\n        \n        # Train with batches of data\n        for xb,yb in train_dl:\n            \n            # 1. Generate predictions\n            pred = model(xb)\n            \n            # 2. Calculate loss\n            loss = loss_fn(pred, yb)\n            \n            # 3. Compute gradients\n            loss.backward()\n            \n            # 4. Update parameters using gradients\n            opt.step()\n            \n            # 5. Reset the gradients to zero\n            opt.zero_grad()\n        \n        # Print the progress\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n</pre> # Utility function to train the model def fit(num_epochs, model, loss_fn, opt, train_dl):          # Repeat for given number of epochs     for epoch in range(num_epochs):                  # Train with batches of data         for xb,yb in train_dl:                          # 1. Generate predictions             pred = model(xb)                          # 2. Calculate loss             loss = loss_fn(pred, yb)                          # 3. Compute gradients             loss.backward()                          # 4. Update parameters using gradients             opt.step()                          # 5. Reset the gradients to zero             opt.zero_grad()                  # Print the progress         if (epoch+1) % 10 == 0:             print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item())) <p>Algunas cosas a tener en cuenta arriba:</p> <p>Usamos el cargador de datos definido anteriormente para obtener lotes de datos para cada iteraci\u00f3n. En lugar de actualizar los par\u00e1metros (pesos y sesgos) manualmente, usamos <code>opt.step</code> para realizar la actualizaci\u00f3n y <code>opt.zero_grad</code> para restablecer los gradientes a cero.</p> <ul> <li>Tambi\u00e9n agregamos una declaraci\u00f3n de registro que imprime la p\u00e9rdida del \u00faltimo lote de datos para cada d\u00e9cima \u00e9poca para realizar un seguimiento del progreso del entrenamiento. <code>loss.item</code> devuelve el valor real almacenado en el tensor de p\u00e9rdida.</li> </ul> <p>Entrenemos el modelo para 100 \u00e9pocas.</p> In\u00a0[\u00a0]: Copied! <pre>fit(100, model, loss_fn, opt, train_dl)\n</pre> fit(100, model, loss_fn, opt, train_dl) <p>Generemos predicciones usando nuestro modelo y verifiquemos que est\u00e9n cerca de nuestros objetivos.</p> In\u00a0[\u00a0]: Copied! <pre># Generate predictions\npreds = model(inputs)\npreds\n</pre> # Generate predictions preds = model(inputs) preds In\u00a0[\u00a0]: Copied! <pre># Compare with targets\ntargets\n</pre> # Compare with targets targets <p>De hecho, las predicciones est\u00e1n bastante cerca de nuestros objetivos. Hemos entrenado un modelo razonablemente bueno para predecir el rendimiento de los cultivos de manzanas y naranjas al observar la temperatura promedio, las precipitaciones y la humedad en una regi\u00f3n. Podemos usarlo para hacer predicciones de rendimiento de cultivos para nuevas regiones pasando un lote que contiene una sola fila de entrada.</p> In\u00a0[\u00a0]: Copied! <pre>model(torch.tensor([[75, 63, 44.]]))\n</pre> model(torch.tensor([[75, 63, 44.]])) <p>El rendimiento previsto de manzanas es de 54,3 toneladas por hect\u00e1rea y el de naranjas de 68,3 toneladas por hect\u00e1rea.</p> In\u00a0[\u00a0]: Copied! <pre>F.mse_loss(model(inputs),targets)**0.5\n</pre> F.mse_loss(model(inputs),targets)**0.5"},{"location":"02_pytorch_linear_regression_es/#02-introduccion-a-redes-neuronales-con-pytorch","title":"02. Introducci\u00f3n a Redes Neuronales con PyTorch\u00b6","text":""},{"location":"02_pytorch_linear_regression_es/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>En este cap\u00edtulo, se presentar\u00e1n los fundamentos de las redes neuronales y c\u00f3mo se implementan en PyTorch. Se presentar\u00e1n los conceptos de redes neuronales, funciones de activaci\u00f3n, funciones de p\u00e9rdida, optimizadores y c\u00f3mo se implementan en PyTorch.</p>"},{"location":"02_pytorch_linear_regression_es/#desarrollando-la-primera-red-neuronal-en-pytorch","title":"Desarrollando la primera red neuronal en PyTorch\u00b6","text":"<p>Una red neuronal se compone de un elemento fundamental llamado neurona. Cada neurona de una red neuronal realiza tres operaciones b\u00e1sicas:</p> <ol> <li>Multiplica cada entrada que recibe por un peso.</li> <li>Suma todas las entradas ponderadas a\u00f1adiendo un sesgo (constante).</li> <li>Aplica una funci\u00f3n no lineal a la salida. Esta funci\u00f3n se denomina funci\u00f3n de activaci\u00f3n.</li> </ol> <p>Los pesos y el sesgo son par\u00e1metros de la neurona que se aprenden durante el entrenamiento. La funci\u00f3n de activaci\u00f3n es una funci\u00f3n no lineal que se aplica a la salida de la neurona. Gracias a la funci\u00f3n de activaci\u00f3n no lineal, la red neuronal puede aprender relaciones no lienales entre las entradas y las salidas.</p> <p>Las operaciones de una neurona se puede describir en forma matem\u00e1tica de la siguiente manera:</p> <pre><code>y = f(w*x + b)\n</code></pre> <p>Siendo, <code>x</code> es un vector de entrada de tama\u00f1o <code>n</code>, <code>w</code> es un vector de pesos de tama\u00f1o <code>n</code>, <code>b</code> es el sesgo (un solo n\u00famero) y <code>f</code> es la funci\u00f3n de activaci\u00f3n. Si no se utiliza funci\u00f3n de activaci\u00f3n, la salida <code>y</code> es simplemente una suma ponderada de las entradas (m\u00e1s el sesgo), es decir, una regresi\u00f3n lineal:</p> <pre><code>y = w1x1 + w2x2 + ... + wnxn + b\n</code></pre> <p>Para entender c\u00f3mo se implementa una arquitectura de deep learning en PyTorch, se comenzar\u00e1 desarrollando un modelo de regresi\u00f3n lineal. Este modelo es el m\u00e1s simple de todos los modelos de redes neuronales, pero es un buen punto de partida para entender c\u00f3mo se implementan las redes neuronales en PyTorch.</p>"},{"location":"02_pytorch_linear_regression_es/#creando-un-conjunto-de-datos-sintetico-prediccion-de-produccion-de-manzanas-y-naranjas","title":"Creando un conjunto de datos sint\u00e9tico: predicci\u00f3n de producci\u00f3n de manzanas y naranjas\u00b6","text":""},{"location":"02_pytorch_linear_regression_es/#datos-de-entrenamiento","title":"Datos de entrenamiento\u00b6","text":"<p>Podemos representar los datos de entrenamiento usando dos matrices: <code>entradas</code> y <code>objetivos</code>, cada una con una fila por observaci\u00f3n y una columna por variable.</p>"},{"location":"02_pytorch_linear_regression_es/#modelo-de-regresion-lineal","title":"Modelo de regresi\u00f3n lineal\u00b6","text":"<p>Los pesos y sesgos (<code>w11, w12,... w23, b1 y b2</code>) tambi\u00e9n se pueden representar como matrices, inicializadas como valores aleatorios. La primera fila de <code>w</code> y el primer elemento de <code>b</code> se utilizan para predecir la primera variable objetivo, es decir, el rendimiento de las manzanas y, de manera similar, el segundo para las naranjas.</p>"},{"location":"02_pytorch_linear_regression_es/#funcion-de-perdida","title":"Funci\u00f3n de p\u00e9rdida\u00b6","text":"<p>El entrenamiento de una red neuronal consiste en determinar los pesos y sesgos que hacen que la predicci\u00f3n sea lo m\u00e1s parecida al conjunto de valores reales (observaciones). Este proceso es en realidad un problema de optimizaci\u00f3n, en concreto de minizaci\u00f3n. La minimizaci\u00f3n de una funci\u00f3n $y=f(x)$ consiste en determinar los par\u00e1metros $x$ que minimizan el valor de la funci\u00f3n $f(x)$.</p> <p>En el caso de la funci\u00f3n $y=x^2$ el valor que minimiza la funci\u00f3n es $x=0$. Calcular la derivada de una funci\u00f3n, y despejar el valor de la $x$ para determinar el valor m\u00ednimo es muy costoso computacionalmente, y en ocasiones no es posible. Por ello, la optimizaci\u00f3n de los par\u00e1metros se realiza con un proceso num\u00e9rico denominado back-propagation. En la siguiente referencia se puede encontrar m\u00e1s informaci\u00f3n al respecto [REFERENCE999].</p> <p>Es necesario por tanto, definir cu\u00e1l es la funci\u00f3n que se quiere minimizar en el entrenamiento de nuestra red neuronal. Esta funci\u00f3n se denomina funci\u00f3n de p\u00e9rdidad.</p> <p>Las funciones de p\u00e9rdida m\u00e1s utilizadas son:</p> <ul> <li>Problemas de regresi\u00f3n: Error cuadr\u00e1tico medio (MSE) y error cuadr\u00e1tico absoluto (MAE).</li> <li>Problemas de clasificaci\u00f3n: Entrop\u00eda cruzada.</li> </ul> <p>Calcula la diferencia entre las dos matrices (<code>preds</code> y <code>targets</code>). Cuadre todos los elementos de la matriz de diferencias para eliminar los valores negativos. *Calcular el promedio de los elementos de la matriz resultante.</p> <p>En el problema planteado, se utilizar\u00e1 como funci\u00f3n de p\u00e9rdida el error cuadr\u00e1tico medio (MSE).</p>"},{"location":"02_pytorch_linear_regression_es/#calcular-gradientes","title":"Calcular gradientes\u00b6","text":"<p>Con PyTorch, podemos calcular autom\u00e1ticamente el gradiente o la derivada de la p\u00e9rdida w.r.t. a los pesos y sesgos porque tienen <code>requires_grad</code> establecido en <code>True</code>. Veremos c\u00f3mo esto es \u00fatil en un momento.</p>"},{"location":"02_pytorch_linear_regression_es/#ajuste-pesos-y-sesgos-para-reducir-la-perdida","title":"Ajuste pesos y sesgos para reducir la p\u00e9rdida\u00b6","text":"<p>La p\u00e9rdida es una funci\u00f3n cuadr\u00e1tica de nuestros pesos y sesgos, y nuestro objetivo es encontrar el conjunto de pesos donde la p\u00e9rdida es la m\u00e1s baja. Si trazamos un gr\u00e1fico de la p\u00e9rdida con cualquier elemento de peso o sesgo individual, se ver\u00e1 como la figura que se muestra a continuaci\u00f3n. Una idea importante del c\u00e1lculo es que el gradiente indica la tasa de cambio de la p\u00e9rdida, es decir, la [pendiente] (https://en.wikipedia.org/wiki/Slope) de la funci\u00f3n de p\u00e9rdida w.r.t. los pesos y sesgos.</p> <p>Si un elemento degradado es positivo:</p> <ul> <li>aumentar el valor del elemento de peso ligeramente aumentar\u00e1 la p\u00e9rdida</li> <li>disminuir el valor del elemento de peso ligeramente disminuir\u00e1 la p\u00e9rdida</li> </ul> <p></p> <p>Si un elemento degradado es negativo:</p> <ul> <li>aumentar el valor del elemento de peso ligeramente disminuir\u00e1 la p\u00e9rdida</li> <li>disminuir el valor del elemento de peso ligeramente aumentar\u00e1 la p\u00e9rdida</li> </ul> <p></p> <p>El aumento o disminuci\u00f3n de la p\u00e9rdida al cambiar un elemento de peso es proporcional al gradiente de la p\u00e9rdida w.r.t. ese elemento Esta observaci\u00f3n forma la base del_descenso de gradiente_algoritmo de optimizaci\u00f3n que usaremos para mejorar nuestro modelo (descendiendo_a lo largo del_gradiente).</p> <p>Podemos restar de cada elemento de peso una peque\u00f1a cantidad proporcional a la derivada de la p\u00e9rdida w.r.t. ese elemento para reducir ligeramente la p\u00e9rdida.</p>"},{"location":"02_pytorch_linear_regression_es/#entrena-el-modelo-usando-descenso-de-gradiente","title":"Entrena el modelo usando descenso de gradiente\u00b6","text":"<p>Como se vio anteriormente, reducimos la p\u00e9rdida y mejoramos nuestro modelo utilizando el algoritmo de optimizaci\u00f3n de descenso de gradiente. Por lo tanto, podemos entrenar el modelo usando los siguientes pasos:</p> <ol> <li><p>Genera predicciones</p> </li> <li><p>Calcular la p\u00e9rdida</p> </li> <li><p>Calcular gradientes con los pesos y sesgos</p> </li> <li><p>Ajuste los pesos restando una peque\u00f1a cantidad proporcional al gradiente</p> </li> <li><p>Restablecer los gradientes a cero</p> </li> </ol> <p>Implementemos lo anterior paso a paso.</p>"},{"location":"02_pytorch_linear_regression_es/#tren-para-multiples-epocas","title":"Tren para m\u00faltiples\u00a0\u00e9pocas\u00b6","text":"<p>Para reducir a\u00fan m\u00e1s la p\u00e9rdida, podemos repetir el proceso de ajustar los pesos y sesgos utilizando los gradientes varias veces. Cada iteraci\u00f3n se denomina \u00e9poca. Entrenemos el modelo para 100 \u00e9pocas.</p>"},{"location":"02_pytorch_linear_regression_es/#regresion-lineal-usando-las-funciones-incorporadas-de-pytorch","title":"Regresi\u00f3n lineal usando las funciones incorporadas de PyTorch\u00b6","text":"<p>Hemos implementado un modelo de regresi\u00f3n lineal y descenso de gradiente utilizando algunas operaciones b\u00e1sicas de tensor. Sin embargo, dado que este es un patr\u00f3n com\u00fan en el aprendizaje profundo, PyTorch proporciona varias funciones y clases integradas para facilitar la creaci\u00f3n y el entrenamiento de modelos con solo unas pocas l\u00edneas de c\u00f3digo.</p> <p>Comencemos importando el paquete <code>torch.nn</code> de PyTorch, que contiene clases de utilidad para construir redes neuronales.</p>"},{"location":"02_pytorch_linear_regression_es/#conjunto-de-datos-y-cargador-de-datos","title":"Conjunto de datos y cargador de datos\u00b6","text":"<p>Crearemos un <code>TensorDataset</code>, que permite el acceso a filas desde <code>inputs</code> y <code>targets</code> como tuplas, y proporciona API est\u00e1ndar para trabajar con muchos tipos diferentes de conjuntos de datos en PyTorch.</p>"},{"location":"02_pytorch_linear_regression_es/#nnlineal","title":"nn.Lineal\u00b6","text":"<p>En lugar de inicializar manualmente los pesos y sesgos, podemos definir el modelo usando la clase <code>nn.Linear</code> de PyTorch, que lo hace autom\u00e1ticamente.</p>"},{"location":"02_pytorch_linear_regression_es/#funcion-de-perdida","title":"Funci\u00f3n de p\u00e9rdida\u00b6","text":"<p>En lugar de definir una funci\u00f3n de p\u00e9rdida manualmente, podemos usar la funci\u00f3n de p\u00e9rdida integrada <code>mse_loss</code>.</p>"},{"location":"02_pytorch_linear_regression_es/#optimizador","title":"Optimizador\u00b6","text":"<p>En lugar de manipular manualmente los pesos y sesgos del modelo usando gradientes, podemos usar el optimizador <code>optim.SGD</code>. SGD es la abreviatura de \"descenso de gradiente estoc\u00e1stico\". El t\u00e9rmino estoc\u00e1stico indica que las muestras se seleccionan en lotes aleatorios en lugar de como un solo grupo.</p>"},{"location":"02_pytorch_linear_regression_es/#entrenar-al-modelo","title":"Entrenar al modelo\u00b6","text":"<p>Ahora estamos listos para entrenar el modelo. Seguiremos el mismo proceso para implementar el descenso de gradiente:</p> <ol> <li><p>Genera predicciones</p> </li> <li><p>Calcular la p\u00e9rdida</p> </li> <li><p>Calcular gradientes con los pesos y sesgos</p> </li> <li><p>Ajuste los pesos restando una peque\u00f1a cantidad proporcional al gradiente</p> </li> <li><p>Restablecer los gradientes a cero</p> </li> </ol> <p>El \u00fanico cambio es que trabajaremos con lotes de datos en lugar de procesar todos los datos de entrenamiento en cada iteraci\u00f3n. Definamos una funci\u00f3n de utilidad <code>fit</code> que entrene el modelo para un n\u00famero determinado de \u00e9pocas.</p>"},{"location":"03_pytorch_logistic_regression_es/","title":"03. Procesamiento de Im\u00e1genes y Regresi\u00f3n Log\u00edstica","text":"<p>Este tutorial cubre los siguientes temas:</p> <ul> <li>Trabajar con im\u00e1genes en PyTorch (usando el conjunto de datos MNIST)</li> <li>Dividir un conjunto de datos en conjuntos de entrenamiento, validaci\u00f3n y prueba</li> <li>Creaci\u00f3n de modelos PyTorch con l\u00f3gica personalizada mediante la ampliaci\u00f3n de la clase <code>nn.Module</code></li> <li>Interpretar los resultados del modelo como probabilidades utilizando Softmax y seleccionando etiquetas predichas</li> <li>Elegir una m\u00e9trica de evaluaci\u00f3n \u00fatil (precisi\u00f3n) y una funci\u00f3n de p\u00e9rdida (entrop\u00eda cruzada) para problemas de clasificaci\u00f3n</li> <li>Configuraci\u00f3n de un ciclo de entrenamiento que tambi\u00e9n eval\u00faa el modelo utilizando el conjunto de validaci\u00f3n</li> <li>Probar el modelo manualmente en ejemplos seleccionados al azar* Guardar y cargar puntos de control del modelo para evitar volver a entrenar desde cero</li> </ul> <p>Comenzamos instalando e importando <code>torch</code> y <code>torchvision</code>. <code>torchvision</code> contiene algunas utilidades para trabajar con datos de imagen. Tambi\u00e9n proporciona clases auxiliares para descargar e importar conjuntos de datos populares como MNIST autom\u00e1ticamente.</p> In\u00a0[5]: Copied! <pre># Imports\nimport torch\nimport torchvision\nfrom torchvision.datasets import MNIST\n</pre> # Imports import torch import torchvision from torchvision.datasets import MNIST In\u00a0[6]: Copied! <pre># Download training dataset\ndataset = MNIST(root='data/', download=True)\n</pre> # Download training dataset dataset = MNIST(root='data/', download=True) <p>Cuando esta instrucci\u00f3n se ejecuta por primera vez, descarga los datos en el directorio <code>data/</code> al lado del cuaderno y crea un PyTorch <code>Dataset</code>. En ejecuciones posteriores, la descarga se omite porque los datos ya se han descargado. Vamos a comprobar el tama\u00f1o del conjunto de datos.</p> <p>El conjunto de datos tiene 60\u00a0000 im\u00e1genes que usaremos para entrenar el modelo. Tambi\u00e9n hay un conjunto de prueba adicional de 10 000 im\u00e1genes que se utilizan para evaluar modelos y reportar m\u00e9tricas en documentos e informes. Podemos crear el conjunto de datos de prueba usando la clase <code>MNIST</code> pasando <code>train=False</code> al constructor.</p> In\u00a0[\u00a0]: Copied! <pre>test_dataset = MNIST(root='data/', train=False)\n</pre> test_dataset = MNIST(root='data/', train=False) <p>Veamos un elemento de muestra del conjunto de datos de entrenamiento.</p> In\u00a0[9]: Copied! <pre>dataset[0]\n</pre> dataset[0] Out[9]: <pre>(&lt;PIL.Image.Image image mode=L size=28x28 at 0x1A79439D198&gt;, 5)</pre> In\u00a0[10]: Copied! <pre>test_dataset[0]\n</pre> test_dataset[0] Out[10]: <pre>(&lt;PIL.Image.Image image mode=L size=28x28 at 0x1A79439D240&gt;, 7)</pre> <p>Es un par, que consiste en una imagen de 28x28px y una etiqueta. La imagen es un objeto de la clase <code>PIL.Image.Image</code>, que forma parte de la biblioteca de im\u00e1genes de Python Pillow. Podemos ver la imagen dentro de Jupyter usando <code>matplotlib</code>, la biblioteca de gr\u00e1ficos y gr\u00e1ficos de facto para la ciencia de datos en Python.</p> In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt %matplotlib inline <p>La instrucci\u00f3n <code>%matplotlib inline</code> indica a Jupyter que queremos trazar los gr\u00e1ficos dentro del cuaderno. Sin esta l\u00ednea, Jupyter mostrar\u00e1 la imagen en una ventana emergente. Las declaraciones que comienzan con <code>%</code> se denominan comandos m\u00e1gicos y se utilizan para configurar el comportamiento del propio Jupyter. Puede encontrar una lista completa de comandos m\u00e1gicos aqu\u00ed: https://ipython.readthedocs.io/en/stable/interactive/magics.html.</p> <p>Veamos un par de im\u00e1genes del conjunto de datos.</p> In\u00a0[12]: Copied! <pre>image, label = dataset[0]\nplt.imshow(image, cmap='gray')\nprint('Label:', label)\n</pre> image, label = dataset[0] plt.imshow(image, cmap='gray') print('Label:', label) <pre>Label: 5\n</pre> In\u00a0[13]: Copied! <pre>image, label = dataset[10]\nplt.imshow(image, cmap='gray')\nprint('Label:', label)\n</pre> image, label = dataset[10] plt.imshow(image, cmap='gray') print('Label:', label) <pre>Label: 3\n</pre> <p>Es obvio que estas im\u00e1genes tienen un tama\u00f1o relativamente peque\u00f1o, y reconocer los d\u00edgitos a veces puede ser un desaf\u00edo incluso para el ojo humano. Si bien es \u00fatil mirar estas im\u00e1genes, solo hay un problema aqu\u00ed: PyTorch no sabe c\u00f3mo trabajar con im\u00e1genes. Necesitamos convertir las im\u00e1genes en tensores. Podemos hacer esto especificando una transformaci\u00f3n mientras creamos nuestro conjunto de datos.</p> In\u00a0[14]: Copied! <pre>import torchvision.transforms as transforms\n</pre> import torchvision.transforms as transforms <p>Los conjuntos de datos de PyTorch nos permiten especificar una o m\u00e1s funciones de transformaci\u00f3n que se aplican a las im\u00e1genes a medida que se cargan. El m\u00f3dulo <code>torchvision.transforms</code> contiene muchas de estas funciones predefinidas. Usaremos la transformaci\u00f3n <code>ToTensor</code> para convertir im\u00e1genes en tensores PyTorch.</p> In\u00a0[15]: Copied! <pre># MNIST dataset (images and labels)\ndataset = MNIST(root='data/', \n                train=True,\n                transform=transforms.ToTensor())\n</pre> # MNIST dataset (images and labels) dataset = MNIST(root='data/',                  train=True,                 transform=transforms.ToTensor()) In\u00a0[16]: Copied! <pre>img_tensor, label = dataset[0]\nprint(img_tensor.shape, label)\n</pre> img_tensor, label = dataset[0] print(img_tensor.shape, label) <pre>torch.Size([1, 28, 28]) 5\n</pre> <p>La imagen ahora se convierte a un tensor de 1x28x28. La primera dimensi\u00f3n rastrea los canales de color. Las dimensiones segunda y tercera representan p\u00edxeles a lo largo de la altura y el ancho de la imagen, respectivamente. Dado que las im\u00e1genes en el conjunto de datos MNIST est\u00e1n en escala de grises, solo hay un canal. Otros conjuntos de datos tienen im\u00e1genes con color, en cuyo caso hay tres canales: rojo, verde y azul (RGB).</p> <p>Veamos algunos valores de muestra dentro del tensor.</p> In\u00a0[17]: Copied! <pre>print(img_tensor[0,10:15,10:15])\nprint(torch.max(img_tensor), torch.min(img_tensor))\n</pre> print(img_tensor[0,10:15,10:15]) print(torch.max(img_tensor), torch.min(img_tensor)) <pre>tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\ntensor(1.) tensor(0.)\n</pre> <p>Los valores var\u00edan de 0 a 1, con <code>0</code> representando el negro, <code>1</code> el blanco y los valores entre diferentes tonos de gris. Tambi\u00e9n podemos trazar el tensor como una imagen usando <code>plt.imshow</code>.</p> In\u00a0[18]: Copied! <pre># Plot the image by passing in the 28x28 matrix\nplt.imshow(img_tensor[0,10:15,10:15], cmap='gray');\n</pre> # Plot the image by passing in the 28x28 matrix plt.imshow(img_tensor[0,10:15,10:15], cmap='gray'); <p>Tenga en cuenta que necesitamos pasar solo la matriz de 28x28 a <code>plt.imshow</code>, sin una dimensi\u00f3n de canal. Tambi\u00e9n pasamos un mapa de colores (<code>cmap=gray</code>) para indicar que queremos ver una imagen en escala de grises.</p> In\u00a0[19]: Copied! <pre>from torch.utils.data import random_split\n\ntrain_ds, val_ds = random_split(dataset, [50000, 10000])\nlen(train_ds), len(val_ds)\n</pre> from torch.utils.data import random_split  train_ds, val_ds = random_split(dataset, [50000, 10000]) len(train_ds), len(val_ds) Out[19]: <pre>(50000, 10000)</pre> <p>Es esencial elegir una muestra aleatoria para crear un conjunto de validaci\u00f3n. Los datos de entrenamiento a menudo se ordenan por las etiquetas de destino, es decir, im\u00e1genes de 0, seguidas de 1, seguidas de 2, etc. Si creamos un conjunto de validaci\u00f3n utilizando el \u00faltimo 20% de las im\u00e1genes, solo constar\u00eda de 8 y 9. Por el contrario, el conjunto de entrenamiento no contendr\u00eda 8 ni 9. Tal entrenamiento-validaci\u00f3n har\u00eda imposible entrenar un modelo \u00fatil.</p> <p>Ahora podemos crear cargadores de datos para ayudarnos a cargar los datos en lotes. Usaremos un tama\u00f1o de lote de 128.</p> In\u00a0[20]: Copied! <pre>from torch.utils.data import DataLoader\n\nbatch_size = 128\n\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size)\n</pre> from torch.utils.data import DataLoader  batch_size = 128  train_loader = DataLoader(train_ds, batch_size, shuffle=True) val_loader = DataLoader(val_ds, batch_size) <p>Configuramos <code>shuffle=True</code> para el cargador de datos de entrenamiento para garantizar que los lotes generados en cada \u00e9poca sean diferentes. Esta aleatorizaci\u00f3n ayuda a generalizar y acelerar el proceso de entrenamiento. Por otro lado, dado que el cargador de datos de validaci\u00f3n se usa solo para evaluar el modelo, no es necesario mezclar las im\u00e1genes.</p> In\u00a0[24]: Copied! <pre>import torch.nn as nn\n\ninput_size = 28*28\nnum_classes = 10\n\n# Logistic regression model\nmodel = nn.Linear(in_features = input_size, out_features = num_classes)\n</pre> import torch.nn as nn  input_size = 28*28 num_classes = 10  # Logistic regression model model = nn.Linear(in_features = input_size, out_features = num_classes) <p>Por supuesto, este modelo es mucho m\u00e1s grande que nuestro modelo anterior en t\u00e9rminos de n\u00famero de par\u00e1metros. Echemos un vistazo a los pesos y sesgos.</p> In\u00a0[25]: Copied! <pre>print(model)\n</pre> print(model) <pre>Linear(in_features=784, out_features=10, bias=True)\n</pre> In\u00a0[26]: Copied! <pre>model.bias\nmodel.bias.shape\n</pre> model.bias model.bias.shape Out[26]: <pre>torch.Size([10])</pre> In\u00a0[27]: Copied! <pre>print(model.weight.shape)\nmodel.weight\n</pre> print(model.weight.shape) model.weight <pre>torch.Size([10, 784])\n</pre> Out[27]: <pre>Parameter containing:\ntensor([[ 0.0272,  0.0254,  0.0070,  ...,  0.0271,  0.0071, -0.0239],\n        [ 0.0008, -0.0183, -0.0132,  ..., -0.0297, -0.0195, -0.0028],\n        [-0.0193, -0.0279,  0.0178,  ...,  0.0179, -0.0306, -0.0301],\n        ...,\n        [ 0.0315, -0.0104, -0.0265,  ..., -0.0106,  0.0091,  0.0350],\n        [-0.0015,  0.0148, -0.0252,  ..., -0.0261,  0.0177, -0.0202],\n        [ 0.0005,  0.0014, -0.0202,  ..., -0.0184,  0.0351, -0.0174]],\n       requires_grad=True)</pre> In\u00a0[28]: Copied! <pre>print(model.bias.shape)\nmodel.bias\n</pre> print(model.bias.shape) model.bias <pre>torch.Size([10])\n</pre> Out[28]: <pre>Parameter containing:\ntensor([-0.0130,  0.0238,  0.0274,  0.0117,  0.0307, -0.0181,  0.0007, -0.0185,\n        -0.0080,  0.0238], requires_grad=True)</pre> <p>Aunque hay un total de 7850 par\u00e1metros aqu\u00ed, conceptualmente, nada ha cambiado hasta ahora. Probemos y generemos algunos resultados usando nuestro modelo. Tomaremos el primer lote de 100 im\u00e1genes de nuestro conjunto de datos y las pasaremos a nuestro modelo.</p> In\u00a0[29]: Copied! <pre>img, lb = train_ds[0]\nimg.shape\n</pre> img, lb = train_ds[0] img.shape Out[29]: <pre>torch.Size([1, 28, 28])</pre> In\u00a0[30]: Copied! <pre>train_loader\n</pre> train_loader Out[30]: <pre>&lt;torch.utils.data.dataloader.DataLoader at 0x1a79b094198&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>list(train_loader)[:4]\n</pre> list(train_loader)[:4] In\u00a0[35]: Copied! <pre>for images, labels in train_loader:\n    print(labels)\n    print(images.shape)\n#     outputs = model(images)\n#     print(outputs)\n    break\n</pre> for images, labels in train_loader:     print(labels)     print(images.shape) #     outputs = model(images) #     print(outputs)     break <pre>tensor([1, 3, 9, 3, 5, 3, 4, 3, 0, 6, 4, 5, 1, 1, 2, 7, 3, 6, 0, 2, 1, 9, 2, 5,\n        0, 4, 7, 1, 9, 8, 0, 6, 6, 1, 3, 2, 5, 0, 3, 4, 0, 5, 9, 2, 4, 5, 5, 6,\n        0, 0, 5, 2, 3, 7, 0, 4, 3, 0, 8, 8, 2, 2, 3, 4, 6, 1, 2, 8, 6, 2, 1, 4,\n        0, 5, 5, 0, 9, 9, 4, 1, 9, 6, 6, 3, 3, 1, 3, 3, 5, 8, 3, 1, 5, 3, 4, 4,\n        2, 7, 6, 7, 5, 0, 1, 9, 2, 3, 7, 9, 5, 1, 4, 5, 9, 3, 8, 7, 1, 0, 0, 9,\n        5, 7, 3, 8, 7, 2, 3, 3])\ntorch.Size([128, 1, 28, 28])\n</pre> In\u00a0[37]: Copied! <pre>images.shape\n</pre> images.shape Out[37]: <pre>torch.Size([128, 1, 28, 28])</pre> In\u00a0[38]: Copied! <pre>images.reshape(batch_size, 784).shape\n</pre> images.reshape(batch_size, 784).shape Out[38]: <pre>torch.Size([128, 784])</pre> <p>El c\u00f3digo anterior genera un error porque nuestros datos de entrada no tienen la forma correcta. Nuestras im\u00e1genes tienen la forma 1x28x28, pero necesitamos que sean vectores de tama\u00f1o 784, es decir, necesitamos aplanarlos. Usaremos el m\u00e9todo <code>.reshape</code> de un tensor, que nos permitir\u00e1 'ver' eficientemente cada imagen como un vector plano sin realmente crear una copia de los datos subyacentes. Para incluir esta funcionalidad adicional dentro de nuestro modelo, necesitamos definir un modelo personalizado extendiendo la clase <code>nn.Module</code> de PyTorch.</p> <p>Una clase en Python proporciona un \"modelo\" para crear objetos. Veamos un ejemplo de definici\u00f3n de una nueva clase en Python.</p> In\u00a0[39]: Copied! <pre>class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return out\n  \nmodel = MnistModel()\n</pre> class MnistModel(nn.Module):     def __init__(self):         super().__init__()         self.linear = nn.Linear(input_size, num_classes)              def forward(self, xb):         xb = xb.reshape(-1, 784)         out = self.linear(xb)         return out    model = MnistModel() <p>Dentro del m\u00e9todo constructor <code>__init__</code>, instanciamos los pesos y sesgos usando <code>nn.Linear</code>. Y dentro del m\u00e9todo <code>forward</code>, que se invoca cuando pasamos un lote de entradas al modelo, aplanamos el tensor de entrada y lo pasamos a <code>self.linear</code>.</p> <p><code>xb.reshape(-1, 28*28)</code> indica a PyTorch que queremos unavistadel tensor <code>xb</code> con dos dimensiones. La longitud a lo largo de la segunda dimensi\u00f3n es 28*28 (es decir, 784). Un argumento para <code>.reshape</code> se puede establecer en <code>-1</code> (en este caso, la primera dimensi\u00f3n) para permitir que PyTorch lo descubra autom\u00e1ticamente en funci\u00f3n de la forma del tensor original.</p> <p>Tenga en cuenta que el modelo ya no tiene los atributos <code>.weight</code> y <code>.bias</code> (ya que ahora est\u00e1n dentro del atributo <code>.linear</code>), pero tiene un m\u00e9todo <code>.parameters</code> que devuelve una lista que contiene los pesos y el sesgo .</p> In\u00a0[40]: Copied! <pre>model.linear\n</pre> model.linear Out[40]: <pre>Linear(in_features=784, out_features=10, bias=True)</pre> In\u00a0[41]: Copied! <pre>model.linear.weight\n</pre> model.linear.weight Out[41]: <pre>Parameter containing:\ntensor([[ 0.0203,  0.0183,  0.0007,  ..., -0.0008, -0.0156,  0.0218],\n        [ 0.0298,  0.0048, -0.0126,  ...,  0.0071,  0.0040, -0.0052],\n        [-0.0020,  0.0249, -0.0317,  ..., -0.0075, -0.0352,  0.0043],\n        ...,\n        [-0.0084, -0.0184, -0.0218,  ..., -0.0340, -0.0063,  0.0113],\n        [ 0.0085,  0.0116, -0.0288,  ..., -0.0040, -0.0013, -0.0339],\n        [ 0.0096, -0.0070,  0.0032,  ..., -0.0012,  0.0349, -0.0125]],\n       requires_grad=True)</pre> In\u00a0[42]: Copied! <pre>print(model.linear.weight.shape, model.linear.bias.shape)\nmodel.parameters()\nlist(model.parameters())\n</pre> print(model.linear.weight.shape, model.linear.bias.shape) model.parameters() list(model.parameters()) <pre>torch.Size([10, 784]) torch.Size([10])\n</pre> Out[42]: <pre>[Parameter containing:\n tensor([[ 0.0203,  0.0183,  0.0007,  ..., -0.0008, -0.0156,  0.0218],\n         [ 0.0298,  0.0048, -0.0126,  ...,  0.0071,  0.0040, -0.0052],\n         [-0.0020,  0.0249, -0.0317,  ..., -0.0075, -0.0352,  0.0043],\n         ...,\n         [-0.0084, -0.0184, -0.0218,  ..., -0.0340, -0.0063,  0.0113],\n         [ 0.0085,  0.0116, -0.0288,  ..., -0.0040, -0.0013, -0.0339],\n         [ 0.0096, -0.0070,  0.0032,  ..., -0.0012,  0.0349, -0.0125]],\n        requires_grad=True), Parameter containing:\n tensor([ 0.0191, -0.0302, -0.0273, -0.0281, -0.0226,  0.0268,  0.0181,  0.0354,\n         -0.0002, -0.0301], requires_grad=True)]</pre> <p>Podemos usar nuestro nuevo modelo personalizado de la misma manera que antes. Veamos si funciona.</p> In\u00a0[43]: Copied! <pre>for images, labels in train_loader:\n    print(images.shape)\n    outputs = model(images)\n    break\n\nprint('outputs.shape : ', outputs.shape)\nprint('Sample outputs :\\n', outputs[:2].data)\n</pre> for images, labels in train_loader:     print(images.shape)     outputs = model(images)     break  print('outputs.shape : ', outputs.shape) print('Sample outputs :\\n', outputs[:2].data) <pre>torch.Size([128, 1, 28, 28])\noutputs.shape :  torch.Size([128, 10])\nSample outputs :\n tensor([[ 0.2190, -0.0496,  0.0746, -0.0122, -0.1212, -0.3471, -0.2582,  0.0699,\n         -0.2685,  0.2092],\n        [ 0.2571, -0.0107, -0.1650,  0.1359,  0.0424, -0.3832,  0.1630,  0.0675,\n         -0.6183,  0.1319]])\n</pre> In\u00a0[45]: Copied! <pre>outputs\n</pre> outputs Out[45]: <pre>tensor([[ 0.2190, -0.0496,  0.0746,  ...,  0.0699, -0.2685,  0.2092],\n        [ 0.2571, -0.0107, -0.1650,  ...,  0.0675, -0.6183,  0.1319],\n        [ 0.1358, -0.1044,  0.0111,  ...,  0.2700, -0.1633, -0.0345],\n        ...,\n        [ 0.2713, -0.1081, -0.1103,  ...,  0.1579, -0.0792,  0.2572],\n        [-0.0306, -0.2571, -0.1545,  ...,  0.3003,  0.0127,  0.1174],\n        [ 0.3300, -0.1934, -0.0621,  ...,  0.3359, -0.3857,  0.1580]],\n       grad_fn=&lt;AddmmBackward&gt;)</pre> <p>Para cada una de las 100 im\u00e1genes de entrada, obtenemos 10 salidas, una para cada clase. Como se discuti\u00f3 anteriormente, nos gustar\u00eda que estos resultados representen probabilidades. Los elementos de cada fila de salida deben estar entre 0 y 1 y sumar 1, lo cual no es el caso.</p> <p>Para convertir las filas de salida en probabilidades, usamos la funci\u00f3n softmax, que tiene la siguiente f\u00f3rmula:</p> <p></p> <p>Primero, reemplazamos cada elemento <code>yi</code> en una fila de salida por <code>e^yi</code>, haciendo que todos los elementos sean positivos.</p> <p></p> <p>Luego, los dividimos por su suma para asegurarnos de que suman 1. El vector resultante puede interpretarse como probabilidades.</p> <p>Si bien es f\u00e1cil implementar la funci\u00f3n softmax (\u00a1debe probarla!), usaremos la implementaci\u00f3n que se proporciona dentro de PyTorch porque funciona bien con tensores multidimensionales (una lista de filas de salida en nuestro caso).</p> In\u00a0[63]: Copied! <pre>import math\n\nc = [-2., 7., 5., 9.]\nprint(c)\nc_exp = [round(math.exp(i), 2) for i in c]\nprint(c_exp)\nsoftmax = [(j/sum(c_exp)) for j in c_exp]\nprint(softmax)\n\nimport torch.nn.functional as F\nimport numpy as np\nprint(torch.as_tensor(c))\nF.softmax(torch.as_tensor(c), dim=0)\n</pre> import math  c = [-2., 7., 5., 9.] print(c) c_exp = [round(math.exp(i), 2) for i in c] print(c_exp) softmax = [(j/sum(c_exp)) for j in c_exp] print(softmax)  import torch.nn.functional as F import numpy as np print(torch.as_tensor(c)) F.softmax(torch.as_tensor(c), dim=0) <pre>[-2.0, 7.0, 5.0, 9.0]\n[0.14, 1096.63, 148.41, 8103.08]\n[1.4976049018747874e-05, 0.11730846168163916, 0.0158756816776598, 0.8668008805916823]\ntensor([-2.,  7.,  5.,  9.])\n</pre> Out[63]: <pre>tensor([1.4477e-05, 1.1731e-01, 1.5876e-02, 8.6680e-01])</pre> In\u00a0[64]: Copied! <pre>import torch.nn.functional as F\n</pre> import torch.nn.functional as F <p>La funci\u00f3n softmax est\u00e1 incluida en el paquete <code>torch.nn.function</code> y requiere que especifiquemos una dimensi\u00f3n a lo largo de la cual se debe aplicar la funci\u00f3n.</p> In\u00a0[65]: Copied! <pre>outputs[:3]\n</pre> outputs[:3] Out[65]: <pre>tensor([[ 0.2190, -0.0496,  0.0746, -0.0122, -0.1212, -0.3471, -0.2582,  0.0699,\n         -0.2685,  0.2092],\n        [ 0.2571, -0.0107, -0.1650,  0.1359,  0.0424, -0.3832,  0.1630,  0.0675,\n         -0.6183,  0.1319],\n        [ 0.1358, -0.1044,  0.0111,  0.1517,  0.1973, -0.2435, -0.1137,  0.2700,\n         -0.1633, -0.0345]], grad_fn=&lt;SliceBackward&gt;)</pre> <p>tenemos 10 salidas para 128 im\u00e1genes (lote).</p> In\u00a0[73]: Copied! <pre>outputs.shape\n</pre> outputs.shape Out[73]: <pre>torch.Size([128, 10])</pre> <p></p> In\u00a0[107]: Copied! <pre>a = np.array([[1,1,1], [2,2,2]])\nprint(a)\nprint(a.sum(axis=0))\nprint(a.sum(axis=1))\nprint(a[:,0])\nprint(a[0])\nprint(a[0,:])\n</pre> a = np.array([[1,1,1], [2,2,2]]) print(a) print(a.sum(axis=0)) print(a.sum(axis=1)) print(a[:,0]) print(a[0]) print(a[0,:]) <pre>[[1 1 1]\n [2 2 2]]\n[3 3 3]\n[3 6]\n[1 2]\n[1 1 1]\n[1 1 1]\n</pre> In\u00a0[98]: Copied! <pre>print(outputs.shape)\nprint(F.softmax(outputs, dim=1).sum(axis=1))\nprint(\"\\n\")\nprint(F.softmax(outputs, dim=0).sum(axis=0))\n</pre> print(outputs.shape) print(F.softmax(outputs, dim=1).sum(axis=1)) print(\"\\n\") print(F.softmax(outputs, dim=0).sum(axis=0)) <pre>torch.Size([128, 10])\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n\n\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000], grad_fn=&lt;SumBackward1&gt;)\n</pre> In\u00a0[84]: Copied! <pre># Apply softmax for each output row\nprobs = F.softmax(outputs, dim=1)\n</pre> # Apply softmax for each output row probs = F.softmax(outputs, dim=1) In\u00a0[90]: Copied! <pre>probs[0]\n</pre> probs[0] Out[90]: <pre>tensor([0.1284, 0.0981, 0.1111, 0.1019, 0.0914, 0.0729, 0.0797, 0.1106, 0.0788,\n        0.1271], grad_fn=&lt;SelectBackward&gt;)</pre> In\u00a0[108]: Copied! <pre># Look at sample probabilities\nprint(\"Sample probabilities:\\n\", probs[:2].data)\n\n# Add up the probabilities of an output row\nprint(\"Sum: \", torch.sum(probs[0]).item())\n</pre> # Look at sample probabilities print(\"Sample probabilities:\\n\", probs[:2].data)  # Add up the probabilities of an output row print(\"Sum: \", torch.sum(probs[0]).item()) <pre>Sample probabilities:\n tensor([[0.1284, 0.0981, 0.1111, 0.1019, 0.0914, 0.0729, 0.0797, 0.1106, 0.0788,\n         0.1271],\n        [0.1303, 0.0997, 0.0854, 0.1154, 0.1051, 0.0687, 0.1186, 0.1078, 0.0543,\n         0.1149]])\nSum:  0.9999998807907104\n</pre> <p>Finalmente, podemos determinar la etiqueta predicha para cada imagen simplemente eligiendo el \u00edndice del elemento con la probabilidad m\u00e1s alta en cada fila de salida. Podemos hacer esto usando <code>torch.max</code>, que devuelve el elemento m\u00e1s grande de cada fila y el \u00edndice correspondiente.</p> In\u00a0[111]: Copied! <pre>max_probs, preds = torch.max(probs[:2], dim=1)\nprint(preds)\nprint(max_probs)\n</pre> max_probs, preds = torch.max(probs[:2], dim=1) print(preds) print(max_probs) <pre>tensor([0, 0])\ntensor([0.1284, 0.1303], grad_fn=&lt;MaxBackward0&gt;)\n</pre> In\u00a0[114]: Copied! <pre>max_probs, preds = torch.max(probs, dim=1)\nprint(preds)\nprint(max_probs)\n</pre> max_probs, preds = torch.max(probs, dim=1) print(preds) print(max_probs) <pre>tensor([0, 0, 7, 3, 7, 3, 0, 3, 6, 3, 9, 3, 0, 7, 6, 7, 7, 9, 0, 3, 3, 9, 9, 3,\n        9, 7, 3, 9, 7, 3, 0, 3, 9, 7, 7, 0, 0, 3, 0, 3, 3, 0, 7, 6, 7, 0, 7, 4,\n        0, 3, 9, 9, 9, 0, 0, 0, 3, 0, 6, 9, 9, 7, 3, 9, 3, 6, 9, 3, 9, 0, 0, 3,\n        9, 0, 6, 0, 3, 9, 6, 7, 9, 0, 6, 3, 9, 7, 0, 3, 3, 3, 3, 6, 0, 0, 9, 3,\n        7, 4, 9, 3, 6, 0, 7, 7, 6, 6, 9, 7, 0, 0, 9, 7, 0, 9, 7, 0, 9, 3, 0, 9,\n        6, 9, 0, 6, 0, 0, 7, 7])\ntensor([0.1284, 0.1303, 0.1279, 0.1646, 0.1446, 0.1247, 0.1309, 0.1304, 0.1168,\n        0.1262, 0.1311, 0.1348, 0.1226, 0.1286, 0.1290, 0.1308, 0.1365, 0.1402,\n        0.1334, 0.1496, 0.1278, 0.1408, 0.1220, 0.1240, 0.1402, 0.1419, 0.1375,\n        0.1193, 0.1208, 0.1503, 0.1325, 0.1407, 0.1487, 0.1294, 0.1537, 0.1301,\n        0.1318, 0.1449, 0.1186, 0.1189, 0.1378, 0.1347, 0.1287, 0.1297, 0.1450,\n        0.1541, 0.1425, 0.1224, 0.1235, 0.1480, 0.1701, 0.1385, 0.1539, 0.1290,\n        0.1370, 0.1264, 0.1346, 0.1346, 0.1262, 0.1475, 0.1373, 0.1309, 0.1403,\n        0.1578, 0.1289, 0.1213, 0.1445, 0.1387, 0.1568, 0.1280, 0.1352, 0.1356,\n        0.1682, 0.1351, 0.1312, 0.1339, 0.1321, 0.1440, 0.1374, 0.1310, 0.1439,\n        0.1330, 0.1305, 0.1162, 0.1504, 0.1415, 0.1531, 0.1209, 0.1504, 0.1402,\n        0.1446, 0.1217, 0.1424, 0.1462, 0.1189, 0.1466, 0.1504, 0.1200, 0.1485,\n        0.1296, 0.1316, 0.1305, 0.1234, 0.1463, 0.1296, 0.1400, 0.1681, 0.1365,\n        0.1533, 0.1225, 0.1502, 0.1296, 0.1475, 0.1105, 0.1503, 0.1335, 0.1257,\n        0.1256, 0.1490, 0.1302, 0.1212, 0.1468, 0.1242, 0.1323, 0.1152, 0.1266,\n        0.1411, 0.1308], grad_fn=&lt;MaxBackward0&gt;)\n</pre> <p>Esto est\u00e1 mal, solo para probar la funci\u00f3n.</p> In\u00a0[112]: Copied! <pre>aux1, aux2 = torch.max(probs[:2], dim=0)\nprint(aux1)\nprint(aux2)\n</pre> aux1, aux2 = torch.max(probs[:2], dim=0) print(aux1) print(aux2) <pre>tensor([0.1303, 0.0997, 0.1111, 0.1154, 0.1051, 0.0729, 0.1186, 0.1106, 0.0788,\n        0.1271], grad_fn=&lt;MaxBackward0&gt;)\ntensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n</pre> <p>Los n\u00fameros impresos arriba son las etiquetas predichas para el primer lote de im\u00e1genes de entrenamiento. Vamos a compararlos con las etiquetas reales.</p> In\u00a0[116]: Copied! <pre>labels\n</pre> labels Out[116]: <pre>tensor([5, 7, 6, 6, 6, 1, 8, 1, 7, 9, 7, 4, 6, 5, 1, 5, 5, 0, 8, 2, 1, 0, 6, 1,\n        7, 5, 4, 4, 2, 1, 4, 7, 9, 3, 3, 0, 8, 2, 4, 8, 6, 9, 9, 1, 5, 2, 6, 7,\n        9, 7, 0, 0, 0, 6, 8, 6, 4, 5, 5, 0, 0, 8, 6, 0, 5, 1, 0, 9, 0, 4, 6, 1,\n        0, 4, 9, 6, 3, 0, 1, 5, 4, 6, 1, 5, 0, 8, 6, 1, 8, 4, 8, 7, 0, 3, 3, 2,\n        6, 1, 7, 4, 1, 6, 3, 3, 1, 1, 0, 3, 3, 1, 0, 8, 4, 7, 8, 5, 5, 5, 4, 7,\n        1, 7, 8, 1, 4, 0, 0, 6])</pre> <p>La mayor\u00eda de las etiquetas previstas son diferentes de las etiquetas reales. Esto se debe a que comenzamos con pesos y sesgos inicializados aleatoriamente. Necesitamos entrenar el modelo, es decir, ajustar los pesos usando el gradiente descendente para hacer mejores predicciones.</p> In\u00a0[117]: Copied! <pre>outputs[:2]\n</pre> outputs[:2] Out[117]: <pre>tensor([[ 0.2190, -0.0496,  0.0746, -0.0122, -0.1212, -0.3471, -0.2582,  0.0699,\n         -0.2685,  0.2092],\n        [ 0.2571, -0.0107, -0.1650,  0.1359,  0.0424, -0.3832,  0.1630,  0.0675,\n         -0.6183,  0.1319]], grad_fn=&lt;SliceBackward&gt;)</pre> In\u00a0[120]: Copied! <pre>torch.sum(preds == labels)\n</pre> torch.sum(preds == labels) Out[120]: <pre>tensor(5)</pre> In\u00a0[121]: Copied! <pre>torch.sum(preds == labels).item()\n</pre> torch.sum(preds == labels).item() Out[121]: <pre>5</pre> In\u00a0[122]: Copied! <pre>def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n</pre> def accuracy(outputs, labels):     _, preds = torch.max(outputs, dim=1)     return torch.tensor(torch.sum(preds == labels).item() / len(preds)) <p>El operador <code>==</code> realiza una comparaci\u00f3n por elementos de dos tensores con la misma forma y devuelve un tensor de la misma forma, que contiene <code>Verdadero</code> para elementos desiguales y <code>Falso</code> para elementos iguales. Pasar el resultado a <code>torch.sum</code> devuelve el n\u00famero de etiquetas que se predijeron correctamente. Finalmente, dividimos por el n\u00famero total de im\u00e1genes para obtener la precisi\u00f3n.</p> <p>Tenga en cuenta que no necesitamos aplicar softmax a las salidas ya que sus resultados tienen el mismo orden relativo. Esto se debe a que <code>e^x</code> es una funci\u00f3n creciente, es decir, si <code>y1 &gt; y2</code>, entonces <code>e^y1 &gt; e^y2</code>. Lo mismo ocurre despu\u00e9s de promediar los valores para obtener el softmax.</p> <p>Calculemos la precisi\u00f3n del modelo actual en el primer lote de datos.</p> In\u00a0[123]: Copied! <pre>accuracy(outputs, labels)\n</pre> accuracy(outputs, labels) Out[123]: <pre>tensor(0.0391)</pre> In\u00a0[124]: Copied! <pre>probs\n</pre> probs Out[124]: <pre>tensor([[0.1284, 0.0981, 0.1111,  ..., 0.1106, 0.0788, 0.1271],\n        [0.1303, 0.0997, 0.0854,  ..., 0.1078, 0.0543, 0.1149],\n        [0.1118, 0.0880, 0.0987,  ..., 0.1279, 0.0829, 0.0943],\n        ...,\n        [0.1266, 0.0866, 0.0864,  ..., 0.1130, 0.0892, 0.1248],\n        [0.1014, 0.0808, 0.0896,  ..., 0.1411, 0.1059, 0.1175],\n        [0.1301, 0.0771, 0.0879,  ..., 0.1308, 0.0636, 0.1095]],\n       grad_fn=&lt;SoftmaxBackward&gt;)</pre> <p>La precisi\u00f3n es una forma excelente para nosotros (los humanos) de evaluar el modelo. Sin embargo, no podemos usarla como una funci\u00f3n de p\u00e9rdida para optimizar nuestro modelo mediante el descenso de gradiente por las siguientes razones:</p> <ol> <li><p>No es una funci\u00f3n diferenciable. <code>torch.max</code> y <code>==</code> son operaciones no continuas y no diferenciables, por lo que no podemos usar la precisi\u00f3n para calcular gradientes con los pesos y sesgos.</p> </li> <li><p>No tiene en cuenta las probabilidades reales predichas por el modelo, por lo que no puede proporcionar suficiente retroalimentaci\u00f3n para mejoras incrementales.</p> </li> </ol> <p>Por estas razones, la precisi\u00f3n se usa a menudo como una m\u00e9trica de evaluaci\u00f3n para la clasificaci\u00f3n, pero no como una funci\u00f3n de p\u00e9rdida. Una funci\u00f3n de p\u00e9rdida com\u00fanmente utilizada para problemas de clasificaci\u00f3n es la entrop\u00eda cruzada, que tiene la siguiente f\u00f3rmula:</p> <p></p> <p>Si bien parece complicado, en realidad es bastante simple:</p> <p>Para cada fila de salida, elija la probabilidad pronosticada para la etiqueta correcta. Por ejemplo, si las probabilidades pronosticadas para una imagen son <code>[0.1, 0.3, 0.2, ...]</code> y la etiqueta correcta es <code>1</code>, elegimos el elemento correspondiente <code>0.3</code> e ignoramos el resto. Luego, toma el logaritmo de la probabilidad seleccionada. Si la probabilidad es alta, es decir, cercana a 1, entonces su logaritmo es un valor negativo muy peque\u00f1o, cercano a 0. Y si la probabilidad es baja (cerca de 0), entonces el logaritmo es un valor negativo muy grande. Tambi\u00e9n multiplicamos el resultado por -1, lo que da como resultado un gran valor positivo de la p\u00e9rdida por malas predicciones.</p> <p></p> <ul> <li>Finalmente, tome el promedio de la entrop\u00eda cruzada en todas las filas de salida para obtener la p\u00e9rdida total de un lote de datos.</li> </ul> <p>A diferencia de la precisi\u00f3n, la entrop\u00eda cruzada es una funci\u00f3n continua y diferenciable. Tambi\u00e9n proporciona retroalimentaci\u00f3n \u00fatil para mejoras incrementales en el modelo (una probabilidad ligeramente mayor para la etiqueta correcta conduce a una p\u00e9rdida menor). Estos dos factores hacen que la entrop\u00eda cruzada sea una mejor opci\u00f3n para la funci\u00f3n de p\u00e9rdida.</p> <p>Como era de esperar, PyTorch proporciona una implementaci\u00f3n de entrop\u00eda cruzada eficiente y compatible con tensores como parte del paquete <code>torch.nn.funcional</code>. Adem\u00e1s, tambi\u00e9n realiza softmax internamente, por lo que podemos pasar directamente los resultados del modelo sin convertirlos en probabilidades.</p> In\u00a0[126]: Copied! <pre>outputs\n</pre> outputs Out[126]: <pre>tensor([[ 0.2190, -0.0496,  0.0746,  ...,  0.0699, -0.2685,  0.2092],\n        [ 0.2571, -0.0107, -0.1650,  ...,  0.0675, -0.6183,  0.1319],\n        [ 0.1358, -0.1044,  0.0111,  ...,  0.2700, -0.1633, -0.0345],\n        ...,\n        [ 0.2713, -0.1081, -0.1103,  ...,  0.1579, -0.0792,  0.2572],\n        [-0.0306, -0.2571, -0.1545,  ...,  0.3003,  0.0127,  0.1174],\n        [ 0.3300, -0.1934, -0.0621,  ...,  0.3359, -0.3857,  0.1580]],\n       grad_fn=&lt;AddmmBackward&gt;)</pre> In\u00a0[127]: Copied! <pre>loss_fn = F.cross_entropy\n</pre> loss_fn = F.cross_entropy In\u00a0[128]: Copied! <pre># Loss for current batch of data\nloss = loss_fn(outputs, labels)\nprint(loss)\n</pre> # Loss for current batch of data loss = loss_fn(outputs, labels) print(loss) <pre>tensor(2.3799, grad_fn=&lt;NllLossBackward&gt;)\n</pre> <p>Sabemos que la entrop\u00eda cruzada es el logaritmo negativo de la probabilidad predicha de la etiqueta correcta promediada sobre todas las muestras de entrenamiento. Por lo tanto, una forma de interpretar el n\u00famero resultante, p. <code>2.23</code> es mirar <code>e^-2.23</code> que es alrededor de <code>0.1</code> como la probabilidad predicha de la etiqueta correcta, en promedio. Cuanto menor sea la p\u00e9rdida, mejor ser\u00e1 el modelo.</p> In\u00a0[134]: Copied! <pre>print(math.exp(-loss.item()))\nprint(math.exp(-10))\nprint(math.exp(-2))\nprint(math.exp(-1))\nprint(math.exp(-0.2))\n</pre> print(math.exp(-loss.item())) print(math.exp(-10)) print(math.exp(-2)) print(math.exp(-1)) print(math.exp(-0.2)) <pre>0.09255626006372962\n4.5399929762484854e-05\n0.1353352832366127\n0.36787944117144233\n0.8187307530779818\n</pre> In\u00a0[135]: Copied! <pre>def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    optimizer = opt_func(model.parameters(), lr)\n    history = [] # for recording epoch-wise results\n    \n    for epoch in range(epochs):\n        \n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n\n    return history\n</pre> def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):     optimizer = opt_func(model.parameters(), lr)     history = [] # for recording epoch-wise results          for epoch in range(epochs):                  # Training Phase          for batch in train_loader:             loss = model.training_step(batch)             loss.backward()             optimizer.step()             optimizer.zero_grad()                  # Validation phase         result = evaluate(model, val_loader)         model.epoch_end(epoch, result)         history.append(result)      return history <p>La funci\u00f3n <code>fit</code> registra la p\u00e9rdida de validaci\u00f3n y la m\u00e9trica de cada \u00e9poca. Devuelve un historial de la formaci\u00f3n, \u00fatil para la depuraci\u00f3n y visualizaci\u00f3n.</p> <p>Las configuraciones como el tama\u00f1o del lote, la tasa de aprendizaje, etc. (llamadas hiperpar\u00e1metros), deben elegirse con anticipaci\u00f3n mientras se entrenan los modelos de aprendizaje autom\u00e1tico. Elegir los hiperpar\u00e1metros correctos es fundamental para entrenar un modelo razonablemente preciso en un tiempo razonable. Es un \u00e1rea activa de investigaci\u00f3n y experimentaci\u00f3n en aprendizaje autom\u00e1tico. Si\u00e9ntase libre de probar diferentes tasas de aprendizaje y ver c\u00f3mo afecta el proceso de capacitaci\u00f3n.</p> <p>Definamos la funci\u00f3n <code>evaluate</code>, utilizada en la fase de validaci\u00f3n de <code>fit</code>.</p> In\u00a0[136]: Copied! <pre>l1 = [1, 2, 3, 4, 5]\n</pre> l1 = [1, 2, 3, 4, 5] In\u00a0[137]: Copied! <pre>l2 = [x*2 for x in l1]\nl2\n</pre> l2 = [x*2 for x in l1] l2 Out[137]: <pre>[2, 4, 6, 8, 10]</pre> In\u00a0[\u00a0]: Copied! <pre>def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n</pre> def evaluate(model, val_loader):     outputs = [model.validation_step(batch) for batch in val_loader]     return model.validation_epoch_end(outputs) <p>Finalmente, redefinamos la clase <code>MnistModel</code> para incluir los m\u00e9todos adicionales <code>training_step</code>, <code>validation_step</code>, <code>validation_epoch_end</code> y <code>epoch_end</code> usados \u200b\u200bpor <code>fit</code> y <code>evaluar</code>.</p> In\u00a0[\u00a0]: Copied! <pre>class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n    \nmodel = MnistModel()\n</pre> class MnistModel(nn.Module):     def __init__(self):         super().__init__()         self.linear = nn.Linear(input_size, num_classes)              def forward(self, xb):         xb = xb.reshape(-1, 784)         out = self.linear(xb)         return out          def training_step(self, batch):         images, labels = batch          out = self(images)                  # Generate predictions         loss = F.cross_entropy(out, labels) # Calculate loss         return loss          def validation_step(self, batch):         images, labels = batch          out = self(images)                    # Generate predictions         loss = F.cross_entropy(out, labels)   # Calculate loss         acc = accuracy(out, labels)           # Calculate accuracy         return {'val_loss': loss, 'val_acc': acc}              def validation_epoch_end(self, outputs):         batch_losses = [x['val_loss'] for x in outputs]         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses         batch_accs = [x['val_acc'] for x in outputs]         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}          def epoch_end(self, epoch, result):         print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))      model = MnistModel() <p>Antes de entrenar el modelo, veamos c\u00f3mo funciona el modelo en el conjunto de validaci\u00f3n con el conjunto inicial de pesos y sesgos inicializados aleatoriamente.</p> In\u00a0[\u00a0]: Copied! <pre>result0 = evaluate(model, val_loader)\nresult0\n</pre> result0 = evaluate(model, val_loader) result0 Out[\u00a0]: <pre>{'val_acc': 0.10977056622505188, 'val_loss': 2.3349318504333496}</pre> <p>La precisi\u00f3n inicial es de alrededor del 10 %, lo que cabr\u00eda esperar de un modelo inicializado aleatoriamente (ya que tiene una probabilidad de 1 en 10 de obtener una etiqueta correcta adivinando aleatoriamente).</p> <p>Ahora estamos listos para entrenar el modelo. Entrenemos durante cinco \u00e9pocas y veamos los resultados.</p> In\u00a0[\u00a0]: Copied! <pre>history1 = fit(5, 0.001, model, train_loader, val_loader)\n</pre> history1 = fit(5, 0.001, model, train_loader, val_loader) <pre>Epoch [0], val_loss: 1.9552, val_acc: 0.6153\nEpoch [1], val_loss: 1.6839, val_acc: 0.7270\nEpoch [2], val_loss: 1.4819, val_acc: 0.7587\nEpoch [3], val_loss: 1.3295, val_acc: 0.7791\nEpoch [4], val_loss: 1.2124, val_acc: 0.7969\n</pre> <p>\u00a1Es un gran resultado! Con solo 5 \u00e9pocas de entrenamiento, nuestro modelo ha alcanzado una precisi\u00f3n de m\u00e1s del 80 % en el conjunto de validaci\u00f3n. Veamos si podemos mejorar eso entrenando para algunas \u00e9pocas m\u00e1s. Intente cambiar las tasas de aprendizaje y el n\u00famero de \u00e9pocas en cada una de las celdas a continuaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>history2 = fit(5, 0.001, model, train_loader, val_loader)\n</pre> history2 = fit(5, 0.001, model, train_loader, val_loader) <pre>Epoch [0], val_loss: 1.1205, val_acc: 0.8081\nEpoch [1], val_loss: 1.0467, val_acc: 0.8165\nEpoch [2], val_loss: 0.9862, val_acc: 0.8237\nEpoch [3], val_loss: 0.9359, val_acc: 0.8281\nEpoch [4], val_loss: 0.8934, val_acc: 0.8322\n</pre> In\u00a0[\u00a0]: Copied! <pre>history3 = fit(5, 0.001, model, train_loader, val_loader)\n</pre> history3 = fit(5, 0.001, model, train_loader, val_loader) <pre>Epoch [0], val_loss: 0.8569, val_acc: 0.8371\nEpoch [1], val_loss: 0.8254, val_acc: 0.8393\nEpoch [2], val_loss: 0.7977, val_acc: 0.8420\nEpoch [3], val_loss: 0.7733, val_acc: 0.8447\nEpoch [4], val_loss: 0.7515, val_acc: 0.8470\n</pre> In\u00a0[\u00a0]: Copied! <pre>history4 = fit(5, 0.001, model, train_loader, val_loader)\n</pre> history4 = fit(5, 0.001, model, train_loader, val_loader) <pre>Epoch [0], val_loss: 0.7320, val_acc: 0.8494\nEpoch [1], val_loss: 0.7144, val_acc: 0.8512\nEpoch [2], val_loss: 0.6985, val_acc: 0.8528\nEpoch [3], val_loss: 0.6839, val_acc: 0.8543\nEpoch [4], val_loss: 0.6706, val_acc: 0.8557\n</pre> <p>Si bien la precisi\u00f3n contin\u00faa aumentando a medida que entrenamos para m\u00e1s \u00e9pocas, las mejoras se reducen con cada \u00e9poca. Visualicemos esto usando un gr\u00e1fico de l\u00edneas.</p> In\u00a0[\u00a0]: Copied! <pre>history = [result0] + history1 + history2 + history3 + history4\naccuracies = [result['val_acc'] for result in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');\n</pre> history = [result0] + history1 + history2 + history3 + history4 accuracies = [result['val_acc'] for result in history] plt.plot(accuracies, '-x') plt.xlabel('epoch') plt.ylabel('accuracy') plt.title('Accuracy vs. No. of epochs'); <p>Est\u00e1 bastante claro en la imagen de arriba que el modelo probablemente no cruzar\u00e1 el umbral de precisi\u00f3n del 90%, incluso despu\u00e9s de entrenar durante mucho tiempo. Una posible raz\u00f3n de esto es que la tasa de aprendizaje puede ser demasiado alta. Los par\u00e1metros del modelo pueden estar \"rebotando\" alrededor del conjunto \u00f3ptimo de par\u00e1metros para la p\u00e9rdida m\u00e1s baja. Puede intentar reducir la tasa de aprendizaje y el entrenamiento durante algunas \u00e9pocas m\u00e1s para ver si ayuda.</p> <p>La raz\u00f3n m\u00e1s probable de que el modelo simplemente no sea lo suficientemente potente. Si recuerdas nuestra hip\u00f3tesis inicial, hemos asumido que la salida (en este caso las probabilidades de clase) es una funci\u00f3n lineal de la entrada (intensidades de p\u00edxeles), obtenida al realizar una multiplicaci\u00f3n matricial con los pesos matriz y sumando el sesgo. Esta es una suposici\u00f3n bastante d\u00e9bil, ya que es posible que en realidad no exista una relaci\u00f3n lineal entre las intensidades de los p\u00edxeles en una imagen y el d\u00edgito que representa. Si bien funciona razonablemente bien para un conjunto de datos simple como MNIST (lo que nos brinda una precisi\u00f3n del 85 %), necesitamos modelos m\u00e1s sofisticados que puedan capturar relaciones no lineales entre los p\u00edxeles de la imagen y las etiquetas para tareas complejas como reconocer objetos cotidianos, animales, etc.</p> <p>Si bien hasta ahora hemos estado rastreando la precisi\u00f3n general de un modelo, tambi\u00e9n es una buena idea observar los resultados del modelo en algunas im\u00e1genes de muestra. Probemos nuestro modelo con algunas im\u00e1genes del conjunto de datos de prueba predefinido de 10000 im\u00e1genes. Comenzamos recreando el conjunto de datos de prueba con la transformaci\u00f3n <code>ToTensor</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Define test dataset\ntest_dataset = MNIST(root='data/', \n                     train=False,\n                     transform=transforms.ToTensor())\n</pre> # Define test dataset test_dataset = MNIST(root='data/',                       train=False,                      transform=transforms.ToTensor()) <p>Aqu\u00ed hay una imagen de muestra del conjunto de datos.</p> In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[0]\nplt.imshow(img[0], cmap='gray')\nprint('Shape:', img.shape)\nprint('Label:', label)\n</pre> img, label = test_dataset[0] plt.imshow(img[0], cmap='gray') print('Shape:', img.shape) print('Label:', label) <pre>Shape: torch.Size([1, 28, 28])\nLabel: 7\n</pre> <p>Definamos una funci\u00f3n auxiliar <code>predict_image</code>, que devuelve la etiqueta pronosticada para un solo tensor de imagen.</p> In\u00a0[\u00a0]: Copied! <pre>def predict_image(img, model):\n    xb = img.unsqueeze(0)\n    yb = model(xb)\n    _, preds = torch.max(yb, dim=1)\n    return preds[0].item()\n</pre> def predict_image(img, model):     xb = img.unsqueeze(0)     yb = model(xb)     _, preds = torch.max(yb, dim=1)     return preds[0].item() <p><code>img.unsqueeze</code> simplemente agrega otra dimensi\u00f3n al comienzo del tensor 1x28x28, convirti\u00e9ndolo en un tensor 1x1x28x28, que el modelo ve como un lote que contiene una sola imagen.</p> <p>Vamos a probarlo con algunas im\u00e1genes.</p> In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[0]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[0] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) <pre>Label: 7 , Predicted: 7\n</pre> In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[10]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[10] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) <pre>Label: 0 , Predicted: 0\n</pre> In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[193]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[193] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) <pre>Label: 9 , Predicted: 4\n</pre> In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[1839]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[1839] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) <pre>Label: 2 , Predicted: 8\n</pre> <p>Identificar d\u00f3nde funciona mal nuestro modelo puede ayudarnos a mejorar el modelo mediante la recopilaci\u00f3n de m\u00e1s datos de entrenamiento, el aumento o la disminuci\u00f3n de la complejidad del modelo y el cambio de los par\u00e1metros exagerados.</p> <p>Como paso final, veamos tambi\u00e9n la p\u00e9rdida general y la precisi\u00f3n del modelo en el conjunto de prueba.</p> In\u00a0[\u00a0]: Copied! <pre>test_loader = DataLoader(test_dataset, batch_size=256)\nresult = evaluate(model, test_loader)\nresult\n</pre> test_loader = DataLoader(test_dataset, batch_size=256) result = evaluate(model, test_loader) result Out[\u00a0]: <pre>{'val_acc': 0.86083984375, 'val_loss': 0.6424765586853027}</pre> <p>Esperamos que esto sea similar a la precisi\u00f3n/p\u00e9rdida en el conjunto de validaci\u00f3n. De lo contrario, es posible que necesitemos un mejor conjunto de validaci\u00f3n que tenga datos y una distribuci\u00f3n similares a los del conjunto de prueba (que a menudo proviene de datos del mundo real).</p> In\u00a0[\u00a0]: Copied! <pre>torch.save(model.state_dict(), 'mnist-logistic.pth')\n</pre> torch.save(model.state_dict(), 'mnist-logistic.pth') <p>El m\u00e9todo <code>.state_dict</code> devuelve un <code>OrderedDict</code> que contiene todas las matrices de ponderaci\u00f3n y sesgo asignadas a los atributos correctos del modelo.</p> In\u00a0[\u00a0]: Copied! <pre>model.state_dict()\n</pre> model.state_dict() Out[\u00a0]: <pre>OrderedDict([('linear.weight',\n              tensor([[ 0.0057,  0.0222, -0.0220,  ..., -0.0021, -0.0115, -0.0308],\n                      [-0.0353,  0.0083, -0.0307,  ...,  0.0345, -0.0087,  0.0200],\n                      [ 0.0104,  0.0158,  0.0225,  ...,  0.0255,  0.0227, -0.0346],\n                      ...,\n                      [-0.0097, -0.0173, -0.0154,  ...,  0.0025, -0.0274, -0.0276],\n                      [ 0.0272, -0.0156,  0.0029,  ...,  0.0217,  0.0286, -0.0114],\n                      [-0.0018, -0.0293, -0.0191,  ..., -0.0297,  0.0291,  0.0212]])),\n             ('linear.bias',\n              tensor([-0.0322,  0.1078, -0.0008, -0.0159,  0.0346,  0.0235, -0.0047,  0.0277,\n                      -0.0684, -0.0356]))])</pre> <p>Para cargar los pesos del modelo, podemos instanciar un nuevo objeto de la clase <code>MnistModel</code> y usar el m\u00e9todo <code>.load_state_dict</code>.</p> In\u00a0[\u00a0]: Copied! <pre>model2 = MnistModel()\n</pre> model2 = MnistModel() In\u00a0[\u00a0]: Copied! <pre>model2.state_dict()\n</pre> model2.state_dict() Out[\u00a0]: <pre>OrderedDict([('linear.weight',\n              tensor([[-0.0168, -0.0088, -0.0010,  ..., -0.0233,  0.0253,  0.0161],\n                      [-0.0139, -0.0039,  0.0011,  ...,  0.0178, -0.0125, -0.0090],\n                      [-0.0341, -0.0001,  0.0089,  ..., -0.0282,  0.0181,  0.0251],\n                      ...,\n                      [-0.0274, -0.0289, -0.0180,  ..., -0.0197, -0.0173,  0.0262],\n                      [ 0.0318,  0.0125,  0.0178,  ..., -0.0192,  0.0083, -0.0032],\n                      [-0.0264, -0.0261,  0.0058,  ..., -0.0005,  0.0135,  0.0287]])),\n             ('linear.bias',\n              tensor([ 0.0014, -0.0254, -0.0085, -0.0081, -0.0333,  0.0109, -0.0128, -0.0342,\n                       0.0204,  0.0232]))])</pre> In\u00a0[\u00a0]: Copied! <pre>evaluate(model2, test_loader)\n</pre> evaluate(model2, test_loader) Out[\u00a0]: <pre>{'val_acc': 0.08339843899011612, 'val_loss': 2.325232744216919}</pre> In\u00a0[\u00a0]: Copied! <pre>model2.load_state_dict(torch.load('mnist-logistic.pth'))\nmodel2.state_dict()\n</pre> model2.load_state_dict(torch.load('mnist-logistic.pth')) model2.state_dict() Out[\u00a0]: <pre>OrderedDict([('linear.weight',\n              tensor([[ 0.0057,  0.0222, -0.0220,  ..., -0.0021, -0.0115, -0.0308],\n                      [-0.0353,  0.0083, -0.0307,  ...,  0.0345, -0.0087,  0.0200],\n                      [ 0.0104,  0.0158,  0.0225,  ...,  0.0255,  0.0227, -0.0346],\n                      ...,\n                      [-0.0097, -0.0173, -0.0154,  ...,  0.0025, -0.0274, -0.0276],\n                      [ 0.0272, -0.0156,  0.0029,  ...,  0.0217,  0.0286, -0.0114],\n                      [-0.0018, -0.0293, -0.0191,  ..., -0.0297,  0.0291,  0.0212]])),\n             ('linear.bias',\n              tensor([-0.0322,  0.1078, -0.0008, -0.0159,  0.0346,  0.0235, -0.0047,  0.0277,\n                      -0.0684, -0.0356]))])</pre> <p>Solo como una verificaci\u00f3n de cordura, verifiquemos que este modelo tenga la misma p\u00e9rdida y precisi\u00f3n en el conjunto de prueba que antes.</p> In\u00a0[\u00a0]: Copied! <pre>test_loader = DataLoader(test_dataset, batch_size=256)\nresult = evaluate(model2, test_loader)\nresult\n</pre> test_loader = DataLoader(test_dataset, batch_size=256) result = evaluate(model2, test_loader) result Out[\u00a0]: <pre>{'val_acc': 0.86083984375, 'val_loss': 0.6424765586853027}</pre> <p>Como paso final, podemos guardar y confirmar nuestro trabajo utilizando la biblioteca <code>jovian</code>. Junto con el cuaderno, tambi\u00e9n podemos acoplar las pesas de nuestro modelo entrenado, para poder utilizarlo m\u00e1s adelante.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"03_pytorch_logistic_regression_es/#03-procesamiento-de-imagenes-y-regresion-logistica","title":"03. Procesamiento de Im\u00e1genes y Regresi\u00f3n Log\u00edstica\u00b6","text":""},{"location":"03_pytorch_logistic_regression_es/#parte-3-de-aprendizaje-profundo-con-pytorch-de-cero-a-gan","title":"Parte 3 de \"Aprendizaje profundo con Pytorch: de cero a GAN\"\u00b6","text":"<p>Esta serie de did\u00e1cticas es una introducci\u00f3n pr\u00e1ctica y amena para principiantes en aprender en profundidad con la ayuda de PyTorch, una biblioteca de recursos de neuronas de c\u00f3digo abierto. Estos tutoriales adoptan un enfoque pr\u00e1ctico centrado en la codificaci\u00f3n. La mejor manera de aprender hardware es ejecutar el c\u00f3digo y experimentarlo usted mismo.</p>"},{"location":"03_pytorch_logistic_regression_es/#como-ejecutar-el-codigo","title":"C\u00f3mo ejecutar el c\u00f3digo\u00b6","text":"<p>Este tutorial es un ejecutable Jupyter notebook alojado en Jovian. Puede ejecutar este tutorial y experimentar con los ejemplos de c\u00f3digo de varias maneras: usando recursos gratuitos en l\u00ednea (recomendado) o en su computadora.</p>"},{"location":"03_pytorch_logistic_regression_es/#opcion-1-ejecutar-usando-recursos-en-linea-gratuitos-1-clic-recomendado","title":"Opci\u00f3n 1: Ejecutar usando recursos en l\u00ednea gratuitos (1-clic, recomendado)\u00b6","text":"<p>La forma m\u00e1s f\u00e1cil de comenzar a ejecutar el c\u00f3digo es hacer clic en el bot\u00f3n Ejecutar en la parte superior de esta p\u00e1gina y seleccionar Ejecutar en Colab. Google Colab es una plataforma en l\u00ednea gratuita para ejecutar port\u00e1tiles Jupyter utilizando la infraestructura de nube de Google. Tambi\u00e9n puede seleccionar \"Ejecutar en Binder\" o \"Ejecutar en Kaggle\" si tiene problemas para ejecutar el cuaderno en Google Colab.</p>"},{"location":"03_pytorch_logistic_regression_es/#opcion-2-ejecutar-en-su-computadora-localmente","title":"Opci\u00f3n 2: Ejecutar en su computadora localmente\u00b6","text":"<p>Para ejecutar el c\u00f3digo en su computadora localmente, deber\u00e1 configurar Python, descargar el cuaderno e instalar las bibliotecas requeridas. Recomendamos usar la distribuci\u00f3n Conda de Python. Haga clic en el bot\u00f3n Ejecutar en la parte superior de esta p\u00e1gina, seleccione la opci\u00f3n Ejecutar localmente y siga las instrucciones.</p> <p>Cuadernos de Jupyter: Este tutorial es un Cuaderno de Jupyter - un documento hecho de celdas. Cada celda puede contener c\u00f3digo escrito en Python o explicaciones en ingl\u00e9s sencillo. Puede ejecutar celdas de c\u00f3digo y ver los resultados, por ejemplo, n\u00fameros, mensajes, gr\u00e1ficos, tablas, archivos, etc., instant\u00e1neamente dentro del cuaderno. Jupyter es una poderosa plataforma para la experimentaci\u00f3n y el an\u00e1lisis. No tenga miedo de perder el tiempo con el c\u00f3digo y romper cosas: aprender\u00e1 mucho al encontrar y corregir errores. Puede utilizar la opci\u00f3n de men\u00fa \"Kernel &gt; Reiniciar y borrar salida\" o \"Editar &gt; Borrar salidas\" para borrar todas las salidas y empezar de nuevo desde arriba.</p>"},{"location":"03_pytorch_logistic_regression_es/#trabajar-con-imagenes","title":"Trabajar con im\u00e1genes\u00b6","text":"<p>En este tutorial, usaremos nuestro conocimiento existente de PyTorch y la regresi\u00f3n lineal para resolver un tipo de problema muy diferente: clasificaci\u00f3n de im\u00e1genes. Usaremos la famosa Base de datos de d\u00edgitos escritos a mano del MNIST como nuestro conjunto de datos de entrenamiento. Consiste en im\u00e1genes en escala de grises de 28 px por 28 px de d\u00edgitos escritos a mano (0 a 9) y etiquetas para cada imagen que indican qu\u00e9 d\u00edgito representa. Aqu\u00ed hay algunas im\u00e1genes de muestra del conjunto de datos:</p> <p></p>"},{"location":"03_pytorch_logistic_regression_es/#conjuntos-de-datos-de-entrenamiento-y-validacion","title":"Conjuntos de datos de entrenamiento y validaci\u00f3n\u00b6","text":"<p>Al construir modelos de aprendizaje autom\u00e1tico del mundo real, es bastante com\u00fan dividir el conjunto de datos en tres partes:</p> <ol> <li>Conjunto de entrenamiento: se utiliza para entrenar el modelo, es decir, calcular la p\u00e9rdida y ajustar los pesos del modelo mediante el descenso de gradiente.</li> <li>Conjunto de validaci\u00f3n: se utiliza para evaluar el modelo durante el entrenamiento, ajustar los hiperpar\u00e1metros (tasa de aprendizaje, etc.) y elegir la mejor versi\u00f3n del modelo.</li> <li>Conjunto de prueba: se utiliza para comparar diferentes modelos o enfoques e informar sobre la precisi\u00f3n final del modelo.</li> </ol> <p>En el conjunto de datos del MNIST, hay 60 000 im\u00e1genes de entrenamiento y 10 000 im\u00e1genes de prueba. El conjunto de prueba est\u00e1 estandarizado para que diferentes investigadores puedan informar los resultados de sus modelos contra la misma colecci\u00f3n de im\u00e1genes.</p> <p>Dado que no hay un conjunto de validaci\u00f3n predefinido, debemos dividir manualmente las 60 000 im\u00e1genes en conjuntos de datos de entrenamiento y validaci\u00f3n. Reservemos 10 000 im\u00e1genes elegidas al azar para la validaci\u00f3n. Podemos hacer esto usando el m\u00e9todo <code>random_spilt</code> de PyTorch.</p>"},{"location":"03_pytorch_logistic_regression_es/#modelo","title":"Modelo\u00b6","text":"<p>Ahora que hemos preparado nuestros cargadores de datos, podemos definir nuestro modelo.</p> <p>Un modelo deregresi\u00f3n log\u00edsticaes casi id\u00e9ntico a un modelo de regresi\u00f3n lineal. Contiene matrices de ponderaci\u00f3n y sesgo, y la salida se obtiene mediante operaciones matriciales simples (<code>pred = x @ w.t() + b</code>). Como hicimos con la regresi\u00f3n lineal, podemos usar <code>nn.Linear</code> para crear el modelo en lugar de crear e inicializar manualmente las matrices.</p> <p>Dado que <code>nn.Linear</code> espera que cada ejemplo de entrenamiento sea un vector, cada tensor de imagen <code>1x28x28</code> se _aplana_ en un vector de tama\u00f1o 784 `(2828)` antes de pasar al modelo.</p> <ul> <li>El resultado de cada imagen es un vector de tama\u00f1o 10, en el que cada elemento representa la probabilidad de una etiqueta de destino en particular (es decir, de 0 a 9). La etiqueta predicha para una imagen es simplemente la que tiene la probabilidad m\u00e1s alta.</li> </ul>"},{"location":"03_pytorch_logistic_regression_es/#metrica-de-evaluacion-y-funcion-de-perdida","title":"M\u00e9trica de evaluaci\u00f3n y funci\u00f3n de p\u00e9rdida\u00b6","text":"<p>Al igual que con la regresi\u00f3n lineal, necesitamos una forma de evaluar qu\u00e9 tan bien est\u00e1 funcionando nuestro modelo. Una forma natural de hacer esto ser\u00eda encontrar el porcentaje de etiquetas que se predijeron correctamente, es decir,. la precisi\u00f3n de las predicciones.</p>"},{"location":"03_pytorch_logistic_regression_es/#entrenando-al-modelo","title":"Entrenando al modelo\u00b6","text":"<p>Ahora que hemos definido los cargadores de datos, el modelo, la funci\u00f3n de p\u00e9rdida y el optimizador, estamos listos para entrenar el modelo. El proceso de entrenamiento es id\u00e9ntico a la regresi\u00f3n lineal, con la adici\u00f3n de una \"fase de validaci\u00f3n\" para evaluar el modelo en cada \u00e9poca. As\u00ed es como se ve en pseudoc\u00f3digo:</p> <pre><code>for epoch in range(num_epochs):\n    # Fase de entrenamiento\n    for batch in train_loader:\n        # Generar predicciones\n        # Calcular p\u00e9rdida\n        # Calcular gradientes\n        # Actualizar pesos\n        # Restablecer gradientes\n    \n    # Fase de validaci\u00f3n\n    for batch in val_loader:\n        # Generar predicciones\n        # Calcular p\u00e9rdida\n        # Calcular m\u00e9tricas (precisi\u00f3n, etc.)\n    # Calcule la p\u00e9rdida de validaci\u00f3n promedio y las m\u00e9tricas\n    \n    # \u00c9poca de registro, p\u00e9rdida y m\u00e9tricas para inspecci\u00f3n\n</code></pre> <p>Algunas partes del ciclo de entrenamiento son espec\u00edficas del problema espec\u00edfico que estamos resolviendo (por ejemplo, funci\u00f3n de p\u00e9rdida, m\u00e9tricas, etc.), mientras que otras son gen\u00e9ricas y se pueden aplicar a cualquier problema de aprendizaje profundo.</p> <p>Incluiremos las partes independientes del problema dentro de una funci\u00f3n llamada <code>fit</code>, que se usar\u00e1 para entrenar el modelo. Las partes espec\u00edficas del problema se implementar\u00e1n agregando nuevos m\u00e9todos a la clase <code>nn.Module</code>.</p>"},{"location":"03_pytorch_logistic_regression_es/#pruebas-con-imagenes-individuales","title":"Pruebas con im\u00e1genes individuales\u00b6","text":""},{"location":"03_pytorch_logistic_regression_es/#saving-and-loading-the-model","title":"Saving and loading the model\u00b6","text":""},{"location":"03_pytorch_logistic_regression_es/#otras-lecturas","title":"Otras lecturas\u00b6","text":"<p>Aqu\u00ed hay algunas referencias para leer m\u00e1s: Para un tratamiento m\u00e1s matem\u00e1tico, consulte el popular curso Aprendizaje autom\u00e1tico en Coursera. La mayor\u00eda de las im\u00e1genes utilizadas en esta serie de tutoriales han sido tomadas de este curso. El ciclo de entrenamiento definido en este cuaderno se inspir\u00f3 en Cuadernos de desarrollo de FastAI que contienen una gran cantidad de otras cosas \u00fatiles si puede leer y comprender el c\u00f3digo.</p> <ul> <li>Para una inmersi\u00f3n profunda en softmax y entrop\u00eda cruzada, consulte [esta publicaci\u00f3n de blog en DeepNotes] (https://deepnotes.io/softmax-crossentropy).</li> </ul>"},{"location":"04-00_pytorch_fundamentals/","title":"00. Fundamentos de PyTorch","text":"<p>Ver c\u00f3digo fuente | Ver diapositivas | Ver v\u00eddeo tutorial</p> In\u00a0[\u00a0]: Copied! <pre>import torch\ntorch.__version__\n</pre> import torch torch.__version__ <p>Maravilloso, parece que tenemos PyTorch 1.10.0+.</p> <p>Esto significa que si est\u00e1 leyendo estos materiales, ver\u00e1 la mayor compatibilidad con PyTorch 1.10.0+; sin embargo, si su n\u00famero de versi\u00f3n es mucho mayor, es posible que note algunas inconsistencias.</p> <p>Y si tiene alg\u00fan problema, publ\u00edquelo en el curso [p\u00e1gina de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).</p> In\u00a0[\u00a0]: Copied! <pre># Escalar\nscalar = torch.tensor(7)\nscalar\n</pre> # Escalar scalar = torch.tensor(7) scalar <p>\u00bfVes c\u00f3mo lo anterior imprimi\u00f3 <code>tensor (7)</code>?</p> <p>Eso significa que aunque \"escalar\" es un n\u00famero \u00fanico, es del tipo \"torch.Tensor\".</p> <p>Podemos verificar las dimensiones de un tensor usando el atributo <code>ndim</code>.</p> In\u00a0[\u00a0]: Copied! <pre>scalar.ndim\n</pre> scalar.ndim <p>\u00bfQu\u00e9 pasar\u00eda si quisi\u00e9ramos recuperar el n\u00famero del tensor?</p> <p>Como en, \u00bfconvertirlo de <code>torch.Tensor</code> a un entero de Python?</p> <p>Para hacerlo podemos usar el m\u00e9todo <code>item()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Obtener el n\u00famero de Python dentro de un tensor (solo funciona con tensores de un elemento)\nscalar.item()\n</pre> # Obtener el n\u00famero de Python dentro de un tensor (solo funciona con tensores de un elemento) scalar.item() <p>Bien, ahora veamos un vector.</p> <p>Un vector es un tensor de una sola dimensi\u00f3n pero puede contener muchos n\u00fameros.</p> <p>Por ejemplo, podr\u00eda tener un vector <code>[3, 2]</code> para describir <code>[dormitorios, ba\u00f1os]</code> en su casa. O podr\u00eda tener \"[3, 2, 2]\" para describir \"[dormitorios, ba\u00f1os, aparcamientos]\" en su casa.</p> <p>La tendencia importante aqu\u00ed es que un vector es flexible en lo que puede representar (lo mismo ocurre con los tensores).</p> In\u00a0[\u00a0]: Copied! <pre># Vector\nvector = torch.tensor([7, 7])\nvector\n</pre> # Vector vector = torch.tensor([7, 7]) vector <p>Maravilloso, \"vector\" ahora contiene dos 7, mi n\u00famero favorito.</p> <p>\u00bfCu\u00e1ntas dimensiones crees que tendr\u00e1?</p> In\u00a0[\u00a0]: Copied! <pre># Verifique el n\u00famero de dimensiones del vector.\nvector.ndim\n</pre> # Verifique el n\u00famero de dimensiones del vector. vector.ndim <p>Hmm, eso es extra\u00f1o, \"vector\" contiene dos n\u00fameros pero solo tiene una dimensi\u00f3n.</p> <p>Te contar\u00e9 un truco.</p> <p>Puede saber la cantidad de dimensiones que tiene un tensor en PyTorch por la cantidad de corchetes en el exterior (<code>[</code>) y solo necesita contar un lado.</p> <p>\u00bfCu\u00e1ntos corchetes tiene \"vector\"?</p> <p>Otro concepto importante para los tensores es su atributo de \"forma\". La forma te dice c\u00f3mo est\u00e1n dispuestos los elementos dentro de ella.</p> <p>Veamos la forma del \"vector\".</p> In\u00a0[\u00a0]: Copied! <pre># Comprobar la forma del vector\nvector.shape\n</pre> # Comprobar la forma del vector vector.shape <p>Lo anterior devuelve <code>torch.Size([2])</code> lo que significa que nuestro vector tiene la forma <code>[2]</code>. Esto se debe a los dos elementos que colocamos entre corchetes (<code>[7, 7]</code>).</p> <p>Veamos ahora una matriz.</p> In\u00a0[\u00a0]: Copied! <pre># Matriz\nMATRIX = torch.tensor([[7, 8], \n                       [9, 10]])\nMATRIX\n</pre> # Matriz MATRIX = torch.tensor([[7, 8],                         [9, 10]]) MATRIX <p>\u00a1Guau! \u00a1M\u00e1s n\u00fameros! Las matrices son tan flexibles como los vectores, excepto que tienen una dimensi\u00f3n extra.</p> In\u00a0[\u00a0]: Copied! <pre># Comprobar n\u00famero de dimensiones\nMATRIX.ndim\n</pre> # Comprobar n\u00famero de dimensiones MATRIX.ndim <p><code>MATRIX</code> tiene dos dimensiones (\u00bfcontaste el n\u00famero de corchetes en el exterior de un lado?). \u00bfQu\u00e9 \"forma\" crees que tendr\u00e1?</p> In\u00a0[\u00a0]: Copied! <pre>MATRIX.shape\n</pre> MATRIX.shape <p>Obtenemos el resultado <code>torch.Size([2, 2])</code> porque <code>MATRIX</code> tiene dos elementos de profundidad y dos elementos de ancho.</p> <p>\u00bfQu\u00e9 tal si creamos un tensor?</p> In\u00a0[\u00a0]: Copied! <pre># Tensor\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n</pre> # Tensor TENSOR = torch.tensor([[[1, 2, 3],                         [3, 6, 9],                         [2, 4, 5]]]) TENSOR <p>\u00a1Guau! Que bonito tensor.</p> <p>Quiero enfatizar que los tensores pueden representar casi cualquier cosa.</p> <p>El que acabamos de crear podr\u00edan ser las cifras de ventas de una tienda de carnes y mantequilla de almendras (dos de mis comidas favoritas).</p> <p></p> <p>\u00bfCu\u00e1ntas dimensiones crees que tiene? (pista: utilice el truco de contar corchetes)</p> In\u00a0[\u00a0]: Copied! <pre># Consultar n\u00famero de dimensiones para TENSOR\nTENSOR.ndim\n</pre> # Consultar n\u00famero de dimensiones para TENSOR TENSOR.ndim <p>\u00bfY qu\u00e9 pasa con su forma?</p> In\u00a0[\u00a0]: Copied! <pre># Comprobar la forma del TENSOR\nTENSOR.shape\n</pre> # Comprobar la forma del TENSOR TENSOR.shape <p>Muy bien, genera <code>torch.Size([1, 3, 3])</code>.</p> <p>Las dimensiones van de exterior a interior.</p> <p>Eso significa que hay 1 dimensi\u00f3n de 3 por 3.</p> <p>![ejemplo de diferentes dimensiones tensoriales](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch- Different-tensor-dimensions.png)</p> <p>Nota: Es posible que hayas notado que uso letras min\u00fasculas para <code>escalar</code> y <code>vector</code> y letras may\u00fasculas para <code>MATRIX</code> y <code>TENSOR</code>. Esto fue a prop\u00f3sito. En la pr\u00e1ctica, a menudo ver\u00e1s escalares y vectores indicados con letras min\u00fasculas como \"y\" o \"a\". Y matrices y tensores indicados con letras may\u00fasculas como \"X\" o \"W\".</p> <p>Tambi\u00e9n puedes notar que los nombres matriz y tensor se usan indistintamente. Esto es com\u00fan. Sin embargo, dado que en PyTorch a menudo se trata con <code>torch.Tensor</code>s (de ah\u00ed el nombre del tensor), la forma y las dimensiones de lo que hay dentro dictar\u00e1n lo que realmente es.</p> <p>Resumamos.</p> Nombre \u00bfQu\u00e9 es? N\u00famero de dimensiones Inferior o superior (normalmente/ejemplo) escalar un solo n\u00famero 0 Inferior (<code>a</code>) vector un n\u00famero con direcci\u00f3n (por ejemplo, velocidad del viento con direcci\u00f3n) pero tambi\u00e9n puede tener muchos otros n\u00fameros 1 Inferior (<code>y</code>) matriz una matriz bidimensional de n\u00fameros 2 Superior (<code>Q</code>) tensor una matriz de n\u00fameros n-dimensional puede ser cualquier n\u00famero, un tensor de dimensi\u00f3n 0 es un escalar, un tensor de dimensi\u00f3n 1 es un vector Superior (<code>X</code>) <p></p> In\u00a0[\u00a0]: Copied! <pre># Crea un tensor aleatorio de tama\u00f1o (3, 4)\nrandom_tensor = torch.rand(size=(3, 4))\nrandom_tensor, random_tensor.dtype\n</pre> # Crea un tensor aleatorio de tama\u00f1o (3, 4) random_tensor = torch.rand(size=(3, 4)) random_tensor, random_tensor.dtype <p>La flexibilidad de <code>torch.rand()</code> es que podemos ajustar el <code>tama\u00f1o</code> para que sea lo que queramos.</p> <p>Por ejemplo, supongamos que desea un tensor aleatorio con la forma de imagen com\u00fan de <code>[224, 224, 3]</code> (<code>[alto, ancho, color_channels</code>]).</p> In\u00a0[\u00a0]: Copied! <pre># Crea un tensor aleatorio de tama\u00f1o (224, 224, 3)\nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n</pre> # Crea un tensor aleatorio de tama\u00f1o (224, 224, 3) random_image_size_tensor = torch.rand(size=(224, 224, 3)) random_image_size_tensor.shape, random_image_size_tensor.ndim In\u00a0[\u00a0]: Copied! <pre># Crea un tensor de todos ceros.\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n</pre> # Crea un tensor de todos ceros. zeros = torch.zeros(size=(3, 4)) zeros, zeros.dtype <p>Podemos hacer lo mismo para crear un tensor de todos unos excepto usar <code>torch.ones()</code>  en su lugar.</p> In\u00a0[\u00a0]: Copied! <pre># Crea un tensor de todos unos.\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n</pre> # Crea un tensor de todos unos. ones = torch.ones(size=(3, 4)) ones, ones.dtype In\u00a0[\u00a0]: Copied! <pre># Utilice torch.arange(), torch.range() est\u00e1 en desuso\nzero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n\n# Crea un rango de valores del 0 al 10\nzero_to_ten = torch.arange(start=0, end=10, step=1)\nzero_to_ten\n</pre> # Utilice torch.arange(), torch.range() est\u00e1 en desuso zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future  # Crea un rango de valores del 0 al 10 zero_to_ten = torch.arange(start=0, end=10, step=1) zero_to_ten <p>A veces es posible que desees un tensor de cierto tipo con la misma forma que otro tensor.</p> <p>Por ejemplo, un tensor de todos ceros con la misma forma que un tensor anterior.</p> <p>Para hacerlo, puede utilizar <code>torch.zeros_like(input)</code> o [<code>torch.ones_like(input)</code>](https ://pytorch.org/docs/1.9.1/generated/torch.ones_like.html) que devuelven un tensor lleno de ceros o unos con la misma forma que la \"entrada\", respectivamente.</p> In\u00a0[\u00a0]: Copied! <pre># Tambi\u00e9n puede crear un tensor de ceros similar a otro tensor.\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n</pre> # Tambi\u00e9n puede crear un tensor de ceros similar a otro tensor. ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape ten_zeros In\u00a0[\u00a0]: Copied! <pre># El tipo de datos predeterminado para tensores es float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n</pre> # El tipo de datos predeterminado para tensores es float32 float_32_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed                                device=None, # defaults to None, which uses the default tensor type                                requires_grad=False) # if True, operations performed on the tensor are recorded   float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device <p>Aparte de los problemas de forma (las formas de los tensores no coinciden), dos de los otros problemas m\u00e1s comunes que encontrar\u00e1 en PyTorch son problemas de tipo de datos y de dispositivo.</p> <p>Por ejemplo, uno de los tensores es <code>torch.float32</code> y el otro es <code>torch.float16</code> (a PyTorch a menudo le gusta que los tensores tengan el mismo formato).</p> <p>O uno de sus tensores est\u00e1 en la CPU y el otro en la GPU (a PyTorch le gusta que los c\u00e1lculos entre tensores se realicen en el mismo dispositivo).</p> <p>Veremos m\u00e1s sobre este dispositivo m\u00e1s adelante.</p> <p>Por ahora creemos un tensor con <code>dtype=torch.float16</code>.</p> In\u00a0[\u00a0]: Copied! <pre>float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n</pre> float_16_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=torch.float16) # torch.half would also work  float_16_tensor.dtype In\u00a0[\u00a0]: Copied! <pre># Crear un tensor\nsome_tensor = torch.rand(3, 4)\n\n# Conoce detalles al respecto\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n</pre> # Crear un tensor some_tensor = torch.rand(3, 4)  # Conoce detalles al respecto print(some_tensor) print(f\"Shape of tensor: {some_tensor.shape}\") print(f\"Datatype of tensor: {some_tensor.dtype}\") print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU <p>Nota: Cuando tienes problemas en PyTorch, muy a menudo tiene que ver con uno de los tres atributos anteriores. Entonces, cuando aparezcan los mensajes de error, cante una peque\u00f1a canci\u00f3n llamada \"qu\u00e9, qu\u00e9, d\u00f3nde\":</p> <ul> <li>\"\u00bfQu\u00e9 forma tienen mis tensores? \u00bfQu\u00e9 tipo de datos son y d\u00f3nde se almacenan? \u00bfQu\u00e9 forma, qu\u00e9 tipo de datos, d\u00f3nde, d\u00f3nde, d\u00f3nde\"</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Crea un tensor de valores y agr\u00e9gale un n\u00famero.\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n</pre> # Crea un tensor de valores y agr\u00e9gale un n\u00famero. tensor = torch.tensor([1, 2, 3]) tensor + 10 In\u00a0[\u00a0]: Copied! <pre># Multiplicalo por 10\ntensor * 10\n</pre> # Multiplicalo por 10 tensor * 10 <p>Observe c\u00f3mo los valores del tensor anteriores no terminaron siendo \"tensor ([110, 120, 130])\", esto se debe a que los valores dentro del tensor no cambian a menos que sean reasignados.</p> In\u00a0[\u00a0]: Copied! <pre># Los tensores no cambian a menos que se reasignen\ntensor\n</pre> # Los tensores no cambian a menos que se reasignen tensor <p>Restemos un n\u00famero y esta vez reasignaremos la variable <code>tensor</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Restar y reasignar\ntensor = tensor - 10\ntensor\n</pre> # Restar y reasignar tensor = tensor - 10 tensor In\u00a0[\u00a0]: Copied! <pre># Agregar y reasignar\ntensor = tensor + 10\ntensor\n</pre> # Agregar y reasignar tensor = tensor + 10 tensor <p>PyTorch tambi\u00e9n tiene un mont\u00f3n de funciones integradas como <code>torch.mul()</code> (abreviatura de multiplicaci\u00f3n) y <code>torch.add()</code> para realizar operaciones b\u00e1sicas.</p> In\u00a0[\u00a0]: Copied! <pre># Tambi\u00e9n puede utilizar funciones de antorcha.\ntorch.multiply(tensor, 10)\n</pre> # Tambi\u00e9n puede utilizar funciones de antorcha. torch.multiply(tensor, 10) In\u00a0[\u00a0]: Copied! <pre># El tensor original a\u00fan no ha cambiado.\ntensor\n</pre> # El tensor original a\u00fan no ha cambiado. tensor <p>Sin embargo, es m\u00e1s com\u00fan usar s\u00edmbolos de operador como <code>*</code> en lugar de <code>torch.mul()</code></p> In\u00a0[\u00a0]: Copied! <pre># Multiplicaci\u00f3n por elementos (cada elemento multiplica su equivalente, \u00edndice 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n</pre> # Multiplicaci\u00f3n por elementos (cada elemento multiplica su equivalente, \u00edndice 0-&gt;0, 1-&gt;1, 2-&gt;2) print(tensor, \"*\", tensor) print(\"Equals:\", tensor * tensor) In\u00a0[\u00a0]: Copied! <pre>import torch\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n</pre> import torch tensor = torch.tensor([1, 2, 3]) tensor.shape <p>La diferencia entre la multiplicaci\u00f3n por elementos y la multiplicaci\u00f3n de matrices es la suma de valores.</p> <p>Para nuestra variable <code>tensor</code> con valores <code>[1, 2, 3]</code>:</p> Operaci\u00f3n C\u00e1lculo C\u00f3digo Multiplicaci\u00f3n por elementos <code>[1*1, 2*2, 3*3]</code> = <code>[1, 4, 9]</code> <code>tensor * tensor</code> Multiplicaci\u00f3n de matrices <code>[1*1 + 2*2 + 3*3]</code> = <code>[14]</code> <code>tensor.matmul(tensor)</code> In\u00a0[\u00a0]: Copied! <pre># Multiplicaci\u00f3n de matrices por elementos\ntensor * tensor\n</pre> # Multiplicaci\u00f3n de matrices por elementos tensor * tensor In\u00a0[\u00a0]: Copied! <pre># Multiplicaci\u00f3n de matrices\ntorch.matmul(tensor, tensor)\n</pre> # Multiplicaci\u00f3n de matrices torch.matmul(tensor, tensor) In\u00a0[\u00a0]: Copied! <pre># Tambi\u00e9n se puede utilizar el s\u00edmbolo \"@\" para la multiplicaci\u00f3n de matrices, aunque no se recomienda.\ntensor @ tensor\n</pre> # Tambi\u00e9n se puede utilizar el s\u00edmbolo \"@\" para la multiplicaci\u00f3n de matrices, aunque no se recomienda. tensor @ tensor <p>Puedes hacer la multiplicaci\u00f3n de matrices a mano, pero no es recomendable.</p> <p>El m\u00e9todo incorporado <code>torch.matmul()</code> es m\u00e1s r\u00e1pido.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\n# Multiplicaci\u00f3n de matrices a mano.\n# (evite a toda costa realizar operaciones con bucles for, son computacionalmente costosos)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n</pre> %%time # Multiplicaci\u00f3n de matrices a mano. # (evite a toda costa realizar operaciones con bucles for, son computacionalmente costosos) value = 0 for i in range(len(tensor)):   value += tensor[i] * tensor[i] value In\u00a0[\u00a0]: Copied! <pre>%%time\ntorch.matmul(tensor, tensor)\n</pre> %%time torch.matmul(tensor, tensor) In\u00a0[\u00a0]: Copied! <pre># Las formas deben estar en la forma correcta.\ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n</pre> # Las formas deben estar en la forma correcta. tensor_A = torch.tensor([[1, 2],                          [3, 4],                          [5, 6]], dtype=torch.float32)  tensor_B = torch.tensor([[7, 10],                          [8, 11],                           [9, 12]], dtype=torch.float32)  torch.matmul(tensor_A, tensor_B) # (this will error) <p>Podemos hacer que la multiplicaci\u00f3n de matrices funcione entre <code>tensor_A</code> y <code>tensor_B</code> haciendo que sus dimensiones internas coincidan.</p> <p>Una de las formas de hacer esto es con una transposici\u00f3n (cambiar las dimensiones de un tensor determinado).</p> <p>Puede realizar transposiciones en PyTorch usando:</p> <ul> <li><code>torch.transpose(input, dim0, dim1)</code> - donde <code>input</code> es el tensor que se desea transponer y <code>dim0</code> y <code>dim1</code> son las dimensiones que se van a intercambiar.</li> <li><code>tensor.T</code> - donde <code>tensor</code> es el tensor que se desea transponer.</li> </ul> <p>Probemos esto \u00faltimo.</p> In\u00a0[\u00a0]: Copied! <pre># Ver tensor_A y tensor_B\nprint(tensor_A)\nprint(tensor_B)\n</pre> # Ver tensor_A y tensor_B print(tensor_A) print(tensor_B) In\u00a0[\u00a0]: Copied! <pre># Ver tensor_A y tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n</pre> # Ver tensor_A y tensor_B.T print(tensor_A) print(tensor_B.T) In\u00a0[\u00a0]: Copied! <pre># La operaci\u00f3n funciona cuando se transpone tensor_B\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n</pre> # La operaci\u00f3n funciona cuando se transpone tensor_B print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\") print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\") print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\") print(\"Output:\\n\") output = torch.matmul(tensor_A, tensor_B.T) print(output)  print(f\"\\nOutput shape: {output.shape}\") <p>Tambi\u00e9n puede usar <code>torch.mm()</code> que es una abreviatura de <code>torch.matmul()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># torch.mm es un atajo para matmul\ntorch.mm(tensor_A, tensor_B.T)\n</pre> # torch.mm es un atajo para matmul torch.mm(tensor_A, tensor_B.T) <p>Sin la transpuesta, las reglas de multiplicaci\u00f3n de matrices no se cumplen y obtenemos un error como el anterior.</p> <p>\u00bfQu\u00e9 tal una imagen?</p> <p></p> <p>Puede crear sus propios elementos visuales de multiplicaci\u00f3n de matrices como este en http://matrixmultiplication.xyz/.</p> <p>Nota: Una multiplicaci\u00f3n de matrices como esta tambi\u00e9n se conoce como producto escalar de dos matrices .</p> <p>Las redes neuronales est\u00e1n llenas de multiplicaciones de matrices y productos escalares.</p> <p>El m\u00f3dulo <code>torch.nn.Linear()</code> (lo veremos en acci\u00f3n m\u00e1s adelante), Tambi\u00e9n conocida como capa de avance o capa completamente conectada, implementa una multiplicaci\u00f3n de matrices entre una entrada \"x\" y una matriz de pesos \"A\".</p> <p>$$ y = x\\cdot{A^T} + b $$</p> <p>D\u00f3nde:</p> <ul> <li><code>x</code> es la entrada a la capa (el aprendizaje profundo es una pila de capas como <code>torch.nn.Linear()</code> y otras una encima de la otra).</li> <li><code>A</code> es la matriz de pesos creada por la capa, esto comienza como n\u00fameros aleatorios que se ajustan a medida que una red neuronal aprende a representar mejor los patrones en los datos (observe la \"<code>T</code>\", eso se debe a que la matriz de pesos se transpone ).<ul> <li>Nota: Es posible que tambi\u00e9n veas a menudo \"W\" u otra letra como \"X\" utilizada para mostrar la matriz de pesos.</li> </ul> </li> <li><code>b</code> es el t\u00e9rmino de sesgo utilizado para compensar ligeramente los pesos y las entradas.</li> <li><code>y</code> es la salida (una manipulaci\u00f3n de la entrada con la esperanza de descubrir patrones en ella).</li> </ul> <p>Esta es una funci\u00f3n lineal (es posible que hayas visto algo como $y = mx+b$ en la escuela secundaria o en cualquier otro lugar) y se puede usar para dibujar una l\u00ednea recta.</p> <p>Juguemos con una capa lineal.</p> <p>Intente cambiar los valores de <code>in_features</code> y <code>out_features</code> a continuaci\u00f3n y vea qu\u00e9 sucede.</p> <p>\u00bfNotas algo que ver con las formas?</p> In\u00a0[\u00a0]: Copied! <pre># Dado que la capa lineal comienza con una matriz de pesos aleatorios, hag\u00e1mosla reproducible (m\u00e1s sobre esto m\u00e1s adelante)\ntorch.manual_seed(42)\n# Esto usa la multiplicaci\u00f3n de matrices.\nlinear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n</pre> # Dado que la capa lineal comienza con una matriz de pesos aleatorios, hag\u00e1mosla reproducible (m\u00e1s sobre esto m\u00e1s adelante) torch.manual_seed(42) # Esto usa la multiplicaci\u00f3n de matrices. linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input                           out_features=6) # out_features = describes outer value  x = tensor_A output = linear(x) print(f\"Input shape: {x.shape}\\n\") print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\") <p>Pregunta: \u00bfQu\u00e9 sucede si cambia <code>in_features</code> del 2 al 3 anterior? \u00bfTiene error? \u00bfC\u00f3mo podr\u00eda cambiar la forma de la entrada (<code>x</code>) para adaptarse al error? Pista: \u00bfqu\u00e9 tuvimos que hacer con el <code>tensor_B</code> anterior?</p> <p>Si nunca lo has hecho antes, la multiplicaci\u00f3n de matrices puede ser un tema confuso al principio.</p> <p>Pero despu\u00e9s de haber jugado con \u00e9l unas cuantas veces e incluso haber abierto algunas redes neuronales, notar\u00e1s que est\u00e1 en todas partes.</p> <p>Recuerde, la multiplicaci\u00f3n de matrices es todo lo que necesita.</p> <p></p> <p>Cuando empieces a profundizar en las capas de la red neuronal y a construir la tuya propia, encontrar\u00e1s multiplicaciones de matrices por todas partes. Fuente: https://marksaroufim.substack.com/p/working-class-deep-learner</p> In\u00a0[\u00a0]: Copied! <pre># Crear un tensor\nx = torch.arange(0, 100, 10)\nx\n</pre> # Crear un tensor x = torch.arange(0, 100, 10) x <p>Ahora realicemos alguna agregaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # esto generar\u00e1 un error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n</pre> print(f\"Minimum: {x.min()}\") print(f\"Maximum: {x.max()}\") # print(f\"Mean: {x.mean()}\") # esto generar\u00e1 un error print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype print(f\"Sum: {x.sum()}\") <p>Nota: Es posible que algunos m\u00e9todos como <code>torch.mean()</code> requieran que los tensores est\u00e9n en <code>torch.float32</code> (el m\u00e1s com\u00fan) u otro tipo de datos espec\u00edfico; de lo contrario, la operaci\u00f3n fallar\u00e1.</p> <p>Tambi\u00e9n puedes hacer lo mismo que arriba con los m\u00e9todos \"antorcha\".</p> In\u00a0[\u00a0]: Copied! <pre>torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n</pre> torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x) In\u00a0[\u00a0]: Copied! <pre># Crear un tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Devuelve el \u00edndice de valores m\u00e1ximo y m\u00ednimo.\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n</pre> # Crear un tensor tensor = torch.arange(10, 100, 10) print(f\"Tensor: {tensor}\")  # Devuelve el \u00edndice de valores m\u00e1ximo y m\u00ednimo. print(f\"Index where max value occurs: {tensor.argmax()}\") print(f\"Index where min value occurs: {tensor.argmin()}\") In\u00a0[\u00a0]: Copied! <pre># Crea un tensor y verifica su tipo de datos.\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n</pre> # Crea un tensor y verifica su tipo de datos. tensor = torch.arange(10., 100., 10.) tensor.dtype <p>Ahora crearemos otro tensor igual que antes pero cambiaremos su tipo de datos a <code>torch.float16</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Crea un tensor float16\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n</pre> # Crea un tensor float16 tensor_float16 = tensor.type(torch.float16) tensor_float16 <p>Y podemos hacer algo similar para crear un tensor <code>torch.int8</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Crear un tensor int8\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n</pre> # Crear un tensor int8 tensor_int8 = tensor.type(torch.int8) tensor_int8 <p>Nota: Para empezar, diferentes tipos de datos pueden resultar confusos. Pero pi\u00e9nselo as\u00ed: cuanto menor es el n\u00famero (por ejemplo, 32, 16, 8), menos precisa es la computadora que almacena el valor. Y con una menor cantidad de almacenamiento, esto generalmente resulta en un c\u00e1lculo m\u00e1s r\u00e1pido y un modelo general m\u00e1s peque\u00f1o. Las redes neuronales m\u00f3viles a menudo operan con n\u00fameros enteros de 8 bits, m\u00e1s peque\u00f1os y m\u00e1s r\u00e1pidos de ejecutar, pero menos precisos que sus contrapartes float32. Para obtener m\u00e1s informaci\u00f3n sobre esto, le\u00ed sobre [precisi\u00f3n en inform\u00e1tica] (https://en.wikipedia.org/wiki/Precision_(computer_science)).</p> <p>Ejercicio: Hasta ahora hemos cubierto algunos m\u00e9todos tensoriales, pero hay muchos m\u00e1s en la documentaci\u00f3n <code>torch.Tensor</code> , recomendar\u00eda pasar 10 minutos desplaz\u00e1ndose y mirando cualquiera que le llame la atenci\u00f3n. Haga clic en ellos y luego escr\u00edbalos usted mismo en c\u00f3digo para ver qu\u00e9 sucede.</p> In\u00a0[\u00a0]: Copied! <pre># Crear un tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n</pre> # Crear un tensor import torch x = torch.arange(1., 8.) x, x.shape <p>Ahora agreguemos una dimensi\u00f3n adicional con <code>torch.reshape()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># A\u00f1ade una dimensi\u00f3n extra\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n</pre> # A\u00f1ade una dimensi\u00f3n extra x_reshaped = x.reshape(1, 7) x_reshaped, x_reshaped.shape <p>Tambi\u00e9n podemos cambiar la vista con <code>torch.view()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Cambiar vista (mantiene los mismos datos que el original pero cambia la vista)\n# Ver m\u00e1s: https://stackoverflow.com/a/54507446/7900723\nz = x.view(1, 7)\nz, z.shape\n</pre> # Cambiar vista (mantiene los mismos datos que el original pero cambia la vista) # Ver m\u00e1s: https://stackoverflow.com/a/54507446/7900723 z = x.view(1, 7) z, z.shape <p>Sin embargo, recuerde que cambiar la vista de un tensor con <code>torch.view()</code> en realidad solo crea una nueva vista del mismo tensor.</p> <p>Entonces, cambiar la vista tambi\u00e9n cambia el tensor original.</p> In\u00a0[\u00a0]: Copied! <pre># Cambiar z cambia x\nz[:, 0] = 5\nz, x\n</pre> # Cambiar z cambia x z[:, 0] = 5 z, x <p>Si quisi\u00e9ramos apilar nuestro nuevo tensor encima de s\u00ed mismo cinco veces, podr\u00edamos hacerlo con <code>torch.stack()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Apilar tensores uno encima del otro\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n</pre> # Apilar tensores uno encima del otro x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens x_stacked <p>\u00bfQu\u00e9 tal eliminar todas las dimensiones individuales de un tensor?</p> <p>Para hacerlo, puedes usar <code>torch.squeeze()</code> (recuerdo esto como apretar el tensor para que solo tenga dimensiones superiores a 1).</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Eliminar dimensi\u00f3n adicional de x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n</pre> print(f\"Previous tensor: {x_reshaped}\") print(f\"Previous shape: {x_reshaped.shape}\")  # Eliminar dimensi\u00f3n adicional de x_reshaped x_squeezed = x_reshaped.squeeze() print(f\"\\nNew tensor: {x_squeezed}\") print(f\"New shape: {x_squeezed.shape}\") <p>Y para hacer lo contrario de <code>torch.squeeze()</code> puedes usar <code>torch.unsqueeze()</code> para agregar un valor de dimensi\u00f3n de 1 en un \u00edndice espec\u00edfico.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Previous tensor: {x_squeezed}\")\nprint(f\"Previous shape: {x_squeezed.shape}\")\n\n# # A\u00f1ade una dimensi\u00f3n extra con unsqueeze\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"\\nNew tensor: {x_unsqueezed}\")\nprint(f\"New shape: {x_unsqueezed.shape}\")\n</pre> print(f\"Previous tensor: {x_squeezed}\") print(f\"Previous shape: {x_squeezed.shape}\")  # # A\u00f1ade una dimensi\u00f3n extra con unsqueeze x_unsqueezed = x_squeezed.unsqueeze(dim=0) print(f\"\\nNew tensor: {x_unsqueezed}\") print(f\"New shape: {x_unsqueezed.shape}\") <p>Tambi\u00e9n puede reorganizar el orden de los valores de los ejes con <code>torch.permute(input, dims)</code>, donde la <code>input</code> se convierte en una vista con nuevas <code>dims</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Crear tensor con forma espec\u00edfica.\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permutar el tensor original para reorganizar el orden de los ejes.\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n</pre> # Crear tensor con forma espec\u00edfica. x_original = torch.rand(size=(224, 224, 3))  # Permutar el tensor original para reorganizar el orden de los ejes. x_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0  print(f\"Previous shape: {x_original.shape}\") print(f\"New shape: {x_permuted.shape}\") <p>Nota: Debido a que la permutaci\u00f3n devuelve una vista (comparte los mismos datos que el original), los valores en el tensor permutado ser\u00e1n los mismos que los del tensor original y si cambia los valores en la vista, tambi\u00e9n cambiar los valores del original.</p> In\u00a0[\u00a0]: Copied! <pre># Crear un tensor\nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n</pre> # Crear un tensor import torch x = torch.arange(1, 10).reshape(1, 3, 3) x, x.shape <p>Los valores de indexaci\u00f3n van a la dimensi\u00f3n exterior -&gt; dimensi\u00f3n interior (consulte los corchetes).</p> In\u00a0[\u00a0]: Copied! <pre># Indexemos par\u00e9ntesis por par\u00e9ntesis\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n</pre> # Indexemos par\u00e9ntesis por par\u00e9ntesis print(f\"First square bracket:\\n{x[0]}\")  print(f\"Second square bracket: {x[0][0]}\")  print(f\"Third square bracket: {x[0][0][0]}\") <p>Tambi\u00e9n puede usar <code>:</code> para especificar \"todos los valores en esta dimensi\u00f3n\" y luego usar una coma (<code>,</code>) para agregar otra dimensi\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga todos los valores de la dimensi\u00f3n 0 y el \u00edndice 0 de la dimensi\u00f3n 1\nx[:, 0]\n</pre> # Obtenga todos los valores de la dimensi\u00f3n 0 y el \u00edndice 0 de la dimensi\u00f3n 1 x[:, 0] In\u00a0[\u00a0]: Copied! <pre># Obtenga todos los valores de la 0.\u00aa y la 1.\u00aa dimensi\u00f3n, pero solo el \u00edndice 1 de la 2.\u00aa dimensi\u00f3n\nx[:, :, 1]\n</pre> # Obtenga todos los valores de la 0.\u00aa y la 1.\u00aa dimensi\u00f3n, pero solo el \u00edndice 1 de la 2.\u00aa dimensi\u00f3n x[:, :, 1] In\u00a0[\u00a0]: Copied! <pre># Obtenga todos los valores de la dimensi\u00f3n 0 pero solo el valor del \u00edndice 1 de la primera y segunda dimensi\u00f3n\nx[:, 1, 1]\n</pre> # Obtenga todos los valores de la dimensi\u00f3n 0 pero solo el valor del \u00edndice 1 de la primera y segunda dimensi\u00f3n x[:, 1, 1] In\u00a0[\u00a0]: Copied! <pre># Obtenga el \u00edndice 0 de la 0.\u00aa y 1.\u00aa dimensi\u00f3n y todos los valores de la 2.\u00aa dimensi\u00f3n\nx[0, 0, :] # same as x[0][0]\n</pre> # Obtenga el \u00edndice 0 de la 0.\u00aa y 1.\u00aa dimensi\u00f3n y todos los valores de la 2.\u00aa dimensi\u00f3n x[0, 0, :] # same as x[0][0] <p>Para empezar, la indexaci\u00f3n puede resultar bastante confusa, especialmente con tensores m\u00e1s grandes (todav\u00eda tengo que intentar indexar varias veces para hacerlo bien). Pero con un poco de pr\u00e1ctica y siguiendo el lema del explorador de datos (visualizar, visualizar, visualizar), empezar\u00e1s a cogerle el tranquillo.</p> In\u00a0[\u00a0]: Copied! <pre># matriz NumPy a tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n</pre> # matriz NumPy a tensor import torch import numpy as np array = np.arange(1.0, 8.0) tensor = torch.from_numpy(array) array, tensor <p>Nota: De forma predeterminada, las matrices NumPy se crean con el tipo de datos <code>float64</code> y si lo convierte a un tensor de PyTorch, mantendr\u00e1 el mismo tipo de datos (como arriba).</p> <p>Sin embargo, muchos c\u00e1lculos de PyTorch utilizan de forma predeterminada <code>float32</code>.</p> <p>Entonces, si desea convertir su matriz NumPy (float64) -&gt; tensor PyTorch (float64) -&gt; tensor PyTorch (float32), puede usar <code>tensor = torch.from_numpy(array).type(torch.float32)</code>.</p> <p>Debido a que reasignamos el \"tensor\" arriba, si cambia el tensor, la matriz permanece igual.</p> In\u00a0[\u00a0]: Copied! <pre># Cambia la matriz, mant\u00e9n el tensor.\narray = array + 1\narray, tensor\n</pre> # Cambia la matriz, mant\u00e9n el tensor. array = array + 1 array, tensor <p>Y si desea pasar del tensor de PyTorch a la matriz NumPy, puede llamar a <code>tensor.numpy()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Tensor a matriz NumPy\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n</pre> # Tensor a matriz NumPy tensor = torch.ones(7) # create a tensor of ones with dtype=float32 numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed tensor, numpy_tensor <p>Y se aplica la misma regla anterior, si cambia el <code>tensor</code> original, el nuevo <code>numpy_tensor</code> permanece igual.</p> In\u00a0[\u00a0]: Copied! <pre># Cambia el tensor, mant\u00e9n la matriz igual\ntensor = tensor + 1\ntensor, numpy_tensor\n</pre> # Cambia el tensor, mant\u00e9n la matriz igual tensor = tensor + 1 tensor, numpy_tensor In\u00a0[\u00a0]: Copied! <pre>import torch\n\n# Crea dos tensores aleatorios\nrandom_tensor_A = torch.rand(3, 4)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"Tensor A:\\n{random_tensor_A}\\n\")\nprint(f\"Tensor B:\\n{random_tensor_B}\\n\")\nprint(f\"Does Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n</pre> import torch  # Crea dos tensores aleatorios random_tensor_A = torch.rand(3, 4) random_tensor_B = torch.rand(3, 4)  print(f\"Tensor A:\\n{random_tensor_A}\\n\") print(f\"Tensor B:\\n{random_tensor_B}\\n\") print(f\"Does Tensor A equal Tensor B? (anywhere)\") random_tensor_A == random_tensor_B <p>Tal como era de esperar, los tensores salen con valores diferentes.</p> <p>Pero, \u00bfqu\u00e9 pasar\u00eda si quisieras crear dos tensores aleatorios con los mismos valores?</p> <p>Como en, los tensores a\u00fan contendr\u00edan valores aleatorios pero ser\u00edan del mismo tipo.</p> <p>Ah\u00ed es donde entra <code>torch.manual_seed(seed)</code>, donde <code>seed</code> es un n\u00famero entero (como <code>42</code> pero podr\u00eda ser cualquier cosa) que le d\u00e9 sabor a la aleatoriedad.</p> <p>Prob\u00e9moslo creando algunos tensores aleatorios m\u00e1s con sabor.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport random\n\n# # Establecer la semilla aleatoria\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Tienes que restablecer la semilla cada vez que se llama a un nuevo rand()\n# Sin esto, tensor_D ser\u00eda diferente a tensor_C\ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n</pre> import torch import random  # # Establecer la semilla aleatoria RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below torch.manual_seed(seed=RANDOM_SEED)  random_tensor_C = torch.rand(3, 4)  # Tienes que restablecer la semilla cada vez que se llama a un nuevo rand() # Sin esto, tensor_D ser\u00eda diferente a tensor_C torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens random_tensor_D = torch.rand(3, 4)  print(f\"Tensor C:\\n{random_tensor_C}\\n\") print(f\"Tensor D:\\n{random_tensor_D}\\n\") print(f\"Does Tensor C equal Tensor D? (anywhere)\") random_tensor_C == random_tensor_D <p>\u00a1Lindo!</p> <p>Parece que la semilla funcion\u00f3.</p> <p>Recurso: Lo que acabamos de cubrir solo roza la superficie de la reproducibilidad en PyTorch. Para obtener m\u00e1s informaci\u00f3n sobre la reproducibilidad en general y las semillas aleatorias, consultar\u00eda:</p> <ul> <li>La documentaci\u00f3n de reproducibilidad de PyTorch (un buen ejercicio ser\u00eda leer esto durante 10 minutos e incluso si no lo entiendes ahora, ser consciente de ello es importante).</li> <li>La p\u00e1gina de semillas aleatorias de Wikipedia (esto brindar\u00e1 una buena descripci\u00f3n general de las semillas aleatorias y la pseudoaleatoriedad en general).</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <p>Si no tiene una GPU Nvidia accesible, lo anterior generar\u00e1 algo como:</p> <pre><code>NVIDIA-SMI fall\u00f3 porque no pudo comunicarse con el controlador NVIDIA. Aseg\u00farese de que el controlador NVIDIA m\u00e1s reciente est\u00e9 instalado y ejecut\u00e1ndose.\n</code></pre> <p>En ese caso, vuelva a subir y siga los pasos de instalaci\u00f3n.</p> <p>Si tiene una GPU, la l\u00ednea anterior generar\u00e1 algo como:</p> <pre><code>mi\u00e9rcoles 19 de enero 22:09:08 2022       \n+------------------------------------------------- ----------------------------+\n| NVIDIA-SMI 495.46 Versi\u00f3n del controlador: 460.32.03 Versi\u00f3n CUDA: 11.2 |\n|-------------------------------+------------------ -----+----------------------+\n| Persistencia del nombre de GPU-M| Visualizaci\u00f3n de ID de bus A | Incorrecci\u00f3n vol\u00e1til. CEC |\n| Fan Temp Perf Pwr:Uso/Cap|         Uso de memoria | GPU-Util Compute M. |\n|                               |                      |               MIG M. |\n|===============================+================== =====+======================|\n|   0 Tesla P100-PCIE... Apagado | 00000000:00:04.0 Apagado |                    0 |\n| N/D 35C P0 27W / 250W |      0MiB/16280MiB |      0% Predeterminado |\n|                               |                      |                  N/A |\n+-------------------------------+------------------ -----+----------------------+\n                                                                               \n+------------------------------------------------- ----------------------------+\n| Procesos: |\n|  GPU GI CI PID Tipo Nombre del proceso GPU Memoria |\n|        Uso de identificaci\u00f3n |\n|=================================================== ============================|\n|  No se encontraron procesos en ejecuci\u00f3n |\n+------------------------------------------------- ----------------------------+\n</code></pre> In\u00a0[\u00a0]: Copied! <pre># Comprobar GPU\nimport torch\ntorch.cuda.is_available()\n</pre> # Comprobar GPU import torch torch.cuda.is_available() <p>Si lo anterior genera \"True\", PyTorch puede ver y usar la GPU, si genera \"False\", no puede ver la GPU y, en ese caso, tendr\u00e1 que volver a realizar los pasos de instalaci\u00f3n.</p> <p>Ahora, digamos que desea configurar su c\u00f3digo para que se ejecute en la CPU o la GPU, si est\u00e1 disponible.</p> <p>De esa manera, si usted o alguien decide ejecutar su c\u00f3digo, funcionar\u00e1 independientemente del dispositivo inform\u00e1tico que est\u00e9 utilizando.</p> <p>Creemos una variable \"dispositivo\" para almacenar qu\u00e9 tipo de dispositivo est\u00e1 disponible.</p> In\u00a0[\u00a0]: Copied! <pre># Establecer tipo de dispositivo\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Establecer tipo de dispositivo device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device <p>Si el resultado anterior es <code>\"cuda\"</code>, significa que podemos configurar todo nuestro c\u00f3digo PyTorch para usar el dispositivo CUDA disponible (una GPU) y si genera <code>\"cpu\"</code>, nuestro c\u00f3digo PyTorch se quedar\u00e1 con la CPU.</p> <p>Nota: En PyTorch, se recomienda escribir c\u00f3digo independiente del dispositivo. Esto significa c\u00f3digo que se ejecutar\u00e1 en la CPU (siempre disponible) o GPU (si est\u00e1 disponible).</p> <p>Si desea realizar una inform\u00e1tica m\u00e1s r\u00e1pida, puede utilizar una GPU, pero si desea realizar una inform\u00e1tica mucho m\u00e1s r\u00e1pida, puede utilizar varias GPU.</p> <p>Puede contar la cantidad de GPU a las que PyTorch tiene acceso usando [<code>torch.cuda.device_count()</code>](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda. recuento_dispositivo).</p> In\u00a0[\u00a0]: Copied! <pre># Contar el n\u00famero de dispositivos\ntorch.cuda.device_count()\n</pre> # Contar el n\u00famero de dispositivos torch.cuda.device_count() <p>Saber la cantidad de GPU a las que tiene acceso PyTorch es \u00fatil en caso de que desee ejecutar un proceso espec\u00edfico en una GPU y otro proceso en otra (PyTorch tambi\u00e9n tiene funciones que le permiten ejecutar un proceso en todas las GPU).</p> In\u00a0[\u00a0]: Copied! <pre># Compruebe si hay GPU Apple Silicon\nimport torch\ntorch.backends.mps.is_available() # Note this will print false if you're not running on a Mac\n</pre> # Compruebe si hay GPU Apple Silicon import torch torch.backends.mps.is_available() # Note this will print false if you're not running on a Mac In\u00a0[\u00a0]: Copied! <pre># Establecer tipo de dispositivo\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\ndevice\n</pre> # Establecer tipo de dispositivo device = \"mps\" if torch.backends.mps.is_available() else \"cpu\" device <p>Como antes, si el resultado anterior es <code>\"mps\"</code>, significa que podemos configurar todo nuestro c\u00f3digo PyTorch para usar la GPU Apple Silicon disponible.</p> In\u00a0[\u00a0]: Copied! <pre>if torch.cuda.is_available():\n    device = \"cuda\" # Use NVIDIA GPU (if available)\nelif torch.backends.mps.is_available():\n    device = \"mps\" # Use Apple Silicon GPU (if available)\nelse:\n    device = \"cpu\" # Default to CPU if no GPU is available\n</pre> if torch.cuda.is_available():     device = \"cuda\" # Use NVIDIA GPU (if available) elif torch.backends.mps.is_available():     device = \"mps\" # Use Apple Silicon GPU (if available) else:     device = \"cpu\" # Default to CPU if no GPU is available In\u00a0[\u00a0]: Copied! <pre># Crear tensor (predeterminado en la CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor no en GPU\nprint(tensor, tensor.device)\n\n# Mover tensor a GPU (si est\u00e1 disponible)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n</pre> # Crear tensor (predeterminado en la CPU) tensor = torch.tensor([1, 2, 3])  # Tensor no en GPU print(tensor, tensor.device)  # Mover tensor a GPU (si est\u00e1 disponible) tensor_on_gpu = tensor.to(device) tensor_on_gpu <p>Si tiene una GPU disponible, el c\u00f3digo anterior generar\u00e1 algo como:</p> <pre><code>tensor([1, 2, 3]) CPU\ntensor([1, 2, 3], dispositivo='cuda:0')\n</code></pre> <p>Observe que el segundo tensor tiene <code>'device='cuda:0'</code>, esto significa que est\u00e1 almacenado en la 0.\u00aa GPU disponible (las GPU est\u00e1n indexadas a 0, si hubiera dos GPU disponibles, ser\u00edan <code>'cuda:0'</code> y <code>' cuda:1'</code> respectivamente, hasta <code>'cuda:n'</code>).</p> In\u00a0[\u00a0]: Copied! <pre># Si el tensor est\u00e1 en la GPU, no se puede transformar a NumPy (esto generar\u00e1 un error)\ntensor_on_gpu.numpy()\n</pre> # Si el tensor est\u00e1 en la GPU, no se puede transformar a NumPy (esto generar\u00e1 un error) tensor_on_gpu.numpy() <p>En su lugar, para devolver un tensor a la CPU y poder usarlo con NumPy, podemos usar <code>Tensor.cpu()</code>.</p> <p>Esto copia el tensor a la memoria de la CPU para que pueda usarse con CPU.</p> In\u00a0[\u00a0]: Copied! <pre># En su lugar, copie el tensor nuevamente a la CPU.\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n</pre> # En su lugar, copie el tensor nuevamente a la CPU. tensor_back_on_cpu = tensor_on_gpu.cpu().numpy() tensor_back_on_cpu <p>Lo anterior devuelve una copia del tensor de la GPU en la memoria de la CPU, por lo que el tensor original todav\u00eda est\u00e1 en la GPU.</p> In\u00a0[\u00a0]: Copied! <pre>tensor_on_gpu\n</pre> tensor_on_gpu"},{"location":"04-00_pytorch_fundamentals/#00-fundamentos-de-pytorch","title":"00. Fundamentos de PyTorch\u00b6","text":""},{"location":"04-00_pytorch_fundamentals/#que-es-pytorch","title":"\u00bfQu\u00e9 es PyTorch?\u00b6","text":"<p>PyTorch es un marco de aprendizaje autom\u00e1tico y aprendizaje profundo de c\u00f3digo abierto.</p>"},{"location":"04-00_pytorch_fundamentals/#para-que-se-puede-utilizar-pytorch","title":"\u00bfPara qu\u00e9 se puede utilizar PyTorch?\u00b6","text":"<p>PyTorch le permite manipular y procesar datos y escribir algoritmos de aprendizaje autom\u00e1tico utilizando c\u00f3digo Python.</p>"},{"location":"04-00_pytorch_fundamentals/#quien-usa-pytorch","title":"\u00bfQui\u00e9n usa PyTorch?\u00b6","text":"<p>Muchas de las empresas de tecnolog\u00eda m\u00e1s grandes del mundo, como Meta (Facebook), Tesla y Microsoft, as\u00ed como empresas de investigaci\u00f3n de inteligencia artificial como OpenAI utiliza PyTorch para impulsar la investigaci\u00f3n y llevar el aprendizaje autom\u00e1tico a sus productos.</p> <p></p> <p>Por ejemplo, Andrej Karpathy (director de IA de Tesla) ha dado varias charlas (PyTorch DevCon 2019, [Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M ?t=2904)) sobre c\u00f3mo Tesla usa PyTorch para impulsar sus modelos de visi\u00f3n por computadora aut\u00f3nomos.</p> <p>PyTorch tambi\u00e9n se utiliza en otras industrias, como la agricultura, para impulsar la visi\u00f3n por computadora en tractores.</p>"},{"location":"04-00_pytorch_fundamentals/#por-que-utilizar-pytorch","title":"\u00bfPor qu\u00e9 utilizar PyTorch?\u00b6","text":"<p>A los investigadores de aprendizaje autom\u00e1tico les encanta usar PyTorch. Y a partir de febrero de 2022, PyTorch es el marco de aprendizaje profundo m\u00e1s utilizado en Papers With Code, un sitio web para realizar un seguimiento de los art\u00edculos de investigaci\u00f3n sobre aprendizaje autom\u00e1tico y los repositorios de c\u00f3digo adjuntos a ellos.</p> <p>PyTorch tambi\u00e9n ayuda a encargarse de muchas cosas, como la aceleraci\u00f3n de GPU (hacer que su c\u00f3digo se ejecute m\u00e1s r\u00e1pido) detr\u00e1s de escena.</p> <p>Por lo tanto, puede concentrarse en manipular datos y escribir algoritmos y PyTorch se asegurar\u00e1 de que se ejecute r\u00e1pidamente.</p> <p>Y si empresas como Tesla y Meta (Facebook) lo utilizan para construir modelos que implementan para impulsar cientos de aplicaciones, conducir miles de autom\u00f3viles y entregar contenido a miles de millones de personas, es evidente que tambi\u00e9n es capaz en el frente del desarrollo.</p>"},{"location":"04-00_pytorch_fundamentals/#que-vamos-a-cubrir-en-este-modulo","title":"Qu\u00e9 vamos a cubrir en este m\u00f3dulo\u00b6","text":"<p>Este curso se divide en diferentes secciones (cuadernos).</p> <p>Cada cuaderno cubre ideas y conceptos importantes dentro de PyTorch.</p> <p>Los cuadernos posteriores se basan en el conocimiento del anterior (la numeraci\u00f3n comienza en 00, 01, 02 y contin\u00faa hasta donde termina).</p> <p>Este cuaderno trata sobre el componente b\u00e1sico del aprendizaje autom\u00e1tico y el aprendizaje profundo, el tensor.</p> <p>Espec\u00edficamente, cubriremos:</p> Tema Contenido Introducci\u00f3n a los tensores Los tensores son el componente b\u00e1sico de todo el aprendizaje autom\u00e1tico y el aprendizaje profundo. Creando tensores Los tensores pueden representar casi cualquier tipo de datos (im\u00e1genes, palabras, tablas de n\u00fameros). Obtener informaci\u00f3n de tensores Si puedes poner informaci\u00f3n en un tensor, tambi\u00e9n querr\u00e1s sacarla. Manipulaci\u00f3n de tensores Los algoritmos de aprendizaje autom\u00e1tico (como las redes neuronales) implican la manipulaci\u00f3n de tensores de muchas formas diferentes, como sumar, multiplicar y combinar. Tratando con formas tensoriales Uno de los problemas m\u00e1s comunes en el aprendizaje autom\u00e1tico es lidiar con desajustes de formas (intentar mezclar tensores con formas incorrectas con otros tensores). Indexaci\u00f3n de tensores Si ha indexado en una lista de Python o una matriz NumPy, es muy similar con los tensores, excepto que pueden tener muchas m\u00e1s dimensiones. Mezcla de tensores de PyTorch y NumPy PyTorch juega con tensores (<code>torch.Tensor</code>), a NumPy le gustan las matrices ([<code>np.ndarray</code>](https://numpy.org /doc/stable/reference/generated/numpy.ndarray.html)) a veces querr\u00e1s mezclarlos y combinarlos. Reproducibilidad El aprendizaje autom\u00e1tico es muy experimental y dado que utiliza mucha aleatoriedad para funcionar, a veces querr\u00e1s que esa aleatoriedad no sea tan aleatoria. Ejecuci\u00f3n de tensores en GPU Las GPU (Unidades de procesamiento de gr\u00e1ficos) hacen que su c\u00f3digo sea m\u00e1s r\u00e1pido, PyTorch facilita la ejecuci\u00f3n de su c\u00f3digo en las GPU."},{"location":"04-00_pytorch_fundamentals/#donde-puedes-obtener-ayuda","title":"\u00bfD\u00f3nde puedes obtener ayuda?\u00b6","text":"<p>Todos los materiales de este curso en vivo en GitHub.</p> <p>Y si tiene problemas, tambi\u00e9n puede hacer una pregunta en la p\u00e1gina de debates.</p> <p>Tambi\u00e9n est\u00e1n los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"04-00_pytorch_fundamentals/#importando-pytorch","title":"Importando PyTorch\u00b6","text":"<p>Nota: Antes de ejecutar cualquier c\u00f3digo de este cuaderno, deber\u00eda haber seguido los pasos de configuraci\u00f3n de PyTorch.</p> <p>Sin embargo, si est\u00e1s ejecutando Google Colab, todo deber\u00eda funcionar (Google Colab viene con PyTorch y otras bibliotecas instaladas).</p> <p>Comencemos importando PyTorch y verificando la versi\u00f3n que estamos usando.</p>"},{"location":"04-00_pytorch_fundamentals/#introduccion-a-los-tensores","title":"Introducci\u00f3n a los tensores\u00b6","text":"<p>Ahora que importamos PyTorch, es hora de aprender sobre los tensores.</p> <p>Los tensores son el componente fundamental del aprendizaje autom\u00e1tico.</p> <p>Su trabajo es representar datos de forma num\u00e9rica.</p> <p>Por ejemplo, podr\u00eda representar una imagen como un tensor con la forma <code>[3, 224, 224]</code> que significar\u00eda <code>[color_channels, height, width]</code>, ya que en la imagen tiene <code>3</code> canales de color (rojo, verde, azul), una altura de <code>224</code> p\u00edxeles y una anchura de <code>224</code> p\u00edxeles.</p> <p>![ejemplo de pasar de una imagen de entrada a una representaci\u00f3n tensorial de la imagen, la imagen se divide en 3 canales de color, as\u00ed como n\u00fameros para representar la altura y el ancho](https://raw.githubusercontent.com/mrdbourke/pytorch -aprendizaje-deep/main/images/00-tensor-shape-example-of-image.png)</p> <p>En lenguaje tensorial (el lenguaje utilizado para describir tensores), el tensor tendr\u00eda tres dimensiones, una para \"color_channels\", \"altura\" y \"ancho\".</p> <p>Pero nos estamos adelantando.</p> <p>Aprendamos m\u00e1s sobre los tensores codific\u00e1ndolos.</p>"},{"location":"04-00_pytorch_fundamentals/#creando-tensores","title":"Creando tensores\u00b6","text":"<p>A PyTorch le encantan los tensores. Tanto es as\u00ed que hay una p\u00e1gina de documentaci\u00f3n completa dedicada a la clase <code>torch.Tensor</code>.</p> <p>Su primera tarea es leer la documentaci\u00f3n en <code>torch.Tensor</code> durante 10 minutos. Pero puedes llegar a eso m\u00e1s tarde.</p> <p>Codifiquemos.</p> <p>Lo primero que vamos a crear es un escalar.</p> <p>Un escalar es un n\u00famero \u00fanico y en t\u00e9rminos tensoriales es un tensor de dimensi\u00f3n cero.</p> <p>Nota: Esa es una tendencia para este curso. Nos centraremos en escribir c\u00f3digo espec\u00edfico. Pero a menudo establezco ejercicios que implican leer y familiarizarse con la documentaci\u00f3n de PyTorch. Porque despu\u00e9s de todo, una vez que hayas terminado este curso, sin duda querr\u00e1s aprender m\u00e1s. Y la documentaci\u00f3n est\u00e1 en alg\u00fan lugar donde se encontrar\u00e1 con bastante frecuencia.</p>"},{"location":"04-00_pytorch_fundamentals/#tensores-aleatorios","title":"Tensores aleatorios\u00b6","text":"<p>Hemos establecido que los tensores representan alg\u00fan tipo de datos.</p> <p>Y los modelos de aprendizaje autom\u00e1tico, como las redes neuronales, manipulan y buscan patrones dentro de los tensores.</p> <p>Pero al construir modelos de aprendizaje autom\u00e1tico con PyTorch, es raro que crees tensores a mano (como lo que estamos haciendo nosotros).</p> <p>En cambio, un modelo de aprendizaje autom\u00e1tico a menudo comienza con grandes tensores de n\u00fameros aleatorios y ajusta estos n\u00fameros aleatorios a medida que trabaja con datos para representarlos mejor.</p> <p>En esencia:</p> <p><code>Comience con n\u00fameros aleatorios -&gt; mire los datos -&gt; actualice los n\u00fameros aleatorios -&gt; mire los datos -&gt; actualice los n\u00fameros aleatorios...</code></p> <p>Como cient\u00edfico de datos, puede definir c\u00f3mo se inicia el modelo de aprendizaje autom\u00e1tico (inicializaci\u00f3n), analiza los datos (representaci\u00f3n) y actualiza (optimizaci\u00f3n) sus n\u00fameros aleatorios.</p> <p>Nos pondremos manos a la obra con estos pasos m\u00e1s adelante.</p> <p>Por ahora, veamos c\u00f3mo crear un tensor de n\u00fameros aleatorios.</p> <p>Podemos hacerlo usando <code>torch.rand()</code> y pasando el par\u00e1metro <code>size</code>.</p>"},{"location":"04-00_pytorch_fundamentals/#ceros-y-unos","title":"Ceros y unos\u00b6","text":"<p>A veces simplemente querr\u00e1s llenar los tensores con ceros o unos.</p> <p>Esto sucede mucho con el enmascaramiento (como enmascarar algunos de los valores en un tensor con ceros para que el modelo sepa que no debe aprenderlos).</p> <p>Creemos un tensor lleno de ceros con <code>torch.zeros()</code></p> <p>Nuevamente entra en juego el par\u00e1metro \"tama\u00f1o\".</p>"},{"location":"04-00_pytorch_fundamentals/#creando-un-rango-y-tensores-como","title":"Creando un rango y tensores como\u00b6","text":"<p>A veces es posible que desees un rango de n\u00fameros, como del 1 al 10 o del 0 al 100.</p> <p>Puede utilizar <code>torch.arange(inicio, fin, paso)</code> para hacerlo.</p> <p>D\u00f3nde:</p> <ul> <li><code>start</code> = inicio del rango (por ejemplo, 0)</li> <li><code>end</code> = fin del rango (por ejemplo, 10)</li> <li><code>paso</code> = cu\u00e1ntos pasos hay entre cada valor (por ejemplo, 1)</li> </ul> <p>Nota: En Python, puedes usar <code>range()</code> para crear un rango. Sin embargo, en PyTorch, <code>torch.range()</code> est\u00e1 en desuso y puede mostrar un error en el futuro.</p>"},{"location":"04-00_pytorch_fundamentals/#tipos-de-datos-tensoriales","title":"Tipos de datos tensoriales\u00b6","text":"<p>Hay muchos [tipos de datos tensoriales disponibles en PyTorch] diferentes (https://pytorch.org/docs/stable/tensors.html#data-types).</p> <p>Algunos son espec\u00edficos para CPU y otros son mejores para GPU.</p> <p>Saber cu\u00e1l es cu\u00e1l puede llevar alg\u00fan tiempo.</p> <p>Generalmente, si ve <code>torch.cuda</code> en cualquier lugar, el tensor se est\u00e1 usando para la GPU (ya que las GPU de Nvidia usan un conjunto de herramientas inform\u00e1ticas llamado CUDA).</p> <p>El tipo m\u00e1s com\u00fan (y generalmente el predeterminado) es <code>torch.float32</code> o <code>torch.float</code>.</p> <p>Esto se conoce como \"coma flotante de 32 bits\".</p> <p>Pero tambi\u00e9n hay punto flotante de 16 bits (<code>torch.float16</code> o <code>torch.half</code>) y punto flotante de 64 bits (<code>torch.float64</code> o <code>torch.double</code>).</p> <p>Y para confundir a\u00fan m\u00e1s las cosas, tambi\u00e9n hay n\u00fameros enteros de 8, 16, 32 y 64 bits.</p> <p>\u00a1Y mucho m\u00e1s!</p> <p>Nota: Un n\u00famero entero es un n\u00famero plano y redondo como \"7\", mientras que un flotante tiene un decimal \"7.0\".</p> <p>La raz\u00f3n de todo esto tiene que ver con la precisi\u00f3n en la inform\u00e1tica.</p> <p>La precisi\u00f3n es la cantidad de detalles utilizados para describir un n\u00famero.</p> <p>Cuanto mayor sea el valor de precisi\u00f3n (8, 16, 32), m\u00e1s detalles y, por tanto, m\u00e1s datos se utilizar\u00e1n para expresar un n\u00famero.</p> <p>Esto es importante en el aprendizaje profundo y la computaci\u00f3n num\u00e9rica porque al realizar tantas operaciones, cuanto m\u00e1s detalles tenga para calcular, m\u00e1s computaci\u00f3n tendr\u00e1 que usar.</p> <p>Por lo tanto, los tipos de datos de menor precisi\u00f3n generalmente son m\u00e1s r\u00e1pidos de calcular, pero sacrifican algo de rendimiento en m\u00e9tricas de evaluaci\u00f3n como la precisi\u00f3n (m\u00e1s r\u00e1pido de calcular pero menos preciso).</p> <p>Recursos:</p> <ul> <li>Consulte la [documentaci\u00f3n de PyTorch para obtener una lista de todos los tipos de datos tensoriales disponibles] (https://pytorch.org/docs/stable/tensors.html#data-types).</li> <li>Lea la [p\u00e1gina de Wikipedia para obtener una descripci\u00f3n general de qu\u00e9 es la precisi\u00f3n en inform\u00e1tica] (https://en.wikipedia.org/wiki/Precision_(computer_science)).</li> </ul> <p>Veamos c\u00f3mo crear algunos tensores con tipos de datos espec\u00edficos. Podemos hacerlo usando el par\u00e1metro <code>dtype</code>.</p>"},{"location":"04-00_pytorch_fundamentals/#obtener-informacion-de-tensores","title":"Obtener informaci\u00f3n de tensores\u00b6","text":"<p>Una vez que haya creado tensores (o alguien m\u00e1s o un m\u00f3dulo de PyTorch los haya creado para usted), es posible que desee obtener informaci\u00f3n de ellos.</p> <p>Ya los hemos visto antes, pero tres de los atributos m\u00e1s comunes que querr\u00e1s conocer sobre los tensores son:</p> <ul> <li><code>forma</code> - \u00bfqu\u00e9 forma tiene el tensor? (algunas operaciones requieren reglas de forma espec\u00edficas)</li> <li><code>dtype</code>: \u00bfen qu\u00e9 tipo de datos se almacenan los elementos dentro del tensor?</li> <li><code>dispositivo</code>: \u00bfen qu\u00e9 dispositivo est\u00e1 almacenado el tensor? (generalmente GPU o CPU)</li> </ul> <p>Creemos un tensor aleatorio y descubramos detalles al respecto.</p>"},{"location":"04-00_pytorch_fundamentals/#manipulacion-de-tensores-operaciones-tensoriales","title":"Manipulaci\u00f3n de tensores (operaciones tensoriales)\u00b6","text":"<p>En el aprendizaje profundo, los datos (im\u00e1genes, texto, v\u00eddeo, audio, estructuras de prote\u00ednas, etc.) se representan como tensores.</p> <p>Un modelo aprende investigando esos tensores y realizando una serie de operaciones (podr\u00edan ser m\u00e1s de 1.000.000 de operaciones) en tensores para crear una representaci\u00f3n de los patrones en los datos de entrada.</p> <p>Estas operaciones suelen ser un baile maravilloso entre:</p> <ul> <li>Suma</li> <li>Resta</li> <li>Multiplicaci\u00f3n (por elementos)</li> <li>Divisi\u00f3n</li> <li>Multiplicaci\u00f3n de matrices</li> </ul> <p>Y eso es. Seguro que hay algunos m\u00e1s aqu\u00ed y all\u00e1, pero estos son los componentes b\u00e1sicos de las redes neuronales.</p> <p>Al apilar estos bloques de construcci\u00f3n de la manera correcta, puedes crear las redes neuronales m\u00e1s sofisticadas (\u00a1como Lego!).</p>"},{"location":"04-00_pytorch_fundamentals/#operaciones-basicas","title":"Operaciones b\u00e1sicas\u00b6","text":"<p>Comencemos con algunas de las operaciones fundamentales, suma (<code>+</code>), resta (<code>-</code>), multiplicaci\u00f3n (<code>*</code>).</p> <p>Funcionan tal como crees que lo har\u00edan.</p>"},{"location":"04-00_pytorch_fundamentals/#multiplicacion-de-matrices-es-todo-lo-que-necesitas","title":"Multiplicaci\u00f3n de matrices (es todo lo que necesitas)\u00b6","text":"<p>Una de las operaciones m\u00e1s comunes en los algoritmos de aprendizaje autom\u00e1tico y aprendizaje profundo (como las redes neuronales) es la [multiplicaci\u00f3n de matrices] (https://www.mathsisfun.com/algebra/matrix-multiplying.html).</p> <p>PyTorch implementa la funcionalidad de multiplicaci\u00f3n de matrices en el m\u00e9todo <code>torch.matmul()</code>.</p> <p>Las dos reglas principales que hay que recordar para la multiplicaci\u00f3n de matrices son:</p> <ol> <li>Las dimensiones interiores deben coincidir:</li> </ol> <ul> <li><code>(3, 2) @ (3, 2)</code> no funcionar\u00e1</li> <li><code>(2, 3) @ (3, 2)</code> funcionar\u00e1</li> <li><code>(3, 2) @ (2, 3)</code> funcionar\u00e1</li> </ul> <ol> <li>La matriz resultante tiene la forma de dimensiones exteriores:</li> </ol> <ul> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code></li> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>Nota: \"<code>@</code>\" en Python es el s\u00edmbolo para la multiplicaci\u00f3n de matrices.</p> <p>Recurso: Puede ver todas las reglas para la multiplicaci\u00f3n de matrices usando <code>torch.matmul()</code> [en la documentaci\u00f3n de PyTorch](https://pytorch.org/docs/stable/generated/torch.matmul. HTML).</p> <p>Creemos un tensor y realicemos una multiplicaci\u00f3n de elementos y una multiplicaci\u00f3n de matrices en \u00e9l.</p>"},{"location":"04-00_pytorch_fundamentals/#uno-de-los-errores-mas-comunes-en-el-aprendizaje-profundo-errores-de-forma","title":"Uno de los errores m\u00e1s comunes en el aprendizaje profundo (errores de forma)\u00b6","text":"<p>Debido a que gran parte del aprendizaje profundo consiste en multiplicar y realizar operaciones en matrices, y las matrices tienen una regla estricta sobre qu\u00e9 formas y tama\u00f1os se pueden combinar, uno de los errores m\u00e1s comunes con los que se encontrar\u00e1 en el aprendizaje profundo son las discrepancias de formas.</p>"},{"location":"04-00_pytorch_fundamentals/#encontrar-el-minimo-maximo-media-suma-etc-agregacion","title":"Encontrar el m\u00ednimo, m\u00e1ximo, media, suma, etc. (agregaci\u00f3n)\u00b6","text":"<p>Ahora que hemos visto algunas formas de manipular tensores, veamos algunas formas de agregarlos (pasar de m\u00e1s valores a menos valores).</p> <p>Primero crearemos un tensor y luego encontraremos su m\u00e1ximo, m\u00ednimo, media y suma.</p>"},{"location":"04-00_pytorch_fundamentals/#posicional-minmax","title":"Posicional m\u00edn/m\u00e1x\u00b6","text":"<p>Tambi\u00e9n puede encontrar el \u00edndice de un tensor donde ocurre el m\u00e1ximo o el m\u00ednimo con <code>torch.argmax()</code> y <code>torch .argmin()</code> respectivamente.</p> <p>Esto es \u00fatil en caso de que solo desee la posici\u00f3n donde est\u00e1 el valor m\u00e1s alto (o m\u00e1s bajo) y no el valor real en s\u00ed (veremos esto en una secci\u00f3n posterior cuando utilice la [funci\u00f3n de activaci\u00f3n softmax](https://pytorch.org /docs/stable/generated/torch.nn.Softmax.html)).</p>"},{"location":"04-00_pytorch_fundamentals/#cambiar-el-tipo-de-datos-del-tensor","title":"Cambiar el tipo de datos del tensor\u00b6","text":"<p>Como se mencion\u00f3, un problema com\u00fan con las operaciones de aprendizaje profundo es tener tensores en diferentes tipos de datos.</p> <p>Si un tensor est\u00e1 en <code>torch.float64</code> y otro est\u00e1 en <code>torch.float32</code>, es posible que se encuentre con algunos errores.</p> <p>Pero hay una soluci\u00f3n.</p> <p>Puede cambiar los tipos de datos de los tensores usando <code>torch.Tensor.type(dtype=None)</code> donde el <code>dtype</code> El par\u00e1metro es el tipo de datos que desea utilizar.</p> <p>Primero crearemos un tensor y verificaremos su tipo de datos (el valor predeterminado es <code>torch.float32</code>).</p>"},{"location":"04-00_pytorch_fundamentals/#remodelar-apilar-apretar-y-descomprimir","title":"Remodelar, apilar, apretar y descomprimir\u00b6","text":"<p>Muchas veces querr\u00e1s remodelar o cambiar las dimensiones de tus tensores sin cambiar realmente los valores dentro de ellos.</p> <p>Para hacerlo, algunos m\u00e9todos populares son:</p> M\u00e9todo Descripci\u00f3n de una l\u00ednea <code>torch.reshape(entrada, forma)</code> Cambia la forma de <code>input</code> a <code>shape</code> (si es compatible), tambi\u00e9n puede usar <code>torch.Tensor.reshape()</code>. <code>Tensor.view(forma)</code> Devuelve una vista del tensor original en una \"forma\" diferente pero comparte los mismos datos que el tensor original. <code>torch.stack(tensores, dim=0)</code> Concatena una secuencia de \"tensores\" a lo largo de una nueva dimensi\u00f3n (\"dim\"), todos los \"tensores\" deben tener el mismo tama\u00f1o. <code>torch.squeeze(entrada)</code> Aprieta <code>input</code> para eliminar todas las dimensiones con valor <code>1</code>. <code>torch.unsqueeze(entrada, tenue)</code> Devuelve \"entrada\" con un valor de dimensi\u00f3n de \"1\" agregado en \"tenue\". <code>torch.permute(entrada, atenuaciones)</code> Devuelve una vista de la \"entrada\" original con sus dimensiones permutadas (reorganizadas) a \"atenuadas\". <p>\u00bfPor qu\u00e9 hacer algo de esto?</p> <p>Porque los modelos de aprendizaje profundo (redes neuronales) tratan de manipular tensores de alguna manera. Y debido a las reglas de la multiplicaci\u00f3n de matrices, si las formas no coinciden, se producir\u00e1n errores. Estos m\u00e9todos le ayudan a asegurarse de que los elementos correctos de sus tensores se mezclen con los elementos correctos de otros tensores.</p> <p>Prob\u00e9moslos.</p> <p>Primero, crearemos un tensor.</p>"},{"location":"04-00_pytorch_fundamentals/#indexacion-seleccionando-datos-de-tensores","title":"Indexaci\u00f3n (seleccionando datos de tensores)\u00b6","text":"<p>A veces querr\u00e1s seleccionar datos espec\u00edficos de los tensores (por ejemplo, solo la primera columna o la segunda fila).</p> <p>Para hacerlo, puede utilizar la indexaci\u00f3n.</p> <p>Si alguna vez ha indexado en listas de Python o matrices NumPy, la indexaci\u00f3n en PyTorch con tensores es muy similar.</p>"},{"location":"04-00_pytorch_fundamentals/#tensores-de-pytorch-y-numpy","title":"Tensores de PyTorch y NumPy\u00b6","text":"<p>Dado que NumPy es una biblioteca de computaci\u00f3n num\u00e9rica popular de Python, PyTorch tiene una funcionalidad para interactuar bien con ella.</p> <p>Los dos m\u00e9todos principales que querr\u00e1s usar para NumPy a PyTorch (y viceversa) son:</p> <ul> <li><code>torch.from_numpy(ndarray)</code> - Matriz NumPy -&gt; tensor de PyTorch.</li> <li><code>torch.Tensor.numpy()</code> - Tensor de PyTorch -&gt; Matriz NumPy.</li> </ul> <p>Prob\u00e9moslos.</p>"},{"location":"04-00_pytorch_fundamentals/#reproducibilidad-tratar-de-sacar-lo-aleatorio-de-lo-aleatorio","title":"Reproducibilidad (tratar de sacar lo aleatorio de lo aleatorio)\u00b6","text":"<p>A medida que aprenda m\u00e1s sobre las redes neuronales y el aprendizaje autom\u00e1tico, comenzar\u00e1 a descubrir en qu\u00e9 medida influye la aleatoriedad.</p> <p>Bueno, pseudoaleatoriedad, eso es. Porque despu\u00e9s de todo, tal como est\u00e1n dise\u00f1adas, una computadora es fundamentalmente determinista (cada paso es predecible), por lo que la aleatoriedad que crean es aleatoriedad simulada (aunque tambi\u00e9n hay debate sobre esto, pero como no soy un cient\u00edfico inform\u00e1tico, no Te dejaremos descubrir m\u00e1s t\u00fa mismo).</p> <p>Entonces, \u00bfc\u00f3mo se relaciona esto con las redes neuronales y el aprendizaje profundo?</p> <p>Hemos discutido que las redes neuronales comienzan con n\u00fameros aleatorios para describir patrones en los datos (estos n\u00fameros son descripciones deficientes) y tratamos de mejorar esos n\u00fameros aleatorios usando operaciones tensoriales (y algunas otras cosas que a\u00fan no hemos discutido) para describir mejor los patrones en datos.</p> <p>En breve:</p> <p><code>comience con n\u00fameros aleatorios -&gt; operaciones tensoriales -&gt; intente hacerlo mejor (una y otra vez)</code></p> <p>Aunque la aleatoriedad es agradable y poderosa, a veces te gustar\u00eda que hubiera un poco menos de aleatoriedad.</p> <p>\u00bfPor qu\u00e9?</p> <p>Para que pueda realizar experimentos repetibles.</p> <p>Por ejemplo, crea un algoritmo capaz de lograr un rendimiento X.</p> <p>Y luego tu amigo lo prueba para comprobar que no est\u00e1s loco.</p> <p>\u00bfC\u00f3mo pudieron hacer tal cosa?</p> <p>Ah\u00ed es donde entra en juego la reproducibilidad.</p> <p>En otras palabras, \u00bfpuedes obtener los mismos resultados (o muy similares) en tu computadora ejecutando el mismo c\u00f3digo que yo obtengo en la m\u00eda?</p> <p>Veamos un breve ejemplo de reproducibilidad en PyTorch.</p> <p>Comenzaremos creando dos tensores aleatorios, ya que son aleatorios, uno esperar\u00eda que fueran diferentes, \u00bfverdad?</p>"},{"location":"04-00_pytorch_fundamentals/#ejecutar-tensores-en-gpu-y-realizar-calculos-mas-rapidos","title":"Ejecutar tensores en GPU (y realizar c\u00e1lculos m\u00e1s r\u00e1pidos)\u00b6","text":"<p>Los algoritmos de aprendizaje profundo requieren muchas operaciones num\u00e9ricas.</p> <p>Y, de forma predeterminada, estas operaciones suelen realizarse en una CPU (unidad de procesamiento de computadora).</p> <p>Sin embargo, existe otra pieza com\u00fan de hardware llamada GPU (unidad de procesamiento de gr\u00e1ficos), que suele ser mucho m\u00e1s r\u00e1pida a la hora de realizar los tipos espec\u00edficos de operaciones que necesitan las redes neuronales (multiplicaciones de matrices) que las CPU.</p> <p>Es posible que su computadora tenga uno.</p> <p>Si es as\u00ed, deber\u00edas intentar usarlo siempre que puedas para entrenar redes neuronales porque es probable que acelere dr\u00e1sticamente el tiempo de entrenamiento.</p> <p>Hay algunas formas de obtener acceso primero a una GPU y, en segundo lugar, hacer que PyTorch use la GPU.</p> <p>Nota: Cuando hago referencia a \"GPU\" a lo largo de este curso, me refiero a una GPU Nvidia con CUDA habilitada (CUDA es una plataforma inform\u00e1tica y API que ayuda a permitir que las GPU se utilicen para computaci\u00f3n de prop\u00f3sito general y no solo para gr\u00e1ficos) a menos que se especifique lo contrario.</p>"},{"location":"04-00_pytorch_fundamentals/#1-obtener-una-gpu","title":"1. Obtener una GPU\u00b6","text":"<p>Quiz\u00e1s ya sepas lo que pasa cuando digo GPU. Pero si no, hay algunas formas de acceder a uno.</p> M\u00e9todo Dificultad de configuraci\u00f3n Ventajas Desventajas C\u00f3mo configurar Colaboraci\u00f3n de Google F\u00e1cil De uso gratuito, casi no requiere configuraci\u00f3n, puede compartir el trabajo con otras personas tan f\u00e1cilmente como un enlace No guarda sus salidas de datos, c\u00e1lculo limitado, sujeto a tiempos de espera Siga la gu\u00eda de Google Colab Utilice el suyo propio Medio Ejecute todo localmente en su propia m\u00e1quina Las GPU no son gratuitas, requieren un costo inicial Siga las directrices de instalaci\u00f3n de PyTorch Computaci\u00f3n en la nube (AWS, GCP, Azure) Medio-Duro Peque\u00f1o costo inicial, acceso a computaci\u00f3n casi infinita Puede resultar costoso si se ejecuta continuamente, lleva algo de tiempo configurarlo correctamente Siga las directrices de instalaci\u00f3n de PyTorch <p>Hay m\u00e1s opciones para usar GPU, pero las tres anteriores ser\u00e1n suficientes por ahora.</p> <p>Personalmente, uso una combinaci\u00f3n de Google Colab y mi propia computadora personal para experimentos a peque\u00f1a escala (y para crear este curso) y recurro a recursos de la nube cuando necesito m\u00e1s potencia inform\u00e1tica.</p> <p>Recurso: Si est\u00e1 pensando en comprar su propia GPU pero no est\u00e1 seguro de qu\u00e9 comprar, [Tim Dettmers tiene una gu\u00eda excelente](https://timdettmers.com/2020/09/07/ Which -gpu-para-aprendizaje-profundo/).</p> <p>Para verificar si tiene acceso a una GPU Nvidia, puede ejecutar <code>!nvidia-smi</code> donde <code>!</code> (tambi\u00e9n llamado bang) significa \"ejecutar esto en la l\u00ednea de comando\".</p>"},{"location":"04-00_pytorch_fundamentals/#2-hacer-que-pytorch-se-ejecute-en-la-gpu","title":"2. Hacer que PyTorch se ejecute en la GPU\u00b6","text":"<p>Una vez que tenga una GPU lista para acceder, el siguiente paso es utilizar PyTorch para almacenar datos (tensores) y calcular datos (realizar operaciones con tensores).</p> <p>Para hacerlo, puede utilizar el paquete <code>torch.cuda</code>.</p> <p>En lugar de hablar de ello, prob\u00e9moslo.</p> <p>Puede probar si PyTorch tiene acceso a una GPU usando <code>torch.cuda.is_available()</code>.</p>"},{"location":"04-00_pytorch_fundamentals/#21-hacer-que-pytorch-se-ejecute-en-apple-silicon","title":"2.1 Hacer que PyTorch se ejecute en Apple Silicon\u00b6","text":"<p>Para ejecutar PyTorch en las GPU M1/M2/M3 de Apple, puede utilizar el m\u00f3dulo <code>torch.backends.mps</code>.</p> <p>Aseg\u00farese de que las versiones de macOS y Pytorch est\u00e9n actualizadas.</p> <p>Puede probar si PyTorch tiene acceso a una GPU usando <code>torch.backends.mps.is_available()</code>.</p>"},{"location":"04-00_pytorch_fundamentals/#3-poner-tensores-y-modelos-en-la-gpu","title":"3. Poner tensores (y modelos) en la GPU\u00b6","text":"<p>Puede colocar tensores (y modelos, lo veremos m\u00e1s adelante) en un dispositivo espec\u00edfico llamando a [<code>to(device)</code>](https://pytorch.org/docs/stable/generated/torch.Tensor.to. html) en ellos. Donde \"dispositivo\" es el dispositivo de destino al que desea que vaya el tensor (o modelo).</p> <p>\u00bfPor qu\u00e9 hacer esto?</p> <p>Las GPU ofrecen computaci\u00f3n num\u00e9rica mucho m\u00e1s r\u00e1pida que las CPU y, si no hay una GPU disponible, debido a nuestro c\u00f3digo independiente del dispositivo (ver arriba), se ejecutar\u00e1 en la CPU.</p> <p>Nota: Poner un tensor en la GPU usando <code>to(dispositivo)</code> (por ejemplo, <code>some_tensor.to(device)</code>) devuelve una copia de ese tensor, p.ej. el mismo tensor estar\u00e1 en CPU y GPU. Para sobrescribir tensores, reas\u00edgnalos:</p> <p><code>alg\u00fan_tensor = alg\u00fan_tensor.to(dispositivo)</code></p> <p>Intentemos crear un tensor y ponerlo en la GPU (si est\u00e1 disponible).</p>"},{"location":"04-00_pytorch_fundamentals/#4-mover-tensores-de-regreso-a-la-cpu","title":"4. Mover tensores de regreso a la CPU\u00b6","text":"<p>\u00bfQu\u00e9 pasar\u00eda si quisi\u00e9ramos mover el tensor de nuevo a la CPU?</p> <p>Por ejemplo, querr\u00e1s hacer esto si quieres interactuar con tus tensores con NumPy (NumPy no aprovecha la GPU).</p> <p>Intentemos usar el m\u00e9todo <code>torch.Tensor.numpy()</code> en nuestro <code>tensor_on_gpu</code>.</p>"},{"location":"04-00_pytorch_fundamentals/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Todos los ejercicios se centran en practicar el c\u00f3digo anterior.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Recursos:</p> <ul> <li>Cuaderno de plantilla de ejercicios para 00.</li> <li>Cuaderno de soluciones de ejemplo para 00 (pruebe los ejercicios antes de mirar esto).</li> </ul> <ol> <li>Lectura de documentaci\u00f3n: una gran parte del aprendizaje profundo (y de aprender a codificar en general) es familiarizarse con la documentaci\u00f3n de un marco determinado que est\u00e1s utilizando. Usaremos mucho la documentaci\u00f3n de PyTorch durante el resto de este curso. As\u00ed que recomendar\u00eda dedicar 10 minutos a leer lo siguiente (est\u00e1 bien si no entiendes algunas cosas por ahora, el enfoque a\u00fan no es la comprensi\u00f3n total, sino la conciencia). Consulte la documentaci\u00f3n en <code>torch.Tensor</code> y para [<code>torch.cuda</code>](https://pytorch.org/ docs/master/notes/cuda.html#cuda-semantics).</li> <li>Crea un tensor aleatorio con forma <code>(7, 7)</code>.</li> <li>Realiza una multiplicaci\u00f3n matricial del tensor de 2 con otro tensor aleatorio con forma <code>(1, 7)</code> (pista: es posible que tengas que transponer el segundo tensor).</li> <li>Establezca la semilla aleatoria en \"0\" y repita los ejercicios 2 y 3.</li> <li>Hablando de semillas aleatorias, vimos c\u00f3mo configurarlas con <code>torch.manual_seed()</code> pero \u00bfexiste una GPU equivalente? (Pista: necesitar\u00e1s consultar la documentaci\u00f3n de <code>torch.cuda</code> para este caso). Si es as\u00ed, configure la semilla aleatoria de la GPU en \"1234\".</li> <li>Cree dos tensores aleatorios de forma <code>(2, 3)</code> y env\u00edelos a la GPU (necesitar\u00e1 acceso a una GPU para esto). Configure <code>torch.manual_seed(1234)</code> al crear los tensores (no tiene que ser la semilla aleatoria de la GPU).</li> <li>Realiza una multiplicaci\u00f3n de matrices en los tensores que creaste en 6 (nuevamente, es posible que tengas que ajustar las formas de uno de los tensores).</li> <li>Encuentre los valores m\u00e1ximo y m\u00ednimo de la salida de 7.</li> <li>Encuentre los valores de \u00edndice m\u00e1ximo y m\u00ednimo de la salida de 7.</li> <li>Haga un tensor aleatorio con forma <code>(1, 1, 1, 10)</code> y luego cree un nuevo tensor con todas las dimensiones <code>1</code> eliminadas para quedar con un tensor de forma <code>(10)</code>. Establezca la semilla en <code>7</code> cuando la cree e imprima el primer tensor y su forma, as\u00ed como el segundo tensor y su forma.</li> </ol>"},{"location":"04-00_pytorch_fundamentals/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Dedique 1 hora a leer el tutorial b\u00e1sico de PyTorch (recomiendo el [Inicio r\u00e1pido](https://pytorch.org/ tutorials/beginner/basics/quickstart_tutorial.html) y Tensores secciones).</li> <li>Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo un tensor puede representar datos, vea este v\u00eddeo: \u00bfQu\u00e9 es un tensor?</li> </ul>"},{"location":"05-01_pytorch_workflow/","title":"01. Fundamentos del flujo de trabajo de PyTorch","text":"<p>Ver c\u00f3digo fuente | Ver diapositivas | Ver v\u00eddeo tutorial</p> In\u00a0[\u00a0]: Copied! <pre>what_were_covering = {1: \"data (prepare and load)\",\n    2: \"build model\",\n    3: \"fitting the model to data (training)\",\n    4: \"making predictions and evaluating a model (inference)\",\n    5: \"saving and loading a model\",\n    6: \"putting it all together\"\n}\n</pre> what_were_covering = {1: \"data (prepare and load)\",     2: \"build model\",     3: \"fitting the model to data (training)\",     4: \"making predictions and evaluating a model (inference)\",     5: \"saving and loading a model\",     6: \"putting it all together\" } <p>Y ahora importemos lo que necesitaremos para este m\u00f3dulo.</p> <p>Obtendremos <code>torch</code>, <code>torch.nn</code> (<code>nn</code> significa red neuronal y este paquete contiene los componentes b\u00e1sicos para crear redes neuronales en PyTorch) y <code>matplotlib</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Verifique la versi\u00f3n de PyTorch\ntorch.__version__\n</pre> import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Verifique la versi\u00f3n de PyTorch torch.__version__ In\u00a0[\u00a0]: Copied! <pre># Crear par\u00e1metros *conocidos*\nweight = 0.7\nbias = 0.3\n\n# Crear datos\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n</pre> # Crear par\u00e1metros *conocidos* weight = 0.7 bias = 0.3  # Crear datos start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) y = weight * X + bias  X[:10], y[:10] <p>\u00a1Hermoso! Ahora vamos a avanzar hacia la construcci\u00f3n de un modelo que pueda aprender la relaci\u00f3n entre \"X\" (caracter\u00edsticas) e \"y\" (etiquetas).</p> In\u00a0[\u00a0]: Copied! <pre># Crear divisi\u00f3n de tren/prueba\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Crear divisi\u00f3n de tren/prueba train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) <p>Maravilloso, tenemos 40 muestras para entrenamiento (<code>X_train</code> &amp; <code>y_train</code>) y 10 muestras para prueba (<code>X_test</code> &amp; <code>y_test</code>).</p> <p>El modelo que creamos intentar\u00e1 aprender la relaci\u00f3n entre <code>X_train</code> e <code>y_train</code> y luego evaluaremos lo que aprende en <code>X_test</code> e <code>y_test</code>.</p> <p>Pero ahora nuestros datos son s\u00f3lo n\u00fameros en una p\u00e1gina.</p> <p>Creemos una funci\u00f3n para visualizarlo.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n  \n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=None):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))    # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")      # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")    if predictions is not None:     # Plot the predictions in red (predictions were made on the test data)     plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")    # Show the legend   plt.legend(prop={\"size\": 14}); In\u00a0[\u00a0]: Copied! <pre>plot_predictions();\n</pre> plot_predictions(); <p>\u00a1\u00c9pico!</p> <p>Ahora, en lugar de ser s\u00f3lo n\u00fameros en una p\u00e1gina, nuestros datos son una l\u00ednea recta.</p> <p>Nota: Ahora es un buen momento para presentarle el lema del explorador de datos... \"\u00a1visualizar, visualizar, visualizar!\"</p> <p>Piensa en esto siempre que trabajes con datos y los conviertas en n\u00fameros: si puedes visualizar algo, puede hacer maravillas para la comprensi\u00f3n.</p> <p>A las m\u00e1quinas les encantan los n\u00fameros y a nosotros, los humanos, tambi\u00e9n nos gustan los n\u00fameros, pero tambi\u00e9n nos gusta mirar las cosas.</p> In\u00a0[\u00a0]: Copied! <pre># Crear una clase de modelo de regresi\u00f3n lineal\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                                dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                   requires_grad=True) # &lt;- can we update this value with gradient descent?)\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                            dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                requires_grad=True) # &lt;- can we update this value with gradient descent?))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n</pre> # Crear una clase de modelo de regresi\u00f3n lineal class LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)     def __init__(self):         super().__init__()          self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)                                                 dtype=torch.float), # &lt;- PyTorch loves float32 by default                                    requires_grad=True) # &lt;- can we update this value with gradient descent?)          self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)                                             dtype=torch.float), # &lt;- PyTorch loves float32 by default                                 requires_grad=True) # &lt;- can we update this value with gradient descent?))      # Forward defines the computation in the model     def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)         return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b) <p>Muy bien, est\u00e1n sucediendo bastantes cosas arriba, pero analic\u00e9moslas poco a poco.</p> <p>Recurso: Usaremos clases de Python para crear fragmentos para construir redes neuronales. Si no est\u00e1 familiarizado con la notaci\u00f3n de clases de Python, le recomiendo leer la [Gu\u00eda de programaci\u00f3n orientada a objetos de Real Python en Python 3] (https://realpython.com/python3-object- Oriented-programming/) varias veces.</p> In\u00a0[\u00a0]: Copied! <pre># Establezca la semilla manual ya que nn. Los par\u00e1metros se inicializan aleatoriamente\ntorch.manual_seed(42)\n\n# Cree una instancia del modelo (esta es una subclase de nn.Module que contiene nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Verifique los nn.Parameter(s) dentro de la subclase nn.Module que creamos\nlist(model_0.parameters())\n</pre> # Establezca la semilla manual ya que nn. Los par\u00e1metros se inicializan aleatoriamente torch.manual_seed(42)  # Cree una instancia del modelo (esta es una subclase de nn.Module que contiene nn.Parameter(s)) model_0 = LinearRegressionModel()  # Verifique los nn.Parameter(s) dentro de la subclase nn.Module que creamos list(model_0.parameters()) <p>Tambi\u00e9n podemos obtener el estado (lo que contiene el modelo) del modelo usando [<code>.state_dict()</code>](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn .Module.state_dict).</p> In\u00a0[\u00a0]: Copied! <pre># Lista de par\u00e1metros con nombre\nmodel_0.state_dict()\n</pre> # Lista de par\u00e1metros con nombre model_0.state_dict() <p>\u00bfObserva c\u00f3mo los valores de <code>pesos</code> y <code>sesgo</code> de <code>model_0.state_dict()</code> aparecen como tensores flotantes aleatorios?</p> <p>Esto se debe a que los inicializamos anteriormente usando <code>torch.randn()</code>.</p> <p>B\u00e1sicamente, queremos comenzar a partir de par\u00e1metros aleatorios y hacer que el modelo los actualice hacia los par\u00e1metros que mejor se ajusten a nuestros datos (los valores codificados de \"peso\" y \"sesgo\" que configuramos al crear nuestros datos en l\u00ednea recta).</p> <p>Ejercicio: Intente cambiar el valor <code>torch.manual_seed()</code> dos celdas arriba, vea qu\u00e9 sucede con los pesos y los valores de sesgo.</p> <p>Debido a que nuestro modelo comienza con valores aleatorios, en este momento tendr\u00e1 poco poder predictivo.</p> In\u00a0[\u00a0]: Copied! <pre># Hacer predicciones con modelo.\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Nota: en el c\u00f3digo PyTorch anterior es posible que tambi\u00e9n vea torch.no_grad()\n# con antorcha.no_grad():\n# y_preds = model_0(X_test)\n</pre> # Hacer predicciones con modelo. with torch.inference_mode():      y_preds = model_0(X_test)  # Nota: en el c\u00f3digo PyTorch anterior es posible que tambi\u00e9n vea torch.no_grad() # con antorcha.no_grad(): # y_preds = model_0(X_test) <p>\u00bfMmm?</p> <p>Probablemente hayas notado que usamos <code>torch.inference_mode()</code> como [administrador de contexto](https://realpython.com/ python-with-statement/) (eso es lo que es <code>with torch.inference_mode():</code>) para hacer las predicciones.</p> <p>Como sugiere el nombre, <code>torch.inference_mode()</code> se utiliza cuando se utiliza un modelo para inferencia (hacer predicciones).</p> <p><code>torch.inference_mode()</code> desactiva un mont\u00f3n de cosas (como el seguimiento de gradiente, que es necesario para el entrenamiento pero no para la inferencia) para hacer pasos hacia adelante (datos que pasan por el m\u00e9todo <code>forward()</code>) m\u00e1s r\u00e1pido .</p> <p>Nota: En el c\u00f3digo PyTorch anterior, tambi\u00e9n puede ver que se usa <code>torch.no_grad()</code> para inferencia. Mientras que <code>torch.inference_mode()</code> y <code>torch.no_grad()</code> hacen cosas similares, <code>torch.inference_mode()</code> es m\u00e1s nuevo, potencialmente m\u00e1s r\u00e1pido y preferido. Consulte este Tweet de PyTorch para obtener m\u00e1s informaci\u00f3n.</p> <p>Hemos hecho algunas predicciones, veamos c\u00f3mo son.</p> In\u00a0[\u00a0]: Copied! <pre># Consulta las predicciones\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n</pre> # Consulta las predicciones print(f\"Number of testing samples: {len(X_test)}\")  print(f\"Number of predictions made: {len(y_preds)}\") print(f\"Predicted values:\\n{y_preds}\") <p>Observe c\u00f3mo hay un valor de predicci\u00f3n por muestra de prueba.</p> <p>Esto se debe al tipo de datos que estamos utilizando. Para nuestra l\u00ednea recta, un valor \"X\" se asigna a un valor \"y\".</p> <p>Sin embargo, los modelos de aprendizaje autom\u00e1tico son muy flexibles. Podr\u00eda tener 100 valores \"X\" asignados a uno, dos, tres o 10 valores \"y\". Todo depende de en qu\u00e9 est\u00e9s trabajando.</p> <p>Nuestras predicciones siguen siendo n\u00fameros en una p\u00e1gina, visualic\u00e9moslas con nuestra funci\u00f3n <code>plot_predictions()</code> que creamos anteriormente.</p> In\u00a0[\u00a0]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) In\u00a0[\u00a0]: Copied! <pre>y_test - y_preds\n</pre> y_test - y_preds <p>\u00a1Guau! Esas predicciones parecen bastante malas...</p> <p>Sin embargo, esto tiene sentido si recuerda que nuestro modelo solo usa valores de par\u00e1metros aleatorios para hacer predicciones.</p> <p>Ni siquiera ha mirado los puntos azules para intentar predecir los puntos verdes.</p> <p>Es hora de cambiar eso.</p> In\u00a0[\u00a0]: Copied! <pre># Crear la funci\u00f3n de p\u00e9rdida\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Crear el optimizador\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n</pre> # Crear la funci\u00f3n de p\u00e9rdida loss_fn = nn.L1Loss() # MAE loss is same as L1Loss  # Crear el optimizador optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize                             lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time)) In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Establezca el n\u00famero de \u00e9pocas (cu\u00e1ntas veces el modelo pasar\u00e1 por los datos de entrenamiento)\nepochs = 100\n\n# Cree listas de p\u00e9rdidas vac\u00edas para realizar un seguimiento de los valores\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n</pre> torch.manual_seed(42)  # Establezca el n\u00famero de \u00e9pocas (cu\u00e1ntas veces el modelo pasar\u00e1 por los datos de entrenamiento) epochs = 100  # Cree listas de p\u00e9rdidas vac\u00edas para realizar un seguimiento de los valores train_loss_values = [] test_loss_values = [] epoch_count = []  for epoch in range(epochs):     ### Training      # Put model in training mode (this is the default state of a model)     model_0.train()      # 1. Forward pass on train data using the forward() method inside      y_pred = model_0(X_train)     # print(y_pred)      # 2. Calculate the loss (how different are our models predictions to the ground truth)     loss = loss_fn(y_pred, y_train)      # 3. Zero grad of the optimizer     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Progress the optimizer     optimizer.step()      ### Testing      # Put the model in evaluation mode     model_0.eval()      with torch.inference_mode():       # 1. Forward pass on test data       test_pred = model_0(X_test)        # 2. Caculate loss on test data       test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type        # Print out what's happening       if epoch % 10 == 0:             epoch_count.append(epoch)             train_loss_values.append(loss.detach().numpy())             test_loss_values.append(test_loss.detach().numpy())             print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \") <p>\u00a1Oh, mirar\u00edas eso! Parece que nuestra p\u00e9rdida disminuye con cada \u00e9poca, tracemos un diagrama para descubrirlo.</p> In\u00a0[\u00a0]: Copied! <pre># Trazar las curvas de p\u00e9rdida\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Trazar las curvas de p\u00e9rdida plt.plot(epoch_count, train_loss_values, label=\"Train loss\") plt.plot(epoch_count, test_loss_values, label=\"Test loss\") plt.title(\"Training and test loss curves\") plt.ylabel(\"Loss\") plt.xlabel(\"Epochs\") plt.legend(); <p>\u00a1Lindo! Las curvas de p\u00e9rdida muestran que la p\u00e9rdida disminuye con el tiempo. Recuerde, la p\u00e9rdida es la medida de qu\u00e9 tan incorrecto es su modelo, por lo que cuanto menor sea, mejor.</p> <p>Pero \u00bfpor qu\u00e9 disminuy\u00f3 la p\u00e9rdida?</p> <p>Bueno, gracias a nuestra funci\u00f3n de p\u00e9rdida y optimizador, los par\u00e1metros internos del modelo (\"pesos\" y \"sesgo\") se actualizaron para reflejar mejor los patrones subyacentes en los datos.</p> <p>Inspeccionemos el <code>.state_dict()</code> de nuestro modelo para ver qu\u00e9 tan cerca llega nuestro modelo de los valores originales que establecimos para ponderaciones y sesgos.</p> In\u00a0[\u00a0]: Copied! <pre># Encuentre los par\u00e1metros aprendidos de nuestro modelo.\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Encuentre los par\u00e1metros aprendidos de nuestro modelo. print(\"The model learned the following values for weights and bias:\") print(model_0.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <p>\u00a1Guau! \u00bfCuan genial es eso?</p> <p>Nuestro modelo se acerc\u00f3 mucho para calcular los valores originales exactos de \"peso\" y \"sesgo\" (y probablemente se acercar\u00eda a\u00fan m\u00e1s si lo entren\u00e1ramos por m\u00e1s tiempo).</p> <p>Ejercicio: Intente cambiar el valor de <code>\u00e9pocas</code> anterior a 200, \u00bfqu\u00e9 sucede con las curvas de p\u00e9rdida y los pesos y valores de los par\u00e1metros de sesgo del modelo?</p> <p>Probablemente nunca los adivine perfectamente (especialmente cuando se usan conjuntos de datos m\u00e1s complicados), pero est\u00e1 bien, a menudo puedes hacer cosas muy interesantes con una aproximaci\u00f3n cercana.</p> <p>Esta es la idea completa del aprendizaje autom\u00e1tico y el aprendizaje profundo, hay algunos valores ideales que describen nuestros datos y en lugar de descifrarlos a mano, podemos entrenar un modelo para descifrarlos mediante programaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Configure el modelo en modo de evaluaci\u00f3n.\nmodel_0.eval()\n\n# 2. Configurar el administrador de contexto del modo de inferencia.\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n</pre> # 1. Configure el modelo en modo de evaluaci\u00f3n. model_0.eval()  # 2. Configurar el administrador de contexto del modo de inferencia. with torch.inference_mode():   # 3. Make sure the calculations are done with the model and data on the same device   # in our case, we haven't setup device-agnostic code yet so our data and model are   # on the CPU by default.   # model_0.to(device)   # X_test = X_test.to(device)   y_preds = model_0(X_test) y_preds <p>\u00a1Lindo! Hemos hecho algunas predicciones con nuestro modelo entrenado, \u00bfc\u00f3mo se ven ahora?</p> In\u00a0[\u00a0]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) <p>\u00a1Guau! \u00a1Esos puntos rojos se ven mucho m\u00e1s cerca que antes!</p> <p>Pasemos a guardar y recargar un modelo en PyTorch.</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# 1. Crear directorio de modelos\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Crear ruta para guardar el modelo\nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Guarde el dictado del estado del modelo.\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # 1. Crear directorio de modelos MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Crear ruta para guardar el modelo MODEL_NAME = \"01_pytorch_workflow_model_0.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Guarde el dictado del estado del modelo. print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  In\u00a0[\u00a0]: Copied! <pre># Verifique la ruta del archivo guardado\n!ls -l models/01_pytorch_workflow_model_0.pth\n</pre> # Verifique la ruta del archivo guardado !ls -l models/01_pytorch_workflow_model_0.pth In\u00a0[\u00a0]: Copied! <pre># Crear una nueva instancia de nuestro modelo (esto se crear\u00e1 con pesos aleatorios)\nloaded_model_0 = LinearRegressionModel()\n\n# Cargue el state_dict de nuestro modelo guardado (esto actualizar\u00e1 la nueva instancia de nuestro modelo con pesos entrenados)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</pre> # Crear una nueva instancia de nuestro modelo (esto se crear\u00e1 con pesos aleatorios) loaded_model_0 = LinearRegressionModel()  # Cargue el state_dict de nuestro modelo guardado (esto actualizar\u00e1 la nueva instancia de nuestro modelo con pesos entrenados) loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) <p>\u00a1Excelente! Parece que las cosas coincidieron.</p> <p>Ahora, para probar nuestro modelo cargado, realicemos inferencias con \u00e9l (hagamos predicciones) en los datos de prueba.</p> <p>\u00bfRecuerda las reglas para realizar inferencias con modelos de PyTorch?</p> <p>Si no, aqu\u00ed hay un repaso:</p> Reglas de inferencia de PyTorch <ol> <li> Establezca el modelo en modo de evaluaci\u00f3n (<code>model.eval()</code>). </li> <li> Realice las predicciones utilizando el administrador de contexto del modo de inferencia (<code>con torch.inference_mode(): ...</code>). </li> <li> Todas las predicciones deben realizarse con objetos en el mismo dispositivo (por ejemplo, datos y modelo solo en GPU o datos y modelo solo en CPU).</li> </ol> In\u00a0[\u00a0]: Copied! <pre># 1. Ponga el modelo cargado en modo de evaluaci\u00f3n.\nloaded_model_0.eval()\n\n# 2. Utilice el administrador de contexto del modo de inferencia para hacer predicciones.\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n</pre> # 1. Ponga el modelo cargado en modo de evaluaci\u00f3n. loaded_model_0.eval()  # 2. Utilice el administrador de contexto del modo de inferencia para hacer predicciones. with torch.inference_mode():     loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model <p>Ahora que hemos hecho algunas predicciones con el modelo cargado, veamos si son las mismas que las predicciones anteriores.</p> In\u00a0[\u00a0]: Copied! <pre># Compare las predicciones del modelo anterior con las predicciones del modelo cargado (deber\u00edan ser iguales)\ny_preds == loaded_model_preds\n</pre> # Compare las predicciones del modelo anterior con las predicciones del modelo cargado (deber\u00edan ser iguales) y_preds == loaded_model_preds <p>\u00a1Lindo!</p> <p>Parece que las predicciones del modelo cargado son las mismas que las predicciones del modelo anterior (predicciones realizadas antes de guardar). Esto indica que nuestro modelo se est\u00e1 guardando y cargando como se esperaba.</p> <p>Nota: Hay m\u00e1s m\u00e9todos para guardar y cargar modelos de PyTorch, pero los dejar\u00e9 para actividades extracurriculares y lecturas adicionales. Consulte la gu\u00eda de PyTorch para guardar y cargar modelos para obtener m\u00e1s informaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># Importar PyTorch y matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Verifique la versi\u00f3n de PyTorch\ntorch.__version__\n</pre> # Importar PyTorch y matplotlib import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Verifique la versi\u00f3n de PyTorch torch.__version__ <p>Ahora comencemos a hacer que nuestro c\u00f3digo sea independiente del dispositivo configurando <code>device=\"cuda\"</code> si est\u00e1 disponible; de \u200b\u200blo contrario, el valor predeterminado ser\u00e1 <code>device=\"cpu\"</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar c\u00f3digo independiente del dispositivo\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</pre> # Configurar c\u00f3digo independiente del dispositivo device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") <p>Si tiene acceso a una GPU, lo anterior deber\u00eda haberse impreso:</p> <pre><code>Dispositivo de uso: cuda\n</code></pre> <p>De lo contrario, utilizar\u00e1 una CPU para los siguientes c\u00e1lculos. Esto est\u00e1 bien para nuestro peque\u00f1o conjunto de datos, pero llevar\u00e1 m\u00e1s tiempo para conjuntos de datos m\u00e1s grandes.</p> In\u00a0[\u00a0]: Copied! <pre># Crear peso y sesgo\nweight = 0.7\nbias = 0.3\n\n# Crear valores de rango\nstart = 0\nend = 1\nstep = 0.02\n\n# Crear X e y (caracter\u00edsticas y etiquetas)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:10], y[:10]\n</pre> # Crear peso y sesgo weight = 0.7 bias = 0.3  # Crear valores de rango start = 0 end = 1 step = 0.02  # Crear X e y (caracter\u00edsticas y etiquetas) X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers) y = weight * X + bias  X[:10], y[:10] <p>\u00a1Maravilloso!</p> <p>Ahora que tenemos algunos datos, divid\u00e1moslos en conjuntos de entrenamiento y prueba.</p> <p>Usaremos una divisi\u00f3n 80/20 con 80% de datos de entrenamiento y 20% de datos de prueba.</p> In\u00a0[\u00a0]: Copied! <pre># Dividir datos\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Dividir datos train_split = int(0.8 * len(X)) X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) <p>Excelente, visualic\u00e9moslos para asegurarnos de que se vean bien.</p> In\u00a0[\u00a0]: Copied! <pre># Nota: Si ha restablecido su tiempo de ejecuci\u00f3n, esta funci\u00f3n no funcionar\u00e1.\n# Tendr\u00e1s que volver a ejecutar la celda de arriba donde se cre\u00f3 la instancia.\nplot_predictions(X_train, y_train, X_test, y_test)\n</pre> # Nota: Si ha restablecido su tiempo de ejecuci\u00f3n, esta funci\u00f3n no funcionar\u00e1. # Tendr\u00e1s que volver a ejecutar la celda de arriba donde se cre\u00f3 la instancia. plot_predictions(X_train, y_train, X_test, y_test) In\u00a0[\u00a0]: Copied! <pre># Subclase nn.M\u00f3dulo para realizar nuestro modelo.\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Establezca la semilla manual al crear el modelo (esto no siempre es necesario, pero se usa con fines demostrativos, intente comentarlo y ver qu\u00e9 sucede)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n</pre> # Subclase nn.M\u00f3dulo para realizar nuestro modelo. class LinearRegressionModelV2(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  # Establezca la semilla manual al crear el modelo (esto no siempre es necesario, pero se usa con fines demostrativos, intente comentarlo y ver qu\u00e9 sucede) torch.manual_seed(42) model_1 = LinearRegressionModelV2() model_1, model_1.state_dict() <p>Observe las salidas de <code>model_1.state_dict()</code>, la capa <code>nn.Linear()</code> cre\u00f3 un par\u00e1metro aleatorio de <code>peso</code> y <code>bias</code> para nosotros.</p> <p>Ahora coloquemos nuestro modelo en la GPU (si est\u00e1 disponible).</p> <p>Podemos cambiar el dispositivo en el que se encuentran nuestros objetos PyTorch usando <code>.to(device)</code>.</p> <p>Primero, verifiquemos el dispositivo actual del modelo.</p> In\u00a0[\u00a0]: Copied! <pre># Comprobar modelo de dispositivo\nnext(model_1.parameters()).device\n</pre> # Comprobar modelo de dispositivo next(model_1.parameters()).device <p>Maravilloso, parece que el modelo est\u00e1 en la CPU de forma predeterminada.</p> <p>Cambi\u00e9moslo para que est\u00e9 en la GPU (si est\u00e1 disponible).</p> In\u00a0[\u00a0]: Copied! <pre># Configure el modelo en GPU si est\u00e1 disponible; de \u200b\u200blo contrario, el valor predeterminado ser\u00e1 CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n</pre> # Configure el modelo en GPU si est\u00e1 disponible; de \u200b\u200blo contrario, el valor predeterminado ser\u00e1 CPU model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not next(model_1.parameters()).device <p>\u00a1Lindo! Debido a nuestro c\u00f3digo independiente del dispositivo, la celda anterior funcionar\u00e1 independientemente de si hay una GPU disponible o no.</p> <p>Si tiene acceso a una GPU habilitada para CUDA, deber\u00eda ver un resultado similar a:</p> <pre><code>dispositivo (tipo = 'cuda', \u00edndice = 0)\n</code></pre> <p>Es hora de crear un circuito de capacitaci\u00f3n y prueba.</p> <p>Primero necesitaremos una funci\u00f3n de p\u00e9rdida y un optimizador.</p> <p>Usemos las mismas funciones que usamos antes, <code>nn.L1Loss()</code> y <code>torch.optim.SGD()</code>.</p> <p>Tendremos que pasar los par\u00e1metros del nuevo modelo (<code>model.parameters()</code>) al optimizador para que los ajuste durante el entrenamiento.</p> <p>La tasa de aprendizaje de <code>0.01</code> tambi\u00e9n funcion\u00f3 bien antes, as\u00ed que us\u00e9mosla nuevamente.</p> In\u00a0[\u00a0]: Copied! <pre># Crear funci\u00f3n de p\u00e9rdida\nloss_fn = nn.L1Loss()\n\n# Crear optimizador\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Crear funci\u00f3n de p\u00e9rdida loss_fn = nn.L1Loss()  # Crear optimizador optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) <p>Hermoso, funci\u00f3n de p\u00e9rdida y optimizador listo, ahora entrenemos y evaluemos nuestro modelo usando un ciclo de entrenamiento y prueba.</p> <p>La \u00fanica diferencia que haremos en este paso en comparaci\u00f3n con el ciclo de entrenamiento anterior es colocar los datos en el \"dispositivo\" de destino.</p> <p>Ya hemos puesto nuestro modelo en el <code>dispositivo</code> de destino usando <code>model_1.to(device)</code>.</p> <p>Y podemos hacer lo mismo con los datos.</p> <p>De esa manera, si el modelo est\u00e1 en la GPU, los datos est\u00e1n en la GPU (y viceversa).</p> <p>Esta vez demos un paso m\u00e1s y establezcamos <code>epochs=1000</code>.</p> <p>Si necesita un recordatorio de los pasos del ciclo de entrenamiento de PyTorch, consulte a continuaci\u00f3n.</p> Pasos del ciclo de entrenamiento de PyTorch <ol> <li>Pase hacia adelante: el modelo revisa todos los datos de entrenamiento una vez y realiza su             Funci\u00f3n adelante()             c\u00e1lculos (<code>model(x_train)</code>).         </li> <li>Calcule la p\u00e9rdida: los resultados del modelo (predicciones) se comparan con la verdad fundamental y se eval\u00faan.             para ver como             Est\u00e1n equivocados (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Gradientes cero: los gradientes del optimizador se establecen en cero (se acumulan de forma predeterminada) para que             puede ser             recalculado para el paso de entrenamiento espec\u00edfico (<code>optimizer.zero_grad()</code>).</li> <li>Realizar retropropagaci\u00f3n de la p\u00e9rdida: calcula el gradiente de la p\u00e9rdida con respecto a cada modelo.             par\u00e1metro a             ser actualizado (cada par\u00e1metro             con <code>requires_grad=True</code>). Esto se conoce como propagaci\u00f3n hacia atr\u00e1s, por lo tanto, \"hacia atr\u00e1s\".             (<code>p\u00e9rdida.backward()</code>).</li> <li>Pasa el optimizador (descenso de gradiente) - Actualiza los par\u00e1metros con <code>requires_grad=True</code>             con respecto a la p\u00e9rdida             gradientes para mejorarlos (<code>optimizer.step()</code>).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Establecer el n\u00famero de \u00e9pocas\nepochs = 1000 \n\n# Poner datos en el dispositivo disponible.\n# Sin esto, se producir\u00e1 un error (no todos los modelos/datos del dispositivo)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Establecer el n\u00famero de \u00e9pocas epochs = 1000   # Poner datos en el dispositivo disponible. # Sin esto, se producir\u00e1 un error (no todos los modelos/datos del dispositivo) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <p>Nota: Debido a la naturaleza aleatoria del aprendizaje autom\u00e1tico, es probable que obtenga resultados ligeramente diferentes (diferentes valores de p\u00e9rdida y predicci\u00f3n) dependiendo de si su modelo fue entrenado en CPU o GPU. Esto es cierto incluso si usa la misma semilla aleatoria en cualquiera de los dispositivos. Si la diferencia es grande, es posible que desee buscar errores; sin embargo, si es peque\u00f1a (idealmente lo es), puede ignorarla.</p> <p>\u00a1Lindo! Esa p\u00e9rdida parece bastante baja.</p> <p>Verifiquemos los par\u00e1metros que nuestro modelo ha aprendido y comp\u00e1relos con los par\u00e1metros originales que codificamos.</p> In\u00a0[\u00a0]: Copied! <pre># Encuentre los par\u00e1metros aprendidos de nuestro modelo.\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Encuentre los par\u00e1metros aprendidos de nuestro modelo. from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html  print(\"The model learned the following values for weights and bias:\") pprint(model_1.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <p>\u00a1Ho, ho! Eso est\u00e1 bastante cerca de ser un modelo perfecto.</p> <p>Sin embargo, recuerde que, en la pr\u00e1ctica, es raro que conozca los par\u00e1metros perfectos de antemano.</p> <p>Y si supiera de antemano los par\u00e1metros que su modelo debe aprender, \u00bfcu\u00e1l ser\u00eda la diversi\u00f3n del aprendizaje autom\u00e1tico?</p> <p>Adem\u00e1s, en muchos problemas de aprendizaje autom\u00e1tico del mundo real, la cantidad de par\u00e1metros puede superar las decenas de millones.</p> <p>No s\u00e9 ustedes, pero prefiero escribir c\u00f3digo para que una computadora los resuelva en lugar de hacerlo a mano.</p> In\u00a0[\u00a0]: Copied! <pre># Convertir el modelo en modo de evaluaci\u00f3n\nmodel_1.eval()\n\n# Hacer predicciones sobre los datos de prueba.\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n</pre> # Convertir el modelo en modo de evaluaci\u00f3n model_1.eval()  # Hacer predicciones sobre los datos de prueba. with torch.inference_mode():     y_preds = model_1(X_test) y_preds <p>Si est\u00e1 haciendo predicciones con datos en la GPU, es posible que observe que el resultado anterior tiene <code>device='cuda:0'</code> hacia el final. Eso significa que los datos est\u00e1n en el dispositivo CUDA 0 (la primera GPU a la que tiene acceso su sistema debido a la indexaci\u00f3n cero); si termina usando varias GPU en el futuro, este n\u00famero puede ser mayor.</p> <p>Ahora tracemos las predicciones de nuestro modelo.</p> <p>Nota: Muchas bibliotecas de ciencia de datos, como pandas, matplotlib y NumPy, no son capaces de utilizar datos almacenados en la GPU. Por lo tanto, es posible que tenga algunos problemas al intentar utilizar una funci\u00f3n de una de estas bibliotecas con datos tensoriales no almacenados en la CPU. Para solucionar este problema, puede llamar a <code>.cpu()</code> en su tensor objetivo para devolver una copia de su tensor objetivo. en la CPU.</p> In\u00a0[\u00a0]: Copied! <pre># plot_predictions(predictions=y_preds) # -&gt; no funcionar\u00e1... los datos no est\u00e1n en la CPU\n\n# Poner datos en la CPU y trazarlos.\nplot_predictions(predictions=y_preds.cpu())\n</pre> # plot_predictions(predictions=y_preds) # -&gt; no funcionar\u00e1... los datos no est\u00e1n en la CPU  # Poner datos en la CPU y trazarlos. plot_predictions(predictions=y_preds.cpu()) <p>\u00a1Guau! Mira esos puntos rojos, se alinean casi perfectamente con los puntos verdes. Supongo que las \u00e9pocas adicionales ayudaron.</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# 1. Crear directorio de modelos\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Crear ruta para guardar el modelo\nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Guarde el dictado del estado del modelo.\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # 1. Crear directorio de modelos MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Crear ruta para guardar el modelo MODEL_NAME = \"01_pytorch_workflow_model_1.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Guarde el dictado del estado del modelo. print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <p>Y solo para asegurarnos de que todo funcion\u00f3 bien, volvamos a cargarlo.</p> <p>Bien:</p> <ul> <li>Crear una nueva instancia de la clase <code>LinearRegressionModelV2()</code></li> <li>Cargar en el dictado de estado del modelo usando <code>torch.nn.Module.load_state_dict()</code></li> <li>Enviar la nueva instancia del modelo al dispositivo de destino (para garantizar que nuestro c\u00f3digo sea independiente del dispositivo)</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Crear una instancia nueva de LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Cargar dictado de estado del modelo\nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Coloque el modelo en el dispositivo de destino (si sus datos est\u00e1n en la GPU, el modelo deber\u00e1 estar en la GPU para hacer predicciones)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n</pre> # Crear una instancia nueva de LinearRegressionModelV2 loaded_model_1 = LinearRegressionModelV2()  # Cargar dictado de estado del modelo loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))  # Coloque el modelo en el dispositivo de destino (si sus datos est\u00e1n en la GPU, el modelo deber\u00e1 estar en la GPU para hacer predicciones) loaded_model_1.to(device)  print(f\"Loaded model:\\n{loaded_model_1}\") print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\") <p>Ahora podemos evaluar el modelo cargado para ver si sus predicciones se alinean con las predicciones realizadas antes de guardar.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluar modelo cargado\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n</pre> # Evaluar modelo cargado loaded_model_1.eval() with torch.inference_mode():     loaded_model_1_preds = loaded_model_1(X_test) y_preds == loaded_model_1_preds <p>\u00a1Todo suma! \u00a1Lindo!</p> <p>Bueno, hemos recorrido un largo camino. \u00a1Ya ha creado y entrenado sus dos primeros modelos de redes neuronales en PyTorch!</p> <p>Es hora de practicar tus habilidades.</p>"},{"location":"05-01_pytorch_workflow/#01-fundamentos-del-flujo-de-trabajo-de-pytorch","title":"01. Fundamentos del flujo de trabajo de PyTorch\u00b6","text":"<p>La esencia del aprendizaje autom\u00e1tico y del aprendizaje profundo es tomar algunos datos del pasado, construir un algoritmo (como una red neuronal) para descubrir patrones en ellos y utilizar los patrones descubiertos para predecir el futuro.</p> <p>Hay muchas maneras de hacer esto y constantemente se descubren muchas nuevas.</p> <p>Pero empecemos poco a poco.</p> <p>\u00bfQu\u00e9 tal si comenzamos con una l\u00ednea recta?</p> <p>Y vemos si podemos construir un modelo de PyTorch que aprenda el patr\u00f3n de la l\u00ednea recta y lo iguale.</p>"},{"location":"05-01_pytorch_workflow/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>En este m\u00f3dulo, cubriremos un flujo de trabajo est\u00e1ndar de PyTorch (se puede cortar y cambiar seg\u00fan sea necesario, pero cubre el esquema principal de pasos).</p> <p></p> <p>Por ahora, usaremos este flujo de trabajo para predecir una l\u00ednea recta simple, pero los pasos del flujo de trabajo se pueden repetir y cambiar seg\u00fan el problema en el que est\u00e9 trabajando.</p> <p>Espec\u00edficamente, cubriremos:</p> Tema Contenido <p>| 1. Preparando datos | Los datos pueden ser casi cualquier cosa, pero para empezar vamos a crear una l\u00ednea recta simple. | 2. Construyendo un modelo | Aqu\u00ed crearemos un modelo para aprender patrones en los datos, tambi\u00e9n elegiremos una funci\u00f3n de p\u00e9rdida, un optimizador y crearemos un bucle de entrenamiento. | | 3. Ajustar el modelo a los datos (entrenamiento) | Tenemos datos y un modelo, ahora dejemos que el modelo (intente) encontrar patrones en los datos (entrenamiento). | | 4. Hacer predicciones y evaluar un modelo (inferencia) | Nuestro modelo encontr\u00f3 patrones en los datos. Comparemos sus hallazgos con los datos reales (pruebas). | | 5. Guardar y cargar un modelo | Es posible que desees utilizar tu modelo en otro lugar o volver a \u00e9l m\u00e1s tarde; aqu\u00ed lo cubriremos. | | 6. Poni\u00e9ndolo todo junto | Tomemos todo lo anterior y combin\u00e9moslo. |</p>"},{"location":"05-01_pytorch_workflow/#donde-puedes-obtener-ayuda","title":"\u00bfD\u00f3nde puedes obtener ayuda?\u00b6","text":"<p>Todos los materiales de este curso est\u00e1n disponibles en GitHub.</p> <p>Y si tiene problemas, tambi\u00e9n puede hacer una pregunta en la p\u00e1gina de debates.</p> <p>Tambi\u00e9n est\u00e1n los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p> <p>Comencemos poniendo lo que estamos cubriendo en un diccionario para consultarlo m\u00e1s adelante.</p>"},{"location":"05-01_pytorch_workflow/#1-datos-preparacion-y-carga","title":"1. Datos (preparaci\u00f3n y carga)\u00b6","text":"<p>Quiero enfatizar que los \"datos\" en el aprendizaje autom\u00e1tico pueden ser casi cualquier cosa que puedas imaginar. Una tabla de n\u00fameros (como una gran hoja de c\u00e1lculo de Excel), im\u00e1genes de cualquier tipo, v\u00eddeos (\u00a1YouTube tiene muchos datos!), archivos de audio como canciones o podcasts, estructuras de prote\u00ednas, texto y m\u00e1s.</p> <p>![El aprendizaje autom\u00e1tico es un juego de dos partes: 1. convertir sus datos en un conjunto representativo de n\u00fameros y 2. construir o elegir un modelo para aprender la representaci\u00f3n lo mejor posible](https://raw.githubusercontent.com/ mrdbourke/pytorch-deep-learning/main/images/01-machine-learning-a-game-of-two-parts.png)</p> <p>El aprendizaje autom\u00e1tico es un juego de dos partes:</p> <ol> <li>Convierte tus datos, sean los que sean, en n\u00fameros (una representaci\u00f3n).</li> <li>Elija o construya un modelo para aprender la representaci\u00f3n lo mejor posible.</li> </ol> <p>A veces se pueden hacer uno y dos al mismo tiempo.</p> <p>\u00bfPero qu\u00e9 pasa si no tienes datos?</p> <p>Bueno, ah\u00ed es donde estamos ahora.</p> <p>Sin datos.</p> <p>Pero podemos crear algunos.</p> <p>Creemos nuestros datos como una l\u00ednea recta.</p> <p>Usaremos regresi\u00f3n lineal para crear los datos con par\u00e1metros conocidos (cosas que un modelo puede aprender) y luego usaremos PyTorch para ver si podemos construir un modelo para estimar estos par\u00e1metros usando descenso de gradiente.</p> <p>No se preocupe si los t\u00e9rminos anteriores no significan mucho ahora, los veremos en acci\u00f3n y pondr\u00e9 recursos adicionales a continuaci\u00f3n donde podr\u00e1 obtener m\u00e1s informaci\u00f3n.</p>"},{"location":"05-01_pytorch_workflow/#dividir-datos-en-conjuntos-de-entrenamiento-y-prueba","title":"Dividir datos en conjuntos de entrenamiento y prueba\u00b6","text":"<p>Tenemos algunos datos.</p> <p>Pero antes de construir un modelo debemos dividirlo.</p> <p>Uno de los pasos m\u00e1s importantes en un proyecto de aprendizaje autom\u00e1tico es crear un conjunto de capacitaci\u00f3n y prueba (y, cuando sea necesario, un conjunto de validaci\u00f3n).</p> <p>Cada divisi\u00f3n del conjunto de datos tiene un prop\u00f3sito espec\u00edfico:</p> Dividir Prop\u00f3sito Cantidad de datos totales \u00bfCon qu\u00e9 frecuencia se usa? Conjunto de entrenamiento El modelo aprende de estos datos (como los materiales del curso que estudia durante el semestre). ~60-80% Siempre Conjunto de validaci\u00f3n El modelo se ajusta a estos datos (como el examen de pr\u00e1ctica que realiza antes del examen final). ~10-20% A menudo pero no siempre Conjunto de prueba El modelo se eval\u00faa con estos datos para probar lo que ha aprendido (como el examen final que realiza al final del semestre). ~10-20% Siempre <p>Por ahora, solo usaremos un conjunto de entrenamiento y prueba, esto significa que tendremos un conjunto de datos para que nuestro modelo aprenda y eval\u00fae.</p> <p>Podemos crearlos dividiendo nuestros tensores <code>X</code> e <code>y</code>.</p> <p>Nota: Cuando se trata de datos del mundo real, este paso generalmente se realiza justo al comienzo de un proyecto (el conjunto de prueba siempre debe mantenerse separado de todos los dem\u00e1s datos). Queremos que nuestro modelo aprenda de los datos de entrenamiento y luego los eval\u00fae con datos de prueba para obtener una indicaci\u00f3n de qu\u00e9 tan bien generaliza a ejemplos no vistos.</p>"},{"location":"05-01_pytorch_workflow/#2-construir-modelo","title":"2. Construir modelo\u00b6","text":"<p>Ahora que tenemos algunos datos, construyamos un modelo para usar los puntos azules para predecir los puntos verdes.</p> <p>Vamos a saltar de inmediato.</p> <p>Primero escribiremos el c\u00f3digo y luego explicaremos todo.</p> <p>Replicamos un modelo de regresi\u00f3n lineal est\u00e1ndar usando PyTorch puro.</p>"},{"location":"05-01_pytorch_workflow/#conceptos-basicos-de-construccion-de-modelos-pytorch","title":"Conceptos b\u00e1sicos de construcci\u00f3n de modelos PyTorch\u00b6","text":"<p>PyTorch tiene cuatro m\u00f3dulos esenciales (m\u00e1s o menos) que puedes usar para crear casi cualquier tipo de red neuronal que puedas imaginar.</p> <p>Son <code>torch.nn</code>, <code>torch.optim</code>, <code>torch.utils.data.Dataset</code> y [<code>torch.utils.data.DataLoader</code>] (https://pytorch.org/docs/stable/data.html). Por ahora, nos centraremos en los dos primeros y llegaremos a los otros dos m\u00e1s adelante (aunque es posible que puedas adivinar qu\u00e9 hacen).</p> M\u00f3dulo PyTorch \u00bfQu\u00e9 hace? <code>torch.nn</code> Contiene todos los componentes b\u00e1sicos de los gr\u00e1ficos computacionales (esencialmente una serie de c\u00e1lculos ejecutados de una manera particular). <code>torch.nn.Parameter</code> Almacena tensores que se pueden usar con <code>nn.Module</code>. Si los gradientes <code>requires_grad=True</code> (utilizados para actualizar los par\u00e1metros del modelo a trav\u00e9s de gradient descent) se calculan autom\u00e1ticamente, esto es a menudo denominado \"autogrado\". <code>torch.nn.Module</code> La clase base para todos los m\u00f3dulos de redes neuronales, todos los componentes b\u00e1sicos de las redes neuronales son subclases. Si est\u00e1 construyendo una red neuronal en PyTorch, sus modelos deben subclasificar <code>nn.Module</code>. Requiere la implementaci\u00f3n de un m\u00e9todo <code>forward()</code>. <code>torch.optim</code> Contiene varios algoritmos de optimizaci\u00f3n (estos indican a los par\u00e1metros del modelo almacenados en <code>nn.Parameter</code> c\u00f3mo cambiar mejor para mejorar el descenso del gradiente y, a su vez, reducir la p\u00e9rdida). <code>def adelante()</code> Todas las subclases de <code>nn.Module</code> requieren un m\u00e9todo <code>forward()</code>, que define el c\u00e1lculo que se realizar\u00e1 con los datos pasados \u200b\u200bal <code>nn.Module</code> particular (por ejemplo, la f\u00f3rmula de regresi\u00f3n lineal anterior). <p>Si lo anterior suena complejo, piense as\u00ed: casi todo en una red neuronal PyTorch proviene de <code>torch.nn</code>,</p> <ul> <li><code>nn.Module</code> contiene los bloques de construcci\u00f3n m\u00e1s grandes (capas)</li> <li><code>nn.Parameter</code> contiene los par\u00e1metros m\u00e1s peque\u00f1os como pesos y sesgos (juntelos para crear <code>nn.Module</code>(s))</li> <li><code>forward()</code> le dice a los bloques m\u00e1s grandes c\u00f3mo hacer c\u00e1lculos en las entradas (tensores llenos de datos) dentro de <code>nn.Module</code>(s)</li> <li><code>torch.optim</code> contiene m\u00e9todos de optimizaci\u00f3n sobre c\u00f3mo mejorar los par\u00e1metros dentro de <code>nn.Parameter</code> para representar mejor los datos de entrada</li> </ul> <p> Bloques de construcci\u00f3n b\u00e1sicos para la creaci\u00f3n de un modelo PyTorch mediante la subclasificaci\u00f3n de <code>nn.Module</code>. Para los objetos que son subclases <code>nn.Module</code>, se debe definir el m\u00e9todo <code>forward()</code>.</p> <p>Recurso: Vea m\u00e1s de estos m\u00f3dulos esenciales y sus casos de uso en la Hoja de referencia de PyTorch.</p>"},{"location":"05-01_pytorch_workflow/#comprobando-el-contenido-de-un-modelo-de-pytorch","title":"Comprobando el contenido de un modelo de PyTorch\u00b6","text":"<p>Ahora que los hemos eliminado, creemos una instancia de modelo con la clase que hemos creado y verifiquemos sus par\u00e1metros usando [<code>.parameters()</code>](https://pytorch.org/docs/stable/generated /torch.nn.Module.html#torch.nn.Module.parameters).</p>"},{"location":"05-01_pytorch_workflow/#hacer-predicciones-usando-torchinference_mode","title":"Hacer predicciones usando <code>torch.inference_mode()</code>\u00b6","text":"<p>Para verificar esto, podemos pasarle los datos de prueba <code>X_test</code> para ver qu\u00e9 tan cerca predice <code>y_test</code>.</p> <p>Cuando pasamos datos a nuestro modelo, pasar\u00e1 por el m\u00e9todo <code>forward()</code> del modelo y producir\u00e1 un resultado utilizando el c\u00e1lculo que hemos definido.</p> <p>Hagamos algunas predicciones.</p>"},{"location":"05-01_pytorch_workflow/#3-modelo-de-tren","title":"3. Modelo de tren\u00b6","text":"<p>En este momento, nuestro modelo est\u00e1 haciendo predicciones utilizando par\u00e1metros aleatorios para realizar c\u00e1lculos, b\u00e1sicamente est\u00e1 adivinando (al azar).</p> <p>Para solucionarlo, podemos actualizar sus par\u00e1metros internos (tambi\u00e9n me refiero a par\u00e1metros como patrones), los valores de <code>pesos</code> y <code>bias</code> que configuramos aleatoriamente usando <code>nn.Parameter()</code> y <code>torch.randn()</code> ser algo que represente mejor los datos.</p> <p>Podr\u00edamos codificar esto (ya que conocemos los valores predeterminados <code>weight=0.7</code> y <code>bias=0.3</code>), pero \u00bfd\u00f3nde est\u00e1 la diversi\u00f3n en eso?</p> <p>Muchas veces no sabr\u00e1s cu\u00e1les son los par\u00e1metros ideales para un modelo.</p> <p>En cambio, es mucho m\u00e1s divertido escribir c\u00f3digo para ver si el modelo puede intentar resolverlos por s\u00ed mismo.</p>"},{"location":"05-01_pytorch_workflow/#creando-una-funcion-de-perdida-y-optimizador-en-pytorch","title":"Creando una funci\u00f3n de p\u00e9rdida y optimizador en PyTorch\u00b6","text":"<p>Para que nuestro modelo actualice sus par\u00e1metros por s\u00ed solo, necesitaremos agregar algunas cosas m\u00e1s a nuestra receta.</p> <p>Y esa es una funci\u00f3n de p\u00e9rdida as\u00ed como un optimizador.</p> <p>Los rollos de estos son:</p> Funci\u00f3n \u00bfQu\u00e9 hace? \u00bfD\u00f3nde vive en PyTorch? Valores comunes Funci\u00f3n de p\u00e9rdida Mide qu\u00e9 tan err\u00f3neas se comparan las predicciones de sus modelos (por ejemplo, <code>y_preds</code>) con las etiquetas de verdad (por ejemplo, <code>y_test</code>). Baja cuanto mejor. PyTorch tiene muchas funciones de p\u00e9rdida integradas en <code>torch.nn</code>. Error absoluto medio (MAE) para problemas de regresi\u00f3n (<code>torch.nn.L1Loss()</code>). Entrop\u00eda cruzada binaria para problemas de clasificaci\u00f3n binaria (<code>torch.nn.BCELoss()</code>). Optimizador Le indica a su modelo c\u00f3mo actualizar sus par\u00e1metros internos para reducir mejor la p\u00e9rdida. Puede encontrar varias implementaciones de funciones de optimizaci\u00f3n en <code>torch.optim</code>. Descenso de gradiente estoc\u00e1stico (<code>torch.optim.SGD()</code>). Optimizador Adam (<code>torch.optim.Adam()</code>). <p>Creemos una funci\u00f3n de p\u00e9rdida y un optimizador que podamos usar para ayudar a mejorar nuestro modelo.</p> <p>Dependiendo del tipo de problema en el que est\u00e9 trabajando, depender\u00e1 de qu\u00e9 funci\u00f3n de p\u00e9rdida y qu\u00e9 optimizador utilice.</p> <p>Sin embargo, hay algunos valores comunes que se sabe que funcionan bien, como el SGD (descenso de gradiente estoc\u00e1stico) o el optimizador Adam. Y la funci\u00f3n de p\u00e9rdida MAE (error absoluto medio) para problemas de regresi\u00f3n (predecir un n\u00famero) o la funci\u00f3n de p\u00e9rdida de entrop\u00eda cruzada binaria para problemas de clasificaci\u00f3n (predecir una cosa u otra).</p> <p>Para nuestro problema, dado que estamos prediciendo un n\u00famero, usemos MAE (que se encuentra en <code>torch.nn.L1Loss()</code>) en PyTorch como nuestra funci\u00f3n de p\u00e9rdida.</p> <p> El error absoluto medio (MAE, en PyTorch: <code>torch.nn.L1Loss</code>) mide la diferencia absoluta entre dos puntos (predicciones y etiquetas) y luego toma la media en todos los ejemplos.</p> <p>Y usaremos SGD, <code>torch.optim.SGD(params, lr)</code> donde:</p> <ul> <li><code>params</code> son los par\u00e1metros del modelo de destino que le gustar\u00eda optimizar (por ejemplo, los valores de <code>pesos</code> y <code>sesgo</code> que configuramos aleatoriamente antes).</li> <li><code>lr</code> es la tasa de aprendizaje a la que desea que el optimizador actualice los par\u00e1metros; mayor significa que el optimizador intentar\u00e1 actualizaciones m\u00e1s grandes (a veces pueden ser demasiado grandes y el optimizador no funcionar\u00e1), menor significa el optimizador intentar\u00e1 actualizaciones m\u00e1s peque\u00f1as (a veces pueden ser demasiado peque\u00f1as y el optimizador tardar\u00e1 demasiado en encontrar los valores ideales). La tasa de aprendizaje se considera un hiperpar\u00e1metro (porque la establece un ingeniero de aprendizaje autom\u00e1tico). Los valores iniciales comunes para la tasa de aprendizaje son <code>0.01</code>, <code>0.001</code>, <code>0.0001</code>; sin embargo, estos tambi\u00e9n se pueden ajustar con el tiempo (esto se llama [programaci\u00f3n de la tasa de aprendizaje](https://pytorch.org/docs/stable /optim.html#how-to-adjust-learning-rate)).</li> </ul> <p>Vaya, eso es mucho, ve\u00e1moslo en c\u00f3digo.</p>"},{"location":"05-01_pytorch_workflow/#creando-un-bucle-de-optimizacion-en-pytorch","title":"Creando un bucle de optimizaci\u00f3n en PyTorch\u00b6","text":"<p>\u00a1Guau! Ahora que tenemos una funci\u00f3n de p\u00e9rdida y un optimizador, es el momento de crear un bucle de entrenamiento (y un bucle de prueba).</p> <p>El ciclo de entrenamiento implica que el modelo revise los datos de entrenamiento y aprenda las relaciones entre las \"caracter\u00edsticas\" y las \"etiquetas\".</p> <p>El ciclo de prueba implica revisar los datos de prueba y evaluar qu\u00e9 tan buenos son los patrones que el modelo aprendi\u00f3 en los datos de entrenamiento (el modelo nunca ve los datos de prueba durante el entrenamiento).</p> <p>Cada uno de estos se denomina \"bucle\" porque queremos que nuestro modelo observe (recorra) cada muestra en cada conjunto de datos.</p> <p>Para crearlos, vamos a escribir un bucle <code>for</code> de Python en el tema de la [canci\u00f3n no oficial del bucle de optimizaci\u00f3n de PyTorch] (https://twitter.com/mrdbourke/status/1450977868406673410?s=20) (hay un  versi\u00f3n en v\u00eddeo tambi\u00e9n).</p> <p> La canci\u00f3n no oficial de los bucles de optimizaci\u00f3n de PyTorch, una forma divertida de recordar los pasos en un bucle de entrenamiento (y prueba) de PyTorch.</p> <p>Habr\u00e1 bastante c\u00f3digo, pero nada que no podamos manejar.</p>"},{"location":"05-01_pytorch_workflow/#bucle-de-entrenamiento-de-pytorch","title":"Bucle de entrenamiento de PyTorch\u00b6","text":"<p>Para el ciclo de entrenamiento, crearemos los siguientes pasos:</p> N\u00famero Nombre del paso \u00bfQu\u00e9 hace? Ejemplo de c\u00f3digo 1 Pase hacia adelante El modelo revisa todos los datos de entrenamiento una vez y realiza los c\u00e1lculos de la funci\u00f3n <code>forward()</code>. <code>modelo(x_train)</code> 2 Calcular la p\u00e9rdida Los resultados del modelo (predicciones) se comparan con la verdad fundamental y se eval\u00faan para ver qu\u00e9 tan equivocados est\u00e1n. <code>p\u00e9rdida = p\u00e9rdida_fn(y_pred, y_train)</code> 3 gradientes cero Los gradientes de los optimizadores se establecen en cero (se acumulan de forma predeterminada) para que puedan recalcularse para el paso de entrenamiento espec\u00edfico. <code>optimizador.zero_grad()</code> 4 Realizar retropropagaci\u00f3n de la p\u00e9rdida Calcula el gradiente de p\u00e9rdida con respecto a cada par\u00e1metro del modelo que se actualizar\u00e1 (cada par\u00e1metro con <code>requires_grad=True</code>). Esto se conoce como propagaci\u00f3n hacia atr\u00e1s, de ah\u00ed \"hacia atr\u00e1s\". <code>p\u00e9rdida.hacia atr\u00e1s()</code> 5 Actualizar el optimizador (descenso de gradiente) Actualice los par\u00e1metros con <code>requires_grad=True</code> con respecto a los gradientes de p\u00e9rdida para mejorarlos. <code>optimizador.paso()</code> <p></p> <p>Nota: Lo anterior es s\u00f3lo un ejemplo de c\u00f3mo se pueden ordenar o describir los pasos. Con experiencia, descubrir\u00e1 que crear bucles de entrenamiento de PyTorch puede ser bastante flexible.</p> <p>Y en cuanto al orden de las cosas, el anterior es un buen orden predeterminado, pero es posible que veas pedidos ligeramente diferentes. Algunas reglas generales:</p> <ul> <li>Calcule la p\u00e9rdida (<code>loss = ...</code>) antes de realizar la retropropagaci\u00f3n (<code>loss.backward()</code>).</li> <li>Cero gradientes (<code>optimizer.zero_grad()</code>) antes de pasarlos por pasos (<code>optimizer.step()</code>).</li> <li>Paso del optimizador (<code>optimizer.step()</code>) despu\u00e9s de realizar la retropropagaci\u00f3n de la p\u00e9rdida (<code>loss.backward()</code>).</li> </ul> <p>Para obtener recursos que le ayuden a comprender lo que sucede detr\u00e1s de escena con la retropropagaci\u00f3n y el descenso de gradiente, consulte la secci\u00f3n extracurricular.</p>"},{"location":"05-01_pytorch_workflow/#bucle-de-prueba-de-pytorch","title":"Bucle de prueba de PyTorch\u00b6","text":"<p>En cuanto al ciclo de prueba (evaluaci\u00f3n de nuestro modelo), los pasos t\u00edpicos incluyen:</p> N\u00famero Nombre del paso \u00bfQu\u00e9 hace? Ejemplo de c\u00f3digo 1 Pase hacia adelante El modelo revisa todos los datos de entrenamiento una vez y realiza los c\u00e1lculos de la funci\u00f3n <code>forward()</code>. <code>modelo(x_test)</code> 2 Calcular la p\u00e9rdida Los resultados del modelo (predicciones) se comparan con la verdad fundamental y se eval\u00faan para ver qu\u00e9 tan equivocados est\u00e1n. <code>p\u00e9rdida = p\u00e9rdida_fn(y_pred, y_test)</code> 3 Calcular m\u00e9tricas de evaluaci\u00f3n (opcional) Adem\u00e1s del valor de p\u00e9rdida, es posible que desee calcular otras m\u00e9tricas de evaluaci\u00f3n, como la precisi\u00f3n en el conjunto de prueba. Funciones personalizadas <p>Observe que el ciclo de prueba no contiene realizar retropropagaci\u00f3n (<code>loss.backward()</code>) ni avanzar el optimizador (<code>optimizer.step()</code>), esto se debe a que no se cambian par\u00e1metros en el modelo durante la prueba, ya ha sido calculado. Para las pruebas, solo nos interesa el resultado del paso directo a trav\u00e9s del modelo.</p> <p></p> <p>Juntemos todo lo anterior y entrenemos nuestro modelo durante 100 \u00e9pocas (pasos directos a trav\u00e9s de los datos) y lo evaluaremos cada 10 \u00e9pocas.</p>"},{"location":"05-01_pytorch_workflow/#4-hacer-predicciones-con-un-modelo-pytorch-entrenado-inferencia","title":"4. Hacer predicciones con un modelo PyTorch entrenado (inferencia)\u00b6","text":"<p>Una vez que haya entrenado un modelo, probablemente querr\u00e1 hacer predicciones con \u00e9l.</p> <p>Ya hemos visto un vistazo de esto en el c\u00f3digo de entrenamiento y prueba anterior; los pasos para hacerlo fuera del ciclo de entrenamiento/prueba son similares.</p> <p>Hay tres cosas que se deben recordar al hacer predicciones (tambi\u00e9n llamadas realizar inferencias) con un modelo de PyTorch:</p> <ol> <li>Configure el modelo en modo de evaluaci\u00f3n (<code>model.eval()</code>).</li> <li>Realice las predicciones utilizando el administrador de contexto del modo de inferencia (<code>with torch.inference_mode(): ...</code>).</li> <li>Todas las predicciones deben realizarse con objetos en el mismo dispositivo (por ejemplo, datos y modelo solo en GPU o datos y modelo solo en CPU).</li> </ol> <p>Los primeros dos elementos garantizan que todos los c\u00e1lculos y configuraciones \u00fatiles que PyTorch utiliza detr\u00e1s de escena durante el entrenamiento, pero que no son necesarios para la inferencia, est\u00e9n desactivados (esto da como resultado un c\u00e1lculo m\u00e1s r\u00e1pido). Y el tercero garantiza que no se encontrar\u00e1 con errores entre dispositivos.</p>"},{"location":"05-01_pytorch_workflow/#5-guardar-y-cargar-un-modelo-de-pytorch","title":"5. Guardar y cargar un modelo de PyTorch\u00b6","text":"<p>Si ha entrenado un modelo de PyTorch, es probable que desee guardarlo y exportarlo a alg\u00fan lugar.</p> <p>Es decir, puede entrenarlo en Google Colab o en su m\u00e1quina local con una GPU, pero ahora le gustar\u00eda exportarlo a alg\u00fan tipo de aplicaci\u00f3n donde otros puedan usarlo.</p> <p>O tal vez quieras guardar tu progreso en un modelo y volver a cargarlo m\u00e1s tarde.</p> <p>Para guardar y cargar modelos en PyTorch, existen tres m\u00e9todos principales que debe conocer (todos los siguientes se han tomado de la [gu\u00eda para guardar y cargar modelos de PyTorch] (https://pytorch.org/tutorials/beginner/ Saving_loading_models. html#ahorrar-cargar-modelo-para-inferencia)):</p> M\u00e9todo PyTorch \u00bfQu\u00e9 hace? <code>torch.save</code> Guarda un objeto serializado en el disco usando la utilidad <code>pickle</code> de Python. Los modelos, tensores y varios otros objetos de Python, como diccionarios, se pueden guardar usando <code>torch.save</code>. <code>torch.load</code> Utiliza las funciones de deseleccionado de <code>pickle</code> para deserializar y cargar archivos de objetos Python encurtidos (como modelos, tensores o diccionarios) en la memoria. Tambi\u00e9n puede configurar en qu\u00e9 dispositivo cargar el objeto (CPU, GPU, etc.). <code>torch.nn.Module.load_state_dict</code> Carga el diccionario de par\u00e1metros de un modelo (<code>model.state_dict()</code>) usando un objeto <code>state_dict()</code> guardado. <p>Nota: Como se indica en la documentaci\u00f3n <code>pickle</code> de Python, el m\u00f3dulo <code>pickle</code> no es seguro. Eso significa que s\u00f3lo debes deshacer (cargar) los datos en los que conf\u00edas. Esto tambi\u00e9n se aplica a la carga de modelos de PyTorch. Utilice \u00fanicamente modelos PyTorch guardados de fuentes en las que conf\u00ede.</p>"},{"location":"05-01_pytorch_workflow/#guardar-el-state_dict-de-un-modelo-de-pytorch","title":"Guardar el <code>state_dict()</code> de un modelo de PyTorch\u00b6","text":"<p>La [forma recomendada](https://pytorch.org/tutorials/beginner/ Saving_loading_models.html#served-loading-model-for-inference) para guardar y cargar un modelo para inferencia (hacer predicciones) es guardando y cargando un <code>state_dict()</code> del modelo.</p> <p>Veamos c\u00f3mo podemos hacerlo en unos pocos pasos:</p> <ol> <li>Crearemos un directorio para guardar modelos llamado \"modelos\" usando el m\u00f3dulo \"pathlib\" de Python.</li> <li>Crearemos una ruta de archivo para guardar el modelo.</li> <li>Llamaremos a <code>torch.save(obj, f)</code> donde <code>obj</code> es el <code>state_dict()</code> del modelo de destino y <code>f</code> es el nombre de archivo donde guardar el modelo.</li> </ol> <p>Nota: Es una convenci\u00f3n com\u00fan que los modelos u objetos guardados de PyTorch terminen con <code>.pt</code> o <code>.pth</code>, como <code>saved_model_01.pth</code>.</p>"},{"location":"05-01_pytorch_workflow/#cargando-state_dict-de-un-modelo-pytorch-guardado","title":"Cargando <code>state_dict()</code> de un modelo PyTorch guardado\u00b6","text":"<p>Como ahora tenemos un modelo guardado <code>state_dict()</code> en <code>models/01_pytorch_workflow_model_0.pth</code>, ahora podemos cargarlo usando <code>torch.nn.Module.load_state_dict(torch.load(f))</code> donde <code>f</code> es la ruta de archivo de nuestro modelo guardado <code>state_dict()</code>.</p> <p>\u00bfPor qu\u00e9 llamar a <code>torch.load()</code> dentro de <code>torch.nn.Module.load_state_dict()</code>?</p> <p>Debido a que solo guardamos el <code>state_dict()</code> del modelo, que es un diccionario de par\u00e1metros aprendidos y no el modelo completo, primero tenemos que cargar el <code>state_dict()</code> con <code>torch.load()</code> y luego pasar ese <code> state_dict()</code> a una nueva instancia de nuestro modelo (que es una subclase de <code>nn.Module</code>).</p> <p>\u00bfPor qu\u00e9 no guardar todo el modelo?</p> <p>Guardar el modelo completo en lugar de solo <code>state_dict()</code> es m\u00e1s intuitivo, sin embargo, para citar PyTorch documentaci\u00f3n (cursiva m\u00eda):</p> <p>La desventaja de este enfoque (guardar el modelo completo) es que los datos serializados est\u00e1n vinculados a las clases espec\u00edficas y a la estructura de directorio exacta utilizada cuando se guarda el modelo...</p> <p>Debido a esto, su c\u00f3digo puede romperse de varias maneras cuando se usa en otros proyectos o despu\u00e9s de refactorizaciones.</p> <p>Entonces, en lugar de eso, estamos usando el m\u00e9todo flexible de guardar y cargar solo <code>state_dict()</code>, que nuevamente es b\u00e1sicamente un diccionario de par\u00e1metros del modelo.</p> <p>Prob\u00e9moslo creando otra instancia de <code>LinearRegressionModel()</code>, que es una subclase de <code>torch.nn.Module</code> y, por lo tanto, tendr\u00e1 el m\u00e9todo incorporado <code>load_state_dict()</code>.</p>"},{"location":"05-01_pytorch_workflow/#6-poniendolo-todo-junto","title":"6. Poni\u00e9ndolo todo junto\u00b6","text":"<p>Hemos cubierto bastante terreno hasta ahora.</p> <p>Pero una vez que hayas practicado un poco, realizar\u00e1s los pasos anteriores como si estuvieras bailando por la calle.</p> <p>Hablando de pr\u00e1ctica, juntemos todo lo que hemos hecho hasta ahora.</p> <p>Excepto que esta vez haremos que nuestro c\u00f3digo sea independiente del dispositivo (de modo que si hay una GPU disponible, la usar\u00e1 y, si no, usar\u00e1 de forma predeterminada la CPU).</p> <p>Habr\u00e1 muchos menos comentarios en esta secci\u00f3n que en la anterior, ya que lo que vamos a ver ya ha sido cubierto.</p> <p>Comenzaremos importando las bibliotecas est\u00e1ndar que necesitamos.</p> <p>Nota: Si est\u00e1 utilizando Google Colab, para configurar una GPU, vaya a Tiempo de ejecuci\u00f3n -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Aceleraci\u00f3n de hardware -&gt; GPU. Si hace esto, se restablecer\u00e1 el tiempo de ejecuci\u00f3n de Colab y perder\u00e1 las variables guardadas.</p>"},{"location":"05-01_pytorch_workflow/#61-datos","title":"6.1 Datos\u00b6","text":"<p>Creemos algunos datos como antes.</p> <p>Primero, codificaremos algunos valores de \"peso\" y \"sesgo\".</p> <p>Luego haremos un rango de n\u00fameros entre 0 y 1, estos ser\u00e1n nuestros valores \"X\".</p> <p>Finalmente, usaremos los valores \"X\", as\u00ed como los valores \"peso\" y \"sesgo\" para crear \"y\" usando la f\u00f3rmula de regresi\u00f3n lineal (<code>y = peso * X + sesgo</code>).</p>"},{"location":"05-01_pytorch_workflow/#62-construyendo-un-modelo-lineal-de-pytorch","title":"6.2 Construyendo un modelo lineal de PyTorch\u00b6","text":"<p>Tenemos algunos datos, ahora es el momento de hacer un modelo.</p> <p>Crearemos el mismo estilo de modelo que antes, excepto que esta vez, en lugar de definir los par\u00e1metros de peso y sesgo de nuestro modelo manualmente usando <code>nn.Parameter()</code>, usaremos <code>nn.Linear(in_features, out_features) </code> para que lo haga por nosotros.</p> <p>Donde <code>in_features</code> es la cantidad de dimensiones que tienen sus datos de entrada y <code>out_features</code> es la cantidad de dimensiones a las que le gustar\u00eda que se generen.</p> <p>En nuestro caso, ambos son <code>1</code> ya que nuestros datos tienen la caracter\u00edstica de entrada <code>1</code> (<code>X</code>) por etiqueta (<code>y</code>).</p> <p>![comparaci\u00f3n del modelo de regresi\u00f3n lineal nn.Parameter y el modelo de regresi\u00f3n lineal nn.Linear](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-regression-model -con-nn-par\u00e1metro-y-nn-lineal-comparado.png) Crear un modelo de regresi\u00f3n lineal usando <code>nn.Parameter</code> versus usar <code>nn.Linear</code>. Hay muchos m\u00e1s ejemplos en los que el m\u00f3dulo <code>torch.nn</code> tiene c\u00e1lculos predise\u00f1ados, incluidas muchas capas de redes neuronales populares y \u00fatiles.</p>"},{"location":"05-01_pytorch_workflow/#63-formacion","title":"6.3 Formaci\u00f3n\u00b6","text":""},{"location":"05-01_pytorch_workflow/#64-hacer-predicciones","title":"6.4 Hacer predicciones\u00b6","text":"<p>Ahora que tenemos un modelo entrenado, activemos su modo de evaluaci\u00f3n y hagamos algunas predicciones.</p>"},{"location":"05-01_pytorch_workflow/#65-guardar-y-cargar-un-modelo","title":"6.5 Guardar y cargar un modelo\u00b6","text":"<p>Estamos contentos con las predicciones de nuestros modelos, as\u00ed que guard\u00e9moslo en un archivo para poder usarlo m\u00e1s tarde.</p>"},{"location":"05-01_pytorch_workflow/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Todos los ejercicios se han inspirado en el c\u00f3digo del cuaderno.</p> <p>Hay un ejercicio por secci\u00f3n principal.</p> <p>Deber\u00eda poder completarlos consultando su secci\u00f3n espec\u00edfica.</p> <p>Nota: Para todos los ejercicios, su c\u00f3digo debe ser independiente del dispositivo (lo que significa que podr\u00eda ejecutarse en CPU o GPU si est\u00e1 disponible).</p> <ol> <li>Cree un conjunto de datos de l\u00ednea recta utilizando la f\u00f3rmula de regresi\u00f3n lineal (<code>peso * X + sesgo</code>).</li> </ol> <ul> <li>Establezca <code>weight=0.3</code> y <code>bias=0.9</code> y debe haber al menos 100 puntos de datos en total.</li> <li>Divida los datos en 80% de entrenamiento y 20% de pruebas.</li> <li>Trazar los datos de entrenamiento y prueba para que sean visuales.</li> </ul> <ol> <li>Cree un modelo de PyTorch subclasificando <code>nn.Module</code>.</li> </ol> <ul> <li>Dentro debe haber un <code>nn.Parameter()</code> inicializado aleatoriamente con <code>requires_grad=True</code>, uno para <code>weights</code> y otro para <code>bias</code>.</li> <li>Implemente el m\u00e9todo <code>forward()</code> para calcular la funci\u00f3n de regresi\u00f3n lineal que utiliz\u00f3 para crear el conjunto de datos en 1.</li> <li>Una vez que haya construido el modelo, cree una instancia del mismo y verifique su <code>state_dict()</code>.</li> <li>Nota: Si desea utilizar <code>nn.Linear()</code> en lugar de <code>nn.Parameter()</code>, puede hacerlo.</li> </ul> <ol> <li>Cree una funci\u00f3n de p\u00e9rdida y un optimizador usando <code>nn.L1Loss()</code> y <code>torch.optim.SGD(params, lr)</code> respectivamente.</li> </ol> <ul> <li>Establezca la tasa de aprendizaje del optimizador en 0,01 y los par\u00e1metros a optimizar deben ser los par\u00e1metros del modelo que cre\u00f3 en 2.</li> <li>Escriba un ciclo de entrenamiento para realizar los pasos de entrenamiento apropiados durante 300 \u00e9pocas.</li> <li>El bucle de entrenamiento debe probar el modelo en el conjunto de datos de prueba cada 20 \u00e9pocas.</li> </ul> <ol> <li>Haga predicciones con el modelo entrenado sobre los datos de prueba.</li> </ol> <ul> <li>Visualice estas predicciones en comparaci\u00f3n con los datos de prueba y entrenamiento originales (nota: es posible que deba asegurarse de que las predicciones no est\u00e9n en la GPU si desea utilizar bibliotecas no habilitadas para CUDA, como matplotlib, para trazar) .</li> </ul> <ol> <li>Guarde el <code>state_dict()</code> de su modelo entrenado en un archivo.</li> </ol> <ul> <li>Cree una nueva instancia de su clase de modelo que cre\u00f3 en 2. y c\u00e1rguela en el <code>state_dict()</code> que acaba de guardar.</li> <li>Realice predicciones sobre los datos de su prueba con el modelo cargado y confirme que coincidan con las predicciones del modelo original de 4.</li> </ul> <p>Recurso: Consulte las plantillas de cuadernos de ejercicios y las [soluciones](https://github. com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions) en el curso GitHub.</p>"},{"location":"05-01_pytorch_workflow/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Escuche [La canci\u00f3n no oficial del bucle de optimizaci\u00f3n de PyTorch] (https://youtu.be/Nutpusq_AFw) (para ayudar a recordar los pasos en un bucle de prueba/entrenamiento de PyTorch).</li> <li>Lea \u00bfQu\u00e9 es realmente <code>torch.nn</code>? de Jeremy Howard para obtener una comprensi\u00f3n m\u00e1s profunda de c\u00f3mo funciona uno de los m\u00f3dulos m\u00e1s importantes de PyTorch.</li> <li>Dedique 10 minutos a desplazarse y consultar la [hoja de referencia de la documentaci\u00f3n de PyTorch] (https://pytorch.org/tutorials/beginner/ptcheat.html) para conocer todos los diferentes m\u00f3dulos de PyTorch que pueda encontrar.</li> <li>Dedique 10 minutos a leer la [documentaci\u00f3n de carga y guardado en el sitio web de PyTorch] (https://pytorch.org/tutorials/beginner/ Saving_loading_models.html) para familiarizarse con las diferentes opciones de guardar y cargar en PyTorch.</li> <li>Dedique 1 a 2 horas a leer/ver lo siguiente para obtener una descripci\u00f3n general de los aspectos internos del descenso de gradiente y la retropropagaci\u00f3n, los dos algoritmos principales que han estado trabajando en segundo plano para ayudar a que nuestro modelo aprenda.</li> <li>P\u00e1gina de Wikipedia para descenso de gradiente</li> <li>[Algoritmo de descenso de gradiente: una inmersi\u00f3n profunda] (https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21) por Robert Kwiatkowski</li> <li>Video de descenso de gradiente, c\u00f3mo aprenden las redes neuronales por 3Blue1Brown</li> <li>\u00bfQu\u00e9 hace realmente la retropropagaci\u00f3n? v\u00eddeo de 3Blue1Brown</li> <li>[P\u00e1gina de Wikipedia sobre retropropagaci\u00f3n] (https://en.wikipedia.org/wiki/Backpropagation)</li> </ul>"},{"location":"06-04_pytorch_custom_datasets/","title":"04. Conjuntos de datos personalizados de PyTorch","text":"<p>Ver c\u00f3digo fuente | Ver diapositivas | Ver v\u00eddeo tutorial</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch import nn\n\n# Nota: este port\u00e1til requiere antorcha &gt;= 1.10.0\ntorch.__version__\n</pre> import torch from torch import nn  # Nota: este port\u00e1til requiere antorcha &gt;= 1.10.0 torch.__version__ <p>Y ahora sigamos las mejores pr\u00e1cticas y configuremos el c\u00f3digo independiente del dispositivo.</p> <p>Nota: Si est\u00e1s usando Google Colab y a\u00fan no tienes una GPU activada, ahora es el momento de activar una a trav\u00e9s de <code>Runtime -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Acelerador de hardware -&gt; GPU</code> . Si hace esto, es probable que su tiempo de ejecuci\u00f3n se reinicie y tendr\u00e1 que ejecutar todas las celdas anteriores yendo a \"Tiempo de ejecuci\u00f3n -&gt; Ejecutar antes\".</p> In\u00a0[\u00a0]: Copied! <pre># Configurar c\u00f3digo independiente del dispositivo\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Configurar c\u00f3digo independiente del dispositivo device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre>import requests\nimport zipfile\nfrom pathlib import Path\n\n# Ruta de configuraci\u00f3n a la carpeta de datos\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# Si la carpeta de im\u00e1genes no existe, desc\u00e1rgala y prep\u00e1rala...\nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n</pre> import requests import zipfile from pathlib import Path  # Ruta de configuraci\u00f3n a la carpeta de datos data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # Si la carpeta de im\u00e1genes no existe, desc\u00e1rgala y prep\u00e1rala... if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path) In\u00a0[\u00a0]: Copied! <pre>import os\ndef walk_through_dir(dir_path):\n  \"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  \n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os def walk_through_dir(dir_path):   \"\"\"   Walks through dir_path returning its contents.   Args:     dir_path (str or pathlib.Path): target directory      Returns:     A print out of:       number of subdiretories in dir_path       number of images (files) in each subdirectory       name of each subdirectory   \"\"\"   for dirpath, dirnames, filenames in os.walk(dir_path):     print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") In\u00a0[\u00a0]: Copied! <pre>walk_through_dir(image_path)\n</pre> walk_through_dir(image_path) <p>\u00a1Excelente!</p> <p>Parece que tenemos alrededor de 75 im\u00e1genes por clase de capacitaci\u00f3n y 25 im\u00e1genes por clase de prueba.</p> <p>Eso deber\u00eda ser suficiente para empezar.</p> <p>Recuerde, estas im\u00e1genes son subconjuntos del conjunto de datos original de Food101.</p> <p>Puede ver c\u00f3mo se crearon en el [cuaderno de creaci\u00f3n de datos] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb).</p> <p>Mientras estamos en eso, configuremos nuestras rutas de capacitaci\u00f3n y prueba.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar rutas de tren y pruebas.\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n</pre> # Configurar rutas de tren y pruebas. train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir In\u00a0[\u00a0]: Copied! <pre>import random\nfrom PIL import Image\n\n# Establecer semilla\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Obtenga todas las rutas de las im\u00e1genes (* significa \"cualquier combinaci\u00f3n\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Obtener ruta de imagen aleatoria\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Obtener la clase de imagen a partir del nombre de la ruta (la clase de imagen es el nombre del directorio donde est\u00e1 almacenada la imagen)\nimage_class = random_image_path.parent.stem\n\n# 4. Abrir imagen\nimg = Image.open(random_image_path)\n\n# 5. Imprimir metadatos\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n</pre> import random from PIL import Image  # Establecer semilla random.seed(42) # &lt;- try changing this and see what happens  # 1. Obtenga todas las rutas de las im\u00e1genes (* significa \"cualquier combinaci\u00f3n\") image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # 2. Obtener ruta de imagen aleatoria random_image_path = random.choice(image_path_list)  # 3. Obtener la clase de imagen a partir del nombre de la ruta (la clase de imagen es el nombre del directorio donde est\u00e1 almacenada la imagen) image_class = random_image_path.parent.stem  # 4. Abrir imagen img = Image.open(random_image_path)  # 5. Imprimir metadatos print(f\"Random image path: {random_image_path}\") print(f\"Image class: {image_class}\") print(f\"Image height: {img.height}\")  print(f\"Image width: {img.width}\") img <p>Podemos hacer lo mismo con <code>matplotlib.pyplot.imshow()</code>, excepto que tenemos que convertir la imagen. a una matriz NumPy primero.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Convertir la imagen en una matriz\nimg_as_array = np.asarray(img)\n\n# Trazar la imagen con matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Convertir la imagen en una matriz img_as_array = np.asarray(img)  # Trazar la imagen con matplotlib plt.figure(figsize=(10, 7)) plt.imshow(img_as_array) plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\") plt.axis(False); In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n</pre> import torch from torch.utils.data import DataLoader from torchvision import datasets, transforms In\u00a0[\u00a0]: Copied! <pre># Escribir transformaci\u00f3n para imagen\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n</pre> # Escribir transformaci\u00f3n para imagen data_transform = transforms.Compose([     # Resize the images to 64x64     transforms.Resize(size=(64, 64)),     # Flip the images randomly on the horizontal     transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance     # Turn the image into a torch.Tensor     transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0  ]) <p>Ahora que tenemos una composici\u00f3n de transformaciones, escribamos una funci\u00f3n para probarlas en varias im\u00e1genes.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_transformed_images(image_paths, transform, n=3, seed=42):\n    \"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # Note: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n</pre> def plot_transformed_images(image_paths, transform, n=3, seed=42):     \"\"\"Plots a series of random images from image_paths.      Will open n image paths from image_paths, transform them     with transform and plot them side by side.      Args:         image_paths (list): List of target image paths.          transform (PyTorch Transforms): Transforms to apply to images.         n (int, optional): Number of images to plot. Defaults to 3.         seed (int, optional): Random seed for the random generator. Defaults to 42.     \"\"\"     random.seed(seed)     random_image_paths = random.sample(image_paths, k=n)     for image_path in random_image_paths:         with Image.open(image_path) as f:             fig, ax = plt.subplots(1, 2)             ax[0].imshow(f)              ax[0].set_title(f\"Original \\nSize: {f.size}\")             ax[0].axis(\"off\")              # Transform and plot image             # Note: permute() will change shape of image to suit matplotlib              # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])             transformed_image = transform(f).permute(1, 2, 0)              ax[1].imshow(transformed_image)              ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")             ax[1].axis(\"off\")              fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)  plot_transformed_images(image_path_list,                          transform=data_transform,                          n=3) <p>\u00a1Lindo!</p> <p>Ahora tenemos una manera de convertir nuestras im\u00e1genes en tensores usando <code>torchvision.transforms</code>.</p> <p>Tambi\u00e9n manipulamos su tama\u00f1o y orientaci\u00f3n si es necesario (algunos modelos prefieren im\u00e1genes de diferentes tama\u00f1os y formas).</p> <p>Generalmente, cuanto mayor sea la forma de la imagen, m\u00e1s informaci\u00f3n podr\u00e1 recuperar un modelo.</p> <p>Por ejemplo, una imagen de tama\u00f1o <code>[256, 256, 3]</code> tendr\u00e1 16 veces m\u00e1s p\u00edxeles que una imagen de tama\u00f1o <code>[64, 64, 3]</code> (<code>(256*256*3)/(64*64* 3)=16</code>).</p> <p>Sin embargo, la desventaja es que m\u00e1s p\u00edxeles requieren m\u00e1s c\u00e1lculos.</p> <p>Ejercicio: Intente comentar una de las transformaciones en <code>data_transform</code> y ejecute la funci\u00f3n de trazado <code>plot_transformed_images()</code> nuevamente, \u00bfqu\u00e9 sucede?</p> In\u00a0[\u00a0]: Copied! <pre># Utilice ImageFolder para crear conjuntos de datos\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n</pre> # Utilice ImageFolder para crear conjuntos de datos from torchvision import datasets train_data = datasets.ImageFolder(root=train_dir, # target folder of images                                   transform=data_transform, # transforms to perform on data (images)                                   target_transform=None) # transforms to perform on labels (if necessary)  test_data = datasets.ImageFolder(root=test_dir,                                   transform=data_transform)  print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\") <p>\u00a1Hermoso!</p> <p>Parece que PyTorch ha registrado nuestro \"Conjunto de datos\".</p> <p>Inspeccion\u00e9moslos revisando los atributos <code>classes</code> y <code>class_to_idx</code>, as\u00ed como la duraci\u00f3n de nuestros conjuntos de entrenamiento y prueba.</p> In\u00a0[\u00a0]: Copied! <pre># Obtener nombres de clases como una lista\nclass_names = train_data.classes\nclass_names\n</pre> # Obtener nombres de clases como una lista class_names = train_data.classes class_names In\u00a0[\u00a0]: Copied! <pre># Tambi\u00e9n puede obtener nombres de clases como un dictado.\nclass_dict = train_data.class_to_idx\nclass_dict\n</pre> # Tambi\u00e9n puede obtener nombres de clases como un dictado. class_dict = train_data.class_to_idx class_dict In\u00a0[\u00a0]: Copied! <pre># Comprueba las longitudes\nlen(train_data), len(test_data)\n</pre> # Comprueba las longitudes len(train_data), len(test_data) <p>\u00a1Lindo! Parece que podremos usarlos como referencia para m\u00e1s adelante.</p> <p>\u00bfQu\u00e9 tal nuestras im\u00e1genes y etiquetas?</p> <p>\u00bfComo se ven?</p> <p>Podemos indexar nuestros <code>train_data</code> y <code>test_data</code> <code>Dataset</code> para encontrar muestras y sus etiquetas de destino.</p> In\u00a0[\u00a0]: Copied! <pre>img, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n</pre> img, label = train_data[0][0], train_data[0][1] print(f\"Image tensor:\\n{img}\") print(f\"Image shape: {img.shape}\") print(f\"Image datatype: {img.dtype}\") print(f\"Image label: {label}\") print(f\"Label datatype: {type(label)}\") <p>Nuestras im\u00e1genes ahora tienen la forma de un tensor (con forma <code>[3, 64, 64]</code>) y las etiquetas tienen la forma de un n\u00famero entero relacionado con una clase espec\u00edfica (como lo indica el atributo <code>class_to_idx</code>).</p> <p>\u00bfQu\u00e9 tal si trazamos un tensor de imagen \u00fanica usando <code>matplotlib</code>?</p> <p>Primero tendremos que permutar (reorganizar el orden de sus dimensiones) para que sea compatible.</p> <p>En este momento, las dimensiones de nuestra imagen est\u00e1n en el formato <code>CHW</code> (canales de color, alto, ancho) pero <code>matplotlib</code> prefiere <code>HWC</code> (alto, ancho, canales de color).</p> In\u00a0[\u00a0]: Copied! <pre># Reorganizar el orden de las dimensiones.\nimg_permute = img.permute(1, 2, 0)\n\n# Imprime diferentes formas (antes y despu\u00e9s de la permutaci\u00f3n)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Trazar la imagen\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n</pre> # Reorganizar el orden de las dimensiones. img_permute = img.permute(1, 2, 0)  # Imprime diferentes formas (antes y despu\u00e9s de la permutaci\u00f3n) print(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\") print(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")  # Trazar la imagen plt.figure(figsize=(10, 7)) plt.imshow(img.permute(1, 2, 0)) plt.axis(\"off\") plt.title(class_names[label], fontsize=14); <p>Observe que la imagen ahora est\u00e1 m\u00e1s pixelada (menos calidad).</p> <p>Esto se debe a que se cambi\u00f3 su tama\u00f1o de \"512x512\" a \"64x64\" p\u00edxeles.</p> <p>La intuici\u00f3n aqu\u00ed es que si crees que la imagen es m\u00e1s dif\u00edcil de reconocer lo que est\u00e1 sucediendo, es probable que al modelo tambi\u00e9n le resulte m\u00e1s dif\u00edcil entenderlo.</p> In\u00a0[\u00a0]: Copied! <pre># Convierta conjuntos de datos de entrenamiento y prueba en cargadores de datos\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n</pre> # Convierta conjuntos de datos de entrenamiento y prueba en cargadores de datos from torch.utils.data import DataLoader train_dataloader = DataLoader(dataset=train_data,                                batch_size=1, # how many samples per batch?                               num_workers=1, # how many subprocesses to use for data loading? (higher = more)                               shuffle=True) # shuffle the data?  test_dataloader = DataLoader(dataset=test_data,                               batch_size=1,                               num_workers=1,                               shuffle=False) # don't usually need to shuffle testing data  train_dataloader, test_dataloader <p>\u00a1Maravilloso!</p> <p>Ahora nuestros datos son iterables.</p> <p>Prob\u00e9moslo y comprobemos las formas.</p> In\u00a0[\u00a0]: Copied! <pre>img, label = next(iter(train_dataloader))\n\n# El tama\u00f1o del lote ahora ser\u00e1 1, intente cambiar el par\u00e1metro de tama\u00f1o de lote anterior y vea qu\u00e9 sucede\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n</pre> img, label = next(iter(train_dataloader))  # El tama\u00f1o del lote ahora ser\u00e1 1, intente cambiar el par\u00e1metro de tama\u00f1o de lote anterior y vea qu\u00e9 sucede print(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label.shape}\") <p>Ahora podr\u00edamos usar estos <code>DataLoader</code> con un bucle de entrenamiento y prueba para entrenar un modelo.</p> <p>Pero antes de hacerlo, veamos otra opci\u00f3n para cargar im\u00e1genes (o casi cualquier otro tipo de datos).</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n</pre> import os import pathlib import torch  from PIL import Image from torch.utils.data import Dataset from torchvision import transforms from typing import Tuple, Dict, List <p>\u00bfRecuerda c\u00f3mo nuestras instancias de <code>torchvision.datasets.ImageFolder()</code> nos permitieron usar los atributos <code>classes</code> y <code>class_to_idx</code>?</p> In\u00a0[\u00a0]: Copied! <pre># Instancia de torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n</pre> # Instancia de torchvision.datasets.ImageFolder() train_data.classes, train_data.class_to_idx In\u00a0[\u00a0]: Copied! <pre># Ruta de configuraci\u00f3n para el directorio de destino\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Obtenga los nombres de las clases del directorio de destino\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n</pre> # Ruta de configuraci\u00f3n para el directorio de destino target_directory = train_dir print(f\"Target directory: {target_directory}\")  # Obtenga los nombres de las clases del directorio de destino class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))]) print(f\"Class names found: {class_names_found}\") <p>\u00a1Excelente!</p> <p>\u00bfQu\u00e9 tal si lo convertimos en una funci\u00f3n completa?</p> In\u00a0[\u00a0]: Copied! <pre># Crear funci\u00f3n para buscar clases en el directorio de destino\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folder names in a target directory.\n    \n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    \n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n</pre> # Crear funci\u00f3n para buscar clases en el directorio de destino def find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:     \"\"\"Finds the class folder names in a target directory.          Assumes target directory is in standard image classification format.      Args:         directory (str): target directory to load classnames from.      Returns:         Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))          Example:         find_classes(\"food_images/train\")         &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})     \"\"\"     # 1. Get the class names by scanning the target directory     classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())          # 2. Raise an error if class names not found     if not classes:         raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")              # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)     class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}     return classes, class_to_idx <p>\u00a1Luciendo bien!</p> <p>Ahora probemos nuestra funci\u00f3n <code>find_classes()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>find_classes(train_dir)\n</pre> find_classes(train_dir) <p>\u00a1Guau! \u00a1Luciendo bien!</p> In\u00a0[\u00a0]: Copied! <pre># Escriba una clase de conjunto de datos personalizada (hereda de torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclase torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n</pre> # Escriba una clase de conjunto de datos personalizada (hereda de torch.utils.data.Dataset) from torch.utils.data import Dataset  # 1. Subclase torch.utils.data.Dataset class ImageFolderCustom(Dataset):          # 2. Initialize with a targ_dir and transform (optional) parameter     def __init__(self, targ_dir: str, transform=None) -&gt; None:                  # 3. Create class attributes         # Get all image paths         self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's         # Setup transforms         self.transform = transform         # Create classes and class_to_idx attributes         self.classes, self.class_to_idx = find_classes(targ_dir)      # 4. Make function to load images     def load_image(self, index: int) -&gt; Image.Image:         \"Opens an image via a path and returns it.\"         image_path = self.paths[index]         return Image.open(image_path)           # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)     def __len__(self) -&gt; int:         \"Returns the total number of samples.\"         return len(self.paths)          # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)     def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:         \"Returns one sample of data, data and label (X, y).\"         img = self.load_image(index)         class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg         class_idx = self.class_to_idx[class_name]          # Transform if necessary         if self.transform:             return self.transform(img), class_idx # return data, label (X, y)         else:             return img, class_idx # return data, label (X, y) <p>\u00a1Guau! Un mont\u00f3n de c\u00f3digo para cargar en nuestras im\u00e1genes.</p> <p>Esta es una de las desventajas de crear su propio \"conjunto de datos\" personalizado.</p> <p>Sin embargo, ahora que lo hemos escrito una vez, podemos moverlo a un archivo <code>.py</code> como <code>data_loader.py</code> junto con algunas otras funciones de datos \u00fatiles y reutilizarlo m\u00e1s adelante.</p> <p>Antes de probar nuestra nueva clase <code>ImageFolderCustom</code>, creemos algunas transformaciones para preparar nuestras im\u00e1genes.</p> In\u00a0[\u00a0]: Copied! <pre># Aumentar los datos del tren\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# No aumente los datos de prueba, solo remodele\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Aumentar los datos del tren train_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.RandomHorizontalFlip(p=0.5),     transforms.ToTensor() ])  # No aumente los datos de prueba, solo remodele test_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>\u00a1Ahora llega la hora de la verdad!</p> <p>Convirtamos nuestras im\u00e1genes de entrenamiento (contenidas en <code>train_dir</code>) y nuestras im\u00e1genes de prueba (contenidas en <code>test_dir</code>) en <code>Dataset</code> usando nuestra propia clase <code>ImageFolderCustom</code>.</p> In\u00a0[\u00a0]: Copied! <pre>train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n</pre> train_data_custom = ImageFolderCustom(targ_dir=train_dir,                                        transform=train_transforms) test_data_custom = ImageFolderCustom(targ_dir=test_dir,                                       transform=test_transforms) train_data_custom, test_data_custom <p>Hmm... no hay errores, \u00bffuncion\u00f3?</p> <p>Intentemos llamar a <code>len()</code> en nuestro nuevo <code>Dataset</code> y busquemos los atributos <code>classes</code> y <code>class_to_idx</code>.</p> In\u00a0[\u00a0]: Copied! <pre>len(train_data_custom), len(test_data_custom)\n</pre> len(train_data_custom), len(test_data_custom) In\u00a0[\u00a0]: Copied! <pre>train_data_custom.classes\n</pre> train_data_custom.classes In\u00a0[\u00a0]: Copied! <pre>train_data_custom.class_to_idx\n</pre> train_data_custom.class_to_idx <p><code>len(test_data_custom) == len(test_data)</code> y <code>len(test_data_custom) == len(test_data)</code> \u00a1\u00a1\u00a1S\u00ed!!!</p> <p>Parece que funcion\u00f3.</p> <p>Tambi\u00e9n podr\u00edamos verificar la igualdad con el <code>Dataset</code> creado por la clase <code>torchvision.datasets.ImageFolder()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Verifique la igualdad entre nuestro conjunto de datos personalizado y el conjunto de datos ImageFolder\nprint((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n</pre> # Verifique la igualdad entre nuestro conjunto de datos personalizado y el conjunto de datos ImageFolder print((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data))) print(train_data_custom.classes == train_data.classes) print(train_data_custom.class_to_idx == train_data.class_to_idx) <p>\u00a1Ho, ho!</p> <p>\u00a1M\u00edranos ir!</p> <p>\u00a1Tres \"verdaderos\"!</p> <p>No hay nada mejor que eso.</p> <p>\u00bfQu\u00e9 tal si lo llevamos a un nivel superior y trazamos algunas im\u00e1genes aleatorias para probar nuestra anulaci\u00f3n <code>__getitem__</code>?</p> In\u00a0[\u00a0]: Copied! <pre># 1. Incorpore un conjunto de datos y una lista de nombres de clases.\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n</pre> # 1. Incorpore un conjunto de datos y una lista de nombres de clases. def display_random_images(dataset: torch.utils.data.dataset.Dataset,                           classes: List[str] = None,                           n: int = 10,                           display_shape: bool = True,                           seed: int = None):          # 2. Adjust display if n too high     if n &gt; 10:         n = 10         display_shape = False         print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")          # 3. Set random seed     if seed:         random.seed(seed)      # 4. Get random sample indexes     random_samples_idx = random.sample(range(len(dataset)), k=n)      # 5. Setup plot     plt.figure(figsize=(16, 8))      # 6. Loop through samples and display random samples      for i, targ_sample in enumerate(random_samples_idx):         targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]          # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]         targ_image_adjust = targ_image.permute(1, 2, 0)          # Plot adjusted samples         plt.subplot(1, n, i+1)         plt.imshow(targ_image_adjust)         plt.axis(\"off\")         if classes:             title = f\"class: {classes[targ_label]}\"             if display_shape:                 title = title + f\"\\nshape: {targ_image_adjust.shape}\"         plt.title(title) <p>\u00a1Qu\u00e9 funci\u00f3n tan atractiva!</p> <p>Prob\u00e9moslo primero con el <code>Dataset</code> que creamos con <code>torchvision.datasets.ImageFolder()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Mostrar im\u00e1genes aleatorias del conjunto de datos creado por ImageFolder\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n</pre> # Mostrar im\u00e1genes aleatorias del conjunto de datos creado por ImageFolder display_random_images(train_data,                        n=5,                        classes=class_names,                       seed=None) <p>Y ahora con el <code>Dataset</code> que creamos con nuestro propio <code>ImageFolderCustom</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Mostrar im\u00e1genes aleatorias del conjunto de datos ImageFolderCustom\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n</pre> # Mostrar im\u00e1genes aleatorias del conjunto de datos ImageFolderCustom display_random_images(train_data_custom,                        n=12,                        classes=class_names,                       seed=None) # Try setting the seed for reproducible images <p>\u00a1\u00a1\u00a1Lindo!!!</p> <p>Parece que nuestro <code>ImageFolderCustom</code> est\u00e1 funcionando tal como nos gustar\u00eda.</p> In\u00a0[\u00a0]: Copied! <pre># Convierta el tren y pruebe conjuntos de datos personalizados en DataLoader\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n</pre> # Convierta el tren y pruebe conjuntos de datos personalizados en DataLoader from torch.utils.data import DataLoader train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset                                      batch_size=1, # how many samples per batch?                                      num_workers=0, # how many subprocesses to use for data loading? (higher = more)                                      shuffle=True) # shuffle the data?  test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset                                     batch_size=1,                                      num_workers=0,                                      shuffle=False) # don't usually need to shuffle testing data  train_dataloader_custom, test_dataloader_custom <p>\u00bfLas formas de las muestras son iguales?</p> In\u00a0[\u00a0]: Copied! <pre># Obtener imagen y etiqueta de DataLoader personalizado\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# El tama\u00f1o del lote ahora ser\u00e1 1, intente cambiar el par\u00e1metro de tama\u00f1o de lote anterior y vea qu\u00e9 sucede\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n</pre> # Obtener imagen y etiqueta de DataLoader personalizado img_custom, label_custom = next(iter(train_dataloader_custom))  # El tama\u00f1o del lote ahora ser\u00e1 1, intente cambiar el par\u00e1metro de tama\u00f1o de lote anterior y vea qu\u00e9 sucede print(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label_custom.shape}\") <p>\u00a1Seguro lo hacen!</p> <p>Ahora analicemos otras formas de transformaciones de datos.</p> In\u00a0[\u00a0]: Copied! <pre>from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1\n])\n\n# No es necesario realizar un aumento en los datos de prueba.\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n</pre> from torchvision import transforms  train_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense      transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1 ])  # No es necesario realizar un aumento en los datos de prueba. test_transforms = transforms.Compose([     transforms.Resize((224, 224)),      transforms.ToTensor() ]) <p>Nota: Generalmente no se realizan aumentos de datos en el conjunto de prueba. La idea del aumento de datos es aumentar artificialmente la diversidad del conjunto de entrenamiento para predecir mejor en el conjunto de prueba.</p> <p>Sin embargo, debe asegurarse de que las im\u00e1genes de su conjunto de prueba se transformen en tensores. Tambi\u00e9n dimensionamos las im\u00e1genes de prueba al mismo tama\u00f1o que nuestras im\u00e1genes de entrenamiento; sin embargo, se puede realizar inferencia en im\u00e1genes de diferentes tama\u00f1os si es necesario (aunque esto puede alterar el rendimiento).</p> <p>Hermoso, ahora tenemos una transformaci\u00f3n de entrenamiento (con aumento de datos) y una transformaci\u00f3n de prueba (sin aumento de datos).</p> <p>\u00a1Probemos nuestro aumento de datos!</p> In\u00a0[\u00a0]: Copied! <pre># Obtener todas las rutas de im\u00e1genes\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Trazar im\u00e1genes aleatorias\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n</pre> # Obtener todas las rutas de im\u00e1genes image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # Trazar im\u00e1genes aleatorias plot_transformed_images(     image_paths=image_path_list,     transform=train_transforms,     n=3,     seed=None ) <p>Intente ejecutar la celda de arriba varias veces y vea c\u00f3mo la imagen original cambia a medida que pasa por la transformaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># Crear transformaci\u00f3n simple\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n</pre> # Crear transformaci\u00f3n simple simple_transform = transforms.Compose([      transforms.Resize((64, 64)),     transforms.ToTensor(), ]) <p>Excelente, ahora tenemos una transformaci\u00f3n simple:</p> <ol> <li>Cargue los datos, convirtiendo primero cada una de nuestras carpetas de entrenamiento y prueba en un <code>Conjunto de datos</code> con <code>torchvision.datasets.ImageFolder()</code></li> <li>Luego, en un <code>DataLoader</code> usando <code>torch.utils.data.DataLoader()</code>.<ul> <li>Configuraremos <code>batch_size=32</code> y <code>num_workers</code> en tantas CPU como sea posible en nuestra m\u00e1quina (esto depender\u00e1 de qu\u00e9 m\u00e1quina est\u00e9 usando).</li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># 1. Cargar y transformar datos\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Convierta datos en DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Configurar el tama\u00f1o del lote y el n\u00famero de trabajadores\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Crear cargador de datos\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n</pre> # 1. Cargar y transformar datos from torchvision import datasets train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform) test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)  # 2. Convierta datos en DataLoaders import os from torch.utils.data import DataLoader  # Configurar el tama\u00f1o del lote y el n\u00famero de trabajadores BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count() print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")  # Crear cargador de datos train_dataloader_simple = DataLoader(train_data_simple,                                       batch_size=BATCH_SIZE,                                       shuffle=True,                                       num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_simple, test_dataloader_simple <p>\u00a1Se ha creado <code>DataLoader</code>!</p> <p>Construyamos un modelo.</p> In\u00a0[\u00a0]: Copied! <pre>class TinyVGG(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n</pre> class TinyVGG(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:         super().__init__()         self.conv_block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.conv_block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*16*16,                       out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.conv_block_1(x)         # print(x.shape)         x = self.conv_block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x         # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion  torch.manual_seed(42) model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device) model_0 <p>Nota: Una de las formas de acelerar la computaci\u00f3n de los modelos de aprendizaje profundo en una GPU es aprovechar la fusi\u00f3n de operadores.</p> <p>Esto significa que en el m\u00e9todo <code>forward()</code> de nuestro modelo anterior, en lugar de llamar a un bloque de capa y reasignar <code>x</code> cada vez, llamamos a cada bloque en sucesi\u00f3n (consulte la \u00faltima l\u00ednea del m\u00e9todo <code>forward()</code> en el modelo anterior como ejemplo).</p> <p>Esto ahorra el tiempo dedicado a reasignar <code>x</code> (memoria pesada) y se centra \u00fanicamente en calcular en <code>x</code>.</p> <p>Consulte C\u00f3mo hacer que el aprendizaje profundo funcione mejor desde los primeros principios de Horace He para conocer m\u00e1s formas de acelerar los modelos de aprendizaje autom\u00e1tico.</p> <p>\u00a1Ese s\u00ed que es un modelo bonito!</p> <p>\u00bfQu\u00e9 tal si lo probamos con un pase hacia adelante en una sola imagen?</p> In\u00a0[\u00a0]: Copied! <pre># 1. Obtenga un lote de im\u00e1genes y etiquetas del DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Obtenga una sola imagen del lote y descomprima la imagen para que su forma se ajuste al modelo.\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Realice un pase hacia adelante en una sola imagen.\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Imprima lo que est\u00e1 sucediendo y convierta los logits del modelo -&gt; problemas pred -&gt; etiqueta pred\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n</pre> # 1. Obtenga un lote de im\u00e1genes y etiquetas del DataLoader img_batch, label_batch = next(iter(train_dataloader_simple))  # 2. Obtenga una sola imagen del lote y descomprima la imagen para que su forma se ajuste al modelo. img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0] print(f\"Single image shape: {img_single.shape}\\n\")  # 3. Realice un pase hacia adelante en una sola imagen. model_0.eval() with torch.inference_mode():     pred = model_0(img_single.to(device))      # 4. Imprima lo que est\u00e1 sucediendo y convierta los logits del modelo -&gt; problemas pred -&gt; etiqueta pred print(f\"Output logits:\\n{pred}\\n\") print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\") print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\") print(f\"Actual label:\\n{label_single}\") <p>Maravilloso, parece que nuestro modelo est\u00e1 generando lo que esper\u00e1bamos.</p> <p>Puede ejecutar la celda de arriba varias veces y cada vez se predecir\u00e1 una imagen diferente.</p> <p>Y probablemente notar\u00e1 que las predicciones a menudo son err\u00f3neas.</p> <p>Esto es de esperarse porque el modelo a\u00fan no ha sido entrenado y esencialmente se trata de adivinar usando pesos aleatorios.</p> In\u00a0[\u00a0]: Copied! <pre># Instale torchinfo si no est\u00e1 disponible, imp\u00f3rtelo si lo est\u00e1\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size\n</pre> # Instale torchinfo si no est\u00e1 disponible, imp\u00f3rtelo si lo est\u00e1 try:      import torchinfo except:     !pip install torchinfo     import torchinfo      from torchinfo import summary summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size  <p>\u00a1Lindo!</p> <p>La salida de <code>torchinfo.summary()</code> nos brinda una gran cantidad de informaci\u00f3n sobre nuestro modelo.</p> <p>Como \"par\u00e1metros totales\", el n\u00famero total de par\u00e1metros en nuestro modelo, el \"tama\u00f1o total estimado (MB)\", que es el tama\u00f1o de nuestro modelo.</p> <p>Tambi\u00e9n puede ver el cambio en las formas de entrada y salida a medida que los datos de un determinado <code>input_size</code> se mueven a trav\u00e9s de nuestro modelo.</p> <p>En este momento, nuestros n\u00fameros de par\u00e1metros y el tama\u00f1o total del modelo son bajos.</p> <p>Esto porque estamos comenzando con un modelo peque\u00f1o.</p> <p>Y si necesitamos aumentar su tama\u00f1o m\u00e1s adelante, podemos hacerlo.</p> In\u00a0[\u00a0]: Copied! <pre>def train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n</pre> def train_step(model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer):     # Put model in train mode     model.train()          # Setup train loss and train accuracy values     train_loss, train_acc = 0, 0          # Loop through data loader data batches     for batch, (X, y) in enumerate(dataloader):         # Send data to target device         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate  and accumulate loss         loss = loss_fn(y_pred, y)         train_loss += loss.item()           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Calculate and accumulate accuracy metric across all batches         y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)         train_acc += (y_pred_class == y).sum().item()/len(y_pred)      # Adjust metrics to get average loss and accuracy per batch      train_loss = train_loss / len(dataloader)     train_acc = train_acc / len(dataloader)     return train_loss, train_acc <p>\u00a1Guau! Funci\u00f3n <code>train_step()</code> realizada.</p> <p>Ahora hagamos lo mismo con la funci\u00f3n <code>test_step()</code>.</p> <p>La principal diferencia aqu\u00ed ser\u00e1 que <code>test_step()</code> no aceptar\u00e1 un optimizador y, por lo tanto, no realizar\u00e1 un descenso de gradiente.</p> <p>Pero como haremos inferencias, nos aseguraremos de activar el administrador de contexto <code>torch.inference_mode()</code> para hacer predicciones.</p> In\u00a0[\u00a0]: Copied! <pre>def test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n</pre> def test_step(model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module):     # Put model in eval mode     model.eval()           # Setup test loss and test accuracy values     test_loss, test_acc = 0, 0          # Turn on inference context manager     with torch.inference_mode():         # Loop through DataLoader batches         for batch, (X, y) in enumerate(dataloader):             # Send data to target device             X, y = X.to(device), y.to(device)                  # 1. Forward pass             test_pred_logits = model(X)              # 2. Calculate and accumulate loss             loss = loss_fn(test_pred_logits, y)             test_loss += loss.item()                          # Calculate and accumulate accuracy             test_pred_labels = test_pred_logits.argmax(dim=1)             test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))                  # Adjust metrics to get average loss and accuracy per batch      test_loss = test_loss / len(dataloader)     test_acc = test_acc / len(dataloader)     return test_loss, test_acc <p>\u00a1Excelente!</p> In\u00a0[\u00a0]: Copied! <pre>from tqdm.auto import tqdm\n\n# 1. Considere varios par\u00e1metros necesarios para los pasos de capacitaci\u00f3n y prueba.\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n</pre> from tqdm.auto import tqdm  # 1. Considere varios par\u00e1metros necesarios para los pasos de capacitaci\u00f3n y prueba. def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),           epochs: int = 5):          # 2. Create empty results dictionary     results = {\"train_loss\": [],         \"train_acc\": [],         \"test_loss\": [],         \"test_acc\": []     }          # 3. Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer)         test_loss, test_acc = test_step(model=model,             dataloader=test_dataloader,             loss_fn=loss_fn)                  # 4. Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # 5. Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)      # 6. Return the filled results at the end of the epochs     return results In\u00a0[\u00a0]: Copied! <pre># Establecer semillas aleatorias\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Establecer n\u00famero de \u00e9pocas\nNUM_EPOCHS = 5\n\n# Recrea una instancia de TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n y optimizador.\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# iniciar el cron\u00f3metro\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Modelo de tren_0\nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# Finalice el cron\u00f3metro e imprima cu\u00e1nto tiempo tard\u00f3\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Establecer semillas aleatorias torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Establecer n\u00famero de \u00e9pocas NUM_EPOCHS = 5  # Recrea una instancia de TinyVGG model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device)  # Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n y optimizador. loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)  # iniciar el cron\u00f3metro from timeit import default_timer as timer  start_time = timer()  # Modelo de tren_0 model_0_results = train(model=model_0,                          train_dataloader=train_dataloader_simple,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # Finalice el cron\u00f3metro e imprima cu\u00e1nto tiempo tard\u00f3 end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <p>Mmm...</p> <p>Parece que nuestro modelo funcion\u00f3 bastante mal.</p> <p>Pero por ahora est\u00e1 bien, seguiremos perseverando.</p> <p>\u00bfCu\u00e1les son algunas formas en las que podr\u00edas mejorarlo?</p> <p>Nota: Consulte la secci\u00f3n [Mejorar un modelo (desde la perspectiva del modelo) en el cuaderno 02](https://www.learnpytorch.io/02_pytorch_classification/#5-improving-a-model-from -a-model-perspective) para obtener ideas sobre c\u00f3mo mejorar nuestro modelo TinyVGG.</p> In\u00a0[\u00a0]: Copied! <pre># Verifique las claves model_0_results\nmodel_0_results.keys()\n</pre> # Verifique las claves model_0_results model_0_results.keys() <p>Necesitaremos extraer cada una de estas claves y convertirlas en una trama.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n</pre> def plot_loss_curves(results: Dict[str, List[float]]):     \"\"\"Plots training curves of a results dictionary.      Args:         results (dict): dictionary containing list of values, e.g.             {\"train_loss\": [...],              \"train_acc\": [...],              \"test_loss\": [...],              \"test_acc\": [...]}     \"\"\"          # Get the loss values of the results dictionary (training and test)     loss = results['train_loss']     test_loss = results['test_loss']      # Get the accuracy values of the results dictionary (training and test)     accuracy = results['train_acc']     test_accuracy = results['test_acc']      # Figure out how many epochs there were     epochs = range(len(results['train_loss']))      # Setup a plot      plt.figure(figsize=(15, 7))      # Plot loss     plt.subplot(1, 2, 1)     plt.plot(epochs, loss, label='train_loss')     plt.plot(epochs, test_loss, label='test_loss')     plt.title('Loss')     plt.xlabel('Epochs')     plt.legend()      # Plot accuracy     plt.subplot(1, 2, 2)     plt.plot(epochs, accuracy, label='train_accuracy')     plt.plot(epochs, test_accuracy, label='test_accuracy')     plt.title('Accuracy')     plt.xlabel('Epochs')     plt.legend(); <p>Bien, probemos nuestra funci\u00f3n <code>plot_loss_curves()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>plot_loss_curves(model_0_results)\n</pre> plot_loss_curves(model_0_results) <p>Vaya.</p> <p>Parece que las cosas est\u00e1n por todos lados...</p> <p>Pero lo sab\u00edamos porque los resultados impresos de nuestro modelo durante el entrenamiento no eran muy prometedores.</p> <p>Podr\u00eda intentar entrenar el modelo durante m\u00e1s tiempo y ver qu\u00e9 sucede cuando traza una curva de p\u00e9rdidas en un horizonte temporal m\u00e1s largo.</p> In\u00a0[\u00a0]: Copied! <pre># Crea transformaci\u00f3n de entrenamiento con TrivialAugment\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# Crear transformaci\u00f3n de prueba (sin aumento de datos)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Crea transformaci\u00f3n de entrenamiento con TrivialAugment train_transform_trivial_augment = transforms.Compose([     transforms.Resize((64, 64)),     transforms.TrivialAugmentWide(num_magnitude_bins=31),     transforms.ToTensor()  ])  # Crear transformaci\u00f3n de prueba (sin aumento de datos) test_transform = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>\u00a1Maravilloso!</p> <p>Ahora convirtamos nuestras im\u00e1genes en <code>Dataset</code> usando <code>torchvision.datasets.ImageFolder()</code> y luego en <code>DataLoader</code> con <code>torch.utils.data.DataLoader()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Convierta carpetas de im\u00e1genes en conjuntos de datos\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n</pre> # Convierta carpetas de im\u00e1genes en conjuntos de datos train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment) test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)  train_data_augmented, test_data_simple <p>Y crearemos <code>DataLoader</code> con <code>batch_size=32</code> y con <code>num_workers</code> configurados seg\u00fan el n\u00famero de CPU disponibles en nuestra m\u00e1quina (podemos obtener esto usando <code>os.cpu_count()</code> de Python).</p> In\u00a0[\u00a0]: Copied! <pre># Convierta conjuntos de datos en DataLoader\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n</pre> # Convierta conjuntos de datos en DataLoader import os BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count()  torch.manual_seed(42) train_dataloader_augmented = DataLoader(train_data_augmented,                                          batch_size=BATCH_SIZE,                                          shuffle=True,                                         num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_augmented, test_dataloader In\u00a0[\u00a0]: Copied! <pre># Cree model_1 y env\u00edelo al dispositivo de destino\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n</pre> # Cree model_1 y env\u00edelo al dispositivo de destino torch.manual_seed(42) model_1 = TinyVGG(     input_shape=3,     hidden_units=10,     output_shape=len(train_data_augmented.classes)).to(device) model_1 <p>\u00a1Modelo listo!</p> <p>\u00a1Es hora de entrenar!</p> <p>Como ya tenemos funciones para el bucle de entrenamiento (<code>train_step()</code>) y el bucle de prueba (<code>test_step()</code>) y una funci\u00f3n para juntarlos en <code>train()</code>, reutilic\u00e9moslas.</p> <p>Usaremos la misma configuraci\u00f3n que <code>model_0</code> con solo variar el par\u00e1metro <code>train_dataloader</code>:</p> <ul> <li>Entrena durante 5 \u00e9pocas.</li> <li>Utilice <code>train_dataloader=train_dataloader_augmented</code> como datos de entrenamiento en <code>train()</code>.</li> <li>Utilice <code>torch.nn.CrossEntropyLoss()</code> como funci\u00f3n de p\u00e9rdida (ya que estamos trabajando con clasificaci\u00f3n de clases m\u00faltiples).</li> <li>Utilice <code>torch.optim.Adam()</code> con <code>lr=0.001</code> como tasa de aprendizaje como optimizador.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Establecer semillas aleatorias\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Establecer n\u00famero de \u00e9pocas\nNUM_EPOCHS = 5\n\n# Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n y optimizador.\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# iniciar el cron\u00f3metro\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Modelo de tren_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# Finalice el cron\u00f3metro e imprima cu\u00e1nto tiempo tard\u00f3\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Establecer semillas aleatorias torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Establecer n\u00famero de \u00e9pocas NUM_EPOCHS = 5  # Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n y optimizador. loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)  # iniciar el cron\u00f3metro from timeit import default_timer as timer  start_time = timer()  # Modelo de tren_1 model_1_results = train(model=model_1,                          train_dataloader=train_dataloader_augmented,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # Finalice el cron\u00f3metro e imprima cu\u00e1nto tiempo tard\u00f3 end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <p>Mmm...</p> <p>No parece que nuestro modelo haya vuelto a funcionar muy bien.</p> <p>Veamos sus curvas de p\u00e9rdidas.</p> In\u00a0[\u00a0]: Copied! <pre>plot_loss_curves(model_1_results)\n</pre> plot_loss_curves(model_1_results) <p>Guau...</p> <p>Estos tampoco tienen muy buena pinta...</p> <p>\u00bfNuestro modelo est\u00e1 insuficiente o sobreajustado?</p> <p>\u00bfO ambos?</p> <p>Idealmente nos gustar\u00eda que tuviera mayor precisi\u00f3n y menor p\u00e9rdida, \u00bfverdad?</p> <p>\u00bfCu\u00e1les son algunos m\u00e9todos que podr\u00eda intentar utilizar para lograrlos?</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n</pre> import pandas as pd model_0_df = pd.DataFrame(model_0_results) model_1_df = pd.DataFrame(model_1_results) model_0_df <p>Y ahora podemos escribir un c\u00f3digo de trazado usando <code>matplotlib</code> para visualizar los resultados de <code>model_0</code> y <code>model_1</code> juntos.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar una trama\nplt.figure(figsize=(15, 10))\n\n# Obtener n\u00famero de \u00e9pocas\nepochs = range(len(model_0_df))\n\n# Trama de p\u00e9rdida del tren\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# P\u00e9rdida de prueba de trama\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Trazar la precisi\u00f3n del tren\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Precisi\u00f3n de la prueba de trazado\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Configurar una trama plt.figure(figsize=(15, 10))  # Obtener n\u00famero de \u00e9pocas epochs = range(len(model_0_df))  # Trama de p\u00e9rdida del tren plt.subplot(2, 2, 1) plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\") plt.title(\"Train Loss\") plt.xlabel(\"Epochs\") plt.legend()  # P\u00e9rdida de prueba de trama plt.subplot(2, 2, 2) plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\") plt.title(\"Test Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Trazar la precisi\u00f3n del tren plt.subplot(2, 2, 3) plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\") plt.title(\"Train Accuracy\") plt.xlabel(\"Epochs\") plt.legend()  # Precisi\u00f3n de la prueba de trazado plt.subplot(2, 2, 4) plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\") plt.title(\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.legend(); <p>Parece que nuestros modelos tuvieron un desempe\u00f1o igualmente pobre y fueron algo espor\u00e1dicos (las m\u00e9tricas suben y bajan bruscamente).</p> <p>Si construyeras \"model_2\", \u00bfqu\u00e9 har\u00edas diferente para intentar mejorar el rendimiento?</p> In\u00a0[\u00a0]: Copied! <pre># Descargar imagen personalizada\nimport requests\n\n# Configurar ruta de imagen personalizada\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Descarga la imagen si a\u00fan no existe\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n</pre> # Descargar imagen personalizada import requests  # Configurar ruta de imagen personalizada custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Descarga la imagen si a\u00fan no existe if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\") In\u00a0[\u00a0]: Copied! <pre>import torchvision\n\n# Leer en imagen personalizada\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Imprimir datos de imagen\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n</pre> import torchvision  # Leer en imagen personalizada custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))  # Imprimir datos de imagen print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\") print(f\"Custom image shape: {custom_image_uint8.shape}\\n\") print(f\"Custom image dtype: {custom_image_uint8.dtype}\") <p>\u00a1Lindo! Parece que nuestra imagen est\u00e1 en formato tensorial; sin embargo, \u00bfeste formato de imagen es compatible con nuestro modelo?</p> <p>Nuestro tensor <code>custom_image</code> es del tipo de datos <code>torch.uint8</code> y sus valores est\u00e1n entre <code>[0, 255]</code>.</p> <p>Pero nuestro modelo toma tensores de imagen del tipo de datos <code>torch.float32</code> y con valores entre <code>[0, 1]</code>.</p> <p>Entonces, antes de usar nuestra imagen personalizada con nuestro modelo, tendremos que convertirla al mismo formato que los datos con los que se entrena nuestro modelo.</p> <p>Si no hacemos esto, nuestro modelo generar\u00e1 un error.</p> In\u00a0[\u00a0]: Copied! <pre># Intente hacer una predicci\u00f3n sobre la imagen en formato uint8 (esto generar\u00e1 un error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n</pre> # Intente hacer una predicci\u00f3n sobre la imagen en formato uint8 (esto generar\u00e1 un error) model_1.eval() with torch.inference_mode():     model_1(custom_image_uint8.to(device)) <p>Si intentamos hacer una predicci\u00f3n sobre una imagen en un tipo de datos diferente al que se entren\u00f3 nuestro modelo, obtenemos un error como el siguiente:</p> <p><code>RuntimeError: El tipo de entrada (torch.cuda.ByteTensor) y el tipo de peso (torch.cuda.FloatTensor) deben ser los mismos</code></p> <p>Arreglemos este problema convirtiendo nuestra imagen personalizada al mismo tipo de datos en el que se entren\u00f3 nuestro modelo (<code>torch.float32</code>).</p> In\u00a0[\u00a0]: Copied! <pre># Cargue una imagen personalizada y convierta los valores del tensor a float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divida los valores de p\u00edxeles de la imagen por 255 para obtenerlos entre [0, 1]\ncustom_image = custom_image / 255. \n\n# Imprimir datos de imagen\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n</pre> # Cargue una imagen personalizada y convierta los valores del tensor a float32 custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)  # Divida los valores de p\u00edxeles de la imagen por 255 para obtenerlos entre [0, 1] custom_image = custom_image / 255.   # Imprimir datos de imagen print(f\"Custom image tensor:\\n{custom_image}\\n\") print(f\"Custom image shape: {custom_image.shape}\\n\") print(f\"Custom image dtype: {custom_image.dtype}\") In\u00a0[\u00a0]: Copied! <pre># Trazar imagen personalizada\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n</pre> # Trazar imagen personalizada plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error plt.title(f\"Image shape: {custom_image.shape}\") plt.axis(False); <p>\u00a1Dos pulgares arriba!</p> <p>Ahora bien, \u00bfc\u00f3mo podr\u00edamos hacer que nuestra imagen tenga el mismo tama\u00f1o que las im\u00e1genes en las que se entren\u00f3 nuestro modelo?</p> <p>Una forma de hacerlo es con <code>torchvision.transforms.Resize()</code>.</p> <p>Compongamos una canalizaci\u00f3n de transformaci\u00f3n para hacerlo.</p> In\u00a0[\u00a0]: Copied! <pre># Crear canal de transformaci\u00f3n para cambiar el tama\u00f1o de la imagen\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transformar imagen de destino\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Imprime la forma original y la nueva forma.\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n</pre> # Crear canal de transformaci\u00f3n para cambiar el tama\u00f1o de la imagen custom_image_transform = transforms.Compose([     transforms.Resize((64, 64)), ])  # Transformar imagen de destino custom_image_transformed = custom_image_transform(custom_image)  # Imprime la forma original y la nueva forma. print(f\"Original shape: {custom_image.shape}\") print(f\"New shape: {custom_image_transformed.shape}\") <p>\u00a1Guau!</p> <p>Finalmente hagamos una predicci\u00f3n sobre nuestra propia imagen personalizada.</p> In\u00a0[\u00a0]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed) <p>Oh Dios m\u00edo...</p> <p>A pesar de nuestros preparativos, nuestra imagen y modelo personalizados est\u00e1n en diferentes dispositivos.</p> <p>Y obtenemos el error:</p> <p><code>RuntimeError: Se esperaba que todos los tensores estuvieran en el mismo dispositivo, pero encontr\u00e9 al menos dos dispositivos, cpu y cuda:0. (al comprobar el peso del argumento en el m\u00e9todo wrapper___slow_conv2d_forward)</code></p> <p>Arreglemos eso poniendo nuestra <code>custom_image_transformed</code> en el dispositivo de destino.</p> In\u00a0[\u00a0]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed.to(device)) <p>\u00bfAhora que?</p> <p>Parece que estamos recibiendo un error de forma.</p> <p>\u00bfPor qu\u00e9 podr\u00eda ser esto?</p> <p>Convertimos nuestra imagen personalizada para que tenga el mismo tama\u00f1o que las im\u00e1genes en las que se entren\u00f3 nuestro modelo...</p> <p>Oh espera...</p> <p>Hay una dimensi\u00f3n que nos olvidamos.</p> <p>El tama\u00f1o del lote.</p> <p>Nuestro modelo espera tensores de imagen con una dimensi\u00f3n de tama\u00f1o de lote al inicio (\"NCHW\" donde \"N\" es el tama\u00f1o de lote).</p> <p>Excepto que nuestra imagen personalizada actualmente es solo \"CHW\".</p> <p>Podemos agregar una dimensi\u00f3n de tama\u00f1o de lote usando <code>torch.unsqueeze(dim=0)</code> para agregar una dimensi\u00f3n adicional a nuestra imagen y finalmente hacer una predicci\u00f3n.</p> <p>B\u00e1sicamente, le indicaremos a nuestro modelo que prediga en una sola imagen (una imagen con un <code>batch_size</code> de 1).</p> In\u00a0[\u00a0]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n</pre> model_1.eval() with torch.inference_mode():     # Add an extra dimension to image     custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)          # Print out different shapes     print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")     print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")          # Make a prediction on image with an extra dimension     custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device)) <p>\u00a1\u00a1\u00a1S\u00ed!!!</p> <p>\u00a1Parece que funcion\u00f3!</p> <p>Nota: Lo que acabamos de analizar son tres de los problemas cl\u00e1sicos y m\u00e1s comunes de aprendizaje profundo y PyTorch:</p> <ol> <li>Tipos de datos incorrectos: nuestro modelo espera <code>torch.float32</code> donde nuestra imagen personalizada original era <code>uint8</code>.</li> <li>Dispositivo incorrecto: nuestro modelo estaba en el \"dispositivo\" de destino (en nuestro caso, la GPU), mientras que nuestros datos de destino a\u00fan no se hab\u00edan movido al \"dispositivo\" de destino.</li> <li>Formas incorrectas: nuestro modelo esperaba una imagen de entrada con la forma <code>[N, C, H, W]</code> o <code>[batch_size, color_channels, height, width]</code> mientras que nuestro tensor de imagen personalizado ten\u00eda la forma <code> [canales_color, alto, ancho]</code>.</li> </ol> <p>Tenga en cuenta que estos errores no son solo para predecir en im\u00e1genes personalizadas.</p> <p>Estar\u00e1n presentes en casi todos los tipos de datos (texto, audio, datos estructurados) y problemas con los que trabaje.</p> <p>Ahora echemos un vistazo a las predicciones de nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre>custom_image_pred\n</pre> custom_image_pred <p>Muy bien, estos todav\u00eda est\u00e1n en forma logit (las salidas sin procesar de un modelo se llaman logits).</p> <p>Convirt\u00e1moslos de logits -&gt; probabilidades de predicci\u00f3n -&gt; etiquetas de predicci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># Imprima logs de predicci\u00f3n\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convertir logits -&gt; probabilidades de predicci\u00f3n (usando torch.softmax() para clasificaci\u00f3n de clases m\u00faltiples)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convertir probabilidades de predicci\u00f3n -&gt; etiquetas de predicci\u00f3n\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n</pre> # Imprima logs de predicci\u00f3n print(f\"Prediction logits: {custom_image_pred}\")  # Convertir logits -&gt; probabilidades de predicci\u00f3n (usando torch.softmax() para clasificaci\u00f3n de clases m\u00faltiples) custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1) print(f\"Prediction probabilities: {custom_image_pred_probs}\")  # Convertir probabilidades de predicci\u00f3n -&gt; etiquetas de predicci\u00f3n custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1) print(f\"Prediction label: {custom_image_pred_label}\") <p>\u00a1Est\u00e1 bien!</p> <p>Luciendo bien.</p> <p>Pero, por supuesto, nuestra etiqueta de predicci\u00f3n todav\u00eda est\u00e1 en forma de \u00edndice/tensor.</p> <p>Podemos convertirlo en una predicci\u00f3n de nombre de clase de cadena indexando en la lista <code>class_names</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Encuentra la etiqueta prevista\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n</pre> # Encuentra la etiqueta prevista custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error custom_image_pred_class <p>Guau.</p> <p>Parece que el modelo hace la predicci\u00f3n correcta, a pesar de que tuvo un desempe\u00f1o deficiente seg\u00fan nuestras m\u00e9tricas de evaluaci\u00f3n.</p> <p>Nota: El modelo en su forma actual predecir\u00e1 \"pizza\", \"filete\" o \"sushi\" sin importar la imagen que se le d\u00e9. Si quisieras que tu modelo predijera en una clase diferente, tendr\u00edas que entrenarlo para hacerlo.</p> <p>Pero si verificamos <code>custom_image_pred_probs</code>, notaremos que el modelo otorga casi el mismo peso (los valores son similares) a cada clase.</p> In\u00a0[\u00a0]: Copied! <pre># Los valores de las probabilidades de predicci\u00f3n son bastante similares.\ncustom_image_pred_probs\n</pre> # Los valores de las probabilidades de predicci\u00f3n son bastante similares. custom_image_pred_probs <p>Tener probabilidades de predicci\u00f3n tan similares podr\u00eda significar un par de cosas:</p> <ol> <li>El modelo intenta predecir las tres clases al mismo tiempo (puede haber una imagen que contenga pizza, bistec y sushi).</li> <li>El modelo no sabe realmente lo que quiere predecir y, a su vez, simplemente asigna valores similares a cada una de las clases.</li> </ol> <p>Nuestro caso es el n\u00famero 2, dado que nuestro modelo est\u00e1 mal entrenado, b\u00e1sicamente se trata de adivinar la predicci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>def pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n</pre> def pred_and_plot_image(model: torch.nn.Module,                          image_path: str,                          class_names: List[str] = None,                          transform=None,                         device: torch.device = device):     \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"          # 1. Load in image and convert the tensor values to float32     target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)          # 2. Divide the image pixel values by 255 to get them between [0, 1]     target_image = target_image / 255.           # 3. Transform if necessary     if transform:         target_image = transform(target_image)          # 4. Make sure the model is on the target device     model.to(device)          # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():         # Add an extra dimension to the image         target_image = target_image.unsqueeze(dim=0)              # Make a prediction on image with an extra dimension and send it to the target device         target_image_pred = model(target_image.to(device))              # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 7. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)          # 8. Plot the image alongside the prediction and prediction probability     plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib     if class_names:         title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     else:          title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     plt.title(title)     plt.axis(False); <p>Qu\u00e9 funci\u00f3n tan bonita, prob\u00e9mosla.</p> In\u00a0[\u00a0]: Copied! <pre># Pred en nuestra imagen personalizada\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n</pre> # Pred en nuestra imagen personalizada pred_and_plot_image(model=model_1,                     image_path=custom_image_path,                     class_names=class_names,                     transform=custom_image_transform,                     device=device) <p>\u00a1Dos pulgares arriba otra vez!</p> <p>Parece que nuestro modelo acert\u00f3 en la predicci\u00f3n con solo adivinar.</p> <p>Sin embargo, este no ser\u00e1 siempre el caso con otras im\u00e1genes...</p> <p>La imagen tambi\u00e9n est\u00e1 pixelada porque cambiamos su tama\u00f1o a \"[64, 64]\" usando \"custom_image_transform\".</p> <p>Ejercicio: Intenta hacer una predicci\u00f3n con una de tus propias im\u00e1genes de pizza, bistec o sushi y observa qu\u00e9 sucede.</p>"},{"location":"06-04_pytorch_custom_datasets/#04-conjuntos-de-datos-personalizados-de-pytorch","title":"04. Conjuntos de datos personalizados de PyTorch\u00b6","text":"<p>En el \u00faltimo cuaderno, [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/), analizamos c\u00f3mo crear modelos de visi\u00f3n por computadora en un conjunto de datos integrado en PyTorch (FashionMNIST).</p> <p>Los pasos que tomamos son similares en muchos problemas diferentes del aprendizaje autom\u00e1tico.</p> <p>Encuentre un conjunto de datos, convierta el conjunto de datos en n\u00fameros, cree un modelo (o encuentre un modelo existente) para encontrar patrones en esos n\u00fameros que puedan usarse para la predicci\u00f3n.</p> <p>PyTorch tiene muchos conjuntos de datos integrados que se utilizan para una gran cantidad de puntos de referencia de aprendizaje autom\u00e1tico; sin embargo, a menudo querr\u00e1s usar tu propio conjunto de datos personalizado.</p>"},{"location":"06-04_pytorch_custom_datasets/#que-es-un-conjunto-de-datos-personalizado","title":"\u00bfQu\u00e9 es un conjunto de datos personalizado?\u00b6","text":"<p>Un conjunto de datos personalizado es una colecci\u00f3n de datos relacionados con un problema espec\u00edfico en el que est\u00e1s trabajando.</p> <p>En esencia, un conjunto de datos personalizado puede estar compuesto por casi cualquier cosa.</p> <p>Por ejemplo, si estuvi\u00e9ramos creando una aplicaci\u00f3n de clasificaci\u00f3n de im\u00e1genes de alimentos como Nutrify, nuestro conjunto de datos personalizado podr\u00edan ser im\u00e1genes de alimentos.</p> <p>O si intent\u00e1ramos crear un modelo para clasificar si una rese\u00f1a basada en texto en un sitio web fue positiva o negativa, nuestro conjunto de datos personalizado podr\u00eda ser ejemplos de rese\u00f1as de clientes existentes y sus calificaciones.</p> <p>O si intent\u00e1ramos crear una aplicaci\u00f3n de clasificaci\u00f3n de sonido, nuestro conjunto de datos personalizado podr\u00eda ser muestras de sonido junto con sus etiquetas de muestra.</p> <p>O si intent\u00e1ramos crear un sistema de recomendaci\u00f3n para los clientes que compran cosas en nuestro sitio web, nuestro conjunto de datos personalizado podr\u00eda ser ejemplos de productos que otras personas han comprado.</p> <p>PyTorch incluye muchas funciones existentes para cargar en varios conjuntos de datos personalizados en <code>TorchVision</code>, [<code>TorchText</code>](https://pytorch.org /text/stable/index.html), <code>TorchAudio</code> y <code>TorchRec</code> bibliotecas de dominio.</p> <p>Pero a veces estas funciones existentes pueden no ser suficientes.</p> <p>En ese caso, siempre podemos crear una subclase de <code>torch.utils.data.Dataset</code> y personalizarla a nuestro gusto.</p>"},{"location":"06-04_pytorch_custom_datasets/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>Aplicaremos el flujo de trabajo de PyTorch que cubrimos en el [cuaderno 01] (https://www.learnpytorch.io/01_pytorch_workflow/) y el [cuaderno 02] (https://www.learnpytorch.io/02_pytorch_classification/) a un problema de visi\u00f3n por computadora.</p> <p>Pero en lugar de utilizar un conjunto de datos PyTorch incorporado, usaremos nuestro propio conjunto de datos de im\u00e1genes de pizza, bistec y sushi.</p> <p>El objetivo ser\u00e1 cargar estas im\u00e1genes y luego construir un modelo para entrenarlas y predecirlas.</p> <p></p> <p>Qu\u00e9 vamos a construir. Usaremos <code>torchvision.datasets</code> as\u00ed como nuestra propia clase <code>Dataset</code> personalizada para cargar im\u00e1genes de alimentos y luego construiremos un modelo de visi\u00f3n por computadora PyTorch para, con suerte, poder clasificarlas.</p> <p>Espec\u00edficamente, cubriremos:</p> Tema Contenido 0. Importaci\u00f3n de PyTorch y configuraci\u00f3n de c\u00f3digo independiente del dispositivo Carguemos PyTorch y luego sigamos las mejores pr\u00e1cticas para configurar nuestro c\u00f3digo para que sea independiente del dispositivo. 1. Obtener datos Usaremos nuestro propio conjunto de datos personalizado de im\u00e1genes de pizza, bistec y sushi. 2. Convi\u00e9rtete en uno con los datos (preparaci\u00f3n de datos) Al comienzo de cualquier problema nuevo de aprendizaje autom\u00e1tico, es fundamental comprender los datos con los que se est\u00e1 trabajando. Aqu\u00ed tomaremos algunos pasos para determinar qu\u00e9 datos tenemos. 3. Transformaci\u00f3n de datos A menudo, los datos que obtienes no estar\u00e1n 100% listos para usar con un modelo de aprendizaje autom\u00e1tico. Aqu\u00ed veremos algunos pasos que podemos seguir para transformar nuestras im\u00e1genes para que est\u00e9n listas para ser usado con un modelo. 4. Cargando datos con <code>ImageFolder</code> (opci\u00f3n 1) PyTorch tiene muchas funciones de carga de datos integradas para tipos de datos comunes. <code>ImageFolder</code> es \u00fatil si nuestras im\u00e1genes est\u00e1n en formato de clasificaci\u00f3n de im\u00e1genes est\u00e1ndar. 5. Cargando datos de imagen con un <code>Conjunto de datos</code> personalizado \u00bfQu\u00e9 pasar\u00eda si PyTorch no tuviera una funci\u00f3n incorporada para cargar datos? Aqu\u00ed es donde podemos crear nuestra propia subclase personalizada de <code>torch.utils.data.Dataset</code>. 6. Otras formas de transformaciones (aumento de datos) El aumento de datos es una t\u00e9cnica com\u00fan para ampliar la diversidad de sus datos de entrenamiento. Aqu\u00ed exploraremos algunas de las funciones de aumento de datos integradas de <code>torchvision</code>. 7. Modelo 0: TinyVGG sin aumento de datos En esta etapa, tendremos nuestros datos listos, construyamos un modelo capaz de ajustarlos. Tambi\u00e9n crearemos algunas funciones de entrenamiento y prueba para entrenar y evaluar nuestro modelo. 8. Explorando curvas de p\u00e9rdidas Las curvas de p\u00e9rdida son una excelente manera de ver c\u00f3mo su modelo se entrena o mejora con el tiempo. Tambi\u00e9n son una buena manera de ver si su modelo est\u00e1 bajo ajuste o sobreajuste. 9. Modelo 1: TinyVGG con aumento de datos Hasta ahora, hemos probado un modelo sin, \u00bfqu\u00e9 tal si probamos uno con aumento de datos? 10. Comparar resultados de modelos Comparemos las curvas de p\u00e9rdida de nuestros diferentes modelos y veamos cu\u00e1l funcion\u00f3 mejor y analicemos algunas opciones para mejorar el rendimiento. 11. Hacer una predicci\u00f3n en una imagen personalizada Nuestro modelo est\u00e1 entrenado en un conjunto de datos de im\u00e1genes de pizza, bistec y sushi. En esta secci\u00f3n cubriremos c\u00f3mo usar nuestro modelo entrenado para predecir en una imagen fuera de nuestro conjunto de datos existente."},{"location":"06-04_pytorch_custom_datasets/#donde-puedes-conseguir-ayuda","title":"\u00bfD\u00f3nde puedes conseguir ayuda?\u00b6","text":"<p>Todos los materiales de este curso en vivo en GitHub.</p> <p>Si tiene problemas, tambi\u00e9n puede hacer una pregunta en el curso p\u00e1gina de debates de GitHub.</p> <p>Y, por supuesto, est\u00e1 la documentaci\u00f3n de PyTorch y los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"06-04_pytorch_custom_datasets/#0-importacion-de-pytorch-y-configuracion-de-codigo-independiente-del-dispositivo","title":"0. Importaci\u00f3n de PyTorch y configuraci\u00f3n de c\u00f3digo independiente del dispositivo\u00b6","text":""},{"location":"06-04_pytorch_custom_datasets/#1-obtener-datos","title":"1. Obtener datos\u00b6","text":"<p>Lo primero es lo primero que necesitamos algunos datos.</p> <p>Y como todo buen programa de cocina, ya nos tienen preparados algunos datos.</p> <p>Vamos a empezar poco a poco.</p> <p>Porque todav\u00eda no buscamos entrenar el modelo m\u00e1s grande ni utilizar el conjunto de datos m\u00e1s grande.</p> <p>El aprendizaje autom\u00e1tico es un proceso iterativo: comience poco a poco, haga que algo funcione y aumente cuando sea necesario.</p> <p>Los datos que usaremos son un subconjunto del [conjunto de datos Food101] (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).</p> <p>Food101 es un popular punto de referencia de visi\u00f3n por computadora, ya que contiene 1000 im\u00e1genes de 101 tipos diferentes de alimentos, con un total de 101 000 im\u00e1genes (75 750 de tren y 25 250 de prueba).</p> <p>\u00bfPuedes pensar en 101 alimentos diferentes?</p> <p>\u00bfSe te ocurre un programa inform\u00e1tico para clasificar 101 alimentos?</p> <p>Puedo.</p> <p>\u00a1Un modelo de aprendizaje autom\u00e1tico!</p> <p>Espec\u00edficamente, un modelo de visi\u00f3n por computadora de PyTorch como el que cubrimos en el [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/).</p> <p>Sin embargo, en lugar de 101 clases de comida, comenzaremos con 3: pizza, bistec y sushi.</p> <p>Y en lugar de 1000 im\u00e1genes por clase, comenzaremos con un 10% aleatorio (comience poco a poco, aumente cuando sea necesario).</p> <p>Si desea ver de d\u00f3nde provienen los datos, consulte los siguientes recursos:</p> <ul> <li>Original [conjunto de datos de Food101 y sitio web en papel] (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).</li> <li><code>torchvision.datasets.Food101</code>: la versi\u00f3n de los datos que descargu\u00e9 para este cuaderno.</li> <li><code>extras/04_custom_data_creation.ipynb</code>: un cuaderno que us\u00e9 para formatear el conjunto de datos de Food101 para usarlo este cuaderno.</li> <li><code>data/pizza_steak_sushi.zip</code>: el archivo zip de im\u00e1genes de pizza, bistec y sushi de Food101 , creado con el cuaderno vinculado anteriormente.</li> </ul> <p>Escribamos un c\u00f3digo para descargar los datos formateados de GitHub.</p> <p>Nota: El conjunto de datos que vamos a utilizar ha sido formateado previamente para el uso que nos gustar\u00eda utilizar. Sin embargo, a menudo tendr\u00e1s que formatear tus propios conjuntos de datos para cualquier problema en el que est\u00e9s trabajando. Esta es una pr\u00e1ctica habitual en el mundo del aprendizaje autom\u00e1tico.</p>"},{"location":"06-04_pytorch_custom_datasets/#2-conviertete-en-uno-con-los-datos-preparacion-de-datos","title":"2. Convi\u00e9rtete en uno con los datos (preparaci\u00f3n de datos)\u00b6","text":"<p>\u00a1Conjunto de datos descargado!</p> <p>Es hora de volverse uno con ello.</p> <p>Este es otro paso importante antes de construir un modelo.</p> <p>Como dijo Abraham Lossfunction...</p> <p>La preparaci\u00f3n de datos es primordial. Antes de construir un modelo, vu\u00e9lvete uno con los datos. Pregunte: \u00bfQu\u00e9 estoy tratando de hacer aqu\u00ed? Fuente: @mrdbourke Twitter.</p> <p>\u00bfQu\u00e9 es inspeccionar los datos y volverse uno con ellos?</p> <p>Antes de comenzar un proyecto o construir cualquier tipo de modelo, es importante saber con qu\u00e9 datos est\u00e1s trabajando.</p> <p>En nuestro caso, tenemos im\u00e1genes de pizza, bistec y sushi en formato de clasificaci\u00f3n de im\u00e1genes est\u00e1ndar.</p> <p>El formato de clasificaci\u00f3n de im\u00e1genes contiene clases separadas de im\u00e1genes en directorios separados titulados con un nombre de clase particular.</p> <p>Por ejemplo, todas las im\u00e1genes de <code>pizza</code> est\u00e1n contenidas en el directorio <code>pizza/</code>.</p> <p>Este formato es popular en muchos puntos de referencia de clasificaci\u00f3n de im\u00e1genes diferentes, incluido ImageNet (de los conjuntos de datos de puntos de referencia de visi\u00f3n por computadora m\u00e1s populares).</p> <p>Puede ver un ejemplo del formato de almacenamiento a continuaci\u00f3n, los n\u00fameros de las im\u00e1genes son arbitrarios.</p> <pre><code>pizza_steak_sushi/ &lt;- carpeta del conjunto de datos general\n    tren/ &lt;- im\u00e1genes de entrenamiento\n        pizza/ &lt;- nombre de clase como nombre de carpeta\n            imagen01.jpeg\n            imagen02.jpeg\n            ...\n        bife/\n            imagen24.jpeg\n            imagen25.jpeg\n            ...\n        Sushi/\n            imagen37.jpeg\n            ...\n    prueba/ &lt;- im\u00e1genes de prueba\n        pizza/\n            imagen101.jpeg\n            imagen102.jpeg\n            ...\n        bife/\n            imagen154.jpeg\n            imagen155.jpeg\n            ...\n        Sushi/\n            imagen167.jpeg\n            ...\n</code></pre> <p>El objetivo ser\u00e1 tomar esta estructura de almacenamiento de datos y convertirla en un conjunto de datos utilizable con PyTorch.</p> <p>Nota: La estructura de los datos con los que trabaja variar\u00e1 seg\u00fan el problema en el que est\u00e9 trabajando. Pero la premisa sigue siendo: volverse uno con los datos y luego encontrar la manera de convertirlos en un conjunto de datos compatible con PyTorch.</p> <p>Podemos inspeccionar lo que hay en nuestro directorio de datos escribiendo una peque\u00f1a funci\u00f3n auxiliar para recorrer cada uno de los subdirectorios y contar los archivos presentes.</p> <p>Para hacerlo, usaremos la [<code>os.walk()</code>] incorporada de Python (https://docs.python.org/3/library/os.html#os.walk).</p>"},{"location":"06-04_pytorch_custom_datasets/#21-visualizar-una-imagen","title":"2.1 Visualizar una imagen\u00b6","text":"<p>Bien, hemos visto c\u00f3mo se formatea nuestra estructura de directorios.</p> <p>Ahora, siguiendo el esp\u00edritu del explorador de datos, es hora de \u00a1visualizar, visualizar, visualizar!</p> <p>Escribamos un c\u00f3digo para:</p> <ol> <li>Obtenga todas las rutas de las im\u00e1genes usando <code>pathlib.Path.glob()</code> para encontrar todas las archivos que terminan en <code>.jpg</code>.</li> <li>Elija una ruta de imagen aleatoria usando <code>random.choice()</code> de Python.</li> <li>Obtenga el nombre de la clase de imagen usando <code>pathlib.Path.parent.stem</code>.</li> <li>Y como estamos trabajando con im\u00e1genes, abriremos la ruta de la imagen aleatoria usando [<code>PIL.Image.open()</code>](https://pillow.readthedocs.io/en/stable/reference/Image. html#PIL.Image.open) (PIL significa Biblioteca de im\u00e1genes de Python).</li> <li>Luego mostraremos la imagen e imprimiremos algunos metadatos.</li> </ol>"},{"location":"06-04_pytorch_custom_datasets/#3-transformar-datos","title":"3. Transformar datos\u00b6","text":"<p>Ahora, \u00bfqu\u00e9 pasar\u00eda si quisi\u00e9ramos cargar los datos de nuestra imagen en PyTorch?</p> <p>Antes de que podamos usar nuestros datos de imagen con PyTorch, necesitamos:</p> <ol> <li>Convertirlo en tensores (representaciones num\u00e9ricas de nuestras im\u00e1genes).</li> <li>Convi\u00e9rtalo en <code>torch.utils.data.Dataset</code> y posteriormente en <code>torch.utils.data.DataLoader</code>; los llamaremos <code>Dataset</code> y <code>DataLoader</code> para abreviar.</li> </ol> <p>Hay varios tipos diferentes de conjuntos de datos y cargadores de conjuntos de datos predise\u00f1ados para PyTorch, seg\u00fan el problema en el que est\u00e9 trabajando.</p> Espacio problem\u00e1tico Conjuntos de datos y funciones predise\u00f1ados Visi\u00f3n <code>torchvision.datasets</code> Audio <code>torchaudio.datasets</code> Texto <code>torchtext.datasets</code> Sistema de recomendaci\u00f3n <code>torchrec.datasets</code> <p>Dado que estamos trabajando con un problema de visi\u00f3n, veremos <code>torchvision.datasets</code> para nuestras funciones de carga de datos, as\u00ed como [<code>torchvision.transforms</code>](https://pytorch.org/vision/stable/transforms .html) para preparar nuestros datos.</p> <p>Importemos algunas bibliotecas base.</p>"},{"location":"06-04_pytorch_custom_datasets/#31-transformando-datos-con-torchvisiontransforms","title":"3.1 Transformando datos con <code>torchvision.transforms</code>\u00b6","text":"<p>Tenemos carpetas de im\u00e1genes, pero antes de poder usarlas con PyTorch, necesitamos convertirlas en tensores.</p> <p>Una de las formas en que podemos hacer esto es usando el m\u00f3dulo <code>torchvision.transforms</code>.</p> <p><code>torchvision.transforms</code> contiene muchos m\u00e9todos predise\u00f1ados para formatear im\u00e1genes, convertirlas en tensores e incluso manipularlas para aumento de datos (la pr\u00e1ctica de alterar datos para dificultar el aprendizaje de un modelo, veremos esto m\u00e1s adelante) prop\u00f3sitos.</p> <p>Para adquirir experiencia con <code>torchvision.transforms</code>, escribamos una serie de pasos de transformaci\u00f3n que:</p> <ol> <li>Cambie el tama\u00f1o de las im\u00e1genes usando <code>transforms.Resize()</code> (de aproximadamente 512x512 a 64x64 , la misma forma que las im\u00e1genes en el [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/)).</li> <li>Voltee nuestras im\u00e1genes aleatoriamente en horizontal usando <code>transforms.RandomHorizontalFlip()</code> (esto podr\u00eda considerarse una forma de aumento de datos porque cambiar\u00e1 artificialmente los datos de nuestra imagen).</li> <li>Convierta nuestras im\u00e1genes de una imagen PIL a un tensor de PyTorch usando [<code>transforms.ToTensor()</code>](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms. A Tensor).</li> </ol> <p>Podemos compilar todos estos pasos usando <code>torchvision.transforms.Compose()</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#4-opcion-1-cargar-datos-de-imagen-usando-imagefolder","title":"4. Opci\u00f3n 1: cargar datos de imagen usando <code>ImageFolder</code>\u00b6","text":"<p>Muy bien, es hora de convertir nuestros datos de imagen en un \"Conjunto de datos\" capaz de usarse con PyTorch.</p> <p>Dado que nuestros datos est\u00e1n en formato de clasificaci\u00f3n de im\u00e1genes est\u00e1ndar, podemos usar la clase [<code>torchvision.datasets.ImageFolder</code>](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets .Carpeta de im\u00e1genes).</p> <p>Donde podemos pasarle la ruta del archivo de un directorio de im\u00e1genes de destino, as\u00ed como una serie de transformaciones que nos gustar\u00eda realizar en nuestras im\u00e1genes.</p> <p>Prob\u00e9moslo en nuestras carpetas de datos <code>train_dir</code> y <code>test_dir</code> pasando <code>transform=data_transform</code> para convertir nuestras im\u00e1genes en tensores.</p>"},{"location":"06-04_pytorch_custom_datasets/#41-convertir-imagenes-cargadas-en-dataloaders","title":"4.1 Convertir im\u00e1genes cargadas en <code>DataLoader</code>'s\u00b6","text":"<p>Tenemos nuestras im\u00e1genes como <code>Dataset</code> de PyTorch, pero ahora convirt\u00e1moslas en <code>DataLoader</code>.</p> <p>Lo haremos usando <code>torch.utils.data.DataLoader</code>.</p> <p>Convertir nuestro \"Conjunto de datos\" en \"Cargador de datos\" los hace iterables para que un modelo pueda aprender las relaciones entre muestras y objetivos (caracter\u00edsticas y etiquetas).</p> <p>Para simplificar las cosas, usaremos <code>batch_size=1</code> y <code>num_workers=1</code>.</p> <p>\u00bfQu\u00e9 es \"num_workers\"?</p> <p>Buena pregunta.</p> <p>Define cu\u00e1ntos subprocesos se crear\u00e1n para cargar sus datos.</p> <p>Pi\u00e9nselo as\u00ed: cuanto mayor sea el valor establecido en <code>num_workers</code>, m\u00e1s potencia de c\u00e1lculo utilizar\u00e1 PyTorch para cargar sus datos.</p> <p>Personalmente, normalmente lo configuro en el n\u00famero total de CPU en mi m\u00e1quina a trav\u00e9s de <code>os.cpu_count()</code> de Python.</p> <p>Esto garantiza que el <code>DataLoader</code> reclute tantos n\u00facleos como sea posible para cargar datos.</p> <p>Nota: Hay m\u00e1s par\u00e1metros con los que puede familiarizarse usando <code>torch.utils.data.DataLoader</code> en la [documentaci\u00f3n de PyTorch](https://pytorch.org/docs/stable/data.html#torch .utils.data.DataLoader).</p>"},{"location":"06-04_pytorch_custom_datasets/#5-opcion-2-cargar-datos-de-imagen-con-un-conjunto-de-datos-personalizado","title":"5. Opci\u00f3n 2: cargar datos de imagen con un <code>conjunto de datos</code> personalizado\u00b6","text":"<p>\u00bfQu\u00e9 pasar\u00eda si no existiera un creador de <code>conjunto de datos</code> predise\u00f1ado como <code>torchvision.datasets.ImageFolder()</code>?</p> <p>\u00bfO no exist\u00eda uno para su problema espec\u00edfico?</p> <p>Bueno, podr\u00edas construir el tuyo propio.</p> <p>Pero espera, \u00bfcu\u00e1les son los pros y los contras de crear tu propia forma personalizada de cargar \"conjuntos de datos\"?</p> Ventajas de crear un \"conjunto de datos\" personalizado Desventajas de crear un <code>conjunto de datos</code> personalizado Puede crear un \"conjunto de datos\" a partir de casi cualquier cosa. Aunque podr\u00edas crear un <code>Conjunto de datos</code> a partir de casi cualquier cosa, eso no significa que vaya a funcionar. No se limita a las funciones de \"conjunto de datos\" predise\u00f1adas de PyTorch. El uso de un \"conjunto de datos\" personalizado a menudo resulta en escribir m\u00e1s c\u00f3digo, lo que podr\u00eda ser propenso a errores o problemas de rendimiento. <p>Para ver esto en acci\u00f3n, trabajemos para replicar <code>torchvision.datasets.ImageFolder()</code> subclasificando <code>torch.utils.data.Dataset</code> (la clase base para todos los <code>Dataset</code> en PyTorch).</p> <p>Comenzaremos importando los m\u00f3dulos que necesitamos:</p> <ul> <li>El <code>os</code> de Python para tratar con directorios (nuestros datos se almacenan en directorios).</li> <li><code>pathlib</code> de Python para tratar con rutas de archivos (cada una de nuestras im\u00e1genes tiene una ruta de archivo \u00fanica).</li> <li><code>antorcha</code> para todo lo relacionado con PyTorch.</li> <li>Clase <code>Imagen</code> de PIL para cargar im\u00e1genes.</li> <li><code>torch.utils.data.Dataset</code> para subclasificar y crear nuestro propio <code>Dataset</code> personalizado.</li> <li><code>torchvision.transforms</code> para convertir nuestras im\u00e1genes en tensores.</li> <li>Varios tipos del m\u00f3dulo <code>typing</code> de Python para agregar sugerencias de tipo a nuestro c\u00f3digo.</li> </ul> <p>Nota: Puede personalizar los siguientes pasos para su propio conjunto de datos. La premisa sigue siendo: escriba c\u00f3digo para cargar sus datos en el formato que desee.</p>"},{"location":"06-04_pytorch_custom_datasets/#51-creando-una-funcion-auxiliar-para-obtener-nombres-de-clases","title":"5.1 Creando una funci\u00f3n auxiliar para obtener nombres de clases\u00b6","text":"<p>Escribamos una funci\u00f3n auxiliar capaz de crear una lista de nombres de clases y un diccionario de nombres de clases y sus \u00edndices dada una ruta de directorio.</p> <p>Para hacerlo, haremos:</p> <ol> <li>Obtenga los nombres de las clases usando <code>os.scandir()</code> para recorrer un directorio de destino (idealmente el directorio est\u00e1 en formato de clasificaci\u00f3n de im\u00e1genes est\u00e1ndar).</li> <li>Genera un error si no se encuentran los nombres de las clases (si esto sucede, es posible que haya alg\u00fan problema con la estructura del directorio).</li> <li>Convierta los nombres de las clases en un diccionario de etiquetas num\u00e9ricas, una para cada clase.</li> </ol> <p>Veamos un peque\u00f1o ejemplo del paso 1 antes de escribir la funci\u00f3n completa.</p>"},{"location":"06-04_pytorch_custom_datasets/#52-crear-un-conjunto-de-datos-personalizado-para-replicar-imagefolder","title":"5.2 Crear un <code>Conjunto de datos</code> personalizado para replicar <code>ImageFolder</code>\u00b6","text":"<p>Ahora estamos listos para crear nuestro propio \"conjunto de datos\" personalizado.</p> <p>Construiremos uno para replicar la funcionalidad de <code>torchvision.datasets.ImageFolder()</code>.</p> <p>Esta ser\u00e1 una buena pr\u00e1ctica y, adem\u00e1s, revelar\u00e1 algunos de los pasos necesarios para crear su propio \"conjunto de datos\" personalizado.</p> <p>Ser\u00e1 bastante c\u00f3digo... \u00a1pero nada que no podamos manejar!</p> <p>Vamos a desglosarlo:</p> <ol> <li>Subclase <code>torch.utils.data.Dataset</code>.</li> <li>Inicialice nuestra subclase con un par\u00e1metro <code>targ_dir</code> (el directorio de datos de destino) y un par\u00e1metro <code>transform</code> (para que tengamos la opci\u00f3n de transformar nuestros datos si es necesario).</li> <li>Cree varios atributos para <code>paths</code> (las rutas de nuestras im\u00e1genes de destino), <code>transform</code> (las transformaciones que nos gustar\u00eda usar, esta puede ser <code>Ninguna</code>), <code>classes</code> y <code>class_to_idx</code> (de nuestro <code>find_classes ()</code> funci\u00f3n).</li> <li>Cree una funci\u00f3n para cargar im\u00e1genes desde un archivo y devolverlas, esto podr\u00eda ser usando <code>PIL</code> o <code>torchvision.io</code> (para entrada/salida de datos de visi\u00f3n).</li> <li>Sobrescriba el m\u00e9todo <code>__len__</code> de <code>torch.utils.data.Dataset</code> para devolver el n\u00famero de muestras en el <code>Dataset</code>. Esto se recomienda pero no es obligatorio. Esto es para que puedas llamar a <code>len (Conjunto de datos)</code>.</li> <li>Sobrescriba el m\u00e9todo <code>__getitem__</code> de <code>torch.utils.data.Dataset</code> para devolver una \u00fanica muestra del <code>Dataset</code>; esto es obligatorio.</li> </ol> <p>\u00a1Vamos a hacerlo!</p>"},{"location":"06-04_pytorch_custom_datasets/#53-crear-una-funcion-para-mostrar-imagenes-aleatorias","title":"5.3 Crear una funci\u00f3n para mostrar im\u00e1genes aleatorias\u00b6","text":"<p>\u00a1Sabes que hora es!</p> <p>Es hora de ponernos el sombrero de explorador de datos y \u00a1visualizar, visualizar, visualizar!</p> <p>Creemos una funci\u00f3n auxiliar llamada <code>display_random_images()</code> que nos ayuda a visualizar im\u00e1genes en nuestro `Conjunto de datos'.</p> <p>Espec\u00edficamente:</p> <ol> <li>Tome un <code>Conjunto de datos</code> y una serie de otros par\u00e1metros como <code>clases</code> (los nombres de nuestras clases de destino), la cantidad de im\u00e1genes para mostrar (<code>n</code>) y una semilla aleatoria.</li> <li>Para evitar que la visualizaci\u00f3n se salga de control, limitaremos \"n\" a 10 im\u00e1genes.</li> <li>Establezca la semilla aleatoria para parcelas reproducibles (si se establece \"semilla\").</li> <li>Obtenga una lista de \u00edndices de muestra aleatorios (podemos usar <code>random.sample()</code> de Python para esto) para trazar.</li> <li>Configure un gr\u00e1fico <code>matplotlib</code>.</li> <li>Recorra los \u00edndices de muestra aleatorios que se encuentran en el paso 4 y gr\u00e1belos con <code>matplotlib</code>.</li> <li>Aseg\u00farese de que las im\u00e1genes de muestra tengan la forma \"HWC\" (alto, ancho, canales de color) para que podamos trazarlas.</li> </ol>"},{"location":"06-04_pytorch_custom_datasets/#54-convierta-imagenes-cargadas-personalizadas-en-dataloader","title":"5.4 Convierta im\u00e1genes cargadas personalizadas en <code>DataLoader</code>\u00b6","text":"<p>Tenemos una manera de convertir nuestras im\u00e1genes sin procesar en <code>Dataset</code> (caracter\u00edsticas asignadas a etiquetas o <code>X</code> asignadas a <code>y</code>) a trav\u00e9s de nuestra clase <code>ImageFolderCustom</code>.</p> <p>Ahora, \u00bfc\u00f3mo podr\u00edamos convertir nuestro \"Conjunto de datos\" personalizado en un \"Cargador de datos\"?</p> <p>Si adivinaste usando <code>torch.utils.data.DataLoader()</code>, \u00a1estar\u00edas en lo cierto!</p> <p>Debido a que la subclase de nuestro <code>Dataset</code> personalizado <code>torch.utils.data.Dataset</code>, podemos usarla directamente con <code>torch.utils.data.DataLoader()</code>.</p> <p>Y podemos hacerlo usando pasos muy similares a los anteriores, excepto que esta vez usaremos nuestro <code>Conjunto de datos</code> personalizado.</p>"},{"location":"06-04_pytorch_custom_datasets/#6-otras-formas-de-transformaciones-aumento-de-datos","title":"6. Otras formas de transformaciones (aumento de datos)\u00b6","text":"<p>Ya hemos visto un par de transformaciones en nuestros datos, pero hay muchas m\u00e1s.</p> <p>Puede verlos todos en la [documentaci\u00f3n <code>torchvision.transforms</code>] (https://pytorch.org/vision/stable/transforms.html).</p> <p>El prop\u00f3sito de las transformaciones es alterar sus im\u00e1genes de alguna manera.</p> <p>Eso puede convertir sus im\u00e1genes en un tensor (como hemos visto antes).</p> <p>O recortarlo o borrar aleatoriamente una parte o rotarla aleatoriamente.</p> <p>Realizar este tipo de transformaciones a menudo se denomina aumento de datos.</p> <p>Aumento de datos es el proceso de alterar tus datos de tal manera que artificialmente aumentes la diversidad de tu conjunto de entrenamiento.</p> <p>Se espera que entrenar un modelo con este conjunto de datos artificialmente alterado d\u00e9 como resultado un modelo que sea capaz de realizar una mejor generalizaci\u00f3n (los patrones que aprende son m\u00e1s s\u00f3lidos para futuros ejemplos no vistos).</p> <p>Puede ver muchos ejemplos diferentes de aumento de datos realizado en im\u00e1genes usando <code>torchvision.transforms</code> en el [ejemplo de ilustraci\u00f3n de transformaciones] de PyTorch (https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#ilustracion-of-transforms ).</p> <p>Pero probemos uno nosotros mismos.</p> <p>El aprendizaje autom\u00e1tico consiste en aprovechar el poder de la aleatoriedad y las investigaciones muestran que las transformaciones aleatorias (como <code>transforms.RandAugment()</code> y  <code>transforms.TrivialAugmentWide()</code>) generalmente funcionan mejor que las transformaciones seleccionadas cuidadosamente.</p> <p>La idea detr\u00e1s de TrivialAugment es... bueno, trivial.</p> <p>Tiene un conjunto de transformaciones y elige aleatoriamente una cantidad de ellas para realizarlas en una imagen y en una magnitud aleatoria entre un rango determinado (una magnitud m\u00e1s alta significa m\u00e1s intensidad).</p> <p>El equipo de PyTorch incluso [us\u00f3 TrivialAugment para entrenar sus \u00faltimos modelos de visi\u00f3n de \u00faltima generaci\u00f3n](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using -torchvision-latest-primitives/#break-down-of-key-accuracy-improvements).</p> <p>![aumento de datos de aumento trivial que se utiliza para la capacitaci\u00f3n de vanguardia de PyTorch] (https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-trivial-augment-being-using-in -PyTorch-resize.png)</p> <p>TrivialAugment fue uno de los ingredientes utilizados en una reciente actualizaci\u00f3n de capacitaci\u00f3n de \u00faltima generaci\u00f3n para varios modelos de visi\u00f3n de PyTorch.</p> <p>\u00bfQu\u00e9 tal si lo probamos en algunas de nuestras propias im\u00e1genes?</p> <p>El par\u00e1metro principal al que prestar atenci\u00f3n en <code>transforms.TrivialAugmentWide()</code> es <code>num_magnitude_bins=31</code>.</p> <p>Define qu\u00e9 parte de un rango se seleccionar\u00e1 un valor de intensidad para aplicar una determinada transformaci\u00f3n, siendo \"0\" ning\u00fan rango y \"31\" siendo el rango m\u00e1ximo (la mayor probabilidad de obtener la mayor intensidad).</p> <p>Podemos incorporar <code>transforms.TrivialAugmentWide()</code> en <code>transforms.Compose()</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#7-modelo-0-tinyvgg-sin-aumento-de-datos","title":"7. Modelo 0: TinyVGG sin aumento de datos\u00b6","text":"<p>Muy bien, hemos visto c\u00f3mo convertir nuestros datos de im\u00e1genes en carpetas a tensores transformados.</p> <p>Ahora construyamos un modelo de visi\u00f3n por computadora para ver si podemos clasificar si una imagen es de pizza, bistec o sushi.</p> <p>Para comenzar, comenzaremos con una transformaci\u00f3n simple, solo cambiaremos el tama\u00f1o de las im\u00e1genes a \"(64, 64)\" y las convertiremos en tensores.</p>"},{"location":"06-04_pytorch_custom_datasets/#71-creando-transformaciones-y-cargando-datos-para-el-modelo-0","title":"7.1 Creando transformaciones y cargando datos para el Modelo 0\u00b6","text":""},{"location":"06-04_pytorch_custom_datasets/#72-crear-clase-de-modelo-tinyvgg","title":"7.2 Crear clase de modelo TinyVGG\u00b6","text":"<p>En [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn), utilizamos el modelo TinyVGG del [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/).</p> <p>Recreemos el mismo modelo, excepto que esta vez usaremos im\u00e1genes en color en lugar de escala de grises (<code>in_channels=3</code> en lugar de <code>in_channels=1</code> para p\u00edxeles RGB).</p>"},{"location":"06-04_pytorch_custom_datasets/#73-pruebe-un-pase-hacia-adelante-en-una-sola-imagen-para-probar-el-modelo","title":"7.3 Pruebe un pase hacia adelante en una sola imagen (para probar el modelo)\u00b6","text":"<p>Una buena forma de probar un modelo es hacer un pase directo a un solo dato.</p> <p>Tambi\u00e9n es una forma pr\u00e1ctica de probar las formas de entrada y salida de nuestras diferentes capas.</p> <p>Para hacer un pase hacia adelante en una sola imagen, hagamos lo siguiente:</p> <ol> <li>Obtenga un lote de im\u00e1genes y etiquetas del <code>DataLoader</code>.</li> <li>Obtenga una sola imagen del lote y \"descomprima()\" la imagen para que tenga un tama\u00f1o de lote de \"1\" (para que su forma se ajuste al modelo).</li> <li>Realice una inferencia en una sola imagen (asegur\u00e1ndose de enviar la imagen al \"dispositivo\" de destino).</li> <li>Imprima lo que est\u00e1 sucediendo y convierta los logits de salida sin procesar del modelo en probabilidades de predicci\u00f3n con <code>torch.softmax()</code> (ya que estamos trabajando con datos de m\u00faltiples clases) y convierta las probabilidades de predicci\u00f3n en etiquetas de predicci\u00f3n con <code>torch.argmax( )</code>.</li> </ol>"},{"location":"06-04_pytorch_custom_datasets/#74-utilice-torchinfo-para-tener-una-idea-de-las-formas-que-atraviesan-nuestro-modelo","title":"7.4 Utilice <code>torchinfo</code> para tener una idea de las formas que atraviesan nuestro modelo.\u00b6","text":"<p>Imprimir nuestro modelo con <code>print(model)</code> nos da una idea de lo que est\u00e1 pasando con nuestro modelo.</p> <p>Y podemos imprimir las formas de nuestros datos a trav\u00e9s del m\u00e9todo <code>forward()</code>.</p> <p>Sin embargo, una forma \u00fatil de obtener informaci\u00f3n de nuestro modelo es usar <code>torchinfo</code>.</p> <p><code>torchinfo</code> viene con un m\u00e9todo <code>summary()</code> que toma un modelo de PyTorch as\u00ed como un <code>input_shape</code> y devuelve lo que sucede cuando un tensor se mueve a trav\u00e9s de su modelo.</p> <p>Nota: Si est\u00e1s utilizando Google Colab, necesitar\u00e1s instalar <code>torchinfo</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#75-crear-funciones-de-tren-y-bucle-de-prueba","title":"7.5 Crear funciones de tren y bucle de prueba\u00b6","text":"<p>Tenemos datos y tenemos un modelo.</p> <p>Ahora creemos algunas funciones de bucle de prueba y entrenamiento para entrenar nuestro modelo con los datos de entrenamiento y evaluar nuestro modelo con los datos de prueba.</p> <p>Y para asegurarnos de que podamos volver a utilizar estos bucles de entrenamiento y prueba, los pondremos en funcionamiento.</p> <p>En concreto vamos a realizar tres funciones:</p> <ol> <li><code>train_step()</code>: toma un modelo, un <code>DataLoader</code>, una funci\u00f3n de p\u00e9rdida y un optimizador y entrena el modelo en el <code>DataLoader</code>.</li> <li><code>test_step()</code>: toma un modelo, un <code>DataLoader</code> y una funci\u00f3n de p\u00e9rdida y eval\u00faa el modelo en el <code>DataLoader</code>.</li> <li><code>train()</code>: realiza 1. y 2. juntos durante un n\u00famero determinado de \u00e9pocas y devuelve un diccionario de resultados.</li> </ol> <p>Nota: Tambi\u00e9n cubrimos los pasos de un bucle de optimizaci\u00f3n de PyTorch en cuaderno 01. como la [Canci\u00f3n de bucle de optimizaci\u00f3n de PyTorch no oficial] (https://youtu.be/Nutpusq_AFw) y hemos creado funciones similares en el [cuaderno 03] (https://www.learnpytorch.io/03_pytorch_computer_vision/#62-functionizing- bucles de entrenamiento y prueba).</p> <p>Comencemos construyendo <code>train_step()</code>.</p> <p>Debido a que estamos tratando con lotes en el <code>DataLoader</code>, acumularemos los valores de precisi\u00f3n y p\u00e9rdida del modelo durante el entrenamiento (sum\u00e1ndolos para cada lote) y luego los ajustaremos al final antes de devolverlos.</p>"},{"location":"06-04_pytorch_custom_datasets/#76-creando-una-funcion-train-para-combinar-train_step-y-test_step","title":"7.6 Creando una funci\u00f3n <code>train()</code> para combinar <code>train_step()</code> y <code>test_step()</code>\u00b6","text":"<p>Ahora necesitamos una manera de juntar nuestras funciones <code>train_step()</code> y <code>test_step()</code>.</p> <p>Para hacerlo, los empaquetaremos en una funci\u00f3n <code>train()</code>.</p> <p>Esta funci\u00f3n entrenar\u00e1 el modelo y lo evaluar\u00e1.</p> <p>Espec\u00edficamente:</p> <ol> <li>Tome un modelo, un <code>DataLoader</code> para conjuntos de entrenamiento y prueba, un optimizador, una funci\u00f3n de p\u00e9rdida y cu\u00e1ntas \u00e9pocas realizar cada paso de entrenamiento y prueba.</li> <li>Cree un diccionario de resultados vac\u00edo para los valores <code>train_loss</code>, <code>train_acc</code>, <code>test_loss</code> y <code>test_acc</code> (podemos llenarlo a medida que avanza el entrenamiento).</li> <li>Recorra las funciones de los pasos de prueba y entrenamiento durante varias \u00e9pocas.</li> <li>Imprime lo que sucede al final de cada \u00e9poca.</li> <li>Actualice el diccionario de resultados vac\u00edo con las m\u00e9tricas actualizadas en cada \u00e9poca.</li> <li>Devuelva el relleno</li> </ol> <p>Para realizar un seguimiento de la cantidad de \u00e9pocas por las que hemos pasado, importemos <code>tqdm</code> desde <code>tqdm.auto</code> (<code>tqdm</code> es uno de los m\u00e1s populares Las bibliotecas de barra de progreso para Python y <code>tqdm.auto</code> deciden autom\u00e1ticamente qu\u00e9 tipo de barra de progreso es mejor para su entorno inform\u00e1tico, por ejemplo, Jupyter Notebook frente a script de Python).</p>"},{"location":"06-04_pytorch_custom_datasets/#77-entrenar-y-evaluar-el-modelo-0","title":"7.7 Entrenar y evaluar el modelo 0\u00b6","text":"<p>Muy bien, muy bien, tenemos todos los ingredientes que necesitamos para entrenar y evaluar nuestro modelo.</p> <p>\u00a1Es hora de juntar nuestro modelo <code>TinyVGG</code>, las funciones <code>DataLoader</code> y <code>train()</code> para ver si podemos construir un modelo capaz de discernir entre pizza, bistec y sushi!</p> <p>Recreemos <code>model_0</code> (no es necesario, pero lo haremos para completarlo) y luego llamemos a nuestra funci\u00f3n <code>train()</code> pasando los par\u00e1metros necesarios.</p> <p>Para que nuestros experimentos sean r\u00e1pidos, entrenaremos nuestro modelo durante 5 \u00e9pocas (aunque puedes aumentar esto si lo deseas).</p> <p>En cuanto a un optimizador y una funci\u00f3n de p\u00e9rdida, usaremos <code>torch.nn.CrossEntropyLoss()</code> (ya que estamos trabajando con datos de clasificaci\u00f3n de clases m\u00faltiples) y <code>torch.optim.Adam( )</code> con una tasa de aprendizaje de <code>1e-3</code> respectivamente.</p> <p>Para ver cu\u00e1nto tardan las cosas, importaremos el m\u00e9todo <code>timeit.default_timer()</code> de Python para calcular el tiempo de entrenamiento. .</p>"},{"location":"06-04_pytorch_custom_datasets/#78-trazar-las-curvas-de-perdidas-del-modelo-0","title":"7.8 Trazar las curvas de p\u00e9rdidas del Modelo 0\u00b6","text":"<p>Seg\u00fan las impresiones de nuestro entrenamiento <code>model_0</code>, no parec\u00eda que le fuera muy bien.</p> <p>Pero podemos evaluarlo mejor trazando las curvas de p\u00e9rdida del modelo.</p> <p>Las curvas de p\u00e9rdida muestran los resultados del modelo a lo largo del tiempo.</p> <p>Y son una excelente manera de ver c\u00f3mo se desempe\u00f1a su modelo en diferentes conjuntos de datos (por ejemplo, entrenamiento y prueba).</p> <p>Creemos una funci\u00f3n para trazar los valores en nuestro diccionario <code>model_0_results</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#8-como-deberia-ser-una-curva-de-perdidas-ideal","title":"8. \u00bfC\u00f3mo deber\u00eda ser una curva de p\u00e9rdidas ideal?\u00b6","text":"<p>Observar las curvas de p\u00e9rdida de prueba y entrenamiento es una excelente manera de ver si su modelo est\u00e1 sobreajustado.</p> <p>Un modelo de sobreajuste es aquel que funciona mejor (a menudo por un margen considerable) en el conjunto de entrenamiento que en el conjunto de validaci\u00f3n/prueba.</p> <p>Si su p\u00e9rdida de entrenamiento es mucho menor que su p\u00e9rdida de prueba, su modelo est\u00e1 sobreajustado.</p> <p>Es decir, se aprenden demasiado bien los patrones en el entrenamiento y esos patrones no se generalizan a los datos de prueba.</p> <p>El otro lado es cuando tu p\u00e9rdida de entrenamiento y pruebas no es tan baja como te gustar\u00eda, esto se considera insuficiencia.</p> <p>La posici\u00f3n ideal para una curva de p\u00e9rdida de entrenamiento y prueba es que se alineen estrechamente entre s\u00ed.</p> <p></p> <p>Izquierda: si tus curvas de p\u00e9rdida de entrenamiento y pruebas no son tan bajas como te gustar\u00eda, esto se considera insuficiencia. *Medio: Cuando su p\u00e9rdida de prueba/validaci\u00f3n es mayor que su p\u00e9rdida de entrenamiento, esto se considera sobreajuste. Derecha: El escenario ideal es cuando las curvas de p\u00e9rdida de entrenamiento y prueba se alinean con el tiempo. Esto significa que su modelo se est\u00e1 generalizando bien. Hay m\u00e1s combinaciones y diferentes cosas que las curvas de p\u00e9rdida pueden hacer; para obtener m\u00e1s informaci\u00f3n sobre esto, consulte la [gu\u00eda de interpretaci\u00f3n de curvas de p\u00e9rdida] de Google (https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic).*</p>"},{"location":"06-04_pytorch_custom_datasets/#81-como-lidiar-con-el-sobreajuste","title":"8.1 C\u00f3mo lidiar con el sobreajuste\u00b6","text":"<p>Dado que el principal problema con el sobreajuste es que su modelo se ajusta demasiado bien a los datos de entrenamiento, querr\u00e1 utilizar t\u00e9cnicas para \"controlarlo\".</p> <p>Una t\u00e9cnica com\u00fan para prevenir el sobreajuste se conoce como regularizaci\u00f3n.</p> <p>Me gusta pensar en esto como \"hacer que nuestros modelos sean m\u00e1s regulares\", es decir, capaces de ajustar m\u00e1s tipos de datos.</p> <p>Analicemos algunos m\u00e9todos para evitar el sobreajuste.</p> M\u00e9todo para evitar el sobreajuste \u00bfQu\u00e9 es? Obtener m\u00e1s datos Tener m\u00e1s datos le da al modelo m\u00e1s oportunidades de aprender patrones, patrones que pueden ser m\u00e1s generalizables a nuevos ejemplos. Simplifica tu modelo Si el modelo actual ya est\u00e1 sobreajustando los datos de entrenamiento, puede ser un modelo demasiado complicado. Esto significa que est\u00e1 aprendiendo demasiado bien los patrones de los datos y no puede generalizar bien a datos invisibles. Una forma de simplificar un modelo es reducir la cantidad de capas que utiliza o reducir la cantidad de unidades ocultas en cada capa. Usar aumento de datos Aumento de datos manipula los datos de entrenamiento de una manera que al modelo le resulta m\u00e1s dif\u00edcil aprender, ya que agrega artificialmente m\u00e1s variedad. a los datos. Si un modelo es capaz de aprender patrones en datos aumentados, es posible que pueda generalizar mejor a datos invisibles. Usar aprendizaje por transferencia Transferir aprendizaje implica aprovechar los patrones (tambi\u00e9n llamados pesos previamente entrenados) que un modelo ha aprendido a usar como base para su propia tarea. En nuestro caso, podr\u00edamos usar un modelo de visi\u00f3n por computadora previamente entrenado en una gran variedad de im\u00e1genes y luego modificarlo ligeramente para que est\u00e9 m\u00e1s especializado en im\u00e1genes de alimentos. Usar capas de abandono Las capas de abandono eliminan aleatoriamente las conexiones entre capas ocultas en las redes neuronales, lo que simplifica efectivamente un modelo pero tambi\u00e9n mejora las conexiones restantes. Consulte <code>torch.nn.Dropout()</code> para obtener m\u00e1s informaci\u00f3n. Usar disminuci\u00f3n de la tasa de aprendizaje La idea aqu\u00ed es disminuir lentamente la tasa de aprendizaje a medida que se entrena un modelo. Esto es similar a alcanzar una moneda en el respaldo de un sof\u00e1. Cuanto m\u00e1s te acercas, m\u00e1s peque\u00f1os son tus pasos. Lo mismo ocurre con la tasa de aprendizaje: cuanto m\u00e1s te acerques a convergencia, m\u00e1s peque\u00f1as querr\u00e1s que sean tus actualizaciones de peso. . Utilice la parada anticipada Detenci\u00f3n temprana detiene el entrenamiento del modelo antes de que comience a sobreajustarse. Por ejemplo, digamos que la p\u00e9rdida del modelo ha dejado de disminuir durante las \u00faltimas 10 \u00e9pocas (este n\u00famero es arbitrario), es posible que desee detener el entrenamiento del modelo aqu\u00ed e ir con los pesos del modelo que tuvieron la p\u00e9rdida m\u00e1s baja (10 \u00e9pocas anteriores). <p>Existen m\u00e1s m\u00e9todos para abordar el sobreajuste, pero estos son algunos de los principales.</p> <p>A medida que comience a construir modelos cada vez m\u00e1s profundos, descubrir\u00e1 que debido a que los aprendizajes profundos son tan buenos para aprender patrones en los datos, lidiar con el sobreajuste es uno de los principales problemas del aprendizaje profundo.</p>"},{"location":"06-04_pytorch_custom_datasets/#82-como-lidiar-con-el-desajuste","title":"8.2 C\u00f3mo lidiar con el desajuste\u00b6","text":"<p>Cuando un modelo es underfitting, se considera que tiene un poder predictivo deficiente en los conjuntos de entrenamiento y prueba.</p> <p>En esencia, un modelo insuficiente no lograr\u00e1 reducir los valores de p\u00e9rdida al nivel deseado.</p> <p>En este momento, al observar nuestras curvas de p\u00e9rdida actuales, consider\u00e9 que nuestro modelo \"TinyVGG\", \"model_0\", no se ajustaba a los datos.</p> <p>La idea principal detr\u00e1s de lidiar con el desajuste es aumentar el poder predictivo de su modelo.</p> <p>Hay varias formas de hacer esto.</p> M\u00e9todo para evitar el desajuste \u00bfQu\u00e9 es? Agregue m\u00e1s capas/unidades a su modelo Si su modelo no se ajusta lo suficiente, es posible que no tenga la capacidad suficiente para aprender los patrones/pesos/representaciones requeridos de los datos para que sean predictivos. Una forma de agregar m\u00e1s poder predictivo a su modelo es aumentar la cantidad de capas/unidades ocultas dentro de esas capas. Ajustar la tasa de aprendizaje Quiz\u00e1s la tasa de aprendizaje de su modelo sea demasiado alta para empezar. Y est\u00e1 tratando de actualizar demasiado sus pesos en cada \u00e9poca, y a su vez no aprende nada. En este caso, puede reducir la tasa de aprendizaje y ver qu\u00e9 sucede. Usar aprendizaje por transferencia El aprendizaje por transferencia es capaz de prevenir el sobreajuste y el desajuste. Implica utilizar los patrones de un modelo que ya funcionaba y ajustarlos a su propio problema. Entrena por m\u00e1s tiempo A veces, un modelo simplemente necesita m\u00e1s tiempo para aprender las representaciones de datos. Si descubre que en sus experimentos m\u00e1s peque\u00f1os su modelo no est\u00e1 aprendiendo nada, tal vez dejarlo entrenar durante m\u00e1s \u00e9pocas pueda dar como resultado un mejor rendimiento. Utilice menos regularizaci\u00f3n Quiz\u00e1s su modelo no se ajuste lo suficiente porque est\u00e1 tratando de evitar un ajuste excesivo. Reprimir las t\u00e9cnicas de regularizaci\u00f3n puede ayudar a que su modelo se ajuste mejor a los datos."},{"location":"06-04_pytorch_custom_datasets/#83-el-equilibrio-entre-sobreajuste-y-desajuste","title":"8.3 El equilibrio entre sobreajuste y desajuste\u00b6","text":"<p>Ninguno de los m\u00e9todos discutidos anteriormente son soluciones m\u00e1gicas, es decir, no siempre funcionan.</p> <p>Y prevenir el sobreajuste y el desajuste es posiblemente el \u00e1rea m\u00e1s activa de la investigaci\u00f3n sobre el aprendizaje autom\u00e1tico.</p> <p>Dado que todo el mundo quiere que sus modelos se ajusten mejor (menos subajuste), pero no tan bien, no generalizan bien ni funcionan en el mundo real (menos sobreajuste).</p> <p>Existe una delgada l\u00ednea entre el sobreajuste y el desajuste.</p> <p>Porque demasiado de cada uno puede causar el otro.</p> <p>El aprendizaje por transferencia es quiz\u00e1s una de las t\u00e9cnicas m\u00e1s poderosas cuando se trata de lidiar con el sobreajuste y el desajuste de sus propios problemas.</p> <p>En lugar de elaborar manualmente diferentes t\u00e9cnicas de sobreajuste y desajuste, el aprendizaje por transferencia le permite tomar un modelo que ya funciona en un espacio problem\u00e1tico similar al suyo (por ejemplo, uno de paperswithcode.com/sota o  Modelos de Hugging Face) y apl\u00edquelo a su propio conjunto de datos.</p> <p>Veremos el poder del aprendizaje por transferencia en un cuaderno posterior.</p>"},{"location":"06-04_pytorch_custom_datasets/#9-modelo-1-tinyvgg-con-aumento-de-datos","title":"9. Modelo 1: TinyVGG con aumento de datos\u00b6","text":"<p>\u00a1Es hora de probar otro modelo!</p> <p>Esta vez, carguemos los datos y usemos aumento de datos para ver si mejora nuestros resultados de alguna manera.</p> <p>Primero, componeremos una transformaci\u00f3n de entrenamiento para incluir <code>transforms.TrivialAugmentWide()</code>, adem\u00e1s de cambiar el tama\u00f1o y convertir nuestras im\u00e1genes en tensores.</p> <p>Haremos lo mismo para una transformaci\u00f3n de prueba excepto sin el aumento de datos.</p>"},{"location":"06-04_pytorch_custom_datasets/#91-crear-transformacion-con-aumento-de-datos","title":"9.1 Crear transformaci\u00f3n con aumento de datos\u00b6","text":""},{"location":"06-04_pytorch_custom_datasets/#92-crear-y-probar-dataset-y-dataloader","title":"9.2 Crear y probar <code>Dataset</code> y <code>DataLoader</code>\u00b6","text":"<p>Nos aseguraremos de que el <code>Dataset</code> del tren use <code>train_transform_trivial_augment</code> y el <code>Dataset</code> de prueba use <code>test_transform</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#93-construir-y-entrenar-el-modelo-1","title":"9.3 Construir y entrenar el Modelo 1\u00b6","text":"<p>\u00a1Datos cargados!</p> <p>Ahora, para construir nuestro pr\u00f3ximo modelo, <code>model_1</code>, podemos reutilizar nuestra clase <code>TinyVGG</code> de antes.</p> <p>Nos aseguraremos de enviarlo al dispositivo de destino.</p>"},{"location":"06-04_pytorch_custom_datasets/#94-trazar-las-curvas-de-perdidas-del-modelo-1","title":"9.4 Trazar las curvas de p\u00e9rdidas del Modelo 1\u00b6","text":"<p>Como tenemos los resultados de <code>model_1</code> guardados en un diccionario de resultados, <code>model_1_results</code>, podemos trazarlos usando <code>plot_loss_curves()</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#10-comparar-los-resultados-del-modelo","title":"10. Comparar los resultados del modelo\u00b6","text":"<p>Aunque nuestros modelos funcionan bastante mal, a\u00fan podemos escribir c\u00f3digo para compararlos.</p> <p>Primero convirtamos los resultados de nuestro modelo en pandas DataFrames.</p>"},{"location":"06-04_pytorch_custom_datasets/#11-haz-una-prediccion-sobre-una-imagen-personalizada","title":"11. Haz una predicci\u00f3n sobre una imagen personalizada.\u00b6","text":"<p>Si ha entrenado un modelo en un determinado conjunto de datos, es probable que desee hacer una predicci\u00f3n con sus propios datos personalizados.</p> <p>En nuestro caso, dado que hemos entrenado un modelo con im\u00e1genes de pizza, bistec y sushi, \u00bfc\u00f3mo podr\u00edamos usar nuestro modelo para hacer una predicci\u00f3n sobre una de nuestras propias im\u00e1genes?</p> <p>Para hacerlo, podemos cargar una imagen y luego preprocesarla de una manera que coincida con el tipo de datos con los que se entren\u00f3 nuestro modelo.</p> <p>En otras palabras, tendremos que convertir nuestra propia imagen personalizada en un tensor y asegurarnos de que est\u00e9 en el tipo de datos correcto antes de pasarla a nuestro modelo.</p> <p>Comencemos descargando una imagen personalizada.</p> <p>Dado que nuestro modelo predice si una imagen contiene pizza, bistec o sushi, descarguemos una foto de [mi pap\u00e1 dando el visto bueno a una pizza grande de Learn PyTorch for Deep Learning GitHub](https://github.com/mrdbourke/ pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg).</p> <p>Descargamos la imagen usando el m\u00f3dulo <code>solicitudes</code> de Python.</p> <p>Nota: Si est\u00e1s usando Google Colab, tambi\u00e9n puedes cargar una imagen a la sesi\u00f3n actual yendo al men\u00fa del lado izquierdo -&gt; Archivos -&gt; Cargar en el almacenamiento de la sesi\u00f3n. Sin embargo, tenga cuidado, esta imagen se eliminar\u00e1 cuando finalice su sesi\u00f3n de Google Colab.</p>"},{"location":"06-04_pytorch_custom_datasets/#111-cargando-una-imagen-personalizada-con-pytorch","title":"11.1 Cargando una imagen personalizada con PyTorch\u00b6","text":"<p>\u00a1Excelente!</p> <p>Parece que tenemos una imagen personalizada descargada y lista para usar en <code>data/04-pizza-dad.jpeg</code>.</p> <p>Es hora de cargarlo.</p> <p><code>torchvision</code> de PyTorch tiene varios m\u00e9todos de entrada y salida (\"IO\" o \"io\" para abreviar) para leer y escribir im\u00e1genes y videos en [<code>torchvision.io</code>](https://pytorch.org/vision/stable/io .html).</p> <p>Como queremos cargar una imagen, usaremos [<code>torchvision.io.read_image()</code>](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io .read_image).</p> <p>Este m\u00e9todo leer\u00e1 una imagen JPEG o PNG y la convertir\u00e1 en un <code>torch.Tensor</code> tridimensional RGB o en escala de grises con valores del tipo de datos <code>uint8</code> en el rango <code>[0, 255]</code>.</p> <p>Prob\u00e9moslo.</p>"},{"location":"06-04_pytorch_custom_datasets/#112-prediccion-de-imagenes-personalizadas-con-un-modelo-pytorch-entrenado","title":"11.2 Predicci\u00f3n de im\u00e1genes personalizadas con un modelo PyTorch entrenado\u00b6","text":"<p>Hermoso, parece que nuestros datos de imagen ahora est\u00e1n en el mismo formato en el que se entren\u00f3 nuestro modelo.</p> <p>Excepto por una cosa...</p> <p>Es \"forma\".</p> <p>Nuestro modelo fue entrenado en im\u00e1genes con forma \"[3, 64, 64]\", mientras que nuestra imagen personalizada actualmente es \"[3, 4032, 3024]\".</p> <p>\u00bfC\u00f3mo podemos asegurarnos de que nuestra imagen personalizada tenga la misma forma que las im\u00e1genes en las que se entren\u00f3 nuestro modelo?</p> <p>\u00bfHay alg\u00fan <code>torchvision.transforms</code> que pueda ayudar?</p> <p>Antes de responder esa pregunta, tracemos la imagen con <code>matplotlib</code> para asegurarnos de que se vea bien. Recuerde que tendremos que permutar las dimensiones de <code>CHW</code> a <code>HWC</code> para adaptarlas a los requisitos de <code>matplotlib</code>.</p>"},{"location":"06-04_pytorch_custom_datasets/#113-armar-la-prediccion-de-imagenes-personalizadas-construir-una-funcion","title":"11.3 Armar la predicci\u00f3n de im\u00e1genes personalizadas: construir una funci\u00f3n\u00b6","text":"<p>Realizar todos los pasos anteriores cada vez que desee hacer una predicci\u00f3n sobre una imagen personalizada r\u00e1pidamente se volver\u00eda tedioso.</p> <p>As\u00ed que junt\u00e9moslos todos en una funci\u00f3n que podamos usar f\u00e1cilmente una y otra vez.</p> <p>Espec\u00edficamente, hagamos una funci\u00f3n que:</p> <ol> <li>Toma una ruta de imagen de destino y la convierte al tipo de datos correcto para nuestro modelo (<code>torch.float32</code>).</li> <li>Se asegura de que los valores de p\u00edxeles de la imagen de destino est\u00e9n en el rango <code>[0, 1]</code>.</li> <li>Transforma la imagen de destino si es necesario.</li> <li>Se asegura de que el modelo est\u00e9 en el dispositivo de destino.</li> <li>Realiza una predicci\u00f3n sobre la imagen de destino con un modelo entrenado (asegur\u00e1ndose de que la imagen tenga el tama\u00f1o correcto y est\u00e9 en el mismo dispositivo que el modelo).</li> <li>Convierte los logits de salida del modelo en probabilidades de predicci\u00f3n.</li> <li>Convierte las probabilidades de predicci\u00f3n en etiquetas de predicci\u00f3n.</li> <li>Traza la imagen de destino junto con la predicci\u00f3n del modelo y la probabilidad de predicci\u00f3n.</li> </ol> <p>\u00a1Unos pocos pasos, pero lo tenemos!</p>"},{"location":"06-04_pytorch_custom_datasets/#principales-conclusiones","title":"Principales conclusiones\u00b6","text":"<p>Hemos cubierto bastante en este m\u00f3dulo.</p> <p>Resum\u00e1moslo con algunos puntos.</p> <ul> <li>PyTorch tiene muchas funciones integradas para manejar todo tipo de datos, desde visi\u00f3n hasta texto, audio y sistemas de recomendaci\u00f3n.</li> <li>Si las funciones de carga de datos integradas de PyTorch no se adaptan a sus necesidades, puede escribir c\u00f3digo para crear sus propios conjuntos de datos personalizados subclasificando <code>torch.utils.data.Dataset</code>.</li> <li><code>torch.utils.data.DataLoader</code>' en PyTorch ayuda a convertir su <code>Dataset</code> en iterables que se pueden usar al entrenar y probar un modelo.</li> <li>Gran parte del aprendizaje autom\u00e1tico trata del equilibrio entre sobreajuste y desajuste (anteriormente analizamos diferentes m\u00e9todos para cada uno, por lo que un buen ejercicio ser\u00eda investigar m\u00e1s y escribir c\u00f3digo para probar las diferentes t\u00e9cnicas). ).</li> <li>Es posible predecir sus propios datos personalizados con un modelo entrenado, siempre y cuando formatee los datos en un formato similar al formato en el que se entren\u00f3 el modelo. Aseg\u00farese de ocuparse de los tres grandes errores de PyTorch y de aprendizaje profundo:<ol> <li>Tipos de datos incorrectos: su modelo esperaba <code>torch.float32</code> cuando sus datos son <code>torch.uint8</code>.</li> <li>Formas de datos incorrectas: su modelo esperaba <code>[batch_size, color_channels, height, width]</code> cuando sus datos son <code>[color_channels, height, width]</code>.</li> <li>Dispositivos incorrectos: su modelo est\u00e1 en la GPU pero sus datos est\u00e1n en la CPU.</li> </ol> </li> </ul>"},{"location":"06-04_pytorch_custom_datasets/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Todos los ejercicios se centran en practicar el c\u00f3digo de las secciones anteriores.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Todos los ejercicios deben completarse utilizando c\u00f3digo independiente del dispositivo.</p> <p>Recursos:</p> <ul> <li>Cuaderno de plantilla de ejercicios para 04</li> <li>Cuaderno de soluciones de ejemplo para 04 (pruebe los ejercicios antes de mirar esto)</li> </ul> <ol> <li>Nuestros modelos tienen un rendimiento deficiente (no se ajustan bien a los datos). \u00bfCu\u00e1les son 3 m\u00e9todos para prevenir el desajuste? Escr\u00edbelas y explica cada una con una frase.</li> <li>Recrea las funciones de carga de datos que creamos en las secciones 1, 2, 3 y 4. Deber\u00edas tener el <code>DataLoader</code> preparado y probado listo para usar.</li> <li>Recrea el <code>model_0</code> que construimos en la secci\u00f3n 7.</li> <li>Cree funciones de entrenamiento y prueba para <code>model_0</code>.</li> <li>Intenta entrenar el modelo que hiciste en el ejercicio 3 durante 5, 20 y 50 \u00e9pocas, \u00bfqu\u00e9 pasa con los resultados?<ul> <li>Utilice <code>torch.optim.Adam()</code> con una tasa de aprendizaje de 0,001 como optimizador.</li> </ul> </li> <li>Duplica la cantidad de unidades ocultas en tu modelo y entr\u00e9nalo durante 20 \u00e9pocas, \u00bfqu\u00e9 pasa con los resultados?</li> <li>Duplica los datos que est\u00e1s usando con tu modelo y entr\u00e9nalo durante 20 \u00e9pocas, \u00bfqu\u00e9 pasa con los resultados?<ul> <li>Nota: Puede utilizar el cuaderno de creaci\u00f3n de datos personalizado para ampliar su conjunto de datos de Food101 .</li> <li>Tambi\u00e9n puede encontrar el conjunto de datos de datos dobles ya formateados (subconjunto del 20% en lugar del 10%) en GitHub, deber\u00e1 escribir el c\u00f3digo de descarga como en el ejercicio 2 para incluirlo en este cuaderno.</li> </ul> </li> <li>Haz una predicci\u00f3n sobre tu propia imagen personalizada de pizza/filete/sushi (incluso puedes descargar una de Internet) y comparte tu predicci\u00f3n.<ul> <li>\u00bfEl modelo que entren\u00f3 en el ejercicio 7 lo hace bien?</li> <li>Si no, \u00bfqu\u00e9 crees que podr\u00edas hacer para mejorarlo?</li> </ul> </li> </ol>"},{"location":"06-04_pytorch_custom_datasets/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Para practicar su conocimiento de los <code>Dataset</code> y <code>DataLoader</code> de PyTorch a trav\u00e9s de PyTorch [cuaderno tutorial de conjuntos de datos y cargadores de datos] (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).</li> <li>Dedique 10 minutos a leer la [documentaci\u00f3n de PyTorch <code>torchvision.transforms</code>] (https://pytorch.org/vision/stable/transforms.html).<ul> <li>Puede ver demostraciones de transformaciones en acci\u00f3n en el tutorial de ilustraciones de transformaciones.</li> </ul> </li> <li>Dedique 10 minutos a leer la [documentaci\u00f3n <code>torchvision.datasets</code>] de PyTorch (https://pytorch.org/vision/stable/datasets.html).<ul> <li>\u00bfCu\u00e1les son algunos conjuntos de datos que le llaman la atenci\u00f3n?</li> <li>\u00bfC\u00f3mo podr\u00edas intentar construir un modelo sobre estos?</li> </ul> </li> <li>TorchData est\u00e1 actualmente en versi\u00f3n beta (a partir de abril de 2022), ser\u00e1 una forma futura de cargar datos en PyTorch, pero puedes comenzar a \u00c9chale un vistazo ahora.</li> <li>Para acelerar los modelos de aprendizaje profundo, puede hacer algunos trucos para mejorar la computaci\u00f3n, la memoria y los c\u00e1lculos generales. Para obtener m\u00e1s informaci\u00f3n, lea la publicaci\u00f3n [C\u00f3mo hacer que el aprendizaje profundo sea mejor desde los primeros principios](https://horace.io/brrr_intro .html) de Horace He.</li> </ul>"},{"location":"07-04-feedforward-nn/","title":"Entrenamiento de redes neuronales profundas en una GPU con PyTorch","text":"<p>Este tutorial cubre los siguientes temas:</p> <ul> <li>Creando una red neuronal profunda con capas ocultas.</li> <li>Usando una funci\u00f3n de activaci\u00f3n no lineal</li> <li>Usar una GPU (cuando est\u00e9 disponible) para acelerar el entrenamiento</li> <li>Experimentar con hiperpar\u00e1metros para mejorar el modelo.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Descomente y ejecute el comando apropiado para su sistema operativo, si es necesario\n\n# Linux / Carpeta\n# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n# ventanas\n# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n# Mac OS\n# !pip instalar numpy matplotlib antorcha torchvision torchaudio\n</pre> # Descomente y ejecute el comando apropiado para su sistema operativo, si es necesario  # Linux / Carpeta # !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html  # ventanas # !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html  # Mac OS # !pip instalar numpy matplotlib antorcha torchvision torchaudio In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torchvision\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n%matplotlib inline\n\n# Utilice un fondo blanco para las figuras matplotlib.\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'\n</pre> import torch import torchvision import numpy as np import matplotlib import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F from torchvision.datasets import MNIST from torchvision.transforms import ToTensor from torchvision.utils import make_grid from torch.utils.data.dataloader import DataLoader from torch.utils.data import random_split %matplotlib inline  # Utilice un fondo blanco para las figuras matplotlib. matplotlib.rcParams['figure.facecolor'] = '#ffffff' <p>Podemos descargar los datos y crear un conjunto de datos de PyTorch usando la clase <code>MNIST</code> de <code>torchvision.datasets</code>.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = MNIST(root='data/', download=True, transform=ToTensor())\n</pre> dataset = MNIST(root='data/', download=True, transform=ToTensor()) <p>Veamos un par de im\u00e1genes del conjunto de datos. Las im\u00e1genes se convierten a tensores de PyTorch con la forma \"1x28x28\" (las dimensiones representan canales de color, ancho y alto). Podemos usar <code>plt.imshow</code> para mostrar las im\u00e1genes. Sin embargo, <code>plt.imshow</code> espera que los canales sean la \u00faltima dimensi\u00f3n en un tensor de imagen, por lo que usamos el m\u00e9todo <code>permute</code> para reordenar las dimensiones de la imagen.</p> In\u00a0[\u00a0]: Copied! <pre>image, label = dataset[0]\nprint('image.shape:', image.shape)\nplt.imshow(image.permute(1, 2, 0), cmap='gray')\nprint('Label:', label)\n</pre> image, label = dataset[0] print('image.shape:', image.shape) plt.imshow(image.permute(1, 2, 0), cmap='gray') print('Label:', label) In\u00a0[\u00a0]: Copied! <pre>image, label = dataset[0]\nprint('image.shape:', image.shape)\nplt.imshow(image.permute(1, 2, 0), cmap='gray')\nprint('Label:', label)\n</pre> image, label = dataset[0] print('image.shape:', image.shape) plt.imshow(image.permute(1, 2, 0), cmap='gray') print('Label:', label) <p>A continuaci\u00f3n, usemos la funci\u00f3n auxiliar <code>random_split</code> para reservar 10000 im\u00e1genes para nuestro conjunto de validaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>val_size = 10000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)\n</pre> val_size = 10000 train_size = len(dataset) - val_size  train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) <p>Ahora podemos crear cargadores de datos PyTorch para entrenamiento y validaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>batch_size=128\n</pre> batch_size=128 In\u00a0[\u00a0]: Copied! <pre>train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n</pre> train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True) <p>\u00bfPuedes descubrir el prop\u00f3sito de los argumentos <code>num_workers</code> y <code>pin_memory</code>? Intente consultar la documentaci\u00f3n: https://pytorch.org/docs/stable/data.html.</p> <p>Visualicemos un lote de datos en una cuadr\u00edcula usando la funci\u00f3n <code>make_grid</code> de <code>torchvision</code>. Tambi\u00e9n usaremos el m\u00e9todo <code>.permute</code> en el tensor para mover los canales a la \u00faltima dimensi\u00f3n, como lo esperaba <code>matplotlib</code>.</p> In\u00a0[\u00a0]: Copied! <pre>for images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break\n</pre> for images, _ in train_loader:     print('images.shape:', images.shape)     plt.figure(figsize=(16,8))     plt.axis('off')     plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))     break In\u00a0[\u00a0]: Copied! <pre>for images, labels in train_loader:\n    print('images.shape:', images.shape)\n    inputs = images.reshape(-1, 784)\n    print('inputs.shape:', inputs.shape)\n    break\n</pre> for images, labels in train_loader:     print('images.shape:', images.shape)     inputs = images.reshape(-1, 784)     print('inputs.shape:', inputs.shape)     break <p>A continuaci\u00f3n, creemos un objeto <code>nn.Linear</code>, que servir\u00e1 como nuestra capa oculta. Estableceremos el tama\u00f1o de la salida de la capa oculta en 32. Este n\u00famero se puede aumentar o disminuir para cambiar la capacidad de aprendizaje del modelo.</p> In\u00a0[\u00a0]: Copied! <pre>input_size = inputs.shape[-1]\nhidden_size = 32\n</pre> input_size = inputs.shape[-1] hidden_size = 32 In\u00a0[\u00a0]: Copied! <pre>layer1 = nn.Linear(input_size, hidden_size)\n</pre> layer1 = nn.Linear(input_size, hidden_size) <p>Ahora podemos calcular salidas intermedias para el lote de im\u00e1genes pasando \"entradas\" a trav\u00e9s de \"capa1\".</p> In\u00a0[\u00a0]: Copied! <pre>inputs.shape\n</pre> inputs.shape In\u00a0[\u00a0]: Copied! <pre>layer1_outputs = layer1(inputs)\nprint('layer1_outputs.shape:', layer1_outputs.shape)\n</pre> layer1_outputs = layer1(inputs) print('layer1_outputs.shape:', layer1_outputs.shape) <p>Los vectores de imagen de tama\u00f1o <code>784</code> se transforman en vectores de salida intermedios de longitud <code>32</code> realizando una multiplicaci\u00f3n matricial de la matriz de <code>inputs</code> con la matriz de pesos transpuesta de <code>layer1</code> y agregando el sesgo. Podemos verificar esto usando <code>torch.allclose</code>. Para obtener una explicaci\u00f3n m\u00e1s detallada, revise el tutorial sobre [regresi\u00f3n lineal] (https://jovian.ai/aakshns/02-linear-regression).</p> In\u00a0[\u00a0]: Copied! <pre>layer1_outputs_direct = inputs @ layer1.weight.t() + layer1.bias\nlayer1_outputs_direct.shape\n</pre> layer1_outputs_direct = inputs @ layer1.weight.t() + layer1.bias layer1_outputs_direct.shape In\u00a0[\u00a0]: Copied! <pre>torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3)\n</pre> torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3) <p>Por lo tanto, \"layer1_outputs\" y \"inputs\" tienen una relaci\u00f3n lineal, es decir, cada elemento de \"layer_outputs\" es una suma ponderada de elementos de \"inputs\". Por lo tanto, incluso cuando entrenamos el modelo y modificamos los pesos, la \"capa1\" solo puede capturar relaciones lineales entre las \"entradas\" y las \"salidas\".</p> <p>A continuaci\u00f3n, usaremos la funci\u00f3n Unidad lineal rectificada (ReLU) como funci\u00f3n de activaci\u00f3n para las salidas. Tiene la f\u00f3rmula <code>relu(x) = max(0,x)</code>, es decir, simplemente reemplaza los valores negativos en un tensor dado con el valor 0. ReLU es una funci\u00f3n no lineal, como se ve aqu\u00ed visualmente:</p> <p>Podemos usar el m\u00e9todo <code>F.relu</code> para aplicar ReLU a los elementos de un tensor.</p> In\u00a0[\u00a0]: Copied! <pre>F.relu(torch.tensor([[1, -1, 0], \n                     [-0.1, .2, 3]]))\n</pre> F.relu(torch.tensor([[1, -1, 0],                       [-0.1, .2, 3]])) <p>Apliquemos la funci\u00f3n de activaci\u00f3n a <code>layer1_outputs</code> y verifiquemos que los valores negativos fueron reemplazados por 0.</p> In\u00a0[\u00a0]: Copied! <pre>relu_outputs = F.relu(layer1_outputs)\nprint('min(layer1_outputs):', torch.min(layer1_outputs).item())\nprint('min(relu_outputs):', torch.min(relu_outputs).item())\n</pre> relu_outputs = F.relu(layer1_outputs) print('min(layer1_outputs):', torch.min(layer1_outputs).item()) print('min(relu_outputs):', torch.min(relu_outputs).item()) <p>Ahora que hemos aplicado una funci\u00f3n de activaci\u00f3n no lineal, <code>relu_outputs</code> y <code>inputs</code> no tienen una relaci\u00f3n lineal. Nos referimos a <code>ReLU</code> como la funci\u00f3n de activaci\u00f3n, porque para cada entrada se activan ciertas salidas (aquellas con valores distintos de cero) mientras que otras se apagan (aquellas con valores distintos de cero)</p> <p>A continuaci\u00f3n, creemos una capa de salida para convertir vectores de longitud <code>hidden_size</code> en <code>relu_outputs</code> en vectores de longitud 10, que es la salida deseada de nuestro modelo (ya que hay 10 etiquetas de destino).</p> In\u00a0[\u00a0]: Copied! <pre>output_size = 10\nlayer2 = nn.Linear(hidden_size, output_size)\n</pre> output_size = 10 layer2 = nn.Linear(hidden_size, output_size) In\u00a0[\u00a0]: Copied! <pre>layer2_outputs = layer2(relu_outputs)\nprint(layer2_outputs.shape)\n</pre> layer2_outputs = layer2(relu_outputs) print(layer2_outputs.shape) In\u00a0[\u00a0]: Copied! <pre>inputs.shape\n</pre> inputs.shape <p>Como era de esperar, <code>layer2_outputs</code> contiene un lote de vectores de tama\u00f1o 10. Ahora podemos usar esta salida para calcular la p\u00e9rdida usando <code>F.cross_entropy</code> y ajustar los pesos de <code>layer1</code> y <code>layer2</code> usando el descenso de gradiente.</p> In\u00a0[\u00a0]: Copied! <pre>F.cross_entropy(layer2_outputs, labels)\n</pre> F.cross_entropy(layer2_outputs, labels) <p>Por lo tanto, nuestro modelo transforma <code>entradas</code> en <code>capa2_salidas</code> aplicando una transformaci\u00f3n lineal (usando <code>capa1</code>), seguida de una activaci\u00f3n no lineal (usando <code>F.relu</code>), seguida de otra transformaci\u00f3n lineal (usando <code>capa2</code> ). Verifiquemos esto volviendo a calcular la salida usando operaciones matriciales b\u00e1sicas.</p> In\u00a0[\u00a0]: Copied! <pre># Versi\u00f3n ampliada de Layer2(F.relu(layer1(inputs)))\noutputs = (F.relu(inputs @ layer1.weight.t() + layer1.bias)) @ layer2.weight.t() + layer2.bias\n</pre> # Versi\u00f3n ampliada de Layer2(F.relu(layer1(inputs))) outputs = (F.relu(inputs @ layer1.weight.t() + layer1.bias)) @ layer2.weight.t() + layer2.bias In\u00a0[\u00a0]: Copied! <pre>torch.allclose(outputs, layer2_outputs, 1e-3)\n</pre> torch.allclose(outputs, layer2_outputs, 1e-3) <p>Tenga en cuenta que las \"salidas\" y las \"entradas\" no tienen una relaci\u00f3n lineal debido a la funci\u00f3n de activaci\u00f3n no lineal \"F.relu\". A medida que entrenamos el modelo y ajustamos los pesos de \"capa1\" y \"capa2\", ahora podemos capturar relaciones no lineales entre las im\u00e1genes y sus etiquetas. En otras palabras, la introducci\u00f3n de la no linealidad hace que el modelo sea m\u00e1s potente y vers\u00e1til. Adem\u00e1s, dado que <code>hidden_size</code> no depende de las dimensiones de las entradas o salidas, lo variamos para aumentar la cantidad de par\u00e1metros dentro del modelo. Tambi\u00e9n podemos introducir nuevas capas ocultas y aplicar la misma activaci\u00f3n no lineal despu\u00e9s de cada capa oculta.</p> <p>El modelo que acabamos de crear se llama red neuronal. Una red neuronal profunda es simplemente una red neuronal con una o m\u00e1s capas ocultas. De hecho, el [Teorema de aproximaci\u00f3n universal] (http://neuralnetworksanddeeplearning.com/chap4.html) establece que una red neuronal suficientemente grande y profunda puede calcular cualquier funci\u00f3n arbitraria, es decir, puede aprender relaciones no lineales ricas y complejas entre entradas y objetivos. Aqu\u00ed hay unos ejemplos:</p> <ul> <li>Identificar si una imagen contiene un gato o un perro (o [algo m\u00e1s] (https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/))</li> <li>Identificar el g\u00e9nero de una canci\u00f3n usando una muestra de 10 segundos.</li> <li>Clasificar las rese\u00f1as de pel\u00edculas como positivas o negativas seg\u00fan su contenido.</li> <li>Navegar con veh\u00edculos aut\u00f3nomos utilizando una transmisi\u00f3n de video de la carretera.</li> <li>Traducir oraciones del ingl\u00e9s al franc\u00e9s (y cientos de otros idiomas)</li> <li>Convertir una grabaci\u00f3n de voz a texto y viceversa</li> <li>Y muchos m\u00e1s...</li> </ul> <p>Es dif\u00edcil imaginar c\u00f3mo el simple proceso de multiplicar entradas con matrices inicializadas aleatoriamente, aplicar activaciones no lineales y ajustar pesos repetidamente mediante el descenso de gradiente puede producir resultados tan sorprendentes. Los modelos de aprendizaje profundo a menudo contienen millones de par\u00e1metros, que en conjunto pueden capturar relaciones mucho m\u00e1s complejas de las que el cerebro humano puede comprender.</p> <p>Si no hubi\u00e9ramos incluido una activaci\u00f3n no lineal entre las dos capas lineales, la relaci\u00f3n final entre entradas y salidas seguir\u00eda siendo lineal. Una simple refactorizaci\u00f3n de los c\u00e1lculos ilustra esto.</p> In\u00a0[\u00a0]: Copied! <pre># Igual que capa2 (capa1 (entradas))\noutputs2 = (inputs @ layer1.weight.t() + layer1.bias) @ layer2.weight.t() + layer2.bias\n</pre> # Igual que capa2 (capa1 (entradas)) outputs2 = (inputs @ layer1.weight.t() + layer1.bias) @ layer2.weight.t() + layer2.bias In\u00a0[\u00a0]: Copied! <pre># Cree una sola capa para reemplazar las dos capas lineales\ncombined_layer = nn.Linear(input_size, output_size)\n\ncombined_layer.weight.data = layer2.weight @ layer1.weight\ncombined_layer.bias.data = layer1.bias @ layer2.weight.t() + layer2.bias\n</pre> # Cree una sola capa para reemplazar las dos capas lineales combined_layer = nn.Linear(input_size, output_size)  combined_layer.weight.data = layer2.weight @ layer1.weight combined_layer.bias.data = layer1.bias @ layer2.weight.t() + layer2.bias In\u00a0[\u00a0]: Copied! <pre># Igual que combine_layer(entradas)\noutputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias\n</pre> # Igual que combine_layer(entradas) outputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias In\u00a0[\u00a0]: Copied! <pre>torch.allclose(outputs2, outputs3, 1e-3)\n</pre> torch.allclose(outputs2, outputs3, 1e-3) In\u00a0[\u00a0]: Copied! <pre># Instalar la biblioteca\n!pip install jovian --upgrade --quiet\n</pre> # Instalar la biblioteca !pip install jovian --upgrade --quiet In\u00a0[\u00a0]: Copied! <pre>import jovian\n</pre> import jovian In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project='04-feedforward-nn')\n</pre> jovian.commit(project='04-feedforward-nn') <p><code>jovian.commit</code> carga el cuaderno en su cuenta Jovian, captura el entorno Python y crea un enlace para compartir para su cuaderno, como se muestra arriba. Puede utilizar este enlace para compartir su trabajo y permitir que cualquiera (incluido usted) ejecute sus cuadernos y reproduzca su trabajo.</p> In\u00a0[\u00a0]: Copied! <pre>class MnistModel(nn.Module):\n    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n    def __init__(self, in_size, hidden_size, out_size):\n        super().__init__()\n        # hidden layer\n        self.linear1 = nn.Linear(in_size, hidden_size)\n        # output layer\n        self.linear2 = nn.Linear(hidden_size, out_size)\n        \n    def forward(self, xb):\n        # Flatten the image tensors\n        xb = xb.view(xb.size(0), -1)\n        # Get intermediate outputs using hidden layer\n        out = self.linear1(xb)\n        # Apply activation function\n        out = F.relu(out)\n        # Get predictions using output layer\n        out = self.linear2(out)\n        return out\n    \n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss, 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n</pre> class MnistModel(nn.Module):     \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"     def __init__(self, in_size, hidden_size, out_size):         super().__init__()         # hidden layer         self.linear1 = nn.Linear(in_size, hidden_size)         # output layer         self.linear2 = nn.Linear(hidden_size, out_size)              def forward(self, xb):         # Flatten the image tensors         xb = xb.view(xb.size(0), -1)         # Get intermediate outputs using hidden layer         out = self.linear1(xb)         # Apply activation function         out = F.relu(out)         # Get predictions using output layer         out = self.linear2(out)         return out          def training_step(self, batch):         images, labels = batch          out = self(images)                  # Generate predictions         loss = F.cross_entropy(out, labels) # Calculate loss         return loss          def validation_step(self, batch):         images, labels = batch          out = self(images)                    # Generate predictions         loss = F.cross_entropy(out, labels)   # Calculate loss         acc = accuracy(out, labels)           # Calculate accuracy         return {'val_loss': loss, 'val_acc': acc}              def validation_epoch_end(self, outputs):         batch_losses = [x['val_loss'] for x in outputs]         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses         batch_accs = [x['val_acc'] for x in outputs]         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}          def epoch_end(self, epoch, result):         print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc'])) <p>Tambi\u00e9n necesitamos definir una funci\u00f3n de \"precisi\u00f3n\" que calcule la precisi\u00f3n de la predicci\u00f3n del modelo en un lote de entradas. Se usa en <code>validation_step</code> arriba.</p> In\u00a0[\u00a0]: Copied! <pre>def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n</pre> def accuracy(outputs, labels):     _, preds = torch.max(outputs, dim=1)     return torch.tensor(torch.sum(preds == labels).item() / len(preds)) <p>Crearemos un modelo que contiene una capa oculta con 32 activaciones.</p> In\u00a0[\u00a0]: Copied! <pre>input_size = 784\nhidden_size = 32 # you can change this\nnum_classes = 10\n</pre> input_size = 784 hidden_size = 32 # you can change this num_classes = 10 In\u00a0[\u00a0]: Copied! <pre>model = MnistModel(input_size, hidden_size=32, out_size=num_classes)\n</pre> model = MnistModel(input_size, hidden_size=32, out_size=num_classes) <p>Echemos un vistazo a los par\u00e1metros del modelo. Esperamos ver una matriz de peso y sesgo para cada una de las capas.</p> In\u00a0[\u00a0]: Copied! <pre>for t in model.parameters():\n    print(t.shape)\n</pre> for t in model.parameters():     print(t.shape) <p>Intentemos generar algunos resultados usando nuestro modelo. Tomaremos el primer lote de 128 im\u00e1genes de nuestro conjunto de datos y las pasaremos a nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre>for images, labels in train_loader:\n    outputs = model(images)\n    loss = F.cross_entropy(outputs, labels)\n    print('Loss:', loss.item())\n    break\n\nprint('outputs.shape : ', outputs.shape)\nprint('Sample outputs :\\n', outputs[:2].data)\n</pre> for images, labels in train_loader:     outputs = model(images)     loss = F.cross_entropy(outputs, labels)     print('Loss:', loss.item())     break  print('outputs.shape : ', outputs.shape) print('Sample outputs :\\n', outputs[:2].data) In\u00a0[\u00a0]: Copied! <pre>torch.cuda.is_available()\n</pre> torch.cuda.is_available() <p>Definamos una funci\u00f3n auxiliar para garantizar que nuestro c\u00f3digo use la GPU si est\u00e1 disponible y use de manera predeterminada la CPU si no lo est\u00e1.</p> In\u00a0[\u00a0]: Copied! <pre>def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n</pre> def get_default_device():     \"\"\"Pick GPU if available, else CPU\"\"\"     if torch.cuda.is_available():         return torch.device('cuda')     else:         return torch.device('cpu') In\u00a0[\u00a0]: Copied! <pre>device = get_default_device()\ndevice\n</pre> device = get_default_device() device <p>A continuaci\u00f3n, definamos una funci\u00f3n que pueda mover datos y modelos a un dispositivo elegido.</p> In\u00a0[\u00a0]: Copied! <pre>def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n</pre> def to_device(data, device):     \"\"\"Move tensor(s) to chosen device\"\"\"     if isinstance(data, (list,tuple)):         return [to_device(x, device) for x in data]     return data.to(device, non_blocking=True) In\u00a0[\u00a0]: Copied! <pre>for images, labels in train_loader:\n    print(images.shape)\n    images = to_device(images, device)\n    print(images.device)\n    break\n</pre> for images, labels in train_loader:     print(images.shape)     images = to_device(images, device)     print(images.device)     break <p>Finalmente, definimos una clase <code>DeviceDataLoader</code> para empaquetar nuestros cargadores de datos existentes y mover lotes de datos al dispositivo seleccionado. Curiosamente, no necesitamos ampliar una clase existente para crear un cargador de datos de PyTorch. Todo lo que necesitamos es un m\u00e9todo <code>__iter__</code> para recuperar lotes de datos y un m\u00e9todo <code>__len__</code> para obtener el n\u00famero de lotes.</p> In\u00a0[\u00a0]: Copied! <pre>class DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n</pre> class DeviceDataLoader():     \"\"\"Wrap a dataloader to move data to a device\"\"\"     def __init__(self, dl, device):         self.dl = dl         self.device = device              def __iter__(self):         \"\"\"Yield a batch of data after moving it to device\"\"\"         for b in self.dl:              yield to_device(b, self.device)      def __len__(self):         \"\"\"Number of batches\"\"\"         return len(self.dl) <p>La palabra clave <code>yield</code> en Python se usa para crear una funci\u00f3n generadora que se puede usar dentro de un bucle <code>for</code>, como se ilustra a continuaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>def some_numbers():\n    yield 10\n    yield 20\n    yield 30\n\nfor value in some_numbers():\n    print(value)\n</pre> def some_numbers():     yield 10     yield 20     yield 30  for value in some_numbers():     print(value) <p>Ahora podemos empaquetar nuestros cargadores de datos usando <code>DeviceDataLoader</code>.</p> In\u00a0[\u00a0]: Copied! <pre>train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\n</pre> train_loader = DeviceDataLoader(train_loader, device) val_loader = DeviceDataLoader(val_loader, device) <p>Los tensores movidos a la GPU tienen una propiedad de \"dispositivo\" que incluye la palabra \"cuda\". Verifiquemos esto mirando un lote de datos de <code>valid_dl</code>.</p> In\u00a0[\u00a0]: Copied! <pre>for xb, yb in val_loader:\n    print('xb.device:', xb.device)\n    print('yb:', yb)\n    break\n</pre> for xb, yb in val_loader:     print('xb.device:', xb.device)     print('yb:', yb)     break In\u00a0[\u00a0]: Copied! <pre>def evaluate(model, val_loader):\n    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    \"\"\"Train the model using gradient descent\"\"\"\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n</pre> def evaluate(model, val_loader):     \"\"\"Evaluate the model's performance on the validation set\"\"\"     outputs = [model.validation_step(batch) for batch in val_loader]     return model.validation_epoch_end(outputs)  def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):     \"\"\"Train the model using gradient descent\"\"\"     history = []     optimizer = opt_func(model.parameters(), lr)     for epoch in range(epochs):         # Training Phase          for batch in train_loader:             loss = model.training_step(batch)             loss.backward()             optimizer.step()             optimizer.zero_grad()         # Validation phase         result = evaluate(model, val_loader)         model.epoch_end(epoch, result)         history.append(result)     return history <p>Antes de entrenar el modelo, debemos asegurarnos de que los datos y los par\u00e1metros del modelo (pesos y sesgos) est\u00e9n en el mismo dispositivo (CPU o GPU). Podemos reutilizar la funci\u00f3n <code>to_device</code> para mover los par\u00e1metros del modelo al dispositivo correcto.</p> In\u00a0[\u00a0]: Copied! <pre># Modelo (en GPU)\nmodel = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes)\nto_device(model, device)\n</pre> # Modelo (en GPU) model = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes) to_device(model, device) <p>Veamos c\u00f3mo se desempe\u00f1a el modelo en el conjunto de validaci\u00f3n con el conjunto inicial de ponderaciones y sesgos.</p> In\u00a0[\u00a0]: Copied! <pre>history = [evaluate(model, val_loader)]\nhistory\n</pre> history = [evaluate(model, val_loader)] history <p>La precisi\u00f3n inicial es de alrededor del 10%, como se podr\u00eda esperar de un modelo inicializado aleatoriamente (ya que tiene una probabilidad de 1 entre 10 de obtener una etiqueta correcta al adivinar al azar).</p> <p>Entrenemos el modelo durante cinco \u00e9pocas y observemos los resultados. Podemos utilizar una tasa de aprendizaje relativamente alta de 0,5.</p> In\u00a0[\u00a0]: Copied! <pre>history += fit(5, 0.5, model, train_loader, val_loader)\n</pre> history += fit(5, 0.5, model, train_loader, val_loader) <p>\u00a196% es bastante bueno! Entrenemos el modelo para cinco \u00e9pocas m\u00e1s a una tasa de aprendizaje m\u00e1s baja de 0,1 para mejorar a\u00fan m\u00e1s la precisi\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>history += fit(5, 0.1, model, train_loader, val_loader)\n</pre> history += fit(5, 0.1, model, train_loader, val_loader) <p>Ahora podemos trazar las p\u00e9rdidas y las precisiones para estudiar c\u00f3mo mejora el modelo con el tiempo.</p> In\u00a0[\u00a0]: Copied! <pre>losses = [x['val_loss'] for x in history]\nplt.plot(losses, '-x')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Loss vs. No. of epochs');\n</pre> losses = [x['val_loss'] for x in history] plt.plot(losses, '-x') plt.xlabel('epoch') plt.ylabel('loss') plt.title('Loss vs. No. of epochs'); In\u00a0[\u00a0]: Copied! <pre>accuracies = [x['val_acc'] for x in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');\n</pre> accuracies = [x['val_acc'] for x in history] plt.plot(accuracies, '-x') plt.xlabel('epoch') plt.ylabel('accuracy') plt.title('Accuracy vs. No. of epochs'); <p>\u00a1Nuestro modelo actual supera al modelo de regresi\u00f3n log\u00edstica (que solo pudo lograr alrededor del 86% de precisi\u00f3n) por un margen considerable! Alcanza r\u00e1pidamente una precisi\u00f3n del 97%, pero no mejora mucho m\u00e1s all\u00e1 de esto. Para mejorar a\u00fan m\u00e1s la precisi\u00f3n, necesitamos hacer que el modelo sea m\u00e1s potente aumentando el tama\u00f1o de la capa oculta o agregando m\u00e1s capas ocultas con activaciones. Le animo a que pruebe ambos enfoques y vea cu\u00e1l funciona mejor.</p> <p>Como paso final, podemos guardar y confirmar nuestro trabajo usando la biblioteca \"joviana\".</p> In\u00a0[\u00a0]: Copied! <pre>!pip install jovian --upgrade -q\n</pre> !pip install jovian --upgrade -q In\u00a0[\u00a0]: Copied! <pre>import jovian\n</pre> import jovian In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project='04-feedforward-nn', environment=None)\n</pre> jovian.commit(project='04-feedforward-nn', environment=None) In\u00a0[\u00a0]: Copied! <pre># Definir conjunto de datos de prueba\ntest_dataset = MNIST(root='data/', \n                     train=False,\n                     transform=ToTensor())\n</pre> # Definir conjunto de datos de prueba test_dataset = MNIST(root='data/',                       train=False,                      transform=ToTensor()) <p>Definamos una funci\u00f3n auxiliar <code>predict_image</code>, que devuelve la etiqueta predicha para un tensor de imagen \u00fanico.</p> In\u00a0[\u00a0]: Copied! <pre>def predict_image(img, model):\n    xb = to_device(img.unsqueeze(0), device)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()\n</pre> def predict_image(img, model):     xb = to_device(img.unsqueeze(0), device)     yb = model(xb)     _, preds  = torch.max(yb, dim=1)     return preds[0].item() <p>Prob\u00e9moslo con algunas im\u00e1genes.</p> In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[0]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[0] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[1839]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[1839] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[193]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', label, ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[193] plt.imshow(img[0], cmap='gray') print('Label:', label, ', Predicted:', predict_image(img, model)) <p>Identificar d\u00f3nde nuestro modelo funciona mal puede ayudarnos a mejorarlo, recopilando m\u00e1s datos de entrenamiento, aumentando/disminuyendo la complejidad del modelo y cambiando los hiperpar\u00e1metros.</p> <p>Como paso final, veamos tambi\u00e9n la p\u00e9rdida general y la precisi\u00f3n del modelo en el conjunto de prueba.</p> In\u00a0[\u00a0]: Copied! <pre>test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=256), device)\nresult = evaluate(model, test_loader)\nresult\n</pre> test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=256), device) result = evaluate(model, test_loader) result <p>Esperamos que esto sea similar a la precisi\u00f3n/p\u00e9rdida en el conjunto de validaci\u00f3n. De lo contrario, es posible que necesitemos un mejor conjunto de validaci\u00f3n que tenga datos y distribuci\u00f3n similares a los del conjunto de prueba (que a menudo proviene de datos del mundo real).</p> <p>Guardemos los pesos del modelo y adjunt\u00e9moslo al cuaderno usando <code>jovian.commit</code>. Tambi\u00e9n registraremos el rendimiento del modelo en el conjunto de datos de prueba usando <code>jovian.log_metrics</code>.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_loss'])\n</pre> jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_loss']) In\u00a0[\u00a0]: Copied! <pre>torch.save(model.state_dict(), 'mnist-feedforward.pth')\n</pre> torch.save(model.state_dict(), 'mnist-feedforward.pth') In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project='04-feedforward-nn', \n              environment=None, \n              outputs=['mnist-feedforward.pth'])\n</pre> jovian.commit(project='04-feedforward-nn',                environment=None,                outputs=['mnist-feedforward.pth'])"},{"location":"07-04-feedforward-nn/#entrenamiento-de-redes-neuronales-profundas-en-una-gpu-con-pytorch","title":"Entrenamiento de redes neuronales profundas en una GPU con PyTorch\u00b6","text":""},{"location":"07-04-feedforward-nn/#parte-4-de-aprendizaje-profundo-con-pytorch-de-cero-a-gan","title":"Parte 4 de \"Aprendizaje profundo con Pytorch: de cero a GAN\"\u00b6","text":"<p>Esta serie de tutoriales es una introducci\u00f3n pr\u00e1ctica y sencilla para principiantes al aprendizaje profundo utilizando PyTorch, una biblioteca de redes neuronales de c\u00f3digo abierto. Estos tutoriales adoptan un enfoque pr\u00e1ctico y centrado en la codificaci\u00f3n. La mejor manera de aprender el material es ejecutar el c\u00f3digo y experimentar con \u00e9l usted mismo. Mira la serie completa aqu\u00ed:</p> <ol> <li>[Conceptos b\u00e1sicos de PyTorch: tensores y degradados] (https://jovian.ai/aakashns/01-pytorch-basics)</li> <li>Descenso de gradiente y regresi\u00f3n lineal</li> <li>Trabajar con im\u00e1genes y regresi\u00f3n log\u00edstica</li> <li>Entrenamiento de redes neuronales profundas en una GPU</li> <li>[Clasificaci\u00f3n de im\u00e1genes mediante redes neuronales convolucionales] (https://jovian.ai/aakashns/05-cifar10-cnn)</li> <li>Aumento de datos, regularizaci\u00f3n y ResNets</li> <li>Generaci\u00f3n de im\u00e1genes mediante redes generativas adversarias</li> </ol>"},{"location":"07-04-feedforward-nn/#como-ejecutar-el-codigo","title":"C\u00f3mo ejecutar el c\u00f3digo\u00b6","text":"<p>Este tutorial es un ejecutable Jupyter notebook alojado en Jovian. Puede ejecutar este tutorial y experimentar con los ejemplos de c\u00f3digo de dos maneras: usando recursos gratuitos en l\u00ednea (recomendado) o en su computadora.</p>"},{"location":"07-04-feedforward-nn/#opcion-1-ejecutar-usando-recursos-en-linea-gratuitos-1-clic-recomendado","title":"Opci\u00f3n 1: Ejecutar usando recursos en l\u00ednea gratuitos (1 clic, recomendado)\u00b6","text":"<p>La forma m\u00e1s sencilla de comenzar a ejecutar el c\u00f3digo es hacer clic en el bot\u00f3n Ejecutar en la parte superior de esta p\u00e1gina y seleccionar Ejecutar en Colab. Google Colab es una plataforma en l\u00ednea gratuita para ejecutar port\u00e1tiles Jupyter utilizando la infraestructura en la nube de Google. Tambi\u00e9n puede seleccionar \"Ejecutar en Binder\" o \"Ejecutar en Kaggle\" si tiene problemas al ejecutar el cuaderno en Google Colab.</p>"},{"location":"07-04-feedforward-nn/#opcion-2-ejecutar-en-su-computadora-localmente","title":"Opci\u00f3n 2: ejecutar en su computadora localmente\u00b6","text":"<p>Para ejecutar el c\u00f3digo en su computadora localmente, deber\u00e1 configurar Python, descargar el cuaderno e instalar las bibliotecas necesarias. Recomendamos utilizar la distribuci\u00f3n Conda de Python. Haga clic en el bot\u00f3n Ejecutar en la parte superior de esta p\u00e1gina, seleccione la opci\u00f3n Ejecutar localmente y siga las instrucciones.</p> <p>Jupyter Notebooks: este tutorial es un Jupyter notebook: un documento compuesto de celdas. Cada celda puede contener c\u00f3digo escrito en Python o explicaciones en ingl\u00e9s sencillo. Puede ejecutar celdas de c\u00f3digo y ver los resultados, por ejemplo, n\u00fameros, mensajes, gr\u00e1ficos, tablas, archivos, etc., instant\u00e1neamente dentro del cuaderno. Jupyter es una poderosa plataforma para la experimentaci\u00f3n y el an\u00e1lisis. No tengas miedo de trastear con el c\u00f3digo y romper cosas: aprender\u00e1s mucho encontrando y corrigiendo errores. Puede utilizar la opci\u00f3n de men\u00fa \"Kernel &gt; Reiniciar y borrar salida\" o \"Editar &gt; Borrar salidas\" para borrar todas las salidas y comenzar de nuevo desde arriba.</p>"},{"location":"07-04-feedforward-nn/#usando-una-gpu-para-un-entrenamiento-mas-rapido","title":"Usando una GPU para un entrenamiento m\u00e1s r\u00e1pido\u00b6","text":"<p>Puede utilizar una Unidad de procesamiento de gr\u00e1ficos (GPU) para entrenar sus modelos m\u00e1s r\u00e1pido si su plataforma de ejecuci\u00f3n est\u00e1 conectada a una GPU fabricada por NVIDIA. Siga estas instrucciones para usar una GPU en la plataforma de su elecci\u00f3n:</p> <ul> <li>Google Colab: utilice la opci\u00f3n de men\u00fa \"Tiempo de ejecuci\u00f3n &gt; Cambiar tipo de tiempo de ejecuci\u00f3n\" y seleccione \"GPU\" en el men\u00fa desplegable \"Acelerador de hardware\".</li> <li>Kaggle: En la secci\u00f3n \"Configuraci\u00f3n\" de la barra lateral, seleccione \"GPU\" en el men\u00fa desplegable \"Acelerador\". Utilice el bot\u00f3n en la parte superior derecha para abrir la barra lateral.</li> <li>Binder: Las computadoras port\u00e1tiles que ejecutan Binder no pueden usar una GPU, ya que las m\u00e1quinas que alimentan Binder no est\u00e1n conectadas a ninguna GPU.</li> <li>Linux: Si su computadora port\u00e1til/escritorio tiene una GPU (tarjeta gr\u00e1fica) NVIDIA, aseg\u00farese de haber instalado los [controladores NVIDIA CUDA] (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index .html).</li> <li>Windows: si su computadora port\u00e1til/escritorio tiene una GPU (tarjeta gr\u00e1fica) NVIDIA, aseg\u00farese de haber instalado los [controladores NVIDIA CUDA] (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows /index.html).</li> <li>macOS: macOS no es compatible con las GPU NVIDIA</li> </ul> <p>Si no tiene acceso a una GPU o no est\u00e1 seguro de cu\u00e1l es, no se preocupe, puede ejecutar todo el c\u00f3digo de este tutorial sin una GPU.</p>"},{"location":"07-04-feedforward-nn/#preparando-los-datos","title":"Preparando los datos\u00b6","text":"<p>En [el tutorial anterior] (https://jovian.ai/aakashns/03-logistic-regression), entrenamos un modelo de regresi\u00f3n log\u00edstica para identificar d\u00edgitos escritos a mano del conjunto de datos MNIST con una precisi\u00f3n de alrededor del 86%. El conjunto de datos consta de im\u00e1genes en escala de grises de 28 px por 28 px de d\u00edgitos escritos a mano (0 a 9) y etiquetas para cada imagen que indican qu\u00e9 d\u00edgito representa. Aqu\u00ed hay algunas im\u00e1genes de muestra del conjunto de datos:</p> <p></p> <p>Notamos que es bastante dif\u00edcil mejorar la precisi\u00f3n de un modelo de regresi\u00f3n log\u00edstica m\u00e1s all\u00e1 del 87%, ya que el modelo supone una relaci\u00f3n lineal entre las intensidades de los p\u00edxeles y las etiquetas de las im\u00e1genes. En esta publicaci\u00f3n, intentaremos mejorarlo utilizando una red neuronal de retroalimentaci\u00f3n que puede capturar relaciones no lineales entre entradas y objetivos.</p> <p>Comencemos instalando e importando los m\u00f3dulos y clases necesarios de <code>torch</code>, <code>torchvision</code>, <code>numpy</code> y <code>matplotlib</code>.</p>"},{"location":"07-04-feedforward-nn/#capas-ocultas-funciones-de-activacion-y-no-linealidad","title":"Capas ocultas, funciones de activaci\u00f3n y no linealidad\u00b6","text":"<p>Crearemos una red neuronal con dos capas: una capa oculta y una capa de salida. Adem\u00e1s, usaremos una funci\u00f3n de activaci\u00f3n entre las dos capas. Veamos un ejemplo paso a paso para aprender c\u00f3mo las capas ocultas y las funciones de activaci\u00f3n pueden ayudar a capturar relaciones no lineales entre entradas y salidas.</p> <p>Primero, creemos un lote de tensores de entrada. Aplanaremos las im\u00e1genes <code>1x28x28</code> en vectores de tama\u00f1o <code>784</code>, para que puedan pasarse a un objeto <code>nn.Linear</code>.</p>"},{"location":"07-04-feedforward-nn/#guarda-y-sube-tu-libreta","title":"Guarda y sube tu libreta\u00b6","text":"<p>Ya sea que est\u00e9 ejecutando este cuaderno Jupyter en l\u00ednea o en su computadora, es esencial guardar su trabajo de vez en cuando. Puede continuar trabajando en un cuaderno guardado m\u00e1s tarde o compartirlo con amigos y colegas para permitirles ejecutar su c\u00f3digo. Jovian ofrece una forma sencilla de guardar y compartir sus cuadernos de Jupyter en l\u00ednea.</p>"},{"location":"07-04-feedforward-nn/#modelo","title":"Modelo\u00b6","text":"<p>Ahora estamos listos para definir nuestro modelo. Como se mencion\u00f3 anteriormente, crearemos una red neuronal con una capa oculta. Esto es lo que eso significa:</p> <ul> <li><p>En lugar de usar un solo objeto <code>nn.Linear</code> para transformar un lote de entradas (intensidades de p\u00edxeles) en salidas (probabilidades de clase), usaremos dos objetos <code>nn.Linear</code>. Cada uno de estos se denomina capa en la red.</p> </li> <li><p>La primera capa (tambi\u00e9n conocida como capa oculta) transformar\u00e1 la matriz de entrada de la forma <code>batch_size x 784</code> en una matriz de salida intermedia de la forma <code>batch_size x hide_size</code>. El par\u00e1metro <code>hidden_size</code> se puede configurar manualmente (por ejemplo, 32 o 64).</p> </li> <li><p>Luego aplicaremos una funci\u00f3n de activaci\u00f3n no lineal a las salidas intermedias. La funci\u00f3n de activaci\u00f3n transforma elementos individuales de la matriz.</p> </li> <li><p>El resultado de la funci\u00f3n de activaci\u00f3n, que tambi\u00e9n es de tama\u00f1o <code>batch_size x hide_size</code>, se pasa a la segunda capa (tambi\u00e9n conocida como capa de salida).  La segunda capa lo transforma en una matriz de tama\u00f1o <code>batch_size x 10</code>. Podemos usar este resultado para calcular la p\u00e9rdida y ajustar los pesos mediante el descenso de gradiente.</p> </li> </ul> <p>Como se mencion\u00f3 anteriormente, nuestro modelo contendr\u00e1 una capa oculta. As\u00ed es como se ve visualmente:</p> <p>Definamos el modelo extendiendo la clase <code>nn.Module</code> de PyTorch.</p>"},{"location":"07-04-feedforward-nn/#usando-una-gpu","title":"Usando una GPU\u00b6","text":"<p>A medida que aumentan los tama\u00f1os de nuestros modelos y conjuntos de datos, necesitamos usar GPU para entrenar nuestros modelos en un per\u00edodo de tiempo razonable. Las GPU contienen cientos de n\u00facleos optimizados para realizar costosas operaciones matriciales en n\u00fameros de punto flotante r\u00e1pidamente, lo que las hace ideales para entrenar redes neuronales profundas. Puede utilizar GPU de forma gratuita en Google Colab y Kaggle o alquilar m\u00e1quinas con GPU en servicios como Google Cloud Platform, Amazon Web Services, y Paperspace.</p> <p>Podemos comprobar si hay una GPU disponible y si los controladores NVIDIA CUDA necesarios est\u00e1n instalados usando <code>torch.cuda.is_available</code>.</p>"},{"location":"07-04-feedforward-nn/#entrenando-el-modelo","title":"Entrenando el modelo\u00b6","text":"<p>Definiremos dos funciones: \"ajustar\" y \"evaluar\" para entrenar el modelo usando el descenso de gradiente y evaluar su desempe\u00f1o en el conjunto de validaci\u00f3n. Para obtener un tutorial detallado de estas funciones, consulte el tutorial anterior.</p>"},{"location":"07-04-feedforward-nn/#pruebas-con-imagenes-individuales","title":"Pruebas con im\u00e1genes individuales\u00b6","text":"<p>Si bien hasta ahora hemos estado rastreando la precisi\u00f3n general de un modelo, tambi\u00e9n es una buena idea observar los resultados del modelo en algunas im\u00e1genes de muestra. Probemos nuestro modelo con algunas im\u00e1genes del conjunto de datos de prueba predefinido de 10000 im\u00e1genes. Comenzamos recreando el conjunto de datos de prueba con la transformaci\u00f3n \"ToTensor\".</p>"},{"location":"07-04-feedforward-nn/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Pruebe los siguientes ejercicios para aplicar los conceptos y t\u00e9cnicas que ha aprendido hasta ahora:</p> <ul> <li>Ejercicios de codificaci\u00f3n sobre entrenamiento de modelos de un extremo a otro: https://jovian.ai/aakashns/03-cifar10-feedforward</li> <li>Cuaderno de inicio para modelos de aprendizaje profundo: https://jovian.ai/aakashns/fashion-feedforward-minimal</li> </ul> <p>Entrenar excelentes modelos de aprendizaje autom\u00e1tico de manera confiable requiere pr\u00e1ctica y experiencia. Intente experimentar con diferentes conjuntos de datos, modelos e hiperpar\u00e1metros, es la mejor manera de adquirir esta habilidad.</p>"},{"location":"07-04-feedforward-nn/#resumen-y-lecturas-adicionales","title":"Resumen y lecturas adicionales\u00b6","text":"<p>Aqu\u00ed hay un resumen de los temas cubiertos en este tutorial:</p> <ul> <li><p>Creamos una red neuronal con una capa oculta para mejorar el modelo de regresi\u00f3n log\u00edstica del tutorial anterior. Tambi\u00e9n utilizamos la funci\u00f3n de activaci\u00f3n ReLU para introducir no linealidad en el modelo, permiti\u00e9ndole aprender relaciones m\u00e1s complejas entre las entradas (densidades de p\u00edxeles) y las salidas (probabilidades de clase).</p> </li> <li><p>Definimos algunas utilidades como <code>get_default_device</code>, <code>to_device</code> y <code>DeviceDataLoader</code> para aprovechar una GPU si est\u00e1 disponible, moviendo los datos de entrada y los par\u00e1metros del modelo al dispositivo apropiado.</p> </li> <li><p>Pudimos usar exactamente el mismo ciclo de entrenamiento: la funci\u00f3n \"ajuste\" que hab\u00edamos definido anteriormente para entrenar el modelo y evaluarlo usando el conjunto de datos de validaci\u00f3n.</p> </li> </ul> <p>Hay muchas posibilidades para experimentar aqu\u00ed y le recomiendo que utilice la naturaleza interactiva de Jupyter para jugar con los distintos par\u00e1metros. Aqui hay algunas ideas:</p> <ul> <li><p>Intente cambiar el tama\u00f1o de la capa oculta o agregue m\u00e1s capas ocultas y vea si puede lograr una mayor precisi\u00f3n.</p> </li> <li><p>Intente cambiar el tama\u00f1o del lote y la tasa de aprendizaje para ver si puede lograr la misma precisi\u00f3n en menos \u00e9pocas.</p> </li> <li><p>Compare los tiempos de entrenamiento en una CPU frente a una GPU. \u00bfVes una diferencia significativa? \u00bfC\u00f3mo var\u00eda con el tama\u00f1o del conjunto de datos y el tama\u00f1o del modelo (n\u00famero de pesos y par\u00e1metros)?</p> </li> <li><p>Intente crear un modelo para un conjunto de datos diferente, como los conjuntos de datos CIFAR10 o CIFAR100.</p> </li> </ul> <p>Aqu\u00ed hay algunas referencias para lectura adicional:</p> <ul> <li><p>[Una prueba visual de que las redes neuronales pueden calcular cualquier funci\u00f3n] (http://neuralnetworksanddeeplearning.com/chap4.html), tambi\u00e9n conocido como teorema de aproximaci\u00f3n universal.</p> </li> <li><p>Pero \u00bfqu\u00e9 es una red neuronal? - Una introducci\u00f3n visual e intuitiva a qu\u00e9 son las redes neuronales y qu\u00e9 representan las capas intermedias</p> </li> <li><p>Notas de la conferencia Stanford CS229 sobre retropropagaci\u00f3n - para un tratamiento m\u00e1s matem\u00e1tico de c\u00f3mo se calculan los gradientes y se actualizan los pesos para Redes neuronales con m\u00faltiples capas.</p> </li> </ul> <p>Ahora est\u00e1 listo para pasar al siguiente tutorial: [Clasificaci\u00f3n de im\u00e1genes mediante redes neuronales convolucionales] (https://jovian.ai/aakashns/05-cifar10-cnn).</p>"},{"location":"08-05-cifar10-cnn/","title":"Clasificaci\u00f3n de im\u00e1genes utilizando redes neuronales convolucionales en PyTorch","text":"<p>Este tutorial cubre los siguientes temas:</p> <ul> <li>Descarga de un conjunto de datos de im\u00e1genes desde la URL web</li> <li>Comprensi\u00f3n de las capas de convoluci\u00f3n y agrupaci\u00f3n.</li> <li>Creaci\u00f3n de una red neuronal convolucional (CNN) usando PyTorch</li> <li>Entrenamiento de una CNN desde cero y seguimiento del rendimiento.</li> <li>Underfitting, overfitting y c\u00f3mo superarlos</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Descomente y ejecute el comando apropiado para su sistema operativo, si es necesario\n\n# Linux/Binder/Windows (sin GPU)\n# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n# Linux/Windows (GPU)\n# pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n \n# MacOS (NO GPU)\n# !pip instalar numpy matplotlib antorcha torchvision torchaudio\n</pre> # Descomente y ejecute el comando apropiado para su sistema operativo, si es necesario  # Linux/Binder/Windows (sin GPU) # !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html  # Linux/Windows (GPU) # pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html   # MacOS (NO GPU) # !pip instalar numpy matplotlib antorcha torchvision torchaudio In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nimport torchvision\nimport tarfile\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split\n</pre> import os import torch import torchvision import tarfile from torchvision.datasets.utils import download_url from torch.utils.data import random_split In\u00a0[\u00a0]: Copied! <pre>project_name='05-cifar10-cnn'\n</pre> project_name='05-cifar10-cnn' <p>Descargaremos las im\u00e1genes en formato PNG desde esta p\u00e1gina, usando algunas funciones auxiliares de los paquetes <code>torchvision</code> y <code>tarfile</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Descargar el conjunto de datos\ndataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\ndownload_url(dataset_url, '.')\n</pre> # Descargar el conjunto de datos dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\" download_url(dataset_url, '.') In\u00a0[\u00a0]: Copied! <pre># Extraer del archivo\nwith tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n    tar.extractall(path='./data')\n</pre> # Extraer del archivo with tarfile.open('./cifar10.tgz', 'r:gz') as tar:     tar.extractall(path='./data') <p>El conjunto de datos se extrae al directorio <code>data/cifar10</code>. Contiene 2 carpetas \"train\" y \"test\", que contienen el conjunto de entrenamiento (50000 im\u00e1genes) y el conjunto de prueba (10000 im\u00e1genes) respectivamente. Cada uno de ellos contiene 10 carpetas, una para cada clase de im\u00e1genes. Verifiquemos esto usando <code>os.listdir</code>.</p> In\u00a0[\u00a0]: Copied! <pre>data_dir = './data/cifar10'\n\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"/train\")\nprint(classes)\n</pre> data_dir = './data/cifar10'  print(os.listdir(data_dir)) classes = os.listdir(data_dir + \"/train\") print(classes) <p>Miremos dentro de un par de carpetas, una del conjunto de entrenamiento y otra del conjunto de prueba. Como ejercicio, puedes comprobar que hay el mismo n\u00famero de im\u00e1genes para cada clase, 5000 en el conjunto de entrenamiento y 1000 en el conjunto de prueba.</p> In\u00a0[\u00a0]: Copied! <pre>airplane_files = os.listdir(data_dir + \"/train/airplane\")\nprint('No. of training examples for airplanes:', len(airplane_files))\nprint(airplane_files[:5])\n</pre> airplane_files = os.listdir(data_dir + \"/train/airplane\") print('No. of training examples for airplanes:', len(airplane_files)) print(airplane_files[:5]) In\u00a0[\u00a0]: Copied! <pre>ship_test_files = os.listdir(data_dir + \"/test/ship\")\nprint(\"No. of test examples for ship:\", len(ship_test_files))\nprint(ship_test_files[:5])\n</pre> ship_test_files = os.listdir(data_dir + \"/test/ship\") print(\"No. of test examples for ship:\", len(ship_test_files)) print(ship_test_files[:5]) <p>Muchos conjuntos de datos de visi\u00f3n por computadora utilizan la estructura de directorios anterior (una carpeta por clase), y la mayor\u00eda de las bibliotecas de aprendizaje profundo proporcionan utilidades para trabajar con dichos conjuntos de datos. Podemos usar la clase <code>ImageFolder</code> de <code>torchvision</code> para cargar los datos como tensores de PyTorch.</p> In\u00a0[\u00a0]: Copied! <pre>from torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\n</pre> from torchvision.datasets import ImageFolder from torchvision.transforms import ToTensor In\u00a0[\u00a0]: Copied! <pre>dataset = ImageFolder(data_dir+'/train', transform=ToTensor())\n</pre> dataset = ImageFolder(data_dir+'/train', transform=ToTensor()) <p>Veamos un elemento de muestra del conjunto de datos de entrenamiento. Cada elemento es una tupla que contiene un tensor de imagen y una etiqueta. Dado que los datos constan de im\u00e1genes en color de 32x32 px con 3 canales (RGB), cada tensor de imagen tiene la forma \"(3, 32, 32)\".</p> In\u00a0[\u00a0]: Copied! <pre>img, label = dataset[0]\nprint(img.shape, label)\nimg\n</pre> img, label = dataset[0] print(img.shape, label) img <p>La lista de clases se almacena en la propiedad <code>.classes</code> del conjunto de datos. La etiqueta num\u00e9rica de cada elemento corresponde al \u00edndice de la etiqueta del elemento en la lista de clases.</p> In\u00a0[\u00a0]: Copied! <pre>print(dataset.classes)\n</pre> print(dataset.classes) <p>Podemos ver la imagen usando <code>matplotlib</code>, pero necesitamos cambiar las dimensiones del tensor a <code>(32,32,3)</code>. Creemos una funci\u00f3n auxiliar para mostrar una imagen y su etiqueta.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'\n</pre> import matplotlib import matplotlib.pyplot as plt %matplotlib inline  matplotlib.rcParams['figure.facecolor'] = '#ffffff' In\u00a0[\u00a0]: Copied! <pre>def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))\n</pre>  def show_example(img, label):     print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")     plt.imshow(img.permute(1, 2, 0)) <p>Veamos un par de im\u00e1genes del conjunto de datos. Como puedes ver, las im\u00e1genes de 32x32px son bastante dif\u00edciles de identificar, incluso para el ojo humano. Intente cambiar los \u00edndices a continuaci\u00f3n para ver im\u00e1genes diferentes.</p> In\u00a0[\u00a0]: Copied! <pre>show_example(*dataset[0])\n</pre> show_example(*dataset[0]) In\u00a0[\u00a0]: Copied! <pre>show_example(*dataset[1099])\n</pre> show_example(*dataset[1099]) In\u00a0[\u00a0]: Copied! <pre>!pip install jovian --upgrade -q\n</pre> !pip install jovian --upgrade -q In\u00a0[\u00a0]: Copied! <pre>import jovian\n</pre> import jovian In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project=project_name)\n</pre> jovian.commit(project=project_name) <p><code>jovian.commit</code> carga el cuaderno en su cuenta Jovian, captura el entorno Python y crea un enlace para compartir para su cuaderno, como se muestra arriba. Puede utilizar este enlace para compartir su trabajo y permitir que cualquiera (incluido usted) ejecute sus cuadernos y reproduzca su trabajo.</p> In\u00a0[\u00a0]: Copied! <pre>random_seed = 42\ntorch.manual_seed(random_seed);\n</pre> random_seed = 42 torch.manual_seed(random_seed); In\u00a0[\u00a0]: Copied! <pre>val_size = 5000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)\n</pre> val_size = 5000 train_size = len(dataset) - val_size  train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) <p>La biblioteca \"jovian\" tambi\u00e9n proporciona una API sencilla para registrar par\u00e1metros importantes relacionados con el conjunto de datos, el entrenamiento del modelo, los resultados, etc. para facilitar la referencia y comparaci\u00f3n entre m\u00faltiples experimentos. Registremos <code>dataset_url</code>, <code>val_pct</code> y <code>rand_seed</code> usando <code>jovian.log_dataset</code>.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)\n</pre> jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed) <p>Ahora podemos crear cargadores de datos para entrenamiento y validaci\u00f3n, para cargar los datos en lotes.</p> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data.dataloader import DataLoader\n\nbatch_size=128\n</pre> from torch.utils.data.dataloader import DataLoader  batch_size=128 In\u00a0[\u00a0]: Copied! <pre>train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n</pre> train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True) <p>Podemos ver lotes de im\u00e1genes del conjunto de datos usando el m\u00e9todo <code>make_grid</code> de <code>torchvision</code>. Cada vez que se ejecuta el siguiente c\u00f3digo, obtenemos un bach diferente, ya que el muestreador mezcla los \u00edndices antes de crear lotes.</p> In\u00a0[\u00a0]: Copied! <pre>from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break\n</pre> from torchvision.utils import make_grid  def show_batch(dl):     for images, labels in dl:         fig, ax = plt.subplots(figsize=(12, 6))         ax.set_xticks([]); ax.set_yticks([])         ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))         break In\u00a0[\u00a0]: Copied! <pre>show_batch(train_dl)\n</pre> show_batch(train_dl) <p>Una vez m\u00e1s, guardemos y confirmemos nuestro trabajo usando \"jovian\" antes de continuar.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project=project_name, environment=None)\n</pre> jovian.commit(project=project_name, environment=None) <p>Despu\u00e9s de la primera confirmaci\u00f3n, todas las confirmaciones posteriores registran una nueva versi\u00f3n del cuaderno dentro del mismo proyecto joviano. Puede usar <code>jovian.commit</code> para versionar los cuadernos de Jupyter (en lugar de hacer <code>Archivo &gt; Guardar como</code>) y mantener organizados sus proyectos de ciencia de datos. Consulte tambi\u00e9n la pesta\u00f1a Records en la p\u00e1gina del proyecto para ver c\u00f3mo se registra la informaci\u00f3n usando <code>jovian.log_dataset </code>aparece en la interfaz de usuario.</p> <p>&gt;a&gt;</p> In\u00a0[\u00a0]: Copied! <pre>def apply_kernel(image, kernel):\n    ri, ci = image.shape       # image dimensions\n    rk, ck = kernel.shape      # kernel dimensions\n    ro, co = ri-rk+1, ci-ck+1  # output dimensions\n    output = torch.zeros([ro, co])\n    for i in range(ro): \n        for j in range(co):\n            output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel)\n    return output\n</pre> def apply_kernel(image, kernel):     ri, ci = image.shape       # image dimensions     rk, ck = kernel.shape      # kernel dimensions     ro, co = ri-rk+1, ci-ck+1  # output dimensions     output = torch.zeros([ro, co])     for i in range(ro):          for j in range(co):             output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel)     return output In\u00a0[\u00a0]: Copied! <pre>sample_image = torch.tensor([\n    [3, 3, 2, 1, 0], \n    [0, 0, 1, 3, 1], \n    [3, 1, 2, 2, 3], \n    [2, 0, 0, 2, 2], \n    [2, 0, 0, 0, 1]\n], dtype=torch.float32)\n\nsample_kernel = torch.tensor([\n    [0, 1, 2], \n    [2, 2, 0], \n    [0, 1, 2]\n], dtype=torch.float32)\n\napply_kernel(sample_image, sample_kernel)\n</pre> sample_image = torch.tensor([     [3, 3, 2, 1, 0],      [0, 0, 1, 3, 1],      [3, 1, 2, 2, 3],      [2, 0, 0, 2, 2],      [2, 0, 0, 0, 1] ], dtype=torch.float32)  sample_kernel = torch.tensor([     [0, 1, 2],      [2, 2, 0],      [0, 1, 2] ], dtype=torch.float32)  apply_kernel(sample_image, sample_kernel) <p>Para im\u00e1genes multicanal, se aplica un n\u00facleo diferente a cada canal y las salidas se suman por p\u00edxeles.</p> <p>Consulte los siguientes art\u00edculos para comprender mejor las convoluciones:</p> <ol> <li>[Comprensi\u00f3n intuitiva de las convoluciones para el aprendizaje profundo] (https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) por Irhum Shafkat</li> <li>[Convoluciones en profundidad] (https://sgugger.github.io/convolution-in- Depth.html) de Sylvian Gugger (este art\u00edculo implementa convoluciones desde cero)</li> </ol> <p>Existen ciertas ventajas que ofrecen las capas convolucionales cuando se trabaja con datos de im\u00e1genes:</p> <ul> <li>Menos par\u00e1metros: se utiliza un peque\u00f1o conjunto de par\u00e1metros (el n\u00facleo) para calcular los resultados de toda la imagen, por lo que el modelo tiene muchos menos par\u00e1metros en comparaci\u00f3n con una capa completamente conectada.</li> <li>Escasez de conexiones: en cada capa, cada elemento de salida solo depende de una peque\u00f1a cantidad de elementos de entrada, lo que hace que los pases hacia adelante y hacia atr\u00e1s sean m\u00e1s eficientes.</li> <li>Compartici\u00f3n de par\u00e1metros e invariancia espacial: las caracter\u00edsticas aprendidas por un n\u00facleo en una parte de la imagen se pueden usar para detectar patrones similares en una parte diferente de otra imagen.</li> </ul> <p>Tambi\u00e9n usaremos capas de max-pooling para disminuir progresivamente la altura y el ancho de los tensores de salida de cada capa convolucional.</p> <p>Antes de definir el modelo completo, veamos c\u00f3mo opera en los datos una \u00fanica capa convolucional seguida de una capa de agrupaci\u00f3n m\u00e1xima.</p> In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\nimport torch.nn.functional as F\n</pre> import torch.nn as nn import torch.nn.functional as F In\u00a0[\u00a0]: Copied! <pre>simple_model = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n    nn.MaxPool2d(2, 2)\n)\n</pre> simple_model = nn.Sequential(     nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),     nn.MaxPool2d(2, 2) ) <p>Consulte la [publicaci\u00f3n de Sylvian] (https://sgugger.github.io/convolution-in- Depth.html) para obtener una explicaci\u00f3n de <code>kernel_size</code>, <code>stride</code> y <code>padding</code>.</p> In\u00a0[\u00a0]: Copied! <pre>for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = simple_model(images)\n    print('out.shape:', out.shape)\n    break\n</pre> for images, labels in train_dl:     print('images.shape:', images.shape)     out = simple_model(images)     print('out.shape:', out.shape)     break <p>La capa <code>Conv2d</code> transforma una imagen de 3 canales en un mapa de caracter\u00edsticas de 16 canales, y la capa <code>MaxPool2d</code> reduce a la mitad la altura y el ancho. El mapa de caracter\u00edsticas se hace m\u00e1s peque\u00f1o a medida que agregamos m\u00e1s capas, hasta que finalmente nos queda un mapa de caracter\u00edsticas peque\u00f1o, que se puede aplanar en un vector. Luego podemos agregar algunas capas completamente conectadas al final para obtener un vector de tama\u00f1o 10 para cada imagen.</p> <p>Definamos el modelo extendiendo una clase <code>ImageClassificationBase</code> que contiene m\u00e9todos auxiliares para entrenamiento y validaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n        \ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n</pre> class ImageClassificationBase(nn.Module):     def training_step(self, batch):         images, labels = batch          out = self(images)                  # Generate predictions         loss = F.cross_entropy(out, labels) # Calculate loss         return loss          def validation_step(self, batch):         images, labels = batch          out = self(images)                    # Generate predictions         loss = F.cross_entropy(out, labels)   # Calculate loss         acc = accuracy(out, labels)           # Calculate accuracy         return {'val_loss': loss.detach(), 'val_acc': acc}              def validation_epoch_end(self, outputs):         batch_losses = [x['val_loss'] for x in outputs]         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses         batch_accs = [x['val_acc'] for x in outputs]         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}          def epoch_end(self, epoch, result):         print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(             epoch, result['train_loss'], result['val_loss'], result['val_acc']))          def accuracy(outputs, labels):     _, preds = torch.max(outputs, dim=1)     return torch.tensor(torch.sum(preds == labels).item() / len(preds)) <p>Usaremos <code>nn.Sequential</code> para encadenar las capas y funciones de activaci\u00f3n en una \u00fanica arquitectura de red.</p> In\u00a0[\u00a0]: Copied! <pre>class Cifar10CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(256*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10))\n        \n    def forward(self, xb):\n        return self.network(xb)\n</pre> class Cifar10CnnModel(ImageClassificationBase):     def __init__(self):         super().__init__()         self.network = nn.Sequential(             nn.Conv2d(3, 32, kernel_size=3, padding=1),             nn.ReLU(),             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),             nn.ReLU(),             nn.MaxPool2d(2, 2), # output: 64 x 16 x 16              nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),             nn.ReLU(),             nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),             nn.ReLU(),             nn.MaxPool2d(2, 2), # output: 128 x 8 x 8              nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),             nn.ReLU(),             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),             nn.ReLU(),             nn.MaxPool2d(2, 2), # output: 256 x 4 x 4              nn.Flatten(),              nn.Linear(256*4*4, 1024),             nn.ReLU(),             nn.Linear(1024, 512),             nn.ReLU(),             nn.Linear(512, 10))              def forward(self, xb):         return self.network(xb) In\u00a0[\u00a0]: Copied! <pre>model = Cifar10CnnModel()\nmodel\n</pre> model = Cifar10CnnModel() model <p>Verifiquemos que el modelo produzca el resultado esperado en un lote de datos de entrenamiento. Las 10 salidas para cada imagen se pueden interpretar como probabilidades para las 10 clases objetivo (despu\u00e9s de aplicar softmax), y la clase con la probabilidad m\u00e1s alta se elige como la etiqueta predicha por el modelo para la imagen de entrada. Consulte la Parte 3 (regresi\u00f3n log\u00edstica) para obtener una discusi\u00f3n m\u00e1s detallada sobre c\u00f3mo interpretar los resultados, aplicar softmax e identificar las etiquetas predichas.</p> In\u00a0[\u00a0]: Copied! <pre>for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break\n</pre> for images, labels in train_dl:     print('images.shape:', images.shape)     out = model(images)     print('out.shape:', out.shape)     print('out[0]:', out[0])     break <p>Para usar sin problemas una GPU, si hay una disponible, definimos un par de funciones auxiliares (<code>get_default_device</code> y <code>to_device</code>) y una clase auxiliar <code>DeviceDataLoader</code> para mover nuestro modelo y datos a la GPU seg\u00fan sea necesario. Estos se describen con m\u00e1s detalle en el tutorial anterior.</p> In\u00a0[\u00a0]: Copied! <pre>def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n</pre> def get_default_device():     \"\"\"Pick GPU if available, else CPU\"\"\"     if torch.cuda.is_available():         return torch.device('cuda')     else:         return torch.device('cpu')      def to_device(data, device):     \"\"\"Move tensor(s) to chosen device\"\"\"     if isinstance(data, (list,tuple)):         return [to_device(x, device) for x in data]     return data.to(device, non_blocking=True)  class DeviceDataLoader():     \"\"\"Wrap a dataloader to move data to a device\"\"\"     def __init__(self, dl, device):         self.dl = dl         self.device = device              def __iter__(self):         \"\"\"Yield a batch of data after moving it to device\"\"\"         for b in self.dl:              yield to_device(b, self.device)      def __len__(self):         \"\"\"Number of batches\"\"\"         return len(self.dl) <p>Seg\u00fan d\u00f3nde est\u00e9 ejecutando esta computadora port\u00e1til, su dispositivo predeterminado podr\u00eda ser una CPU (<code>torch.device('cpu')</code>) o una GPU (<code>torch.device('cuda')</code>)</p> In\u00a0[\u00a0]: Copied! <pre>device = get_default_device()\ndevice\n</pre> device = get_default_device() device <p>Ahora podemos empaquetar nuestros cargadores de datos de entrenamiento y validaci\u00f3n usando <code>DeviceDataLoader</code> para transferir autom\u00e1ticamente lotes de datos a la GPU (si est\u00e1 disponible) y usar <code>to_device</code> para mover nuestro modelo a la GPU (si est\u00e1 disponible).</p> In\u00a0[\u00a0]: Copied! <pre>train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device);\n</pre> train_dl = DeviceDataLoader(train_dl, device) val_dl = DeviceDataLoader(val_dl, device) to_device(model, device); <p>Una vez m\u00e1s, guardemos y confirmemos el cuaderno antes de continuar.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project=project_name)\n</pre> jovian.commit(project=project_name) In\u00a0[\u00a0]: Copied! <pre>@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n</pre> @torch.no_grad() def evaluate(model, val_loader):     model.eval()     outputs = [model.validation_step(batch) for batch in val_loader]     return model.validation_epoch_end(outputs)  def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):     history = []     optimizer = opt_func(model.parameters(), lr)     for epoch in range(epochs):         # Training Phase          model.train()         train_losses = []         for batch in train_loader:             loss = model.training_step(batch)             train_losses.append(loss)             loss.backward()             optimizer.step()             optimizer.zero_grad()         # Validation phase         result = evaluate(model, val_loader)         result['train_loss'] = torch.stack(train_losses).mean().item()         model.epoch_end(epoch, result)         history.append(result)     return history <p>Antes de comenzar a entrenar, creemos una instancia del modelo una vez m\u00e1s y veamos c\u00f3mo se desempe\u00f1a en el conjunto de validaci\u00f3n con el conjunto inicial de par\u00e1metros.</p> In\u00a0[\u00a0]: Copied! <pre>model = to_device(Cifar10CnnModel(), device)\n</pre> model = to_device(Cifar10CnnModel(), device) In\u00a0[\u00a0]: Copied! <pre>evaluate(model, val_dl)\n</pre> evaluate(model, val_dl) <p>La precisi\u00f3n inicial es de alrededor del 10%, que es lo que uno podr\u00eda esperar de un modelo inicializado aleatoriamente (ya que tiene una probabilidad de 1 entre 10 de obtener una etiqueta correcta al adivinar al azar).</p> <p>Usaremos los siguientes hiperpar\u00e1metros (tasa de aprendizaje, n\u00famero de \u00e9pocas, tama\u00f1o de lote, etc.) para entrenar nuestro modelo. Como ejercicio, puedes intentar cambiarlos para ver si logras una mayor precisi\u00f3n en menos tiempo.</p> In\u00a0[\u00a0]: Copied! <pre>num_epochs = 10\nopt_func = torch.optim.Adam\nlr = 0.001\n</pre> num_epochs = 10 opt_func = torch.optim.Adam lr = 0.001 <p>Es importante registrar los hiperpar\u00e1metros de cada experimento que realice, para replicarlo m\u00e1s tarde y compararlo con otros experimentos. Podemos grabarlos usando <code>jovian.log_hyperparams</code>.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.reset()\njovian.log_hyperparams({\n    'num_epochs': num_epochs,\n    'opt_func': opt_func.__name__,\n    'batch_size': batch_size,\n    'lr': lr,\n})\n</pre> jovian.reset() jovian.log_hyperparams({     'num_epochs': num_epochs,     'opt_func': opt_func.__name__,     'batch_size': batch_size,     'lr': lr, }) In\u00a0[\u00a0]: Copied! <pre>history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n</pre> history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func) <p>As\u00ed como hemos registrado los hiperpar\u00e1metros, tambi\u00e9n podemos registrar las m\u00e9tricas finales logradas por el modelo usando <code>jovian.log_metrics</code> como referencia, an\u00e1lisis y comparaci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.log_metrics(train_loss=history[-1]['train_loss'], \n                   val_loss=history[-1]['val_loss'], \n                   val_acc=history[-1]['val_acc'])\n</pre> jovian.log_metrics(train_loss=history[-1]['train_loss'],                     val_loss=history[-1]['val_loss'],                     val_acc=history[-1]['val_acc']) <p>Tambi\u00e9n podemos trazar las precisiones del conjunto de validaci\u00f3n para estudiar c\u00f3mo mejora el modelo con el tiempo.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n</pre> def plot_accuracies(history):     accuracies = [x['val_acc'] for x in history]     plt.plot(accuracies, '-x')     plt.xlabel('epoch')     plt.ylabel('accuracy')     plt.title('Accuracy vs. No. of epochs'); In\u00a0[\u00a0]: Copied! <pre>plot_accuracies(history)\n</pre> plot_accuracies(history) <p>Nuestro modelo alcanza una precisi\u00f3n de alrededor del 75% y, al observar el gr\u00e1fico, parece poco probable que el modelo alcance una precisi\u00f3n superior al 80% incluso despu\u00e9s de un entrenamiento prolongado. Esto sugiere que es posible que necesitemos utilizar un modelo m\u00e1s potente para capturar la relaci\u00f3n entre las im\u00e1genes y las etiquetas con mayor precisi\u00f3n. Esto se puede hacer agregando m\u00e1s capas convolucionales a nuestro modelo o aumentando el n\u00famero. de canales en cada capa convolucional, o mediante el uso de t\u00e9cnicas de regularizaci\u00f3n.</p> <p>Tambi\u00e9n podemos trazar las p\u00e9rdidas de entrenamiento y validaci\u00f3n para estudiar la tendencia.</p> In\u00a0[\u00a0]: Copied! <pre>def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n</pre> def plot_losses(history):     train_losses = [x.get('train_loss') for x in history]     val_losses = [x['val_loss'] for x in history]     plt.plot(train_losses, '-bx')     plt.plot(val_losses, '-rx')     plt.xlabel('epoch')     plt.ylabel('loss')     plt.legend(['Training', 'Validation'])     plt.title('Loss vs. No. of epochs'); In\u00a0[\u00a0]: Copied! <pre>plot_losses(history)\n</pre> plot_losses(history) <p>Inicialmente, tanto las p\u00e9rdidas de formaci\u00f3n como las de validaci\u00f3n parecen disminuir con el tiempo. Sin embargo, si entrena el modelo durante el tiempo suficiente, notar\u00e1 que la p\u00e9rdida de entrenamiento contin\u00faa disminuyendo, mientras que la p\u00e9rdida de validaci\u00f3n deja de disminuir e incluso comienza a aumentar despu\u00e9s de cierto punto.</p> <p>Este fen\u00f3meno se llama sobreajuste y es el no. 1 por qu\u00e9 muchos modelos de aprendizaje autom\u00e1tico dan resultados bastante terribles con datos del mundo real. Esto sucede porque el modelo, en un intento por minimizar la p\u00e9rdida, comienza a aprender patrones que son exclusivos de los datos de entrenamiento, a veces incluso memorizando ejemplos de entrenamiento espec\u00edficos. Debido a esto, el modelo no se generaliza bien a datos nunca antes vistos.</p> <p>A continuaci\u00f3n se presentan algunas estrategias comunes para evitar el sobreajuste:</p> <ul> <li>Recopilar y generar m\u00e1s datos de entrenamiento o agregarles ruido.</li> <li>Uso de t\u00e9cnicas de regularizaci\u00f3n como normalizaci\u00f3n y abandono por lotes.</li> <li>Detenci\u00f3n anticipada del entrenamiento del modelo, cuando la p\u00e9rdida de validaci\u00f3n comienza a aumentar</li> </ul> <p>Cubriremos estos temas con m\u00e1s detalle en el pr\u00f3ximo tutorial de esta serie y aprenderemos c\u00f3mo podemos alcanzar una precisi\u00f3n de m\u00e1s del 90 % realizando cambios menores pero importantes en nuestro modelo.</p> <p>Antes de continuar, guardemos nuestro trabajo en la nube usando <code>jovian.commit</code>.</p> In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project=project_name)\n</pre> jovian.commit(project=project_name) <p>Cuando prueba diferentes experimentos (cambiando la tasa de aprendizaje, el tama\u00f1o del lote, el optimizador, etc.) y registra hiperpar\u00e1metros y m\u00e9tricas con cada versi\u00f3n de su computadora port\u00e1til, puede usar [Comparar](https://jovian.ml /aakshns/05-cifar10-cnn/compare) en la p\u00e1gina del proyecto para analizar qu\u00e9 enfoques est\u00e1n funcionando bien y cu\u00e1les no. Usted ordena/filtra por precisi\u00f3n, p\u00e9rdida, etc., agrega notas para cada versi\u00f3n e incluso invita a colaboradores a contribuir a su proyecto con sus propios experimentos.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())\n</pre> test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor()) <p>Definamos una funci\u00f3n auxiliar <code>predict_image</code>, que devuelve la etiqueta predicha para un tensor de imagen \u00fanico.</p> In\u00a0[\u00a0]: Copied! <pre>def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]\n</pre> def predict_image(img, model):     # Convert to a batch of 1     xb = to_device(img.unsqueeze(0), device)     # Get predictions from model     yb = model(xb)     # Pick index with highest probability     _, preds  = torch.max(yb, dim=1)     # Retrieve the class label     return dataset.classes[preds[0].item()] In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[0]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[0] plt.imshow(img.permute(1, 2, 0)) print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model)) In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[1002]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[1002] plt.imshow(img.permute(1, 2, 0)) print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model)) In\u00a0[\u00a0]: Copied! <pre>img, label = test_dataset[6153]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))\n</pre> img, label = test_dataset[6153] plt.imshow(img.permute(1, 2, 0)) print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model)) <p>Identificar d\u00f3nde nuestro modelo funciona mal puede ayudarnos a mejorarlo, recopilando m\u00e1s datos de entrenamiento, aumentando/disminuyendo la complejidad del modelo y cambiando los hiperpar\u00e1metros.</p> <p>Como paso final, observemos tambi\u00e9n la p\u00e9rdida y precisi\u00f3n general del modelo en el conjunto de prueba y registremos usando \"joviano\". Esperamos que estos valores sean similares a los del conjunto de validaci\u00f3n. De lo contrario, es posible que necesitemos un mejor conjunto de validaci\u00f3n que tenga datos y distribuci\u00f3n similares a los del conjunto de prueba (que a menudo proviene de datos del mundo real).</p> In\u00a0[\u00a0]: Copied! <pre>test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\nresult = evaluate(model, test_loader)\nresult\n</pre> test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device) result = evaluate(model, test_loader) result In\u00a0[\u00a0]: Copied! <pre>jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])\n</pre> jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc']) In\u00a0[\u00a0]: Copied! <pre>torch.save(model.state_dict(), 'cifar10-cnn.pth')\n</pre> torch.save(model.state_dict(), 'cifar10-cnn.pth') <p>El m\u00e9todo <code>.state_dict</code> devuelve un <code>OrderedDict</code> que contiene todos los pesos y matrices de sesgo asignados a los atributos correctos del modelo. Para cargar los pesos del modelo, podemos redefinir el modelo con la misma estructura y usar el m\u00e9todo <code>.load_state_dict</code>.</p> In\u00a0[\u00a0]: Copied! <pre>model2 = to_device(Cifar10CnnModel(), device)\n</pre> model2 = to_device(Cifar10CnnModel(), device) In\u00a0[\u00a0]: Copied! <pre>model2.load_state_dict(torch.load('cifar10-cnn.pth'))\n</pre> model2.load_state_dict(torch.load('cifar10-cnn.pth')) <p>Solo como control de cordura, verifiquemos que este modelo tenga la misma p\u00e9rdida y precisi\u00f3n en el conjunto de prueba que antes.</p> In\u00a0[\u00a0]: Copied! <pre>evaluate(model2, test_loader)\n</pre> evaluate(model2, test_loader) <p>Hagamos una confirmaci\u00f3n final usando \"jovian\".</p> In\u00a0[\u00a0]: Copied! <pre>jovian.commit(project=project_name)\n</pre> jovian.commit(project=project_name) <p>Consulte la pesta\u00f1a Archivos en la p\u00e1gina del proyecto para ver o descargar los pesos del modelo entrenado. Tambi\u00e9n puedes descargar todos los archivos juntos usando la opci\u00f3n Descargar Zip en el men\u00fa desplegable Clonar.</p> <p>El trabajo de ciencia de datos a menudo est\u00e1 fragmentado en muchas plataformas diferentes (Git para c\u00f3digo, Dropbox/S3 para conjuntos de datos y artefactos, hojas de c\u00e1lculo para hiperpar\u00e1metros, m\u00e9tricas, etc.), lo que puede dificultar compartir y reproducir experimentos. Jovian.ml resuelve esto capturando todo lo relacionado con un proyecto de ciencia de datos en una \u00fanica plataforma, al tiempo que proporciona un flujo de trabajo perfecto para capturar, compartir y reproducir su trabajo. Para saber qu\u00e9 puede hacer con Jovian.ml, consulte los documentos: https://docs.jovian.ml.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"08-05-cifar10-cnn/#clasificacion-de-imagenes-utilizando-redes-neuronales-convolucionales-en-pytorch","title":"Clasificaci\u00f3n de im\u00e1genes utilizando redes neuronales convolucionales en PyTorch\u00b6","text":""},{"location":"08-05-cifar10-cnn/#parte-5-de-aprendizaje-profundo-con-pytorch-de-cero-a-gan","title":"Parte 5 de \"Aprendizaje profundo con Pytorch: de cero a GAN\"\u00b6","text":"<p>Esta serie de tutoriales es una introducci\u00f3n pr\u00e1ctica y sencilla para principiantes al aprendizaje profundo utilizando PyTorch, una biblioteca de redes neuronales de c\u00f3digo abierto. Estos tutoriales adoptan un enfoque pr\u00e1ctico y centrado en la codificaci\u00f3n. La mejor manera de aprender el material es ejecutar el c\u00f3digo y experimentar con \u00e9l usted mismo. Mira la serie completa aqu\u00ed:</p> <ol> <li>[Conceptos b\u00e1sicos de PyTorch: tensores y degradados] (https://jovian.ai/aakashns/01-pytorch-basics)</li> <li>Descenso de gradiente y regresi\u00f3n lineal</li> <li>Trabajar con im\u00e1genes y regresi\u00f3n log\u00edstica</li> <li>Entrenamiento de redes neuronales profundas en una GPU</li> <li>[Clasificaci\u00f3n de im\u00e1genes mediante redes neuronales convolucionales] (https://jovian.ai/aakashns/05-cifar10-cnn)</li> <li>Aumento de datos, regularizaci\u00f3n y ResNets</li> <li>Generaci\u00f3n de im\u00e1genes mediante redes generativas adversarias</li> </ol>"},{"location":"08-05-cifar10-cnn/#como-ejecutar-el-codigo","title":"C\u00f3mo ejecutar el c\u00f3digo\u00b6","text":"<p>Este tutorial es un ejecutable Jupyter notebook alojado en Jovian. Puede ejecutar este tutorial y experimentar con los ejemplos de c\u00f3digo de dos maneras: usando recursos gratuitos en l\u00ednea (recomendado) o en su computadora.</p>"},{"location":"08-05-cifar10-cnn/#opcion-1-ejecutar-usando-recursos-en-linea-gratuitos-1-clic-recomendado","title":"Opci\u00f3n 1: Ejecutar usando recursos en l\u00ednea gratuitos (1 clic, recomendado)\u00b6","text":"<p>La forma m\u00e1s sencilla de comenzar a ejecutar el c\u00f3digo es hacer clic en el bot\u00f3n Ejecutar en la parte superior de esta p\u00e1gina y seleccionar Ejecutar en Colab. Google Colab es una plataforma en l\u00ednea gratuita para ejecutar port\u00e1tiles Jupyter utilizando la infraestructura en la nube de Google. Tambi\u00e9n puede seleccionar \"Ejecutar en Binder\" o \"Ejecutar en Kaggle\" si tiene problemas al ejecutar el cuaderno en Google Colab.</p>"},{"location":"08-05-cifar10-cnn/#opcion-2-ejecutar-en-su-computadora-localmente","title":"Opci\u00f3n 2: ejecutar en su computadora localmente\u00b6","text":"<p>Para ejecutar el c\u00f3digo en su computadora localmente, deber\u00e1 configurar Python, descargar el cuaderno e instalar las bibliotecas necesarias. Recomendamos utilizar la distribuci\u00f3n Conda de Python. Haga clic en el bot\u00f3n Ejecutar en la parte superior de esta p\u00e1gina, seleccione la opci\u00f3n Ejecutar localmente y siga las instrucciones.</p>"},{"location":"08-05-cifar10-cnn/#usando-una-gpu-para-un-entrenamiento-mas-rapido","title":"Usando una GPU para un entrenamiento m\u00e1s r\u00e1pido\u00b6","text":"<p>Puede utilizar una Unidad de procesamiento de gr\u00e1ficos (GPU) para entrenar sus modelos m\u00e1s r\u00e1pido si su plataforma de ejecuci\u00f3n est\u00e1 conectada a una GPU fabricada por NVIDIA. Siga estas instrucciones para usar una GPU en la plataforma de su elecci\u00f3n:</p> <ul> <li>Google Colab: utilice la opci\u00f3n de men\u00fa \"Tiempo de ejecuci\u00f3n &gt; Cambiar tipo de tiempo de ejecuci\u00f3n\" y seleccione \"GPU\" en el men\u00fa desplegable \"Acelerador de hardware\".</li> <li>Kaggle: En la secci\u00f3n \"Configuraci\u00f3n\" de la barra lateral, seleccione \"GPU\" en el men\u00fa desplegable \"Acelerador\". Utilice el bot\u00f3n en la parte superior derecha para abrir la barra lateral.</li> <li>Binder: Las computadoras port\u00e1tiles que ejecutan Binder no pueden usar una GPU, ya que las m\u00e1quinas que alimentan Binder no est\u00e1n conectadas a ninguna GPU.</li> <li>Linux: Si su computadora port\u00e1til/escritorio tiene una GPU (tarjeta gr\u00e1fica) NVIDIA, aseg\u00farese de haber instalado los [controladores NVIDIA CUDA] (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index .html).</li> <li>Windows: si su computadora port\u00e1til/escritorio tiene una GPU (tarjeta gr\u00e1fica) NVIDIA, aseg\u00farese de haber instalado los [controladores NVIDIA CUDA] (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows /index.html).</li> <li>macOS: macOS no es compatible con las GPU NVIDIA</li> </ul> <p>Si no tiene acceso a una GPU o no est\u00e1 seguro de cu\u00e1l es, no se preocupe, puede ejecutar todo el c\u00f3digo de este tutorial sin una GPU.</p>"},{"location":"08-05-cifar10-cnn/#explorando-el-conjunto-de-datos-cifar10","title":"Explorando el conjunto de datos CIFAR10\u00b6","text":"<p>En el tutorial anterior, entrenamos redes neuronales feedfoward con una \u00fanica capa oculta para clasificar d\u00edgitos escritos a mano del [conjunto de datos MNIST](http:// yann.lecun.com/exdb/mnist) con m\u00e1s del 97% de precisi\u00f3n. Para este tutorial, usaremos el conjunto de datos CIFAR10, que consta de 60000 im\u00e1genes en color de 32x32 px en 10 clases. Aqu\u00ed hay algunas im\u00e1genes de muestra del conjunto de datos:</p>"},{"location":"08-05-cifar10-cnn/#guarda-y-sube-tu-libreta","title":"Guarda y sube tu libreta\u00b6","text":"<p>Ya sea que est\u00e9 ejecutando este cuaderno Jupyter en l\u00ednea o en su computadora, es esencial guardar su trabajo de vez en cuando. Puede continuar trabajando en un cuaderno guardado m\u00e1s tarde o compartirlo con amigos y colegas para permitirles ejecutar su c\u00f3digo. Jovian ofrece una forma sencilla de guardar y compartir sus cuadernos de Jupyter en l\u00ednea.</p>"},{"location":"08-05-cifar10-cnn/#conjuntos-de-datos-de-capacitacion-y-validacion","title":"Conjuntos de datos de capacitaci\u00f3n y validaci\u00f3n\u00b6","text":"<p>Al crear modelos de aprendizaje autom\u00e1tico del mundo real, es bastante com\u00fan dividir el conjunto de datos en 3 partes:</p> <ol> <li>Conjunto de entrenamiento: se utiliza para entrenar el modelo, es decir, calcular la p\u00e9rdida y ajustar los pesos del modelo mediante el descenso de gradiente.</li> <li>Conjunto de validaci\u00f3n: se utiliza para evaluar el modelo durante el entrenamiento, ajustar los hiperpar\u00e1metros (tasa de aprendizaje, etc.) y elegir la mejor versi\u00f3n del modelo.</li> <li>Conjunto de pruebas: se utiliza para comparar diferentes modelos o diferentes tipos de enfoques de modelado e informar la precisi\u00f3n final del modelo.</li> </ol> <p>Dado que no hay un conjunto de validaci\u00f3n predefinido, podemos reservar una peque\u00f1a porci\u00f3n (5000 im\u00e1genes) del conjunto de entrenamiento para usarla como conjunto de validaci\u00f3n. Usaremos el m\u00e9todo auxiliar <code>random_split</code> de PyTorch para hacer esto. Para garantizar que siempre creemos el mismo conjunto de validaci\u00f3n, tambi\u00e9n estableceremos una semilla para el generador de n\u00fameros aleatorios.</p>"},{"location":"08-05-cifar10-cnn/#definicion-del-modelo-red-neuronal-convolucional","title":"Definici\u00f3n del modelo (red neuronal convolucional)\u00b6","text":"<p>En nuestro [tutorial anterior] (https://jovian.ml/aakashns/04-feedforward-nn), definimos una red neuronal profunda con capas completamente conectadas usando <code>nn.Linear</code>. Sin embargo, para este tutorial usaremos una red neuronal convolucional, utilizando la clase <code>nn.Conv2d</code> de PyTorch.</p> <p>La convoluci\u00f3n 2D es una operaci\u00f3n bastante simple en el fondo: se comienza con un n\u00facleo, que es simplemente una peque\u00f1a matriz de pesos. Este n\u00facleo se \"desliza\" sobre los datos de entrada 2D, realiza una multiplicaci\u00f3n por elementos con la parte de la entrada en la que se encuentra actualmente y luego resume los resultados en un solo p\u00edxel de salida. - Fuente</p> <p>Implementemos una operaci\u00f3n de convoluci\u00f3n en una imagen de 1 canal con un n\u00facleo de 3x3.</p>"},{"location":"08-05-cifar10-cnn/#entrenando-el-modelo","title":"Entrenando el modelo\u00b6","text":"<p>Definiremos dos funciones: \"ajustar\" y \"evaluar\" para entrenar el modelo usando el descenso de gradiente y evaluar su desempe\u00f1o en el conjunto de validaci\u00f3n. Para obtener un tutorial detallado de estas funciones, consulte el tutorial anterior.</p>"},{"location":"08-05-cifar10-cnn/#pruebas-con-imagenes-individuales","title":"Pruebas con im\u00e1genes individuales\u00b6","text":"<p>Si bien hasta ahora hemos estado rastreando la precisi\u00f3n general de un modelo, tambi\u00e9n es una buena idea observar los resultados del modelo en algunas im\u00e1genes de muestra. Probemos nuestro modelo con algunas im\u00e1genes del conjunto de datos de prueba predefinido de 10000 im\u00e1genes. Comenzamos creando un conjunto de datos de prueba usando la clase <code>ImageFolder</code>.</p>"},{"location":"08-05-cifar10-cnn/#guardando-y-cargando-el-modelo","title":"Guardando y cargando el modelo\u00b6","text":"<p>Dado que hemos entrenado nuestro modelo durante mucho tiempo y logramos una precisi\u00f3n razonable, ser\u00eda una buena idea guardar los pesos del modelo en el disco, para que podamos reutilizar el modelo m\u00e1s adelante y evitar volver a entrenar desde cero. As\u00ed es como puedes guardar el modelo.</p>"},{"location":"08-05-cifar10-cnn/#resumen-y-lecturas-adicionalesejercicios","title":"Resumen y lecturas adicionales/ejercicios\u00b6","text":"<p>Hemos cubierto mucho terreno en este tutorial. Aqu\u00ed hay un resumen r\u00e1pido de los temas:</p> <ul> <li>Introducci\u00f3n al conjunto de datos CIFAR10 para clasificaci\u00f3n de im\u00e1genes.</li> <li>Descargar, extraer y cargar un conjunto de datos de im\u00e1genes usando <code>torchvision</code></li> <li>Mostrar lotes aleatorios de im\u00e1genes en una cuadr\u00edcula usando <code>torchvision.utils.make_grid</code></li> <li>Creaci\u00f3n de una red neuronal convolucional usando las capas <code>nn.Conv2d</code> y <code>nn.MaxPool2d</code></li> <li>Captura de informaci\u00f3n del conjunto de datos, m\u00e9tricas e hiperpar\u00e1metros utilizando la biblioteca \"joviana\".</li> <li>Entrenar una red neuronal convolucional y visualizar las p\u00e9rdidas y errores.</li> <li>Comprender el sobreajuste y las estrategias para evitarlo (m\u00e1s sobre esto m\u00e1s adelante)</li> <li>Generar predicciones sobre im\u00e1genes individuales del conjunto de prueba.</li> <li>Guardar y cargar los pesos del modelo y adjuntarlos a la instant\u00e1nea del experimento usando <code>jovian</code></li> </ul> <p>Hay muchas posibilidades para experimentar aqu\u00ed y le recomiendo que utilice la naturaleza interactiva de Jupyter para jugar con los distintos par\u00e1metros. Aqui hay algunas ideas:</p> <ul> <li>Intente cambiar los hiperpar\u00e1metros para lograr una mayor precisi\u00f3n en menos \u00e9pocas. Utilice la tabla de comparaci\u00f3n en la p\u00e1gina del proyecto Jovian.ml para comparar sus experimentos.</li> <li>Intente agregar m\u00e1s capas convolucionales o aumentar la cantidad de canales en cada capa convolucional</li> <li>Intente utilizar una red neuronal de avance y vea cu\u00e1l es la m\u00e1xima precisi\u00f3n que puede lograr</li> <li>Lea acerca de algunas de las estrategias mencionadas anteriormente para reducir el sobreajuste y lograr mejores resultados, e intente implementarlas consultando los documentos de PyTorch.</li> <li>Modifique este cuaderno para entrenar un modelo para un conjunto de datos diferente (por ejemplo, CIFAR100 o ImageNet)</li> </ul> <p>En el pr\u00f3ximo tutorial, continuaremos mejorando la precisi\u00f3n de nuestro modelo utilizando t\u00e9cnicas como aumento de datos, normalizaci\u00f3n por lotes y abandono. Tambi\u00e9n aprenderemos sobre las redes residuales (o ResNets), un cambio peque\u00f1o pero cr\u00edtico en la arquitectura del modelo que aumentar\u00e1 significativamente el rendimiento de nuestro modelo. \u00a1Mant\u00e9nganse al tanto!</p>"},{"location":"09-03_pytorch_computer_vision/","title":"03. Visi\u00f3n por computadora PyTorch","text":"<p>Ver c\u00f3digo fuente | Ver diapositivas | Ver v\u00eddeo tutorial</p> In\u00a0[\u00a0]: Copied! <pre># Importar PyTorch\nimport torch\nfrom torch import nn\n\n# Importar visi\u00f3n de antorcha\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Importar matplotlib para visualizaci\u00f3n\nimport matplotlib.pyplot as plt\n\n# Consultar versiones\n# Nota: su versi\u00f3n de PyTorch no debe ser inferior a 1.10.0 y la versi\u00f3n de torchvision no debe ser inferior a 0.11\nprint(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")\n</pre> # Importar PyTorch import torch from torch import nn  # Importar visi\u00f3n de antorcha import torchvision from torchvision import datasets from torchvision.transforms import ToTensor  # Importar matplotlib para visualizaci\u00f3n import matplotlib.pyplot as plt  # Consultar versiones # Nota: su versi\u00f3n de PyTorch no debe ser inferior a 1.10.0 y la versi\u00f3n de torchvision no debe ser inferior a 0.11 print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\") In\u00a0[\u00a0]: Copied! <pre># Configurar datos de entrenamiento\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Configurar datos de prueba\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=ToTensor()\n)\n</pre> # Configurar datos de entrenamiento train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Configurar datos de prueba test_data = datasets.FashionMNIST(     root=\"data\",     train=False, # get test data     download=True,     transform=ToTensor() ) <p>Veamos la primera muestra de los datos de entrenamiento.</p> In\u00a0[\u00a0]: Copied! <pre># Ver el primer ejemplo de entrenamiento\nimage, label = train_data[0]\nimage, label\n</pre> # Ver el primer ejemplo de entrenamiento image, label = train_data[0] image, label In\u00a0[\u00a0]: Copied! <pre># \u00bfCu\u00e1l es la forma de la imagen?\nimage.shape\n</pre> # \u00bfCu\u00e1l es la forma de la imagen? image.shape <p>La forma del tensor de imagen es <code>[1, 28, 28]</code> o m\u00e1s espec\u00edficamente:</p> <pre><code>[color_channels=1, alto=28, ancho=28]\n</code></pre> <p>Tener <code>color_channels=1</code> significa que la imagen est\u00e1 en escala de grises.</p> <p> Varios problemas tendr\u00e1n diversas formas de entrada y salida. Pero la premisa sigue siendo: codificar datos en n\u00fameros, construir un modelo para encontrar patrones en esos n\u00fameros, convertir esos patrones en algo significativo.</p> <p>Si <code>color_channels=3</code>, la imagen viene en valores de p\u00edxeles para rojo, verde y azul (esto tambi\u00e9n se conoce como [modelo de color RGB] (https://en.wikipedia.org/wiki/RGB_color_model)).</p> <p>El orden de nuestro tensor actual a menudo se denomina \"CHW\" (Canales de color, alto, ancho).</p> <p>Existe un debate sobre si las im\u00e1genes deben representarse como \"CHW\" (canales de color primero) o \"HWC\" (canales de color al final).</p> <p>Nota: Tambi\u00e9n ver\u00e1 los formatos <code>NCHW</code> y <code>NHWC</code> donde <code>N</code> significa n\u00famero de im\u00e1genes. Por ejemplo, si tiene un <code>batch_size=32</code>, la forma de su tensor puede ser <code>[32, 1, 28, 28]</code>. Cubriremos los tama\u00f1os de lote m\u00e1s adelante.</p> <p>PyTorch generalmente acepta <code>NCHW</code> (canales primero) como valor predeterminado para muchos operadores.</p> <p>Sin embargo, PyTorch tambi\u00e9n explica que \"NHWC\" (los \u00faltimos canales) funcionan mejor y se [considera una mejor pr\u00e1ctica] (https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice).</p> <p>Por ahora, dado que nuestro conjunto de datos y modelos son relativamente peque\u00f1os, esto no supondr\u00e1 una gran diferencia.</p> <p>Pero t\u00e9ngalo en cuenta cuando trabaje en conjuntos de datos de im\u00e1genes m\u00e1s grandes y utilice redes neuronales convolucionales (las veremos m\u00e1s adelante).</p> <p>Veamos m\u00e1s formas de nuestros datos.</p> In\u00a0[\u00a0]: Copied! <pre># \u00bfCu\u00e1ntas muestras hay?\nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n</pre> # \u00bfCu\u00e1ntas muestras hay? len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets) <p>Tenemos 60.000 muestras de entrenamiento y 10.000 muestras de prueba.</p> <p>\u00bfQu\u00e9 clases hay?</p> <p>Podemos encontrarlos a trav\u00e9s del atributo <code>.classes</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Ver clases\nclass_names = train_data.classes\nclass_names\n</pre> # Ver clases class_names = train_data.classes class_names <p>\u00a1Dulce! Parece que estamos ante 10 tipos diferentes de ropa.</p> <p>Debido a que estamos trabajando con 10 clases diferentes, significa que nuestro problema es clasificaci\u00f3n de clases m\u00faltiples.</p> <p>Seamos visuales.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\nplt.title(label);\n</pre> import matplotlib.pyplot as plt image, label = train_data[0] print(f\"Image shape: {image.shape}\") plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width) plt.title(label); <p>Podemos convertir la imagen en escala de grises usando el par\u00e1metro <code>cmap</code> de <code>plt.imshow()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>plt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(class_names[label]);\n</pre> plt.imshow(image.squeeze(), cmap=\"gray\") plt.title(class_names[label]); <p>Hermoso, tan hermoso como lo puede ser un bot\u00edn pixelado en escala de grises.</p> <p>Veamos algunos m\u00e1s.</p> In\u00a0[\u00a0]: Copied! <pre># Trazar m\u00e1s im\u00e1genes\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows * cols + 1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n</pre> # Trazar m\u00e1s im\u00e1genes torch.manual_seed(42) fig = plt.figure(figsize=(9, 9)) rows, cols = 4, 4 for i in range(1, rows * cols + 1):     random_idx = torch.randint(0, len(train_data), size=[1]).item()     img, label = train_data[random_idx]     fig.add_subplot(rows, cols, i)     plt.imshow(img.squeeze(), cmap=\"gray\")     plt.title(class_names[label])     plt.axis(False); <p>Hmmm, este conjunto de datos no parece demasiado est\u00e9tico.</p> <p>Pero los principios que aprenderemos sobre c\u00f3mo construir un modelo ser\u00e1n similares en una amplia gama de problemas de visi\u00f3n por computadora.</p> <p>En esencia, tomar valores de p\u00edxeles y construir un modelo para encontrar patrones en ellos para usarlos en valores de p\u00edxeles futuros.</p> <p>Adem\u00e1s, incluso para este peque\u00f1o conjunto de datos (s\u00ed, incluso 60.000 im\u00e1genes en aprendizaje profundo se consideran bastante peque\u00f1as), \u00bfpodr\u00edas escribir un programa para clasificar cada una de ellas?</p> <p>Probablemente podr\u00edas.</p> <p>Pero creo que codificar un modelo en PyTorch ser\u00eda m\u00e1s r\u00e1pido.</p> <p>Pregunta: \u00bfCrees que los datos anteriores se pueden modelar solo con l\u00edneas rectas (lineales)? \u00bfO crees que tambi\u00e9n necesitar\u00edas l\u00edneas no rectas (no lineales)?</p> In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Configurar el hiperpar\u00e1metro de tama\u00f1o de lote\nBATCH_SIZE = 32\n\n# Convierta conjuntos de datos en iterables (lotes)\ntrain_dataloader = DataLoader(train_data, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True # shuffle data every epoch?\n)\n\ntest_dataloader = DataLoader(test_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False # don't necessarily have to shuffle the testing data\n)\n\n# Veamos lo que hemos creado.\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n</pre> from torch.utils.data import DataLoader  # Configurar el hiperpar\u00e1metro de tama\u00f1o de lote BATCH_SIZE = 32  # Convierta conjuntos de datos en iterables (lotes) train_dataloader = DataLoader(train_data, # dataset to turn into iterable     batch_size=BATCH_SIZE, # how many samples per batch?      shuffle=True # shuffle data every epoch? )  test_dataloader = DataLoader(test_data,     batch_size=BATCH_SIZE,     shuffle=False # don't necessarily have to shuffle the testing data )  # Veamos lo que hemos creado. print(f\"Dataloaders: {train_dataloader, test_dataloader}\")  print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") In\u00a0[\u00a0]: Copied! <pre># Mira lo que hay dentro del cargador de datos de entrenamiento\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n</pre> # Mira lo que hay dentro del cargador de datos de entrenamiento train_features_batch, train_labels_batch = next(iter(train_dataloader)) train_features_batch.shape, train_labels_batch.shape <p>Y podemos ver que los datos permanecen sin cambios al verificar una sola muestra.</p> In\u00a0[\u00a0]: Copied! <pre># Mostrar una muestra\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n</pre> # Mostrar una muestra torch.manual_seed(42) random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() img, label = train_features_batch[random_idx], train_labels_batch[random_idx] plt.imshow(img.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(\"Off\"); print(f\"Image size: {img.shape}\") print(f\"Label: {label}, label size: {label.shape}\") In\u00a0[\u00a0]: Copied! <pre># Crear una capa aplanada\nflatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n\n# Obtenga una sola muestra\nx = train_features_batch[0]\n\n# Aplanar la muestra\noutput = flatten_model(x) # perform forward pass\n\n# Imprime lo que pas\u00f3\nprint(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n\n# Intente descomentar a continuaci\u00f3n y vea qu\u00e9 sucede\n# imprimir(x)\n# imprimir (salida)\n</pre> # Crear una capa aplanada flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)  # Obtenga una sola muestra x = train_features_batch[0]  # Aplanar la muestra output = flatten_model(x) # perform forward pass  # Imprime lo que pas\u00f3 print(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\") print(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")  # Intente descomentar a continuaci\u00f3n y vea qu\u00e9 sucede # imprimir(x) # imprimir (salida) <p>La capa <code>nn.Flatten()</code> tom\u00f3 nuestra forma de <code>[color_channels, height, width]</code> a <code>[color_channels, height*width]</code>.</p> <p>\u00bfPor qu\u00e9 hacer esto?</p> <p>Porque ahora hemos convertido nuestros datos de p\u00edxeles de las dimensiones de alto y ancho en un vector de caracter\u00edsticas largo.</p> <p>Y a las capas <code>nn.Linear()</code> les gusta que sus entradas est\u00e9n en forma de vectores de caracter\u00edsticas.</p> <p>Creemos nuestro primer modelo usando <code>nn.Flatten()</code> como primera capa.</p> In\u00a0[\u00a0]: Copied! <pre>from torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # neural networks like their inputs in vector form\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n</pre> from torch import nn class FashionMNISTModelV0(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # neural networks like their inputs in vector form             nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)             nn.Linear(in_features=hidden_units, out_features=output_shape)         )          def forward(self, x):         return self.layer_stack(x) <p>\u00a1Maravilloso!</p> <p>Tenemos una clase de modelo de referencia que podemos usar, ahora creemos una instancia de un modelo.</p> <p>Necesitaremos establecer los siguientes par\u00e1metros:</p> <ul> <li><code>input_shape=784</code>: esta es la cantidad de funciones que tienes en el modelo; en nuestro caso, es una por cada p\u00edxel de la imagen de destino (28 p\u00edxeles de alto por 28 p\u00edxeles de ancho = 784 funciones).</li> <li><code>hidden_units=10</code> - n\u00famero de unidades/neuronas en las capas ocultas, este n\u00famero puede ser el que quieras, pero para mantener el modelo peque\u00f1o comenzaremos con <code>10</code>.</li> <li><code>output_shape=len(class_names)</code>: dado que estamos trabajando con un problema de clasificaci\u00f3n de clases m\u00faltiples, necesitamos una neurona de salida por clase en nuestro conjunto de datos.</li> </ul> <p>Creemos una instancia de nuestro modelo y envi\u00e9mosla a la CPU por ahora (pronto ejecutaremos una peque\u00f1a prueba para ejecutar <code>model_0</code> en la CPU frente a un modelo similar en la GPU).</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Necesidad de configurar el modelo con par\u00e1metros de entrada\nmodel_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n    hidden_units=10, # how many units in the hiden layer\n    output_shape=len(class_names) # one for every class\n)\nmodel_0.to(\"cpu\") # keep model on CPU to begin with\n</pre> torch.manual_seed(42)  # Necesidad de configurar el modelo con par\u00e1metros de entrada model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)     hidden_units=10, # how many units in the hiden layer     output_shape=len(class_names) # one for every class ) model_0.to(\"cpu\") # keep model on CPU to begin with  In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Descargue funciones auxiliares del repositorio de Learn PyTorch (si a\u00fan no las ha descargado)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # Note: you need the \"raw\" GitHub URL for this to work\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n</pre> import requests from pathlib import Path   # Descargue funciones auxiliares del repositorio de Learn PyTorch (si a\u00fan no las ha descargado) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   # Note: you need the \"raw\" GitHub URL for this to work   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content) In\u00a0[\u00a0]: Copied! <pre># M\u00e9trica de precisi\u00f3n de importaci\u00f3n\nfrom helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n\n# Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n y optimizador.\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n</pre> # M\u00e9trica de precisi\u00f3n de importaci\u00f3n from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)  # Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n y optimizador. loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) In\u00a0[\u00a0]: Copied! <pre>from timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n    \"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</pre> from timeit import default_timer as timer  def print_train_time(start: float, end: float, device: torch.device = None):     \"\"\"Prints difference between start and end time.      Args:         start (float): Start time of computation (preferred in timeit format).          end (float): End time of computation.         device ([type], optional): Device that compute is running on. Defaults to None.      Returns:         float: time between start and end in seconds (higher is longer).     \"\"\"     total_time = end - start     print(f\"Train time on {device}: {total_time:.3f} seconds\")     return total_time In\u00a0[\u00a0]: Copied! <pre># Importar tqdm para la barra de progreso\nfrom tqdm.auto import tqdm\n\n# Establecer la semilla y poner en marcha el cron\u00f3metro\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Establece el n\u00famero de \u00e9pocas (lo mantendremos peque\u00f1o para tiempos de entrenamiento m\u00e1s r\u00e1pidos)\nepochs = 3\n\n# Crear un ciclo de entrenamiento y prueba\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate loss (per batch)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulatively add up the loss per epoch \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Print out how many samples have been seen\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n\n    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n    train_loss /= len(train_dataloader)\n    \n    ### Testing\n    # Setup variables for accumulatively adding up loss and accuracy \n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X)\n           \n            # 2. Calculate loss (accumatively)\n            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n\n            # 3. Calculate accuracy (preds need to be same as y_true)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # Calculations on test metrics need to happen inside torch.inference_mode()\n        # Divide total test loss by length of test dataloader (per batch)\n        test_loss /= len(test_dataloader)\n\n        # Divide total accuracy by length of test dataloader (per batch)\n        test_acc /= len(test_dataloader)\n\n    ## Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n\n# Calcular el tiempo de entrenamiento.\ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n</pre> # Importar tqdm para la barra de progreso from tqdm.auto import tqdm  # Establecer la semilla y poner en marcha el cron\u00f3metro torch.manual_seed(42) train_time_start_on_cpu = timer()  # Establece el n\u00famero de \u00e9pocas (lo mantendremos peque\u00f1o para tiempos de entrenamiento m\u00e1s r\u00e1pidos) epochs = 3  # Crear un ciclo de entrenamiento y prueba for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n-------\")     ### Training     train_loss = 0     # Add a loop to loop through training batches     for batch, (X, y) in enumerate(train_dataloader):         model_0.train()          # 1. Forward pass         y_pred = model_0(X)          # 2. Calculate loss (per batch)         loss = loss_fn(y_pred, y)         train_loss += loss # accumulatively add up the loss per epoch           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Print out how many samples have been seen         if batch % 400 == 0:             print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")      # Divide total train loss by length of train dataloader (average loss per batch per epoch)     train_loss /= len(train_dataloader)          ### Testing     # Setup variables for accumulatively adding up loss and accuracy      test_loss, test_acc = 0, 0      model_0.eval()     with torch.inference_mode():         for X, y in test_dataloader:             # 1. Forward pass             test_pred = model_0(X)                         # 2. Calculate loss (accumatively)             test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch              # 3. Calculate accuracy (preds need to be same as y_true)             test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))                  # Calculations on test metrics need to happen inside torch.inference_mode()         # Divide total test loss by length of test dataloader (per batch)         test_loss /= len(test_dataloader)          # Divide total accuracy by length of test dataloader (per batch)         test_acc /= len(test_dataloader)      ## Print out what's happening     print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")  # Calcular el tiempo de entrenamiento. train_time_end_on_cpu = timer() total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,                                             end=train_time_end_on_cpu,                                            device=str(next(model_0.parameters()).device)) <p>\u00a1Lindo! Parece que a nuestro modelo de referencia le fue bastante bien.</p> <p>Tampoco tom\u00f3 mucho tiempo entrenar, incluso solo en la CPU. Me pregunto si se acelerar\u00e1 en la GPU.</p> <p>Escribamos un c\u00f3digo para evaluar nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Make predictions with the model\n            y_pred = model(X)\n            \n            # Accumulate the loss and accuracy values per batch\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # Scale loss and acc to find the average loss/acc per batch\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calcular los resultados del modelo 0 en el conjunto de datos de prueba\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n</pre> torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn):     \"\"\"Returns a dictionary containing the results of model predicting on data_loader.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Make predictions with the model             y_pred = model(X)                          # Accumulate the loss and accuracy values per batch             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)                  # Scale loss and acc to find the average loss/acc per batch         loss /= len(data_loader)         acc /= len(data_loader)              return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calcular los resultados del modelo 0 en el conjunto de datos de prueba model_0_results = eval_model(model=model_0, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn ) model_0_results <p>\u00a1Luciendo bien!</p> <p>Podemos utilizar este diccionario para comparar los resultados del modelo de referencia con otros modelos m\u00e1s adelante.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar c\u00f3digo independiente del dispositivo\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Configurar c\u00f3digo independiente del dispositivo import torch device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device <p>\u00a1Hermoso!</p> <p>Construyamos otro modelo.</p> In\u00a0[\u00a0]: Copied! <pre># Crear un modelo con capas lineales y no lineales.\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n</pre> # Crear un modelo con capas lineales y no lineales. class FashionMNISTModelV1(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # flatten inputs into single vector             nn.Linear(in_features=input_shape, out_features=hidden_units),             nn.ReLU(),             nn.Linear(in_features=hidden_units, out_features=output_shape),             nn.ReLU()         )          def forward(self, x: torch.Tensor):         return self.layer_stack(x) <p>Eso se ve bien.</p> <p>Ahora vamos a crear una instancia con la misma configuraci\u00f3n que usamos antes.</p> <p>Necesitaremos <code>input_shape=784</code> (igual al n\u00famero de caracter\u00edsticas de nuestros datos de imagen), <code>hidden_units=10</code> (comenzando poco a poco y lo mismo que nuestro modelo de referencia) y <code>output_shape=len(class_names)</code> (una salida unidad por clase).</p> <p>Nota: Observe c\u00f3mo mantuvimos la mayor\u00eda de las configuraciones de nuestro modelo iguales excepto por un cambio: agregar capas no lineales. Esta es una pr\u00e1ctica est\u00e1ndar para ejecutar una serie de experimentos de aprendizaje autom\u00e1tico, cambiar una cosa y ver qu\u00e9 sucede, luego hacerlo una y otra vez.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n    hidden_units=10,\n    output_shape=len(class_names) # number of output classes desired\n).to(device) # send model to GPU if it's available\nnext(model_1.parameters()).device # check model device\n</pre> torch.manual_seed(42) model_1 = FashionMNISTModelV1(input_shape=784, # number of input features     hidden_units=10,     output_shape=len(class_names) # number of output classes desired ).to(device) # send model to GPU if it's available next(model_1.parameters()).device # check model device In\u00a0[\u00a0]: Copied! <pre>from helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n</pre> from helper_functions import accuracy_fn loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_1.parameters(),                              lr=0.1) In\u00a0[\u00a0]: Copied! <pre>def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    model.to(device)\n    for batch, (X, y) in enumerate(data_loader):\n        # Send data to GPU\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.to(device)\n    model.eval() # put model in eval mode\n    # Turn on inference context manager\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred = model(X)\n            \n            # 2. Calculate loss and accuracy\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels\n            )\n        \n        # Adjust metrics and print out\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n</pre> def train_step(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,                optimizer: torch.optim.Optimizer,                accuracy_fn,                device: torch.device = device):     train_loss, train_acc = 0, 0     model.to(device)     for batch, (X, y) in enumerate(data_loader):         # Send data to GPU         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate loss         loss = loss_fn(y_pred, y)         train_loss += loss         train_acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels          # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()      # Calculate loss and accuracy per epoch and print out what's happening     train_loss /= len(data_loader)     train_acc /= len(data_loader)     print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")  def test_step(data_loader: torch.utils.data.DataLoader,               model: torch.nn.Module,               loss_fn: torch.nn.Module,               accuracy_fn,               device: torch.device = device):     test_loss, test_acc = 0, 0     model.to(device)     model.eval() # put model in eval mode     # Turn on inference context manager     with torch.inference_mode():          for X, y in data_loader:             # Send data to GPU             X, y = X.to(device), y.to(device)                          # 1. Forward pass             test_pred = model(X)                          # 2. Calculate loss and accuracy             test_loss += loss_fn(test_pred, y)             test_acc += accuracy_fn(y_true=y,                 y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels             )                  # Adjust metrics and print out         test_loss /= len(data_loader)         test_acc /= len(data_loader)         print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\") <p>\u00a1Guau!</p> <p>Ahora que tenemos algunas funciones para entrenar y probar nuestro modelo, ejecut\u00e9moslas.</p> <p>Lo haremos dentro de otro bucle para cada \u00e9poca.</p> <p>De esa manera, para cada \u00e9poca vamos a realizar un paso de entrenamiento y de prueba.</p> <p>Nota: Puede personalizar la frecuencia con la que realiza un paso de prueba. A veces la gente los hace cada cinco o diez \u00e9pocas o, en nuestro caso, cada \u00e9poca.</p> <p>Tambi\u00e9n cronometremos las cosas para ver cu\u00e1nto tiempo tarda nuestro c\u00f3digo en ejecutarse en la GPU.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Medir el tiempo\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n</pre> torch.manual_seed(42)  # Medir el tiempo from timeit import default_timer as timer train_time_start_on_gpu = timer()  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_1,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn     )     test_step(data_loader=test_dataloader,         model=model_1,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn     )  train_time_end_on_gpu = timer() total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,                                             end=train_time_end_on_gpu,                                             device=device) <p>\u00a1Excelente!</p> <p>\u00bfNuestro modelo se entren\u00f3 pero el tiempo de entrenamiento tom\u00f3 m\u00e1s tiempo?</p> <p>Nota: El tiempo de entrenamiento en CUDA versus CPU depender\u00e1 en gran medida de la calidad de la CPU/GPU que est\u00e9s usando. Siga leyendo para obtener una respuesta m\u00e1s explicada.</p> <p>Pregunta: \"Us\u00e9 una GPU pero mi modelo no se entren\u00f3 m\u00e1s r\u00e1pido, \u00bfa qu\u00e9 se debe?\"</p> <p>Respuesta: Bueno, una raz\u00f3n podr\u00eda ser que su conjunto de datos y su modelo son tan peque\u00f1os (como el conjunto de datos y el modelo con el que estamos trabajando) que los beneficios de usar una GPU se ven superados por el tiempo que realmente lleva la transferencia. los datos all\u00ed.</p> <p>Existe un peque\u00f1o cuello de botella entre la copia de datos de la memoria de la CPU (predeterminada) a la memoria de la GPU.</p> <p>Entonces, para modelos y conjuntos de datos m\u00e1s peque\u00f1os, la CPU podr\u00eda ser el lugar \u00f3ptimo para calcular.</p> <p>Pero para conjuntos de datos y modelos m\u00e1s grandes, la velocidad de computaci\u00f3n que la GPU puede ofrecer generalmente supera con creces el costo de llevar los datos all\u00ed.</p> <p>Sin embargo, esto depende en gran medida del hardware que est\u00e1s utilizando. Con la pr\u00e1ctica, te acostumbrar\u00e1s a cu\u00e1l es el mejor lugar para entrenar a tus modelos.</p> <p>Evaluemos nuestro <code>model_1</code> entrenado usando nuestra funci\u00f3n <code>eval_model()</code> y veamos c\u00f3mo fue.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Nota: Esto generar\u00e1 un error debido a que `eval_model()` no utiliza c\u00f3digo independiente del dispositivo.\nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results\n</pre> torch.manual_seed(42)  # Nota: Esto generar\u00e1 un error debido a que `eval_model()` no utiliza c\u00f3digo independiente del dispositivo. model_1_results = eval_model(model=model_1,      data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn)  model_1_results  <p>\u00a1Oh, no!</p> <p>Parece que nuestra funci\u00f3n <code>eval_model()</code> falla con:</p> <p><code>RuntimeError: Se esperaba que todos los tensores estuvieran en el mismo dispositivo, pero encontr\u00e9 al menos dos dispositivos, cuda:0 y cpu. (al verificar el argumento mat1 en el m\u00e9todo wrapper_addmm)</code></p> <p>Es porque hemos configurado nuestros datos y modelo para usar c\u00f3digo independiente del dispositivo, pero no nuestra funci\u00f3n de evaluaci\u00f3n.</p> <p>\u00bfQu\u00e9 tal si solucionamos eso pasando un par\u00e1metro de <code>dispositivo</code> de destino a nuestra funci\u00f3n <code>eval_model()</code>?</p> <p>Luego intentaremos calcular los resultados nuevamente.</p> In\u00a0[\u00a0]: Copied! <pre># Mover valores al dispositivo\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n    \"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calcule los resultados del modelo 1 con c\u00f3digo independiente del dispositivo\nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n</pre> # Mover valores al dispositivo torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn,                 device: torch.device = device):     \"\"\"Evaluates a given model on a given dataset.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.         device (str, optional): Target device to compute on. Defaults to device.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Send data to the target device             X, y = X.to(device), y.to(device)             y_pred = model(X)             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))                  # Scale loss and acc         loss /= len(data_loader)         acc /= len(data_loader)     return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calcule los resultados del modelo 1 con c\u00f3digo independiente del dispositivo model_1_results = eval_model(model=model_1, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn,     device=device ) model_1_results In\u00a0[\u00a0]: Copied! <pre># Verificar resultados de referencia\nmodel_0_results\n</pre> # Verificar resultados de referencia model_0_results <p>Vaya, en este caso, parece que agregar no linealidades a nuestro modelo hizo que funcionara peor que la l\u00ednea base.</p> <p>Eso es algo a tener en cuenta en el aprendizaje autom\u00e1tico: a veces lo que pensaba que deber\u00eda funcionar no funciona.</p> <p>Y luego lo que pensabas que podr\u00eda no funcionar, funciona.</p> <p>Es en parte ciencia, en parte arte.</p> <p>Por lo que parece, parece que nuestro modelo se est\u00e1 sobreajustando en los datos de entrenamiento.</p> <p>El sobreajuste significa que nuestro modelo est\u00e1 aprendiendo bien los datos de entrenamiento, pero esos patrones no se generalizan a los datos de prueba.</p> <p>Dos de los principales para solucionar el sobreajuste incluyen:</p> <ol> <li>Usar un modelo m\u00e1s peque\u00f1o o diferente (algunos modelos se ajustan mejor a ciertos tipos de datos que otros).</li> <li>Usar un conjunto de datos m\u00e1s grande (cuantos m\u00e1s datos, m\u00e1s posibilidades tiene un modelo de aprender patrones generalizables).</li> </ol> <p>Hay m\u00e1s, pero lo dejar\u00e9 como un desaf\u00edo para que lo explores.</p> <p>Intente buscar en l\u00ednea \"formas de evitar el sobreajuste en el aprendizaje autom\u00e1tico\" y vea qu\u00e9 aparece.</p> <p>Mientras tanto, echemos un vistazo al n\u00famero 1: usar un modelo diferente.</p> In\u00a0[\u00a0]: Copied! <pre># Crear una red neuronal convolucional\nclass FashionMNISTModelV2(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n</pre> # Crear una red neuronal convolucional class FashionMNISTModelV2(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*7*7,                        out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.block_1(x)         # print(x.shape)         x = self.block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x  torch.manual_seed(42) model_2 = FashionMNISTModelV2(input_shape=1,      hidden_units=10,      output_shape=len(class_names)).to(device) model_2 <p>\u00a1Lindo!</p> <p>\u00a1Nuestro modelo m\u00e1s grande hasta el momento!</p> <p>Lo que hemos hecho es una pr\u00e1ctica com\u00fan en el aprendizaje autom\u00e1tico.</p> <p>Encuentre una arquitectura modelo en alg\u00fan lugar y repl\u00edquela con c\u00f3digo.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Cree un lote de muestra de n\u00fameros aleatorios con el mismo tama\u00f1o que el lote de im\u00e1genes\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # get a single image for testing\nprint(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"Single image pixel values:\\n{test_image}\")\n</pre> torch.manual_seed(42)  # Cree un lote de muestra de n\u00fameros aleatorios con el mismo tama\u00f1o que el lote de im\u00e1genes images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width] test_image = images[0] # get a single image for testing print(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\")  print(f\"Single image pixel values:\\n{test_image}\") <p>Creemos un ejemplo <code>nn.Conv2d()</code> con varios par\u00e1metros:</p> <ul> <li><code>in_channels</code> (int) - N\u00famero de canales en la imagen de entrada.</li> <li><code>out_channels</code> (int) - N\u00famero de canales producidos por la convoluci\u00f3n.</li> <li><code>kernel_size</code> (int o tupla): tama\u00f1o del kernel/filtro convolutivo.</li> <li><code>stride</code> (int o tuple, opcional): qu\u00e9 tan grande es el paso que da el n\u00facleo convolutivo a la vez. Predeterminado: 1.</li> <li><code>padding</code> (int, tuple, str): relleno agregado a los cuatro lados de la entrada. Predeterminado: 0.</li> </ul> <p></p> <p>Ejemplo de lo que sucede cuando cambias los hiperpar\u00e1metros de una capa <code>nn.Conv2d()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Crea una capa convolucional con las mismas dimensiones que TinyVGG\n# (intente cambiar cualquiera de los par\u00e1metros y vea qu\u00e9 sucede)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # also try using \"valid\" or \"same\" here \n\n# Pasar los datos a trav\u00e9s de la capa convolucional.\nconv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)\n</pre> torch.manual_seed(42)  # Crea una capa convolucional con las mismas dimensiones que TinyVGG # (intente cambiar cualquiera de los par\u00e1metros y vea qu\u00e9 sucede) conv_layer = nn.Conv2d(in_channels=3,                        out_channels=10,                        kernel_size=3,                        stride=1,                        padding=0) # also try using \"valid\" or \"same\" here   # Pasar los datos a trav\u00e9s de la capa convolucional. conv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)  <p>Si intentamos pasar una sola imagen, obtenemos un error de falta de coincidencia de forma:</p> <p><code>RuntimeError: Se esperaba una entrada de 4 dimensiones para un peso de 4 dimensiones [10, 3, 3, 3], pero en su lugar obtuve una entrada de 3 dimensiones de tama\u00f1o [3, 64, 64]</code></p> <p>Nota: Si est\u00e1 ejecutando PyTorch 1.11.0+, este error no ocurrir\u00e1.</p> <p>Esto se debe a que nuestra capa <code>nn.Conv2d()</code> espera un tensor de 4 dimensiones como entrada con tama\u00f1o <code>(N, C, H, W)</code> o <code>[batch_size, color_channels, height, width]</code>.</p> <p>En este momento, nuestra imagen \u00fanica <code>test_image</code> solo tiene la forma <code>[color_channels, height, width]</code> o <code>[3, 64, 64]</code>.</p> <p>Podemos solucionar este problema para una sola imagen usando <code>test_image.unsqueeze(dim=0)</code> para agregar una dimensi\u00f3n adicional para <code>N</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Agregue una dimensi\u00f3n adicional a la imagen de prueba\ntest_image.unsqueeze(dim=0).shape\n</pre> # Agregue una dimensi\u00f3n adicional a la imagen de prueba test_image.unsqueeze(dim=0).shape In\u00a0[\u00a0]: Copied! <pre># Pase la imagen de prueba con dimensi\u00f3n adicional a trav\u00e9s de conv_layer\nconv_layer(test_image.unsqueeze(dim=0)).shape\n</pre> # Pase la imagen de prueba con dimensi\u00f3n adicional a trav\u00e9s de conv_layer conv_layer(test_image.unsqueeze(dim=0)).shape <p>Hmm, observe lo que sucede con nuestra forma (la misma forma que la primera capa de TinyVGG en [CNN Explicador] (https://poloclub.github.io/cnn-explainer/)), obtenemos diferentes tama\u00f1os de canal, as\u00ed como diferentes tama\u00f1os de p\u00edxeles.</p> <p>\u00bfQu\u00e9 pasa si cambiamos los valores de <code>conv_layer</code>?</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n# Crea una nueva conv_layer con diferentes valores (intenta configurarlos como quieras)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pase una sola imagen a trav\u00e9s del nuevo conv_layer_2 (esto llama al m\u00e9todo forward() de nn.Conv2d() en la entrada)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n</pre> torch.manual_seed(42) # Crea una nueva conv_layer con diferentes valores (intenta configurarlos como quieras) conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image                          out_channels=10,                          kernel_size=(5, 5), # kernel is usually a square so a tuple also works                          stride=2,                          padding=0)  # Pase una sola imagen a trav\u00e9s del nuevo conv_layer_2 (esto llama al m\u00e9todo forward() de nn.Conv2d() en la entrada) conv_layer_2(test_image.unsqueeze(dim=0)).shape <p>Vaya, tenemos otro cambio de forma.</p> <p>Ahora nuestra imagen tiene la forma <code>[1, 10, 30, 30]</code> (ser\u00e1 diferente si usa valores diferentes) o <code>[batch_size=1, color_channels=10, height=30, width=30]</code>.</p> <p>\u00bfQue est\u00e1 pasando aqui?</p> <p>Detr\u00e1s de escena, nuestro <code>nn.Conv2d()</code> est\u00e1 comprimiendo la informaci\u00f3n almacenada en la imagen.</p> <p>Para ello, realiza operaciones en la entrada (nuestra imagen de prueba) con sus par\u00e1metros internos.</p> <p>El objetivo de esto es similar al de todas las dem\u00e1s redes neuronales que hemos estado construyendo.</p> <p>Los datos entran y las capas intentan actualizar sus par\u00e1metros internos (patrones) para reducir la funci\u00f3n de p\u00e9rdida gracias a la ayuda del optimizador.</p> <p>La \u00fanica diferencia es c\u00f3mo las diferentes capas calculan sus actualizaciones de par\u00e1metros o, en t\u00e9rminos de PyTorch, la operaci\u00f3n presente en el m\u00e9todo <code>forward()</code> de la capa.</p> <p>Si revisamos nuestro <code>conv_layer_2.state_dict()</code> encontraremos una configuraci\u00f3n de peso y sesgo similar a la que hemos visto antes.</p> In\u00a0[\u00a0]: Copied! <pre># Consulte los par\u00e1metros internos de conv_layer_2\nprint(conv_layer_2.state_dict())\n</pre> # Consulte los par\u00e1metros internos de conv_layer_2 print(conv_layer_2.state_dict()) <p>\u00a1Mira eso! Un mont\u00f3n de n\u00fameros aleatorios para un tensor de peso y sesgo.</p> <p>Las formas de estos son manipuladas por las entradas que le pasamos a <code>nn.Conv2d()</code> cuando lo configuramos.</p> <p>Echemos un vistazo.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga formas de tensores de peso y sesgo dentro de conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n</pre> # Obtenga formas de tensores de peso y sesgo dentro de conv_layer_2 print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\") print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\") <p>Pregunta: \u00bfQu\u00e9 debemos configurar los par\u00e1metros de nuestras capas <code>nn.Conv2d()</code>?</p> <p>Esa es buena. Pero al igual que muchas otras cosas en el aprendizaje autom\u00e1tico, los valores de estos no est\u00e1n escritos en piedra (y recuerden, debido a que estos valores son los que podemos establecer nosotros mismos, se los conoce como \"hiperpar\u00e1metros\").</p> <p>La mejor manera de averiguarlo es probar diferentes valores y ver c\u00f3mo afectan el rendimiento de su modelo.</p> <p>O mejor a\u00fan, busque un ejemplo funcional sobre un problema similar al suyo (como lo hemos hecho con TinyVGG) y c\u00f3pielo.</p> <p>Estamos trabajando con una capa diferente a la que hemos visto antes.</p> <p>Pero la premisa sigue siendo la misma: empezar con n\u00fameros aleatorios y actualizarlos para representar mejor los datos.</p> In\u00a0[\u00a0]: Copied! <pre># Imprima la forma de la imagen original sin y con dimensi\u00f3n sin comprimir\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Cree una capa de muestra nn.MaxPoo2d()\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pasar datos solo a trav\u00e9s de conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pasar datos a trav\u00e9s de la capa de grupo m\u00e1ximo\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n</pre> # Imprima la forma de la imagen original sin y con dimensi\u00f3n sin comprimir print(f\"Test image original shape: {test_image.shape}\") print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")  # Cree una capa de muestra nn.MaxPoo2d() max_pool_layer = nn.MaxPool2d(kernel_size=2)  # Pasar datos solo a trav\u00e9s de conv_layer test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0)) print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")  # Pasar datos a trav\u00e9s de la capa de grupo m\u00e1ximo test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv) print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\") <p>Observe el cambio en las formas de lo que sucede dentro y fuera de una capa <code>nn.MaxPool2d()</code>.</p> <p>El <code>kernel_size</code> de la capa <code>nn.MaxPool2d()</code> afectar\u00e1 el tama\u00f1o de la forma de salida.</p> <p>En nuestro caso, la forma se reduce a la mitad de una imagen de \"62x62\" a una imagen de \"31x31\".</p> <p>Veamos c\u00f3mo funciona con un tensor m\u00e1s peque\u00f1o.</p> In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n# Crea un tensor aleatorio con un n\u00famero de dimensiones similar a nuestras im\u00e1genes.\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Crear una capa de grupo m\u00e1xima\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pase el tensor aleatorio a trav\u00e9s de la capa de grupo m\u00e1xima\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n</pre> torch.manual_seed(42) # Crea un tensor aleatorio con un n\u00famero de dimensiones similar a nuestras im\u00e1genes. random_tensor = torch.randn(size=(1, 1, 2, 2)) print(f\"Random tensor:\\n{random_tensor}\") print(f\"Random tensor shape: {random_tensor.shape}\")  # Crear una capa de grupo m\u00e1xima max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value   # Pase el tensor aleatorio a trav\u00e9s de la capa de grupo m\u00e1xima max_pool_tensor = max_pool_layer(random_tensor) print(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\") print(f\"Max pool tensor shape: {max_pool_tensor.shape}\") <p>Observe las dos dimensiones finales entre <code>random_tensor</code> y <code>max_pool_tensor</code>, van de <code>[2, 2]</code> a <code>[1, 1]</code>.</p> <p>En esencia, se reducen a la mitad.</p> <p>Y el cambio ser\u00eda diferente para diferentes valores de <code>kernel_size</code> para <code>nn.MaxPool2d()</code>.</p> <p>Observe tambi\u00e9n que el valor sobrante en <code>max_pool_tensor</code> es el valor m\u00e1ximo de <code>random_tensor</code>.</p> <p>\u00bfQue esta pasando aqui?</p> <p>\u00c9sta es otra pieza importante del rompecabezas de las redes neuronales.</p> <p>Esencialmente, cada capa de una red neuronal intenta comprimir datos desde un espacio de dimensiones superiores a un espacio de dimensiones inferiores.</p> <p>En otras palabras, tome muchos n\u00fameros (datos sin procesar) y aprenda patrones en esos n\u00fameros, patrones que sean predictivos y al mismo tiempo sean m\u00e1s peque\u00f1os en tama\u00f1o que los valores originales.</p> <p>Desde una perspectiva de inteligencia artificial, se podr\u00eda considerar el objetivo completo de una red neuronal de comprimir informaci\u00f3n.</p> <p>![cada capa de una red neuronal comprime los datos de entrada originales en una representaci\u00f3n m\u00e1s peque\u00f1a que (con suerte) es capaz de hacer predicciones sobre datos de entrada futuros](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/ principal/images/03-conv-net-as-compression.png)</p> <p>Esto significa que, desde el punto de vista de una red neuronal, la inteligencia es compresi\u00f3n.</p> <p>Esta es la idea del uso de una capa <code>nn.MaxPool2d()</code>: tomar el valor m\u00e1ximo de una parte de un tensor y ignorar el resto.</p> <p>En esencia, reducir la dimensionalidad de un tensor y al mismo tiempo conservar una (con suerte) parte significativa de la informaci\u00f3n.</p> <p>Es la misma historia para una capa <code>nn.Conv2d()</code>.</p> <p>Excepto que en lugar de simplemente tomar el m\u00e1ximo, <code>nn.Conv2d()</code> realiza una operaci\u00f3n convolucional en los datos (vea esto en acci\u00f3n en la p\u00e1gina web de CNN Explicador).</p> <p>Ejercicio: \u00bfQu\u00e9 crees que hace la capa <code>nn.AvgPool2d()</code>? Intente hacer un tensor aleatorio como hicimos arriba y p\u00e1selo. Verifique las formas de entrada y salida, as\u00ed como los valores de entrada y salida.</p> <p>Extracurricular: Busque \"redes neuronales convolucionales m\u00e1s comunes\", \u00bfqu\u00e9 arquitecturas encuentra? \u00bfAlguno de ellos est\u00e1 contenido en la biblioteca <code>torchvision.models</code>? \u00bfQu\u00e9 crees que podr\u00edas hacer con estos?</p> In\u00a0[\u00a0]: Copied! <pre># P\u00e9rdida de configuraci\u00f3n y optimizador.\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n</pre> # P\u00e9rdida de configuraci\u00f3n y optimizador. loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_2.parameters(),                               lr=0.1) In\u00a0[\u00a0]: Copied! <pre>torch.manual_seed(42)\n\n# Medir el tiempo\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# Modelo de entrenamiento y prueba.\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n</pre> torch.manual_seed(42)  # Medir el tiempo from timeit import default_timer as timer train_time_start_model_2 = timer()  # Modelo de entrenamiento y prueba. epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_2,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn,         device=device     )     test_step(data_loader=test_dataloader,         model=model_2,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn,         device=device     )  train_time_end_model_2 = timer() total_train_time_model_2 = print_train_time(start=train_time_start_model_2,                                            end=train_time_end_model_2,                                            device=device) <p>\u00a1Guau! Parece que las capas convolucional y de agrupaci\u00f3n m\u00e1xima ayudaron a mejorar un poco el rendimiento.</p> <p>Evaluemos los resultados de <code>model_2</code> con nuestra funci\u00f3n <code>eval_model()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Obtener resultados del modelo_2\nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results\n</pre> # Obtener resultados del modelo_2 model_2_results = eval_model(     model=model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn ) model_2_results In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n</pre> import pandas as pd compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results]) compare_results <p>\u00a1Lindo!</p> <p>Tambi\u00e9n podemos agregar los valores del tiempo de entrenamiento.</p> In\u00a0[\u00a0]: Copied! <pre># A\u00f1adir tiempos de entrenamiento a la comparaci\u00f3n de resultados\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n</pre> # A\u00f1adir tiempos de entrenamiento a la comparaci\u00f3n de resultados compare_results[\"training_time\"] = [total_train_time_model_0,                                     total_train_time_model_1,                                     total_train_time_model_2] compare_results <p>Parece que nuestro modelo CNN (<code>FashionMNISTModelV2</code>) tuvo el mejor rendimiento (p\u00e9rdida m\u00e1s baja, mayor precisi\u00f3n) pero tuvo el tiempo de entrenamiento m\u00e1s largo.</p> <p>Y nuestro modelo de referencia (<code>FashionMNISTModelV0</code>) funcion\u00f3 mejor que <code>model_1</code> (<code>FashionMNISTModelV1</code>).</p> In\u00a0[\u00a0]: Copied! <pre># Visualice los resultados de nuestro modelo.\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");\n</pre> # Visualice los resultados de nuestro modelo. compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\") plt.xlabel(\"accuracy (%)\") plt.ylabel(\"model\"); In\u00a0[\u00a0]: Copied! <pre>def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n</pre> def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):     pred_probs = []     model.eval()     with torch.inference_mode():         for sample in data:             # Prepare sample             sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device              # Forward pass (model outputs raw logit)             pred_logit = model(sample)              # Get prediction probability (logit -&gt; prediction probability)             pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)              # Get pred_prob off GPU for further calculations             pred_probs.append(pred_prob.cpu())                  # Stack the pred_probs to turn list into a tensor     return torch.stack(pred_probs) In\u00a0[\u00a0]: Copied! <pre>import random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# Ver la forma y la etiqueta de la primera muestra de prueba\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n</pre> import random random.seed(42) test_samples = [] test_labels = [] for sample, label in random.sample(list(test_data), k=9):     test_samples.append(sample)     test_labels.append(label)  # Ver la forma y la etiqueta de la primera muestra de prueba print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\") In\u00a0[\u00a0]: Copied! <pre># Haga predicciones sobre muestras de prueba con el modelo 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# Ver la lista de las dos primeras probabilidades de predicci\u00f3n\npred_probs[:2]\n</pre> # Haga predicciones sobre muestras de prueba con el modelo 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # Ver la lista de las dos primeras probabilidades de predicci\u00f3n pred_probs[:2] <p>Y ahora podemos usar nuestra funci\u00f3n <code>make_predictions()</code> para predecir en <code>test_samples</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Haga predicciones sobre muestras de prueba con el modelo 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# Ver la lista de las dos primeras probabilidades de predicci\u00f3n\npred_probs[:2]\n</pre> # Haga predicciones sobre muestras de prueba con el modelo 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # Ver la lista de las dos primeras probabilidades de predicci\u00f3n pred_probs[:2] <p>\u00a1Excelente!</p> <p>Y ahora podemos pasar de probabilidades de predicci\u00f3n a etiquetas de predicci\u00f3n tomando el <code>torch.argmax()</code> de la salida de la funci\u00f3n de activaci\u00f3n <code>torch.softmax()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Convierta las probabilidades de predicci\u00f3n en etiquetas de predicci\u00f3n tomando argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n</pre> # Convierta las probabilidades de predicci\u00f3n en etiquetas de predicci\u00f3n tomando argmax() pred_classes = pred_probs.argmax(dim=1) pred_classes In\u00a0[\u00a0]: Copied! <pre># \u00bfNuestras predicciones tienen la misma forma que nuestras etiquetas de prueba?\ntest_labels, pred_classes\n</pre> # \u00bfNuestras predicciones tienen la misma forma que nuestras etiquetas de prueba? test_labels, pred_classes <p>Ahora nuestras clases previstas tienen el mismo formato que nuestras etiquetas de prueba, podemos comparar.</p> <p>Dado que estamos tratando con datos de im\u00e1genes, seamos fieles al lema del explorador de datos.</p> <p>\"\u00a1Visualiza, visualiza, visualiza!\"</p> In\u00a0[\u00a0]: Copied! <pre># Predicciones de la trama\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n</pre> # Predicciones de la trama plt.figure(figsize=(9, 9)) nrows = 3 ncols = 3 for i, sample in enumerate(test_samples):   # Create a subplot   plt.subplot(nrows, ncols, i+1)    # Plot the target image   plt.imshow(sample.squeeze(), cmap=\"gray\")    # Find the prediction label (in text form, e.g. \"Sandal\")   pred_label = class_names[pred_classes[i]]    # Get the truth label (in text form, e.g. \"T-shirt\")   truth_label = class_names[test_labels[i]]     # Create the title text of the plot   title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"      # Check for equality and change title colour accordingly   if pred_label == truth_label:       plt.title(title_text, fontsize=10, c=\"g\") # green text if correct   else:       plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong   plt.axis(False); <p>Bueno, bueno, bueno, \u00bfno se ve bien?</p> <p>\u00a1Nada mal para un par de docenas de l\u00edneas de c\u00f3digo PyTorch!</p> In\u00a0[\u00a0]: Copied! <pre># Importar tqdm para la barra de progreso\nfrom tqdm.auto import tqdm\n\n# 1.Hacer predicciones con un modelo entrenado.\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenar lista de predicciones en un tensor\ny_pred_tensor = torch.cat(y_preds)\n</pre> # Importar tqdm para la barra de progreso from tqdm.auto import tqdm  # 1.Hacer predicciones con un modelo entrenado. y_preds = [] model_2.eval() with torch.inference_mode():   for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):     # Send data and targets to target device     X, y = X.to(device), y.to(device)     # Do the forward pass     y_logit = model_2(X)     # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels     y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)     # Put predictions on CPU for evaluation     y_preds.append(y_pred.cpu()) # Concatenar lista de predicciones en un tensor y_pred_tensor = torch.cat(y_preds) <p>\u00a1Maravilloso!</p> <p>Ahora que tenemos predicciones, veamos los pasos 2 y 3: 2. Haga una matriz de confusi\u00f3n usando <code>torchmetrics.ConfusionMatrix</code>. 3. Trace la matriz de confusi\u00f3n usando <code>mlxtend.plotting.plot_confusion_matrix()</code>.</p> <p>Primero necesitaremos asegurarnos de tener instalados <code>torchmetrics</code> y <code>mlxtend</code> (estas dos bibliotecas nos ayudar\u00e1n a crear y visualizar una matriz de confusi\u00f3n).</p> <p>Nota: Si est\u00e1 utilizando Google Colab, la versi\u00f3n predeterminada de <code>mlxtend</code> instalada es 0.14.0 (a partir de marzo de 2022); sin embargo, para los par\u00e1metros de la funci\u00f3n <code>plot_confusion_matrix()</code> Como uso, necesitamos 0.19.0 o superior.</p> In\u00a0[\u00a0]: Copied! <pre># Vea si existe torchmetrics, si no, inst\u00e1lelo\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n</pre> # Vea si existe torchmetrics, si no, inst\u00e1lelo try:     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\")     assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\" except:     !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\") <p>Para trazar la matriz de confusi\u00f3n, debemos asegurarnos de tener una versi\u00f3n <code>mlxtend</code> de 0.19.0 o superior.</p> In\u00a0[\u00a0]: Copied! <pre># Importar versi\u00f3n actualizada de mlxtend\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n</pre> # Importar versi\u00f3n actualizada de mlxtend import mlxtend  print(mlxtend.__version__) assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher <p><code>torchmetrics</code> y <code>mlxtend</code> instalados, \u00a1hagamos una matriz de confusi\u00f3n!</p> <p>Primero crearemos una instancia <code>torchmetrics.ConfusionMatrix</code> dici\u00e9ndole con cu\u00e1ntas clases estamos tratando configurando <code>num_classes=len(class_names)</code>.</p> <p>Luego crearemos una matriz de confusi\u00f3n (en formato tensorial) pasando a nuestra instancia las predicciones de nuestro modelo (<code>preds=y_pred_tensor</code>) y los objetivos (<code>target=test_data.targets</code>).</p> <p>Finalmente podemos trazar nuestra matriz de configuraci\u00f3n usando la funci\u00f3n <code>plot_confusion_matrix()</code> de <code>mlxtend.plotting</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Configure una instancia de matriz de confusi\u00f3n y compare las predicciones con los objetivos.\nconfmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Traza la matriz de confusi\u00f3n\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);\n</pre> from torchmetrics import ConfusionMatrix from mlxtend.plotting import plot_confusion_matrix  # 2. Configure una instancia de matriz de confusi\u00f3n y compare las predicciones con los objetivos. confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass') confmat_tensor = confmat(preds=y_pred_tensor,                          target=test_data.targets)  # 3. Traza la matriz de confusi\u00f3n fig, ax = plot_confusion_matrix(     conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy      class_names=class_names, # turn the row and column labels into class names     figsize=(10, 7) ); <p>\u00a1Guau! \u00bfNo se ve bien?</p> <p>Podemos ver que nuestro modelo funciona bastante bien ya que la mayor\u00eda de los cuadrados oscuros est\u00e1n en la diagonal desde la parte superior izquierda hasta la inferior derecha (y el modelo ideal solo tendr\u00e1 valores en estos cuadrados y 0 en el resto).</p> <p>El modelo se \"confunde\" m\u00e1s en clases que son similares, por ejemplo, prediciendo \"Pullover\" para im\u00e1genes que en realidad est\u00e1n etiquetadas como \"Camisa\".</p> <p>Y lo mismo para predecir \"Camisa\" para clases que en realidad est\u00e1n etiquetadas como \"Camiseta/top\".</p> <p>Este tipo de informaci\u00f3n suele ser m\u00e1s \u00fatil que una \u00fanica m\u00e9trica de precisi\u00f3n porque indica al usuario d\u00f3nde un modelo est\u00e1 haciendo las cosas mal.</p> <p>Tambi\u00e9n da pistas de por qu\u00e9 el modelo puede estar haciendo ciertas cosas mal.</p> <p>Es comprensible que el modelo a veces prediga \"Camisa\" para im\u00e1genes etiquetadas como \"Camiseta/top\".</p> <p>Podemos utilizar este tipo de informaci\u00f3n para inspeccionar m\u00e1s a fondo nuestros modelos y datos y ver c\u00f3mo podr\u00edan mejorarse.</p> <p>Ejercicio: Utilice el <code>model_2</code> entrenado para hacer predicciones en el conjunto de datos de prueba FashionMNIST. Luego, traza algunas predicciones en las que el modelo se equivoc\u00f3 junto con cu\u00e1l deber\u00eda haber sido la etiqueta de la imagen. Despu\u00e9s de visualizar estas predicciones, \u00bfcrees que se trata m\u00e1s de un error de modelado o de un error de datos? Por ejemplo, \u00bfpodr\u00eda funcionar mejor el modelo o las etiquetas de los datos est\u00e1n demasiado cerca entre s\u00ed (por ejemplo, una etiqueta de \"Camisa\" est\u00e1 demasiado cerca de \"Camiseta/top\")?</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Cree el directorio de modelos (si a\u00fan no existe), consulte: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # create parent directories if needed\n                 exist_ok=True # if models directory already exists, don't error\n)\n\n# Crear ruta para guardar el modelo\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Guarde el dictado del estado del modelo\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # Cree el directorio de modelos (si a\u00fan no existe), consulte: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, # create parent directories if needed                  exist_ok=True # if models directory already exists, don't error )  # Crear ruta para guardar el modelo MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # Guarde el dictado del estado del modelo print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters            f=MODEL_SAVE_PATH) <p>Ahora que tenemos un modelo guardado <code>state_dict()</code> podemos volver a cargarlo usando una combinaci\u00f3n de <code>load_state_dict()</code> y <code>torch.load()</code>.</p> <p>Como estamos usando <code>load_state_dict()</code>, necesitaremos crear una nueva instancia de <code>FashionMNISTModelV2()</code> con los mismos par\u00e1metros de entrada que nuestro modelo guardado <code>state_dict()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Cree una nueva instancia de FashionMNISTModelV2 (la misma clase que nuestro state_dict() guardado)\n# Nota: al cargar el modelo se producir\u00e1 un error si las formas aqu\u00ed no son las mismas que las de la versi\u00f3n guardada.\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # try changing this to 128 and seeing what happens \n                                    output_shape=10) \n\n# Cargar en el state_dict() guardado\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# Enviar modelo a GPU\nloaded_model_2 = loaded_model_2.to(device)\n</pre> # Cree una nueva instancia de FashionMNISTModelV2 (la misma clase que nuestro state_dict() guardado) # Nota: al cargar el modelo se producir\u00e1 un error si las formas aqu\u00ed no son las mismas que las de la versi\u00f3n guardada. loaded_model_2 = FashionMNISTModelV2(input_shape=1,                                      hidden_units=10, # try changing this to 128 and seeing what happens                                      output_shape=10)   # Cargar en el state_dict() guardado loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))  # Enviar modelo a GPU loaded_model_2 = loaded_model_2.to(device) <p>Y ahora que tenemos un modelo cargado, podemos evaluarlo con <code>eval_model()</code> para asegurarnos de que sus par\u00e1metros funcionen de manera similar a <code>model_2</code> antes de guardarlo.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluar modelo cargado\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n</pre> # Evaluar modelo cargado torch.manual_seed(42)  loaded_model_2_results = eval_model(     model=loaded_model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn )  loaded_model_2_results <p>\u00bfEstos resultados tienen el mismo aspecto que <code>model_2_results</code>?</p> In\u00a0[\u00a0]: Copied! <pre>model_2_results\n</pre> model_2_results <p>Podemos averiguar si dos tensores est\u00e1n cerca uno del otro usando <code>torch.isclose()</code> y pasando un nivel de tolerancia de cercan\u00eda a trav\u00e9s de los par\u00e1metros <code>atol</code> (tolerancia absoluta) y <code>rtol</code> (tolerancia relativa).</p> <p>Si los resultados de nuestro modelo son similares, la salida de <code>torch.isclose()</code> deber\u00eda ser verdadera.</p> In\u00a0[\u00a0]: Copied! <pre># Comprueba si los resultados est\u00e1n cerca uno del otro (si est\u00e1n muy lejos puede haber un error)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # absolute tolerance\n              rtol=0.0001) # relative tolerance\n</pre> # Comprueba si los resultados est\u00e1n cerca uno del otro (si est\u00e1n muy lejos puede haber un error) torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),                torch.tensor(loaded_model_2_results[\"model_loss\"]),               atol=1e-08, # absolute tolerance               rtol=0.0001) # relative tolerance"},{"location":"09-03_pytorch_computer_vision/#03-vision-por-computadora-pytorch","title":"03. Visi\u00f3n por computadora PyTorch\u00b6","text":"<p>Visi\u00f3n por computadora es el arte de ense\u00f1arle a ver a una computadora.</p> <p>Por ejemplo, podr\u00eda implicar la construcci\u00f3n de un modelo para clasificar si una foto es de un gato o de un perro (clasificaci\u00f3n binaria).</p> <p>O si una foto es de un gato, un perro o una gallina (clasificaci\u00f3n multiclase).</p> <p>O identificar d\u00f3nde aparece un autom\u00f3vil en un cuadro de video (detecci\u00f3n de objetos).</p> <p>O descubrir d\u00f3nde se pueden separar los diferentes objetos de una imagen (segmentaci\u00f3n pan\u00f3ptica).</p> <p> Ejemplos de problemas de visi\u00f3n por computadora para clasificaci\u00f3n binaria, clasificaci\u00f3n multiclase, detecci\u00f3n y segmentaci\u00f3n de objetos.</p>"},{"location":"09-03_pytorch_computer_vision/#donde-se-utiliza-la-vision-por-computadora","title":"\u00bfD\u00f3nde se utiliza la visi\u00f3n por computadora?\u00b6","text":"<p>Si usa un tel\u00e9fono inteligente, ya ha utilizado la visi\u00f3n por computadora.</p> <p>Las aplicaciones de c\u00e1mara y fotograf\u00eda utilizan visi\u00f3n por computadora para mejorar y ordenan im\u00e1genes.</p> <p>Los autom\u00f3viles modernos utilizan visi\u00f3n por computadora para evitar otros autom\u00f3viles y mantenerse dentro de las l\u00edneas de los carriles.</p> <p>Los fabricantes utilizan la visi\u00f3n por computadora para identificar defectos en varios productos.</p> <p>Las c\u00e1maras de seguridad utilizan visi\u00f3n por computadora para detectar posibles intrusos.</p> <p>En esencia, cualquier cosa que pueda describirse en un sentido visual puede ser un posible problema de visi\u00f3n por computadora.</p>"},{"location":"09-03_pytorch_computer_vision/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>Aplicaremos el flujo de trabajo de PyTorch que hemos estado aprendiendo en las \u00faltimas secciones a la visi\u00f3n por computadora.</p> <p></p> <p>Espec\u00edficamente, cubriremos:</p> Tema Contenido 0. Bibliotecas de visi\u00f3n por computadora en PyTorch PyTorch tiene un mont\u00f3n de bibliotecas de visi\u00f3n por computadora \u00fatiles integradas, ech\u00e9mosle un vistazo. 1. Cargar datos Para practicar la visi\u00f3n por computadora, comenzaremos con algunas im\u00e1genes de diferentes prendas de vestir de [FashionMNIST] (https://github.com/zalandoresearch/fashion-mnist). 2. Preparar datos Tenemos algunas im\u00e1genes, cargu\u00e9moslas con un PyTorch <code>DataLoader</code> para que podamos usarlas con nuestro bucle de entrenamiento. 3. Modelo 0: construcci\u00f3n de un modelo de referencia Aqu\u00ed crearemos un modelo de clasificaci\u00f3n de m\u00faltiples clases para aprender patrones en los datos, tambi\u00e9n elegiremos una funci\u00f3n de p\u00e9rdida, un optimizador y crearemos un bucle de entrenamiento. 4. Hacer predicciones y evaluar el modelo 0 Hagamos algunas predicciones con nuestro modelo de referencia y eval\u00fa\u00e9moslas. 5. Configurar c\u00f3digo independiente del dispositivo para modelos futuros Es una buena pr\u00e1ctica escribir c\u00f3digo independiente del dispositivo, as\u00ed que configur\u00e9moslo. 6. Modelo 1: Agregar no linealidad Experimentar es una gran parte del aprendizaje autom\u00e1tico. Intentemos mejorar nuestro modelo de referencia agregando capas no lineales. 7. Modelo 2: Red neuronal convolucional (CNN) Es hora de especificar la visi\u00f3n por computadora e introducir la poderosa arquitectura de red neuronal convolucional. 8. Comparando nuestros modelos Hemos construido tres modelos diferentes, compar\u00e9moslos. 9. Evaluando nuestro mejor modelo Hagamos algunas predicciones sobre im\u00e1genes aleatorias y evaluemos nuestro mejor modelo. 10. Haciendo una matriz de confusi\u00f3n Una matriz de confusi\u00f3n es una excelente manera de evaluar un modelo de clasificaci\u00f3n; veamos c\u00f3mo podemos crear una. 11. Guardar y cargar el modelo con mejor rendimiento Dado que es posible que queramos usar nuestro modelo para m\u00e1s adelante, guard\u00e9moslo y asegur\u00e9monos de que se vuelva a cargar correctamente."},{"location":"09-03_pytorch_computer_vision/#donde-puedes-conseguir-ayuda","title":"\u00bfD\u00f3nde puedes conseguir ayuda?\u00b6","text":"<p>Todos los materiales de este curso en vivo en GitHub.</p> <p>Si tiene problemas, tambi\u00e9n puede hacer una pregunta en el curso p\u00e1gina de debates de GitHub.</p> <p>Y, por supuesto, est\u00e1 la documentaci\u00f3n de PyTorch y los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"09-03_pytorch_computer_vision/#0-bibliotecas-de-vision-por-computadora-en-pytorch","title":"0. Bibliotecas de visi\u00f3n por computadora en PyTorch\u00b6","text":"<p>Antes de comenzar a escribir c\u00f3digo, hablemos de algunas bibliotecas de visi\u00f3n por computadora de PyTorch que debe conocer.</p> M\u00f3dulo PyTorch \u00bfQu\u00e9 hace? <code>torchvision</code> Contiene conjuntos de datos, arquitecturas de modelos y transformaciones de im\u00e1genes que se utilizan a menudo para problemas de visi\u00f3n por computadora. <code>torchvision.datasets</code> Aqu\u00ed encontrar\u00e1 muchos conjuntos de datos de visi\u00f3n por computadora de ejemplo para una variedad de problemas, desde clasificaci\u00f3n de im\u00e1genes, detecci\u00f3n de objetos, subt\u00edtulos de im\u00e1genes, clasificaci\u00f3n de videos y m\u00e1s. Tambi\u00e9n contiene una serie de clases base para crear conjuntos de datos personalizados. <code>torchvision.models</code> Este m\u00f3dulo contiene arquitecturas de modelos de visi\u00f3n por computadora de buen rendimiento y de uso com\u00fan implementadas en PyTorch; puede usarlas con sus propios problemas. <code>torchvision.transforms</code> A menudo, las im\u00e1genes deben transformarse (convertirse en n\u00fameros/procesarse/aumentarse) antes de usarse con un modelo; las transformaciones de im\u00e1genes comunes se encuentran aqu\u00ed. <code>torch.utils.data.Dataset</code> Clase de conjunto de datos base para PyTorch. <code>torch.utils.data.DataLoader</code> Crea un iterable de Python sobre un conjunto de datos (creado con <code>torch.utils.data.Dataset</code>). <p>Nota: Las clases <code>torch.utils.data.Dataset</code> y <code>torch.utils.data.DataLoader</code> no son solo para visi\u00f3n por computadora en PyTorch, sino que son capaces de manejar muchos tipos diferentes de datos.</p> <p>Ahora que hemos cubierto algunas de las bibliotecas de visi\u00f3n por computadora de PyTorch m\u00e1s importantes, importemos las dependencias relevantes.</p>"},{"location":"09-03_pytorch_computer_vision/#1-obtener-un-conjunto-de-datos","title":"1. Obtener un conjunto de datos\u00b6","text":"<p>Para comenzar a trabajar en un problema de visi\u00f3n por computadora, obtengamos un conjunto de datos de visi\u00f3n por computadora.</p> <p>Vamos a empezar con FashionMNIST.</p> <p>MNIST significa Instituto Nacional Modificado de Est\u00e1ndares y Tecnolog\u00eda.</p> <p>El [conjunto de datos MNIST original] (https://en.wikipedia.org/wiki/MNIST_database) contiene miles de ejemplos de d\u00edgitos escritos a mano (del 0 al 9) y se utiliz\u00f3 para crear modelos de visi\u00f3n por computadora para identificar n\u00fameros para los servicios postales.</p> <p>FashionMNIST, creado por Zalando Research, es una configuraci\u00f3n similar.</p> <p>Excepto que contiene im\u00e1genes en escala de grises de 10 tipos diferentes de ropa.</p> <p> <code>torchvision.datasets</code> contiene muchos conjuntos de datos de ejemplo que puedes usar para practicar la escritura de c\u00f3digo de visi\u00f3n por computadora. FashionMNIST es uno de esos conjuntos de datos. Y dado que tiene 10 clases de im\u00e1genes diferentes (diferentes tipos de ropa), es un problema de clasificaci\u00f3n de clases m\u00faltiples.</p> <p>M\u00e1s adelante, construiremos una red neuronal de visi\u00f3n por computadora para identificar los diferentes estilos de ropa en estas im\u00e1genes.</p> <p>PyTorch tiene un mont\u00f3n de conjuntos de datos de visi\u00f3n por computadora comunes almacenados en \"torchvision.datasets\".</p> <p>Incluyendo FashionMNIST en <code>torchvision.datasets.FashionMNIST()</code>.</p> <p>Para descargarlo, proporcionamos los siguientes par\u00e1metros:</p> <ul> <li><code>root: str</code> - \u00bfa qu\u00e9 carpeta desea descargar los datos?</li> <li><code>train: Bool</code> - \u00bfQuieres dividir el entrenamiento o la prueba?</li> <li><code>descargar: Bool</code> - \u00bfdeben descargarse los datos?</li> <li><code>transform: torchvision.transforms</code>: \u00bfqu\u00e9 transformaciones le gustar\u00eda realizar en los datos?</li> <li><code>target_transform</code>: tambi\u00e9n puedes transformar los objetivos (etiquetas) si lo deseas.</li> </ul> <p>Muchos otros conjuntos de datos en \"torchvision\" tienen estas opciones de par\u00e1metros.</p>"},{"location":"09-03_pytorch_computer_vision/#11-formas-de-entrada-y-salida-de-un-modelo-de-vision-por-computadora","title":"1.1 Formas de entrada y salida de un modelo de visi\u00f3n por computadora\u00b6","text":"<p>Tenemos un gran tensor de valores (la imagen) que conduce a un valor \u00fanico para el objetivo (la etiqueta).</p> <p>Veamos la forma de la imagen.</p>"},{"location":"09-03_pytorch_computer_vision/#12-visualizando-nuestros-datos","title":"1.2 Visualizando nuestros datos\u00b6","text":""},{"location":"09-03_pytorch_computer_vision/#2-preparar-el-cargador-de-datos","title":"2. Preparar el cargador de datos\u00b6","text":"<p>Ahora tenemos un conjunto de datos listo para funcionar.</p> <p>El siguiente paso es prepararlo con un <code>torch.utils.data.DataLoader</code> o <code>DataLoader</code> para corto.</p> <p>El <code>DataLoader</code> hace lo que usted cree que podr\u00eda hacer.</p> <p>Ayuda a cargar datos en un modelo.</p> <p>Para entrenamiento y para inferencia.</p> <p>Convierte un gran \"conjunto de datos\" en un Python iterable de fragmentos m\u00e1s peque\u00f1os.</p> <p>Estos fragmentos m\u00e1s peque\u00f1os se denominan lotes o minilotes y se pueden configurar mediante el par\u00e1metro <code>batch_size</code>.</p> <p>\u00bfPor qu\u00e9 hacer esto?</p> <p>Porque es m\u00e1s eficiente computacionalmente.</p> <p>En un mundo ideal, podr\u00eda realizar el pase hacia adelante y hacia atr\u00e1s a trav\u00e9s de todos sus datos a la vez.</p> <p>Pero una vez que empiezas a utilizar conjuntos de datos realmente grandes, a menos que tengas una potencia inform\u00e1tica infinita, es m\u00e1s f\u00e1cil dividirlos en lotes.</p> <p>Tambi\u00e9n le brinda a su modelo m\u00e1s oportunidades de mejorar.</p> <p>Con minilotes (peque\u00f1as porciones de datos), el descenso de gradiente se realiza con m\u00e1s frecuencia por \u00e9poca (una vez por minilote en lugar de una vez por \u00e9poca).</p> <p>\u00bfCu\u00e1l es un buen tama\u00f1o de lote?</p> <p>32 es un buen lugar para comenzar para una buena cantidad de problemas.</p> <p>Pero dado que este es un valor que puede establecer (un hiperpar\u00e1metro), puede probar todos los tipos diferentes de valores, aunque generalmente se usan potencias de 2 con mayor frecuencia (por ejemplo, 32, 64, 128, 256, 512).</p> <p> Lote de FashionMNIST con un tama\u00f1o de lote de 32 y reproducci\u00f3n aleatoria activada. Se producir\u00e1 un proceso de procesamiento por lotes similar para otros conjuntos de datos, pero diferir\u00e1 seg\u00fan el tama\u00f1o del lote.</p> <p>Creemos <code>DataLoader</code> para nuestros conjuntos de entrenamiento y prueba.</p>"},{"location":"09-03_pytorch_computer_vision/#3-modelo-0-construir-un-modelo-de-referencia","title":"3. Modelo 0: construir un modelo de referencia\u00b6","text":"<p>\u00a1Datos cargados y preparados!</p> <p>Es hora de crear un modelo de referencia subclasificando <code>nn.Module</code>.</p> <p>Un modelo de referencia es uno de los modelos m\u00e1s simples que puedas imaginar.</p> <p>Utiliza la l\u00ednea de base como punto de partida e intenta mejorarla con modelos posteriores m\u00e1s complicados.</p> <p>Nuestra l\u00ednea base constar\u00e1 de dos capas <code>nn.Linear()</code>.</p> <p>Hemos hecho esto en una secci\u00f3n anterior, pero habr\u00e1 una peque\u00f1a diferencia.</p> <p>Debido a que estamos trabajando con datos de im\u00e1genes, usaremos una capa diferente para comenzar.</p> <p>Y esa es la capa <code>nn.Flatten()</code>.</p> <p><code>nn.Flatten()</code> comprime las dimensiones de un tensor en un solo vector.</p> <p>Esto es m\u00e1s f\u00e1cil de entender cuando lo ves.</p>"},{"location":"09-03_pytorch_computer_vision/#31-perdida-de-configuracion-optimizador-y-metricas-de-evaluacion","title":"3.1 P\u00e9rdida de configuraci\u00f3n, optimizador y m\u00e9tricas de evaluaci\u00f3n\u00b6","text":"<p>Dado que estamos trabajando en un problema de clasificaci\u00f3n, introduzcamos nuestro [script <code>helper_functions.py</code>] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) y posteriormente el <code>accuracy_fn()</code> lo definimos en [cuaderno 02] (https://www.learnpytorch.io/02_pytorch_classification/).</p> <p>Nota: En lugar de importar y utilizar nuestra propia funci\u00f3n de precisi\u00f3n o m\u00e9trica(s) de evaluaci\u00f3n, puede importar varias m\u00e9tricas de evaluaci\u00f3n desde el paquete TorchMetrics.</p>"},{"location":"09-03_pytorch_computer_vision/#32-creando-una-funcion-para-cronometrar-nuestros-experimentos","title":"3.2 Creando una funci\u00f3n para cronometrar nuestros experimentos\u00b6","text":"<p>\u00a1Funci\u00f3n de p\u00e9rdida y optimizador listos!</p> <p>Es hora de empezar a entrenar un modelo.</p> <p>Pero \u00bfqu\u00e9 tal si hacemos un peque\u00f1o experimento mientras entrenamos?</p> <p>Quiero decir, creemos una funci\u00f3n de sincronizaci\u00f3n para medir el tiempo que le toma a nuestro modelo entrenarse en la CPU en comparaci\u00f3n con usar una GPU.</p> <p>Entrenaremos este modelo en la CPU pero el siguiente en la GPU y veremos qu\u00e9 sucede.</p> <p>Nuestra funci\u00f3n de sincronizaci\u00f3n importar\u00e1 la funci\u00f3n <code>timeit.default_timer()</code> del [m\u00f3dulo <code>timeit</code>](https ://docs.python.org/3/library/timeit.html).</p>"},{"location":"09-03_pytorch_computer_vision/#33-crear-un-bucle-de-entrenamiento-y-entrenar-un-modelo-en-lotes-de-datos","title":"3.3 Crear un bucle de entrenamiento y entrenar un modelo en lotes de datos\u00b6","text":"<p>\u00a1Hermoso!</p> <p>Parece que tenemos todas las piezas del rompecabezas listas para funcionar: un temporizador, una funci\u00f3n de p\u00e9rdida, un optimizador, un modelo y, lo m\u00e1s importante, algunos datos.</p> <p>Ahora creemos un bucle de entrenamiento y un bucle de prueba para entrenar y evaluar nuestro modelo.</p> <p>Usaremos los mismos pasos que en los cuadernos anteriores, aunque como nuestros datos ahora est\u00e1n en forma de lotes, agregaremos otro bucle para recorrer nuestros lotes de datos.</p> <p>Nuestros lotes de datos est\u00e1n contenidos en nuestros <code>DataLoader</code>s, <code>train_dataloader</code> y <code>test_dataloader</code> para las divisiones de datos de entrenamiento y prueba respectivamente.</p> <p>Un lote son muestras <code>BATCH_SIZE</code> de <code>X</code> (caracter\u00edsticas) e <code>y</code> (etiquetas), ya que estamos usando <code>BATCH_SIZE=32</code>, nuestros lotes tienen 32 muestras de im\u00e1genes y objetivos.</p> <p>Y dado que estamos calculando lotes de datos, nuestras m\u00e9tricas de p\u00e9rdida y evaluaci\u00f3n se calcular\u00e1n por lote en lugar de hacerlo en todo el conjunto de datos.</p> <p>Esto significa que tendremos que dividir nuestros valores de p\u00e9rdida y precisi\u00f3n por la cantidad de lotes en el cargador de datos respectivo de cada conjunto de datos.</p> <p>Repas\u00e9moslo:</p> <ol> <li>Recorre \u00e9pocas.</li> <li>Recorra los lotes de entrenamiento, realice los pasos de entrenamiento, calcule la p\u00e9rdida del tren por lote.</li> <li>Recorra los lotes de prueba, realice los pasos de prueba, calcule la p\u00e9rdida de prueba por lote.</li> <li>Imprime lo que est\u00e1 pasando.</li> <li>Calcula el tiempo (por diversi\u00f3n).</li> </ol> <p>Unos cuantos pasos, pero...</p> <p>...en caso de duda, codif\u00edquelo.</p>"},{"location":"09-03_pytorch_computer_vision/#4-haga-predicciones-y-obtenga-resultados-del-modelo-0","title":"4. Haga predicciones y obtenga resultados del Modelo 0\u00b6","text":"<p>Dado que vamos a construir algunos modelos, es una buena idea escribir c\u00f3digo para evaluarlos todos de manera similar.</p> <p>Es decir, creemos una funci\u00f3n que admita un modelo entrenado, un <code>DataLoader</code>, una funci\u00f3n de p\u00e9rdida y una funci\u00f3n de precisi\u00f3n.</p> <p>La funci\u00f3n utilizar\u00e1 el modelo para hacer predicciones sobre los datos en el <code>DataLoader</code> y luego podremos evaluar esas predicciones usando la funci\u00f3n de p\u00e9rdida y la funci\u00f3n de precisi\u00f3n.</p>"},{"location":"09-03_pytorch_computer_vision/#5-configurar-codigo-independiente-del-dispositivo-para-usar-una-gpu-si-la-hay","title":"5. Configurar c\u00f3digo independiente del dispositivo (para usar una GPU, si la hay)\u00b6","text":"<p>Hemos visto cu\u00e1nto tiempo lleva entrenar mi modelo PyTorch en 60.000 muestras en la CPU.</p> <p>Nota: El tiempo de entrenamiento del modelo depende del hardware utilizado. Generalmente, m\u00e1s procesadores significan un entrenamiento m\u00e1s r\u00e1pido y los modelos m\u00e1s peque\u00f1os en conjuntos de datos m\u00e1s peque\u00f1os a menudo se entrenar\u00e1n m\u00e1s r\u00e1pido que los modelos y conjuntos de datos grandes.</p> <p>Ahora configuremos algo de c\u00f3digo independiente del dispositivo para que nuestros modelos y datos se ejecuten en GPU si est\u00e1 disponible.</p> <p>Si est\u00e1 ejecutando esta computadora port\u00e1til en Google Colab y a\u00fan no tiene una GPU encendida, ahora es el momento de encender una a trav\u00e9s de <code>Runtime -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Acelerador de hardware -&gt; GPU</code>. Si hace esto, es probable que su tiempo de ejecuci\u00f3n se reinicie y tendr\u00e1 que ejecutar todas las celdas anteriores yendo a \"Tiempo de ejecuci\u00f3n -&gt; Ejecutar antes\".</p>"},{"location":"09-03_pytorch_computer_vision/#6-modelo-1-construccion-de-un-modelo-mejor-con-no-linealidad","title":"6. Modelo 1: construcci\u00f3n de un modelo mejor con no linealidad\u00b6","text":"<p>Aprendimos sobre [el poder de la no linealidad en el cuaderno 02] (https://www.learnpytorch.io/02_pytorch_classification/#6-the-missing-piece-non-linearity).</p> <p>Viendo los datos con los que hemos estado trabajando, \u00bfcrees que necesitan funciones no lineales?</p> <p>Y recuerda, lineal significa recto y no lineal significa no recto.</p> <p>Vamos a averiguar.</p> <p>Lo haremos recreando un modelo similar al anterior, excepto que esta vez colocaremos funciones no lineales (<code>nn.ReLU()</code>) entre cada capa lineal.</p>"},{"location":"09-03_pytorch_computer_vision/#61-perdida-de-configuracion-optimizador-y-metricas-de-evaluacion","title":"6.1 P\u00e9rdida de configuraci\u00f3n, optimizador y m\u00e9tricas de evaluaci\u00f3n\u00b6","text":"<p>Como de costumbre, configuraremos una funci\u00f3n de p\u00e9rdida, un optimizador y una m\u00e9trica de evaluaci\u00f3n (podr\u00edamos hacer m\u00faltiples m\u00e9tricas de evaluaci\u00f3n, pero por ahora nos limitaremos a la precisi\u00f3n).</p>"},{"location":"09-03_pytorch_computer_vision/#62-funcionalizacion-de-bucles-de-entrenamiento-y-prueba","title":"6.2 Funcionalizaci\u00f3n de bucles de entrenamiento y prueba\u00b6","text":"<p>Hasta ahora hemos estado escribiendo bucles de entrenamiento y prueba una y otra vez.</p> <p>Escrib\u00e1moslos nuevamente pero esta vez los pondremos en funciones para que puedan ser llamados una y otra vez.</p> <p>Y debido a que ahora estamos usando c\u00f3digo independiente del dispositivo, nos aseguraremos de llamar a <code>.to(device)</code> en nuestros tensores de funci\u00f3n (<code>X</code>) y objetivo (<code>y</code>).</p> <p>Para el ciclo de entrenamiento crearemos una funci\u00f3n llamada <code>train_step()</code> que toma un modelo, un <code>DataLoader</code>, una funci\u00f3n de p\u00e9rdida y un optimizador.</p> <p>El ciclo de prueba ser\u00e1 similar pero se llamar\u00e1 <code>test_step()</code> y aceptar\u00e1 un modelo, un <code>DataLoader</code>, una funci\u00f3n de p\u00e9rdida y una funci\u00f3n de evaluaci\u00f3n.</p> <p>Nota: Dado que estas son funciones, puedes personalizarlas como quieras. Lo que estamos creando aqu\u00ed pueden considerarse funciones b\u00e1sicas de entrenamiento y prueba para nuestro caso de uso de clasificaci\u00f3n espec\u00edfico.</p>"},{"location":"09-03_pytorch_computer_vision/#7-modelo-2-construccion-de-una-red-neuronal-convolucional-cnn","title":"7. Modelo 2: Construcci\u00f3n de una red neuronal convolucional (CNN)\u00b6","text":"<p>Muy bien, es hora de dar un paso m\u00e1s.</p> <p>Es hora de crear una [red neuronal convolucional] (https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN o ConvNet).</p> <p>Las CNN son conocidas por sus capacidades para encontrar patrones en datos visuales.</p> <p>Y dado que estamos tratando con datos visuales, veamos si el uso de un modelo CNN puede mejorar nuestra l\u00ednea de base.</p> <p>El modelo de CNN que vamos a utilizar se conoce como TinyVGG del sitio web [CNN Explicador] (https://poloclub.github.io/cnn-explainer/).</p> <p>Sigue la estructura t\u00edpica de una red neuronal convolucional:</p> <p><code>Capa de entrada -&gt; [Capa convolucional -&gt; capa de activaci\u00f3n -&gt; capa de agrupaci\u00f3n] -&gt; Capa de salida</code></p> <p>Donde el contenido de <code>[Capa convolucional -&gt; capa de activaci\u00f3n -&gt; capa de agrupaci\u00f3n]</code> se puede ampliar y repetir varias veces, seg\u00fan los requisitos.</p>"},{"location":"09-03_pytorch_computer_vision/#que-modelo-debo-usar","title":"\u00bfQu\u00e9 modelo debo usar?\u00b6","text":"<p>Pregunta: Espera, dices que las CNN son buenas para im\u00e1genes, \u00bfhay alg\u00fan otro tipo de modelo que deba tener en cuenta?</p> <p>Buena pregunta.</p> <p>Esta tabla es una buena gu\u00eda general sobre qu\u00e9 modelo utilizar (aunque hay excepciones).</p> Tipo de problema Modelo a utilizar (generalmente) Ejemplo de c\u00f3digo Datos estructurados (hojas de c\u00e1lculo Excel, datos de filas y columnas) Modelos mejorados con gradiente, bosques aleatorios, XGBoost <code>sklearn.ensemble</code>, [biblioteca XGBoost](https://xgboost.readthedocs.io/en/ estable/) Datos no estructurados (im\u00e1genes, audio, idioma) Redes Neuronales Convolucionales, Transformadores <code>torchvision.models</code>, Transformadores HuggingFace <p>Nota: La tabla anterior es solo como referencia; el modelo que termine usando depender\u00e1 en gran medida del problema en el que est\u00e9 trabajando y de las limitaciones que tenga (cantidad de datos, requisitos de latencia).</p> <p>Basta de hablar de modelos, ahora construyamos una CNN que replique el modelo en el [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/).</p> <p></p> <p>Para hacerlo, aprovecharemos <code>nn.Conv2d()</code> y <code>nn.MaxPool2d()</code>  capas de <code>torch.nn</code>.</p>"},{"location":"09-03_pytorch_computer_vision/#71-paso-a-paso-por-nnconv2d","title":"7.1 Paso a paso por <code>nn.Conv2d()</code>\u00b6","text":"<p>Podr\u00edamos comenzar a usar nuestro modelo anterior y ver qu\u00e9 sucede, pero primero veamos las dos nuevas capas que hemos agregado:</p> <ul> <li><code>nn.Conv2d()</code>, tambi\u00e9n conocida como capa convolucional.</li> <li><code>nn.MaxPool2d()</code>, tambi\u00e9n conocida como capa de agrupaci\u00f3n m\u00e1xima.</li> </ul> <p>Pregunta: \u00bfQu\u00e9 significa \"2d\" en <code>nn.Conv2d()</code>?</p> <p>El 2d es para datos bidimensionales. Como en, nuestras im\u00e1genes tienen dos dimensiones: alto y ancho. S\u00ed, hay una dimensi\u00f3n del canal de color, pero cada una de las dimensiones del canal de color tambi\u00e9n tiene dos dimensiones: alto y ancho.</p> <p>Para otros datos dimensionales (como 1D para texto o 3D para objetos 3D) tambi\u00e9n est\u00e1n <code>nn.Conv1d()</code> y <code>nn.Conv3d()</code>.</p> <p>Para probar las capas, creemos algunos datos de juguetes similares a los datos utilizados en CNN Explicador.</p>"},{"location":"09-03_pytorch_computer_vision/#72-paso-a-paso-por-nnmaxpool2d","title":"7.2 Paso a paso por <code>nn.MaxPool2d()</code>\u00b6","text":"<p>Ahora veamos qu\u00e9 sucede cuando movemos datos a trav\u00e9s de <code>nn.MaxPool2d()</code>.</p>"},{"location":"09-03_pytorch_computer_vision/#73-configurar-una-funcion-de-perdida-y-un-optimizador-para-model_2","title":"7.3 Configurar una funci\u00f3n de p\u00e9rdida y un optimizador para <code>model_2</code>\u00b6","text":"<p>Hemos recorrido suficientes capas en nuestra primera CNN.</p> <p>Pero recuerde, si algo a\u00fan no est\u00e1 claro, intente empezar poco a poco.</p> <p>Elija una sola capa de un modelo, pase algunos datos a trav\u00e9s de ella y vea qu\u00e9 sucede.</p> <p>\u00a1Ahora es el momento de seguir adelante y ponerse a entrenar!</p> <p>Configuremos una funci\u00f3n de p\u00e9rdida y un optimizador.</p> <p>Usaremos las funciones como antes, <code>nn.CrossEntropyLoss()</code> como funci\u00f3n de p\u00e9rdida (ya que estamos trabajando con datos de clasificaci\u00f3n de m\u00faltiples clases).</p> <p>Y <code>torch.optim.SGD()</code> como optimizador para optimizar <code>model_2.parameters()</code> con una tasa de aprendizaje de <code>0.1</code>.</p>"},{"location":"09-03_pytorch_computer_vision/#74-entrenamiento-y-prueba-model_2-usando-nuestras-funciones-de-entrenamiento-y-prueba","title":"7.4 Entrenamiento y prueba <code>model_2</code> usando nuestras funciones de entrenamiento y prueba\u00b6","text":"<p>\u00a1P\u00e9rdida y optimizador listos!</p> <p>Es hora de entrenar y probar.</p> <p>Usaremos nuestras funciones <code>train_step()</code> y <code>test_step()</code> que creamos antes.</p> <p>Tambi\u00e9n mediremos el tiempo para compararlo con nuestros otros modelos.</p>"},{"location":"09-03_pytorch_computer_vision/#8-compare-los-resultados-del-modelo-y-el-tiempo-de-entrenamiento","title":"8. Compare los resultados del modelo y el tiempo de entrenamiento\u00b6","text":"<p>Hemos entrenado tres modelos diferentes.</p> <ol> <li><code>model_0</code>: nuestro modelo de referencia con dos capas <code>nn.Linear()</code>.</li> <li><code>model_1</code>: la misma configuraci\u00f3n que nuestro modelo de referencia, excepto con capas <code>nn.ReLU()</code> entre las capas <code>nn.Linear()</code>.</li> <li><code>model_2</code>: nuestro primer modelo de CNN que imita la arquitectura TinyVGG en el sitio web CNN Explicador.</li> </ol> <p>Esta es una pr\u00e1ctica habitual en el aprendizaje autom\u00e1tico.</p> <p>Construya m\u00faltiples modelos y realice m\u00faltiples experimentos de entrenamiento para ver cu\u00e1l funciona mejor.</p> <p>Combinemos los diccionarios de resultados de nuestros modelos en un DataFrame y averig\u00fc\u00e9moslo.</p>"},{"location":"09-03_pytorch_computer_vision/#compensacion-rendimiento-velocidad","title":"Compensaci\u00f3n rendimiento-velocidad\u00b6","text":"<p>Algo a tener en cuenta en el aprendizaje autom\u00e1tico es la compensaci\u00f3n rendimiento-velocidad.</p> <p>Generalmente, se obtiene un mejor rendimiento con un modelo m\u00e1s grande y complejo (como hicimos con <code>model_2</code>).</p> <p>Sin embargo, este aumento del rendimiento a menudo se produce sacrificando la velocidad de entrenamiento y la velocidad de inferencia.</p> <p>Nota: Los tiempos de capacitaci\u00f3n que obtenga depender\u00e1n en gran medida del hardware que utilice.</p> <p>Generalmente, cuantos m\u00e1s n\u00facleos de CPU tenga, m\u00e1s r\u00e1pido se entrenar\u00e1n sus modelos en la CPU. Y similar para las GPU.</p> <p>El hardware m\u00e1s nuevo (en t\u00e9rminos de antig\u00fcedad) tambi\u00e9n suele entrenar modelos m\u00e1s r\u00e1pido debido a la incorporaci\u00f3n de avances tecnol\u00f3gicos.</p> <p>\u00bfQu\u00e9 tal si nos volvemos visuales?</p>"},{"location":"09-03_pytorch_computer_vision/#9-realice-y-evalue-predicciones-aleatorias-con-el-mejor-modelo","title":"9. Realice y eval\u00fae predicciones aleatorias con el mejor modelo.\u00b6","text":"<p>Muy bien, hemos comparado nuestros modelos entre s\u00ed, evaluemos m\u00e1s a fondo nuestro modelo de mejor rendimiento, \"model_2\".</p> <p>Para hacerlo, creemos una funci\u00f3n <code>make_predictions()</code> donde podemos pasar el modelo y algunos datos para que prediga.</p>"},{"location":"09-03_pytorch_computer_vision/#10-hacer-una-matriz-de-confusion-para-una-evaluacion-de-prediccion-adicional","title":"10. Hacer una matriz de confusi\u00f3n para una evaluaci\u00f3n de predicci\u00f3n adicional\u00b6","text":"<p>Hay muchas m\u00e9tricas de evaluaci\u00f3n diferentes que podemos usar para problemas de clasificaci\u00f3n.</p> <p>Uno de los m\u00e1s visuales es una matriz de confusi\u00f3n.</p> <p>Una matriz de confusi\u00f3n le muestra d\u00f3nde se confundi\u00f3 su modelo de clasificaci\u00f3n entre predicciones y etiquetas verdaderas.</p> <p>Para crear una matriz de confusi\u00f3n, seguiremos tres pasos:</p> <ol> <li>Haga predicciones con nuestro modelo entrenado, <code>model_2</code> (una matriz de confusi\u00f3n compara las predicciones con etiquetas verdaderas).</li> <li>Haga una matriz de confusi\u00f3n usando <code>torchmetrics.ConfusionMatrix</code>.</li> <li>Trace la matriz de confusi\u00f3n usando <code>mlxtend.plotting.plot_confusion_matrix()</code>.</li> </ol> <p>Comencemos haciendo predicciones con nuestro modelo entrenado.</p>"},{"location":"09-03_pytorch_computer_vision/#11-guarde-y-cargue-el-modelo-con-mejor-rendimiento","title":"11. Guarde y cargue el modelo con mejor rendimiento\u00b6","text":"<p>Terminemos esta secci\u00f3n guardando y cargando en nuestro modelo de mejor rendimiento.</p> <p>Recuerde del [cuaderno 01] (https://www.learnpytorch.io/01_pytorch_workflow/#5-served-and-loading-a-pytorch-model) que podemos guardar y cargar un modelo de PyTorch usando una combinaci\u00f3n de:</p> <ul> <li><code>torch.save</code>: una funci\u00f3n para guardar un modelo PyTorch completo o el <code>state_dict()</code> de un modelo.</li> <li><code>torch.load</code>: una funci\u00f3n para cargar en un objeto PyTorch guardado.</li> <li><code>torch.nn.Module.load_state_dict()</code> - una funci\u00f3n para cargar un <code>state_dict()</code> guardado en una instancia de modelo existente.</li> </ul> <p>Puede ver m\u00e1s de estos tres en la [documentaci\u00f3n de modelos de carga y guardado de PyTorch] (https://pytorch.org/tutorials/beginner/ Saving_loading_models.html).</p> <p>Por ahora, guardemos el <code>state_dict()</code> de nuestro <code>model_2</code>, luego volvamos a cargarlo y evalu\u00e9moslo para asegurarnos de que el guardado y la carga se realizaron correctamente.</p>"},{"location":"09-03_pytorch_computer_vision/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Todos los ejercicios se centran en practicar el c\u00f3digo de las secciones anteriores.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Todos los ejercicios deben completarse utilizando c\u00f3digo independiente del dispositivo.</p> <p>Recursos:</p> <ul> <li>Cuaderno de plantilla de ejercicios para 03</li> <li>Cuaderno de soluciones de ejemplo para 03 (pruebe los ejercicios antes de mirar esto)</li> </ul> <ol> <li>\u00bfCu\u00e1les son las 3 \u00e1reas de la industria donde se utiliza actualmente la visi\u00f3n por computadora?</li> <li>Busque \"qu\u00e9 es el sobreajuste en el aprendizaje autom\u00e1tico\" y escriba una oraci\u00f3n sobre lo que encuentre.</li> <li>Busca \"formas de prevenir el sobreajuste en el aprendizaje autom\u00e1tico\", escribe 3 de las cosas que encuentres y una oraci\u00f3n sobre cada una. Nota: hay muchos de estos, as\u00ed que no te preocupes demasiado por todos, simplemente elige 3 y comienza con ellos.</li> <li>Dedique 20 minutos a leer y hacer clic en el [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/).<ul> <li>Cargue su propia imagen de ejemplo usando el bot\u00f3n \"cargar\" y vea qu\u00e9 sucede en cada capa de una CNN a medida que su imagen la atraviesa.</li> </ul> </li> <li>Cargue el tren <code>torchvision.datasets.MNIST()</code> y pruebe los conjuntos de datos.</li> <li>Visualice al menos 5 muestras diferentes del conjunto de datos de entrenamiento MNIST.</li> <li>Convierta el tren MNIST y los conjuntos de datos de prueba en cargadores de datos usando <code>torch.utils.data.DataLoader</code>, establezca <code>batch_size=32</code>.</li> <li>Recrea el <code>model_2</code> usado en este cuaderno (el mismo modelo del [sitio web de CNN Explicador] (https://poloclub.github.io/cnn-explainer/), tambi\u00e9n conocido como TinyVGG) capaz de ajustarse al conjunto de datos MNIST. .</li> <li>Entrene el modelo que cre\u00f3 en el ejercicio 8. en CPU y GPU y vea cu\u00e1nto tiempo lleva cada uno.</li> <li>Haga predicciones utilizando su modelo entrenado y visualice al menos 5 de ellas comparando la predicci\u00f3n con la etiqueta objetivo.</li> <li>Traza una matriz de confusi\u00f3n comparando las predicciones de tu modelo con las etiquetas de verdad.</li> <li>Cree un tensor aleatorio de forma <code>[1, 3, 64, 64]</code> y p\u00e1selo a trav\u00e9s de una capa <code>nn.Conv2d()</code> con varias configuraciones de hiperpar\u00e1metros (pueden ser cualquier configuraci\u00f3n que elija), \u00bfqu\u00e9 nota? \u00bfSi el par\u00e1metro <code>kernel_size</code> sube y baja?</li> <li>Utilice un modelo similar al <code>model_2</code> entrenado de este cuaderno para hacer predicciones en la prueba [<code>torchvision.datasets.FashionMNIST</code>](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST .html) conjunto de datos.<ul> <li>Luego, traza algunas predicciones en las que el modelo se equivoc\u00f3 junto con cu\u00e1l deber\u00eda haber sido la etiqueta de la imagen.</li> <li>Despu\u00e9s de visualizar estas predicciones, \u00bfcrees que se trata m\u00e1s de un error de modelado o de un error de datos?</li> <li>Como en, \u00bfpodr\u00eda funcionar mejor el modelo o las etiquetas de los datos est\u00e1n demasiado cerca entre s\u00ed (por ejemplo, una etiqueta de \"Camisa\" est\u00e1 demasiado cerca de \"Camiseta/top\")?</li> </ul> </li> </ol>"},{"location":"09-03_pytorch_computer_vision/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Ver: conferencia Introducci\u00f3n del MIT a la visi\u00f3n inform\u00e1tica profunda. Esto le dar\u00e1 una gran intuici\u00f3n detr\u00e1s de las redes neuronales convolucionales.</li> <li>Dedique 10 minutos a hacer clic en las diferentes opciones de la [biblioteca de visi\u00f3n de PyTorch] (https://pytorch.org/vision/stable/index.html), \u00bfqu\u00e9 diferentes m\u00f3dulos est\u00e1n disponibles?</li> <li>Busque \"redes neuronales convolucionales m\u00e1s comunes\", \u00bfqu\u00e9 arquitecturas encuentra? \u00bfAlguno de ellos est\u00e1 contenido en la biblioteca <code>torchvision.models</code>? \u00bfQu\u00e9 crees que podr\u00edas hacer con estos?</li> <li>Para obtener una gran cantidad de modelos de visi\u00f3n por computadora de PyTorch previamente entrenados, as\u00ed como muchas extensiones diferentes de las funcionalidades de visi\u00f3n por computadora de PyTorch, consulte la [biblioteca de modelos de im\u00e1genes de PyTorch <code>timm</code>] (https://github.com/rwightman/pytorch-image-models /) (Modelos de im\u00e1genes de antorcha) de Ross Wightman.</li> </ul>"},{"location":"10-07_pytorch_experiment_tracking/","title":"07. Seguimiento del experimento de PyTorch","text":"<p>Ver c\u00f3digo fuente | Ver diapositivas</p> In\u00a0[\u00a0]: Copied! <pre># Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+. try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <p>Nota: Si est\u00e1 utilizando Google Colab, es posible que deba reiniciar su tiempo de ejecuci\u00f3n despu\u00e9s de ejecutar la celda anterior. Despu\u00e9s de reiniciar, puede ejecutar la celda nuevamente y verificar que tiene las versiones correctas de <code>torch</code> (0.12+) y <code>torchvision</code> (0.13+).</p> In\u00a0[\u00a0]: Copied! <pre># Continuar con las importaciones regulares\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Intente obtener torchinfo, inst\u00e1lelo si no funciona\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continuar con las importaciones regulares import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Intente obtener torchinfo, inst\u00e1lelo si no funciona try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Ahora configuremos el c\u00f3digo independiente del dispositivo.</p> <p>Nota: Si est\u00e1s usando Google Colab y a\u00fan no tienes una GPU activada, ahora es el momento de activar una a trav\u00e9s de <code>Runtime -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Acelerador de hardware -&gt; GPU</code> .</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre># Establecer semillas\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n</pre> # Establecer semillas def set_seeds(seed: int=42):     \"\"\"Sets random sets for torch operations.      Args:         seed (int, optional): Random seed to set. Defaults to 42.     \"\"\"     # Set the seed for general torch operations     torch.manual_seed(seed)     # Set the seed for CUDA torch operations (ones that happen on the GPU)     torch.cuda.manual_seed(seed) In\u00a0[\u00a0]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    \n    Returns:\n        pathlib.Path to downloaded data.\n    \n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> import os import zipfile  from pathlib import Path  import requests  def download_data(source: str,                    destination: str,                   remove_source: bool = True) -&gt; Path:     \"\"\"Downloads a zipped dataset from source and unzips to destination.      Args:         source (str): A link to a zipped file containing data.         destination (str): A target directory to unzip data to.         remove_source (bool): Whether to remove the source after downloading and extracting.          Returns:         pathlib.Path to downloaded data.          Example usage:         download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                       destination=\"pizza_steak_sushi\")     \"\"\"     # Setup path to data folder     data_path = Path(\"data/\")     image_path = data_path / destination      # If the image folder doesn't exist, download it and prepare it...      if image_path.is_dir():         print(f\"[INFO] {image_path} directory exists, skipping download.\")     else:         print(f\"[INFO] Did not find {image_path} directory, creating one...\")         image_path.mkdir(parents=True, exist_ok=True)                  # Download pizza, steak, sushi data         target_file = Path(source).name         with open(data_path / target_file, \"wb\") as f:             request = requests.get(source)             print(f\"[INFO] Downloading {target_file} from {source}...\")             f.write(request.content)          # Unzip pizza, steak, sushi data         with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:             print(f\"[INFO] Unzipping {target_file} data...\")              zip_ref.extractall(image_path)          # Remove .zip file         if remove_source:             os.remove(data_path / target_file)          return image_path  image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <p>\u00a1Excelente! Parece que tenemos nuestras im\u00e1genes de pizza, bistec y sushi en formato de clasificaci\u00f3n de im\u00e1genes est\u00e1ndar listas para usar.</p> In\u00a0[\u00a0]: Copied! <pre># Directorios de configuraci\u00f3n\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Configurar los niveles de normalizaci\u00f3n de ImageNet (convierte todas las im\u00e1genes en una distribuci\u00f3n similar a la de ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Crear canalizaci\u00f3n de transformaci\u00f3n manualmente\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Crear cargadores de datos\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Directorios de configuraci\u00f3n train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Configurar los niveles de normalizaci\u00f3n de ImageNet (convierte todas las im\u00e1genes en una distribuci\u00f3n similar a la de ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Crear canalizaci\u00f3n de transformaci\u00f3n manualmente manual_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])            print(f\"Manually created transforms: {manual_transforms}\")  # Crear cargadores de datos train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names In\u00a0[\u00a0]: Copied! <pre># Directorios de configuraci\u00f3n\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Configure pesas previamente entrenadas (muchas de ellas disponibles en torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Obtener transformaciones a partir de pesos (estas son las transformaciones que se utilizaron para obtener los pesos)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Crear cargadores de datos\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Directorios de configuraci\u00f3n train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Configure pesas previamente entrenadas (muchas de ellas disponibles en torchvision.models) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # Obtener transformaciones a partir de pesos (estas son las transformaciones que se utilizaron para obtener los pesos) automatic_transforms = weights.transforms()  print(f\"Automatically created transforms: {automatic_transforms}\")  # Crear cargadores de datos train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=automatic_transforms, # use automatic created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names In\u00a0[\u00a0]: Copied! <pre># Nota: As\u00ed es como se crear\u00eda un modelo previamente entrenado en torchvision &gt; 0.13; quedar\u00e1 obsoleto en versiones futuras.\n# modelo = torchvision.models.ficientnet_b0(preentrenado=True).to(dispositivo) # ANTIGUO\n\n# Descargue los pesos previamente entrenados para EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Configure el modelo con los pesos previamente entrenados y env\u00edelo al dispositivo de destino.\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# Ver el resultado del modelo.\n# modelo\n</pre> # Nota: As\u00ed es como se crear\u00eda un modelo previamente entrenado en torchvision &gt; 0.13; quedar\u00e1 obsoleto en versiones futuras. # modelo = torchvision.models.ficientnet_b0(preentrenado=True).to(dispositivo) # ANTIGUO  # Descargue los pesos previamente entrenados para EfficientNet_B0 weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"  # Configure el modelo con los pesos previamente entrenados y env\u00edelo al dispositivo de destino. model = torchvision.models.efficientnet_b0(weights=weights).to(device)  # Ver el resultado del modelo. # modelo <p>\u00a1Maravilloso!</p> <p>Ahora que tenemos un modelo previamente entrenado, convirt\u00e1moslo en un modelo de extracci\u00f3n de caracter\u00edsticas.</p> <p>En esencia, congelaremos las capas base del modelo (las usaremos para extraer caracter\u00edsticas de nuestras im\u00e1genes de entrada) y cambiaremos el encabezado del clasificador (capa de salida) para adaptarlo a la cantidad de clases con las que estamos trabajando. (tenemos 3 clases: pizza, bistec, sushi).</p> <p>Nota: La idea de crear un modelo de extracci\u00f3n de caracter\u00edsticas (lo que estamos haciendo aqu\u00ed) se cubri\u00f3 con m\u00e1s profundidad en [06. Secci\u00f3n 3.2 de PyTorch Transfer Learning: Configuraci\u00f3n de un modelo previamente entrenado] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model).</p> In\u00a0[\u00a0]: Copied! <pre># Congele todas las capas base estableciendo el atributo require_grad en Falso\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Dado que estamos creando una nueva capa con pesos aleatorios (torch.nn.Linear),\n# vamos a poner las semillas\nset_seeds() \n\n# Actualice el cabezal clasificador para adaptarlo a nuestro problema.\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n</pre> # Congele todas las capas base estableciendo el atributo require_grad en Falso for param in model.features.parameters():     param.requires_grad = False      # Dado que estamos creando una nueva capa con pesos aleatorios (torch.nn.Linear), # vamos a poner las semillas set_seeds()   # Actualice el cabezal clasificador para adaptarlo a nuestro problema. model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features=1280,                out_features=len(class_names),               bias=True).to(device)) <p>Capas base congeladas, cabezal clasificador cambiado, obtengamos un resumen de nuestro modelo con <code>torchinfo.summary()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from torchinfo import summary\n\n# # Obtener un resumen del modelo (descomentar para obtener un resultado completo)\n# resumen (modelo,\n# input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\" (batch_size, color_channels, alto, ancho)\n# detallado = 0,\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Obtener un resumen del modelo (descomentar para obtener un resultado completo) # resumen (modelo, # input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\" (batch_size, color_channels, alto, ancho) # detallado = 0, # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"] # ) <p>Salida de <code>torchinfo.summary()</code> con nuestro modelo de extractor de caracter\u00edsticas EffNetB0, observe c\u00f3mo las capas base est\u00e1n congeladas (no entrenables) y las capas de salida se personalizan seg\u00fan nuestro propio problema.</p> In\u00a0[\u00a0]: Copied! <pre># Definir p\u00e9rdida y optimizador\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Definir p\u00e9rdida y optimizador loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[\u00a0]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n\n# Crea un escritor con todas las configuraciones predeterminadas\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter  # Crea un escritor con todas las configuraciones predeterminadas writer = SummaryWriter() <p>Ahora, para usar el escritor, podr\u00edamos escribir un nuevo bucle de entrenamiento o podr\u00edamos ajustar la funci\u00f3n <code>train()</code> existente que creamos en 05. PyTorch Going Modular secci\u00f3n 4.</p> <p>Tomemos la \u00faltima opci\u00f3n.</p> <p>Obtendremos la funci\u00f3n <code>train()</code> de <code>engine.py</code> y ajustaremos utilizar \"escritor\".</p> <p>Espec\u00edficamente, agregaremos la capacidad de nuestra funci\u00f3n <code>train()</code> para registrar el entrenamiento de nuestro modelo y probar los valores de p\u00e9rdida y precisi\u00f3n.</p> <p>Podemos hacer esto con <code>writer.add_scalars(main_tag, tag_scalar_dict)</code>, donde:</p> <ul> <li><code>main_tag</code> (cadena): el nombre de los escalares que se rastrean (por ejemplo, \"Precisi\u00f3n\")</li> <li><code>tag_scalar_dict</code> (dict): un diccionario de los valores que se est\u00e1n rastreando (por ejemplo, <code>{\"train_loss\": 0.3454}</code>)<ul> <li> <p>Nota: El m\u00e9todo se llama <code>add_scalars()</code> porque nuestros valores de p\u00e9rdida y precisi\u00f3n son generalmente escalares (valores \u00fanicos).</p> </li> </ul> </li> </ul> <p>Una vez que hayamos terminado de rastrear los valores, llamaremos a <code>writer.close()</code> para decirle al <code>writer</code> que deje de buscar valores para rastrear.</p> <p>Para comenzar a modificar <code>train()</code> tambi\u00e9n importaremos <code>train_step()</code> y <code>test_step()</code> desde [<code>engine.py</code>](https://github.com/mrdbourke/pytorch-deep-learning/blob /main/going_modular/going_modular/engine.py).</p> <p>Nota: Puede realizar un seguimiento de la informaci\u00f3n sobre su modelo casi en cualquier parte de su c\u00f3digo. Pero muy a menudo se realizar\u00e1 un seguimiento de los experimentos mientras se entrena un modelo (dentro de un ciclo de entrenamiento/prueba).</p> <p>La clase <code>torch.utils.tensorboard.SummaryWriter()</code> tambi\u00e9n tiene muchos m\u00e9todos diferentes para rastrear diferentes cosas sobre su modelo/datos, como [<code>add_graph()</code>](https://pytorch.org/docs/stable /tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph) que rastrea el gr\u00e1fico de c\u00e1lculo de su modelo. Para obtener m\u00e1s opciones, consulte la documentaci\u00f3n <code>SummaryWriter()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Importar funci\u00f3n train() desde:\n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      \n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  from going_modular.going_modular.engine import train_step, test_step  # Importar funci\u00f3n train() desde: # https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").            Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer,                                            device=device)         test_loss, test_acc = test_step(model=model,                                         dataloader=test_dataloader,                                         loss_fn=loss_fn,                                         device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          ### New: Experiment tracking ###         # Add loss results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)          # Add accuracy results to SummaryWriter         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)                  # Track the PyTorch model architecture         writer.add_graph(model=model,                           # Pass in an example input                          input_to_model=torch.randn(32, 3, 224, 224).to(device))          # Close the writer     writer.close()          ### End new ###      # Return the filled results at the end of the epochs     return results <p>\u00a1Guau!</p> <p>Nuestra funci\u00f3n <code>train()</code> ahora est\u00e1 actualizada para usar una instancia <code>SummaryWriter()</code> para rastrear los resultados de nuestro modelo.</p> <p>\u00bfQu\u00e9 tal si lo probamos durante 5 \u00e9pocas?</p> In\u00a0[\u00a0]: Copied! <pre># modelo de tren\n# Nota: No usar Engine.train() ya que el script original no est\u00e1 actualizado para usar Writer.\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n</pre> # modelo de tren # Nota: No usar Engine.train() ya que el script original no est\u00e1 actualizado para usar Writer. set_seeds() results = train(model=model,                 train_dataloader=train_dataloader,                 test_dataloader=test_dataloader,                 optimizer=optimizer,                 loss_fn=loss_fn,                 epochs=5,                 device=device) <p>Nota: Es posible que observe que los resultados aqu\u00ed son ligeramente diferentes a los que obtuvo nuestro modelo en 06. Aprendizaje por transferencia de PyTorch. La diferencia proviene del uso de <code>engine.train()</code> y nuestra funci\u00f3n <code>train()</code> modificada. \u00bfPuedes adivinar por qu\u00e9? La documentaci\u00f3n de PyTorch sobre aleatoriedad puede ayudar m\u00e1s.</p> <p>Al ejecutar la celda de arriba obtenemos resultados similares a los que obtuvimos en 06. PyTorch Transfer Learning secci\u00f3n 4: Entrenar modelo pero la diferencia est\u00e1 detr\u00e1s de escena, nuestra instancia <code>writer</code> ha creado un directorio <code>runs/</code> que almacena los resultados de nuestro modelo.</p> <p>Por ejemplo, la ubicaci\u00f3n para guardar podr\u00eda verse as\u00ed:</p> <pre><code>carreras/Jun21_00-46-03_daniels_macbook_pro\n</code></pre> <p>Donde el formato predeterminado es <code>runs/CURRENT_DATETIME_HOSTNAME</code>.</p> <p>Los comprobaremos en un segundo, pero solo como recordatorio, anteriormente est\u00e1bamos rastreando los resultados de nuestro modelo en un diccionario.</p> In\u00a0[\u00a0]: Copied! <pre># Mira los resultados del modelo.\nresults\n</pre> # Mira los resultados del modelo. results <p>Hmmm, podr\u00edamos formatear esto para que sea una buena trama, pero \u00bfte imaginas hacer un seguimiento de varios de estos diccionarios?</p> <p>Tiene que haber una mejor manera...</p> In\u00a0[\u00a0]: Copied! <pre># C\u00f3digo de ejemplo para ejecutar en Jupyter o Google Colab Notebook (descom\u00e9ntalo para probarlo)\n# %load_ext tensorboard\n# % tensorboard --logdir se ejecuta\n</pre> # C\u00f3digo de ejemplo para ejecutar en Jupyter o Google Colab Notebook (descom\u00e9ntalo para probarlo) # %load_ext tensorboard # % tensorboard --logdir se ejecuta <p>Si todo sali\u00f3 correctamente, deber\u00edas ver algo como lo siguiente:</p> <p>Ver los resultados de un \u00fanico experimento de modelado para determinar la precisi\u00f3n y la p\u00e9rdida en TensorBoard.</p> <p>Nota: Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo ejecutar TensorBoard en notebooks o en otras ubicaciones, consulte lo siguiente:</p> <ul> <li>Gu\u00eda de uso de TensorBoard en Notebooks de TensorFlow</li> <li>Comience con TensorBoard.dev (\u00fatil para cargar sus registros de TensorBoard en un enlace para compartir)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n</pre> def create_writer(experiment_name: str,                    model_name: str,                    extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():     \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.      log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.      Where timestamp is the current date in YYYY-MM-DD format.      Args:         experiment_name (str): Name of experiment.         model_name (str): Name of model.         extra (str, optional): Anything extra to add to the directory. Defaults to None.      Returns:         torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.      Example usage:         # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"         writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb2\",                                extra=\"5_epochs\")         # The above is the same as:         writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")     \"\"\"     from datetime import datetime     import os      # Get timestamp of current date (all experiments on certain day live in same folder)     timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format      if extra:         # Create log directory path         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)     else:         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)              print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")     return SummaryWriter(log_dir=log_dir) <p>\u00a1Hermoso!</p> <p>Ahora que tenemos una funci\u00f3n <code>create_writer()</code>, prob\u00e9mosla.</p> In\u00a0[\u00a0]: Copied! <pre># Crear un escritor de ejemplo\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n</pre> # Crear un escritor de ejemplo example_writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb0\",                                extra=\"5_epochs\") <p>Viendo bien, ahora tenemos una manera de registrar y rastrear nuestros diversos experimentos.</p> In\u00a0[\u00a0]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Agregar par\u00e1metro de escritor a entrenar()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  # Agregar par\u00e1metro de escritor a entrenar() def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,            writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer           ) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Stores metrics to specified writer log_dir if present.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").       writer: A SummaryWriter() instance to log model results to.      Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                           dataloader=train_dataloader,                                           loss_fn=loss_fn,                                           optimizer=optimizer,                                           device=device)         test_loss, test_acc = test_step(model=model,           dataloader=test_dataloader,           loss_fn=loss_fn,           device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)           ### New: Use the writer parameter to track experiments ###         # See if there's a writer, if so, log to it         if writer:             # Add results to SummaryWriter             writer.add_scalars(main_tag=\"Loss\",                                 tag_scalar_dict={\"train_loss\": train_loss,                                                 \"test_loss\": test_loss},                                global_step=epoch)             writer.add_scalars(main_tag=\"Accuracy\",                                 tag_scalar_dict={\"train_acc\": train_acc,                                                 \"test_acc\": test_acc},                                 global_step=epoch)              # Close the writer             writer.close()         else:             pass     ### End new ###      # Return the filled results at the end of the epochs     return results In\u00a0[\u00a0]: Copied! <pre># Descargue datos de entrenamiento del 10 por ciento y del 20 por ciento (si es necesario)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n</pre> # Descargue datos de entrenamiento del 10 por ciento y del 20 por ciento (si es necesario) data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                                      destination=\"pizza_steak_sushi\")  data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\") <p>\u00a1Datos descargados!</p> <p>Ahora configuremos las rutas de archivo de los datos que usaremos para los diferentes experimentos.</p> <p>Crearemos diferentes rutas de directorio de entrenamiento, pero solo necesitaremos una ruta de directorio de prueba, ya que todos los experimentos utilizar\u00e1n el mismo conjunto de datos de prueba (el conjunto de datos de prueba de pizza, bistec y sushi 10%).</p> In\u00a0[\u00a0]: Copied! <pre># Configurar rutas del directorio de capacitaci\u00f3n\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Configure las rutas del directorio de prueba (nota: use el mismo conjunto de datos de prueba para ambos para comparar los resultados)\ntest_dir = data_10_percent_path / \"test\"\n\n# Consulta los directorios\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n</pre> # Configurar rutas del directorio de capacitaci\u00f3n train_dir_10_percent = data_10_percent_path / \"train\" train_dir_20_percent = data_20_percent_path / \"train\"  # Configure las rutas del directorio de prueba (nota: use el mismo conjunto de datos de prueba para ambos para comparar los resultados) test_dir = data_10_percent_path / \"test\"  # Consulta los directorios print(f\"Training directory 10%: {train_dir_10_percent}\") print(f\"Training directory 20%: {train_dir_20_percent}\") print(f\"Testing directory: {test_dir}\") In\u00a0[\u00a0]: Copied! <pre>from torchvision import transforms\n\n# Cree una transformaci\u00f3n para normalizar la distribuci\u00f3n de datos para que est\u00e9 en l\u00ednea con ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose se transforma en una canalizaci\u00f3n\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n</pre> from torchvision import transforms  # Cree una transformaci\u00f3n para normalizar la distribuci\u00f3n de datos para que est\u00e9 en l\u00ednea con ImageNet normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]                                  std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]  # Compose se transforma en una canalizaci\u00f3n simple_transform = transforms.Compose([     transforms.Resize((224, 224)), # 1. Resize the images     transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1     normalize # 3. Normalize the images so their distributions match the ImageNet dataset  ]) <p>\u00a1Transf\u00f3rmate listo!</p> <p>Ahora creemos nuestros DataLoaders usando la funci\u00f3n <code>create_dataloaders()</code> de <code>data_setup.py</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 2.</p> <p>Crearemos los DataLoaders con un tama\u00f1o de lote de 32.</p> <p>Para todos nuestros experimentos usaremos el mismo <code>test_dataloader</code> (para mantener las comparaciones consistentes).</p> In\u00a0[\u00a0]: Copied! <pre>BATCH_SIZE = 32\n\n# Cree un 10% de capacitaci\u00f3n y pruebe DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Cree un 20 % de datos de prueba y entrenamiento DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Encuentre la cantidad de muestras/lotes por cargador de datos (usando el mismo test_dataloader para ambos experimentos)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n</pre> BATCH_SIZE = 32  # Cree un 10% de capacitaci\u00f3n y pruebe DataLoaders train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,     test_dir=test_dir,      transform=simple_transform,     batch_size=BATCH_SIZE )  # Cree un 20 % de datos de prueba y entrenamiento DataLoders train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,     test_dir=test_dir,     transform=simple_transform,     batch_size=BATCH_SIZE )  # Encuentre la cantidad de muestras/lotes por cargador de datos (usando el mismo test_dataloader para ambos experimentos) print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\") print(f\"Number of classes: {len(class_names)}, class names: {class_names}\") In\u00a0[\u00a0]: Copied! <pre>import torchvision\nfrom torchinfo import summary\n\n# 1. Cree una instancia de EffNetB2 con pesos previamente entrenados.\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Obtenga un resumen del EffNetB2 est\u00e1ndar de torchvision.models (descomente el comentario para obtener un resultado completo)\n# resumen (modelo = effnetb2,\n# input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\"\n# # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"]\n# )\n\n# 3. Obtenga el n\u00famero de caracter\u00edsticas internas de la capa clasificadora EfficientNetB2.\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n</pre> import torchvision from torchinfo import summary  # 1. Cree una instancia de EffNetB2 con pesos previamente entrenados. effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)  # # 2. Obtenga un resumen del EffNetB2 est\u00e1ndar de torchvision.models (descomente el comentario para obtener un resultado completo) # resumen (modelo = effnetb2, # input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\" # # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"] # )  # 3. Obtenga el n\u00famero de caracter\u00edsticas internas de la capa clasificadora EfficientNetB2. print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\") <p>Resumen del modelo de extracci\u00f3n de caracter\u00edsticas de EffNetB2 con todas las capas descongeladas (entrenables) y el cabezal clasificador predeterminado del preentrenamiento de ImageNet.</p> <p>Ahora que sabemos la cantidad requerida de <code>in_features</code> para el modelo EffNetB2, creemos un par de funciones auxiliares para configurar nuestros modelos de extracci\u00f3n de caracter\u00edsticas EffNetB0 y EffNetB2.</p> <p>Queremos que estas funciones:</p> <ol> <li>Obtenga el modelo base de <code>torchvision.models</code></li> <li>Congele las capas base en el modelo (establezca <code>requires_grad=False</code>)</li> <li>Establezca las semillas aleatorias (no necesitamos hacer esto, pero como estamos ejecutando una serie de experimentos e inicializando una nueva capa con pesos aleatorios, queremos que la aleatoriedad sea similar para cada experimento)</li> <li>Cambiar el cabezal clasificador (para adaptarlo a nuestro problema)</li> <li>Asigne un nombre al modelo (por ejemplo, \"effnetb0\" para EffNetB0)</li> </ol> In\u00a0[\u00a0]: Copied! <pre>import torchvision\nfrom torch import nn\n\n# Obtenga funciones num\u00e9ricas (una para cada clase de pizza, bistec y sushi)\nOUT_FEATURES = len(class_names)\n\n# Cree un extractor de funciones EffNetB0\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Cree un extractor de funciones EffNetB2\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n</pre> import torchvision from torch import nn  # Obtenga funciones num\u00e9ricas (una para cada clase de pizza, bistec y sushi) OUT_FEATURES = len(class_names)  # Cree un extractor de funciones EffNetB0 def create_effnetb0():     # 1. Get the base mdoel with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT     model = torchvision.models.efficientnet_b0(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.2),         nn.Linear(in_features=1280, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb0\"     print(f\"[INFO] Created new {model.name} model.\")     return model  # Cree un extractor de funciones EffNetB2 def create_effnetb2():     # 1. Get the base model with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     model = torchvision.models.efficientnet_b2(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.3),         nn.Linear(in_features=1408, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb2\"     print(f\"[INFO] Created new {model.name} model.\")     return model <p>\u00a1Esas son algunas funciones bonitas!</p> <p>Probemoslos creando una instancia de EffNetB0 y EffNetB2 y revisando su <code>resumen()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>effnetb0 = create_effnetb0() \n\n# Obtenga un resumen de salida de las capas en nuestro modelo de extracci\u00f3n de caracter\u00edsticas EffNetB0 (descomente el comentario para ver el resultado completo)\n# resumen (modelo = effnetb0,\n# input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\"\n# # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"]\n# )\n</pre> effnetb0 = create_effnetb0()   # Obtenga un resumen de salida de las capas en nuestro modelo de extracci\u00f3n de caracter\u00edsticas EffNetB0 (descomente el comentario para ver el resultado completo) # resumen (modelo = effnetb0, # input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\" # # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"] # ) <p>Resumen del modelo EffNetB0 con capas base congeladas (no entrenables) y cabezal clasificador actualizado (adecuado para clasificaci\u00f3n de im\u00e1genes de pizza, bistec y sushi).</p> In\u00a0[\u00a0]: Copied! <pre>effnetb2 = create_effnetb2()\n\n# Obtenga un resumen de salida de las capas en nuestro modelo de extracci\u00f3n de caracter\u00edsticas EffNetB2 (elimine el comentario para ver la salida completa)\n# resumen (modelo = effnetb2,\n# input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\"\n# # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"]\n# )\n</pre> effnetb2 = create_effnetb2()  # Obtenga un resumen de salida de las capas en nuestro modelo de extracci\u00f3n de caracter\u00edsticas EffNetB2 (elimine el comentario para ver la salida completa) # resumen (modelo = effnetb2, # input_size=(32, 3, 224, 224), # aseg\u00farese de que sea \"input_size\", no \"input_shape\" # # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"] # ) <p>Resumen del modelo EffNetB2 con capas base congeladas (no entrenables) y cabezal clasificador actualizado (adecuado para clasificaci\u00f3n de im\u00e1genes de pizza, bistec y sushi).</p> <p>Al observar los resultados de los res\u00famenes, parece que la columna vertebral de EffNetB2 tiene casi el doble de par\u00e1metros que EffNetB0.</p> Modelo Par\u00e1metros totales (antes de congelar/cambiar cabezal) Par\u00e1metros totales (despu\u00e9s de congelar/cambiar cabezal) Par\u00e1metros totales entrenables (despu\u00e9s de congelar/cambiar el cabezal) EfficientNetB0 5.288.548 4.011.391 3.843 EfficientNetB2 9.109.994 7.705.221 4.227 <p>Esto le da a la columna vertebral del modelo EffNetB2 m\u00e1s oportunidades para formar una representaci\u00f3n de nuestros datos de pizza, bistec y sushi.</p> <p>Sin embargo, los par\u00e1metros entrenables para cada modelo (los cabezales clasificadores) no son muy diferentes.</p> <p>\u00bfEstos par\u00e1metros adicionales conducir\u00e1n a mejores resultados?</p> <p>Tendremos que esperar y ver...</p> <p>Nota: Con el \u00e1nimo de experimentar, realmente podr\u00edas probar casi cualquier modelo de <code>torchvision.models</code> de manera similar a lo que estamos haciendo aqu\u00ed. Solo eleg\u00ed EffNetB0 y EffNetB2 como ejemplos. Quiz\u00e1s quieras incluir algo como <code>torchvision.models.convnext_tiny()</code> o <code>torchvision.models.convnext_small()</code> en la mezcla.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Crear lista de \u00e9pocas\nnum_epochs = [5, 10]\n\n# 2. Crear lista de modelos (es necesario crear un nuevo modelo para cada experimento)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Cree un diccionario de cargadores de datos para varios cargadores de datos.\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n</pre> # 1. Crear lista de \u00e9pocas num_epochs = [5, 10]  # 2. Crear lista de modelos (es necesario crear un nuevo modelo para cada experimento) models = [\"effnetb0\", \"effnetb2\"]  # 3. Cree un diccionario de cargadores de datos para varios cargadores de datos. train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,                      \"data_20_percent\": train_dataloader_20_percent} <p>\u00a1Listas y diccionario creados!</p> <p>Ahora podemos escribir c\u00f3digo para recorrer cada una de las diferentes opciones y probar cada una de las diferentes combinaciones.</p> <p>Tambi\u00e9n guardaremos el modelo al final de cada experimento para que luego podamos volver a cargarlo en el mejor modelo y usarlo para hacer predicciones.</p> <p>Espec\u00edficamente, sigamos los siguientes pasos:</p> <ol> <li>Establezca las semillas aleatorias (para que los resultados de nuestro experimento sean reproducibles; en la pr\u00e1ctica, puede ejecutar el mismo experimento en ~3 semillas diferentes y promediar los resultados).</li> <li>Mantenga un registro de los diferentes n\u00fameros de experimentos (esto es principalmente para impresiones bonitas).</li> <li>Recorra los elementos del diccionario <code>train_dataloaders</code> para cada uno de los diferentes DataLoaders de entrenamiento.</li> <li>Recorra la lista de n\u00fameros de \u00e9poca.</li> <li>Recorra la lista de diferentes nombres de modelos.</li> <li>Cree impresiones de informaci\u00f3n para el experimento en ejecuci\u00f3n actual (para que sepamos qu\u00e9 est\u00e1 sucediendo).</li> <li>Verifique qu\u00e9 modelo es el modelo objetivo y cree una nueva instancia de EffNetB0 o EffNetB2 (creamos una nueva instancia de modelo en cada experimento para que todos los modelos comiencen desde el mismo punto de vista).</li> <li>Cree una nueva funci\u00f3n de p\u00e9rdida (<code>torch.nn.CrossEntropyLoss()</code>) y un optimizador (<code>torch.optim.Adam(params=model.parameters(), lr=0.001)</code>) para cada nuevo experimento.</li> <li>Entrene el modelo con la funci\u00f3n <code>train()</code> modificada pasando los detalles apropiados al par\u00e1metro <code>writer</code>.</li> <li>Guarde el modelo entrenado con un nombre de archivo apropiado en el archivo <code>save_model()</code> de [<code>utils.py</code>](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/ Going_modular/utils.py).</li> </ol> <p>Tambi\u00e9n podemos usar la magia <code>%%time</code> para ver cu\u00e1nto tiempo toman todos nuestros experimentos juntos en una sola celda de Jupyter/Google Colab.</p> <p>\u00a1Vamos a hacerlo!</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Establece las semillas aleatorias\nset_seeds(seed=42)\n\n# 2. Realice un seguimiento de los n\u00fameros de los experimentos\nexperiment_number = 0\n\n# 3. Recorra cada DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n</pre> %%time from going_modular.going_modular.utils import save_model  # 1. Establece las semillas aleatorias set_seeds(seed=42)  # 2. Realice un seguimiento de los n\u00fameros de los experimentos experiment_number = 0  # 3. Recorra cada DataLoader for dataloader_name, train_dataloader in train_dataloaders.items():      # 4. Loop through each number of epochs     for epochs in num_epochs:           # 5. Loop through each model name and create a new model based on the name         for model_name in models:              # 6. Create information print outs             experiment_number += 1             print(f\"[INFO] Experiment number: {experiment_number}\")             print(f\"[INFO] Model: {model_name}\")             print(f\"[INFO] DataLoader: {dataloader_name}\")             print(f\"[INFO] Number of epochs: {epochs}\")                # 7. Select the model             if model_name == \"effnetb0\":                 model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)             else:                 model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)                          # 8. Create a new loss and optimizer for every model             loss_fn = nn.CrossEntropyLoss()             optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)              # 9. Train target model with target dataloaders and track experiments             train(model=model,                   train_dataloader=train_dataloader,                   test_dataloader=test_dataloader,                    optimizer=optimizer,                   loss_fn=loss_fn,                   epochs=epochs,                   device=device,                   writer=create_writer(experiment_name=dataloader_name,                                        model_name=model_name,                                        extra=f\"{epochs}_epochs\"))                          # 10. Save the model to file so we can get back the best model             save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"             save_model(model=model,                        target_dir=\"models\",                        model_name=save_filepath)             print(\"-\"*50 + \"\\n\") In\u00a0[\u00a0]: Copied! <pre># Visualizaci\u00f3n de TensorBoard en Jupyter y Google Colab Notebooks (elimine el comentario para ver la instancia completa de TensorBoard)\n# %load_ext tensorboard\n# % tensorboard --logdir se ejecuta\n</pre> # Visualizaci\u00f3n de TensorBoard en Jupyter y Google Colab Notebooks (elimine el comentario para ver la instancia completa de TensorBoard) # %load_ext tensorboard # % tensorboard --logdir se ejecuta <p>Al ejecutar la celda de arriba deber\u00edamos obtener un resultado similar al siguiente.</p> <p>Nota: Dependiendo de las semillas aleatorias que usaste/el hardware que usaste, existe la posibilidad de que tus n\u00fameros no sean exactamente los mismos que los que aparecen aqu\u00ed. Esto est\u00e1 bien. Se debe a la aleatoriedad inherente del aprendizaje profundo. Lo que m\u00e1s importa es la tendencia. Hacia d\u00f3nde se dirigen tus n\u00fameros. Si est\u00e1n muy desviados, tal vez haya alg\u00fan problema y sea mejor volver atr\u00e1s y verificar el c\u00f3digo. Pero si est\u00e1n ligeramente desviados (digamos un par de decimales m\u00e1s o menos), est\u00e1 bien.</p> <p>Al visualizar los valores de p\u00e9rdida de prueba para los diferentes experimentos de modelado en TensorBoard, se puede ver que el modelo EffNetB0 entrenado durante 10 \u00e9pocas y con el 20% de los datos logra la p\u00e9rdida m\u00e1s baja. Esto se mantiene con la tendencia general de los experimentos de que: m\u00e1s datos, un modelo m\u00e1s grande y un tiempo de entrenamiento m\u00e1s largo es generalmente mejor.</p> <p>Tambi\u00e9n puede cargar los resultados de su experimento de TensorBoard en tensorboard.dev para alojarlos p\u00fablicamente de forma gratuita.</p> <p>Por ejemplo, ejecutando c\u00f3digo similar al siguiente:</p> In\u00a0[\u00a0]: Copied! <pre># # Sube los resultados a TensorBoard.dev (descomenta para probarlo)\n# !tensorboard dev upload --logdir se ejecuta \\\n# --name \"07. Seguimiento de experimentos de PyTorch: resultados del modelo FoodVision Mini\" \\\n# --description \"Comparaci\u00f3n de resultados de diferentes tama\u00f1os de modelo, cantidad de datos de entrenamiento y tiempo de entrenamiento\".\n</pre> # # Sube los resultados a TensorBoard.dev (descomenta para probarlo) # !tensorboard dev upload --logdir se ejecuta \\ # --name \"07. Seguimiento de experimentos de PyTorch: resultados del modelo FoodVision Mini\" \\ # --description \"Comparaci\u00f3n de resultados de diferentes tama\u00f1os de modelo, cantidad de datos de entrenamiento y tiempo de entrenamiento\". <p>Al ejecutar la celda anterior, los experimentos de este cuaderno se pueden ver p\u00fablicamente en: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/</p> <p>Nota: Tenga en cuenta que todo lo que cargue en tensorboard.dev estar\u00e1 disponible p\u00fablicamente para que cualquiera lo vea. Entonces, si carga sus experimentos, tenga cuidado de que no contengan informaci\u00f3n confidencial.</p> In\u00a0[\u00a0]: Copied! <pre># Configure la mejor ruta de archivo del modelo\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Crear una nueva instancia de EffNetB2 (para cargar el state_dict() guardado en)\nbest_model = create_effnetb2()\n\n# Cargue el mejor modelo guardado state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n</pre> # Configure la mejor ruta de archivo del modelo best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"  # Crear una nueva instancia de EffNetB2 (para cargar el state_dict() guardado en) best_model = create_effnetb2()  # Cargue el mejor modelo guardado state_dict() best_model.load_state_dict(torch.load(best_model_path)) <p>\u00a1El mejor modelo cargado!</p> <p>Mientras estamos aqu\u00ed, revisemos su tama\u00f1o de archivo.</p> <p>Esta es una consideraci\u00f3n importante m\u00e1s adelante al implementar el modelo (incorporarlo a una aplicaci\u00f3n).</p> <p>Si el modelo es demasiado grande, puede resultar dif\u00edcil implementarlo.</p> In\u00a0[\u00a0]: Copied! <pre># Verifique el tama\u00f1o del archivo del modelo\nfrom pathlib import Path\n\n# Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n</pre> # Verifique el tama\u00f1o del archivo del modelo from pathlib import Path  # Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024) print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\") <p>Parece que nuestro mejor modelo hasta ahora tiene un tama\u00f1o de 29 MB. Tendremos esto en cuenta si quisi\u00e9ramos implementarlo m\u00e1s adelante.</p> <p>Es hora de hacer y visualizar algunas predicciones.</p> <p>Creamos una funci\u00f3n <code>pred_and_plot_image()</code> para usar un modelo entrenado para hacer predicciones sobre una imagen en [06. Secci\u00f3n 6 del aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set).</p> <p>Y podemos reutilizar esta funci\u00f3n import\u00e1ndola desde <code>going_modular.going_modular.predictions.py</code> (Puse la funci\u00f3n <code>pred_and_plot_image()</code> en un script para poder reutilizarla).</p> <p>Entonces, para hacer predicciones sobre varias im\u00e1genes que el modelo no ha visto antes, primero obtendremos una lista de todas las rutas de archivos de im\u00e1genes del conjunto de datos de prueba de 20% de pizza, bistec y sushi y luego seleccionaremos aleatoriamente un subconjunto de estas rutas de archivos. para pasar a nuestra funci\u00f3n <code>pred_and_plot_image()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Funci\u00f3n de importaci\u00f3n para hacer predicciones sobre im\u00e1genes y trazarlas.\n# Consulte la funci\u00f3n creada anteriormente en la secci\u00f3n: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Obtenga una lista aleatoria de 3 im\u00e1genes del 20 % del conjunto de prueba\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterar a trav\u00e9s de rutas de im\u00e1genes de prueba aleatorias, hacer predicciones sobre ellas y trazarlas\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n</pre> # Funci\u00f3n de importaci\u00f3n para hacer predicciones sobre im\u00e1genes y trazarlas. # Consulte la funci\u00f3n creada anteriormente en la secci\u00f3n: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set from going_modular.going_modular.predictions import pred_and_plot_image  # Obtenga una lista aleatoria de 3 im\u00e1genes del 20 % del conjunto de prueba import random num_images_to_plot = 3 test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset test_image_path_sample = random.sample(population=test_image_path_list,                                        k=num_images_to_plot) # randomly select k number of images  # Iterar a trav\u00e9s de rutas de im\u00e1genes de prueba aleatorias, hacer predicciones sobre ellas y trazarlas for image_path in test_image_path_sample:     pred_and_plot_image(model=best_model,                         image_path=image_path,                         class_names=class_names,                         image_size=(224, 224)) <p>\u00a1Lindo!</p> <p>Al ejecutar la celda de arriba varias veces, podemos ver que nuestro modelo funciona bastante bien y, a menudo, tiene mayores probabilidades de predicci\u00f3n que los modelos anteriores que hemos creado.</p> <p>Esto sugiere que el modelo tiene m\u00e1s confianza en las decisiones que toma.</p> In\u00a0[\u00a0]: Copied! <pre># Descargar imagen personalizada\nimport requests\n\n# Configurar ruta de imagen personalizada\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Descarga la imagen si a\u00fan no existe\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predecir en imagen personalizada\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Descargar imagen personalizada import requests  # Configurar ruta de imagen personalizada custom_image_path = Path(\"data/04-pizza-dad.jpeg\")  # Descarga la imagen si a\u00fan no existe if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predecir en imagen personalizada pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <p>\u00a1Guau!</p> <p>\u00a1Dos pulgares otra vez!</p> <p>Nuestro mejor modelo predice \"pizza\" correctamente y esta vez con una probabilidad de predicci\u00f3n a\u00fan mayor (0,978) que el primer modelo de extracci\u00f3n de caracter\u00edsticas que entrenamos y utilizamos en [06. Secci\u00f3n 6.1 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#61-making-predictions-on-a-custom-image).</p> <p>Esto nuevamente sugiere que nuestro mejor modelo actual (el extractor de caracter\u00edsticas EffNetB2 entrenado en el 20% de los datos de entrenamiento de pizza, bistec y sushi y durante 10 \u00e9pocas) ha aprendido patrones para que tenga m\u00e1s confianza en su decisi\u00f3n de predecir la pizza.</p> <p>Me pregunto qu\u00e9 podr\u00eda mejorar a\u00fan m\u00e1s el rendimiento de nuestro modelo.</p> <p>Te lo dejar\u00e9 como un desaf\u00edo para que investigues.</p>"},{"location":"10-07_pytorch_experiment_tracking/#07-seguimiento-del-experimento-de-pytorch","title":"07. Seguimiento del experimento de PyTorch\u00b6","text":"<p>Nota: Este cuaderno utiliza la nueva [API de soporte multipeso de <code>torchvision</code> (disponible en <code>torchvision</code> v0.13+)](https://pytorch.org/blog/introtaining-torchvision-new -api-soporte-multi-peso/).</p> <p>Hemos entrenado a unos cuantos modelos en el camino hacia la creaci\u00f3n de FoodVision Mini (un modelo de clasificaci\u00f3n de im\u00e1genes para clasificar im\u00e1genes de pizza, bistec o sushi).</p> <p>Y hasta ahora los hemos rastreado a trav\u00e9s de diccionarios de Python.</p> <p>O simplemente compararlos seg\u00fan las impresiones m\u00e9tricas durante el entrenamiento.</p> <p>\u00bfQu\u00e9 pasar\u00eda si quisieras ejecutar una docena (o m\u00e1s) de modelos diferentes a la vez?</p> <p>Seguramente hay una mejor manera...</p> <p>Hay.</p> <p>Seguimiento del experimento.</p> <p>Y dado que el seguimiento de experimentos es tan importante e integral para el aprendizaje autom\u00e1tico, puede considerar este cuaderno como su primer proyecto importante.</p> <p>Bienvenido al Proyecto Milestone 1: Seguimiento del mini experimento de FoodVision.</p> <p>Responderemos la pregunta: \u00bfc\u00f3mo hago un seguimiento de mis experimentos de aprendizaje autom\u00e1tico?</p>"},{"location":"10-07_pytorch_experiment_tracking/#que-es-el-seguimiento-de-experimentos","title":"\u00bfQu\u00e9 es el seguimiento de experimentos?\u00b6","text":"<p>El aprendizaje autom\u00e1tico y el aprendizaje profundo son muy experimentales.</p> <p>Tendr\u00e1s que ponerte tu boina de artista/gorro de chef para cocinar muchos modelos diferentes.</p> <p>Y hay que ponerse la bata de cient\u00edfico para seguir los resultados de diversas combinaciones de datos, arquitecturas de modelos y reg\u00edmenes de entrenamiento.</p> <p>Ah\u00ed es donde entra en juego el seguimiento de experimentos.</p> <p>Si est\u00e1 ejecutando muchos experimentos diferentes, el seguimiento de experimentos le ayudar\u00e1 a descubrir qu\u00e9 funciona y qu\u00e9 no.</p>"},{"location":"10-07_pytorch_experiment_tracking/#por-que-realizar-un-seguimiento-de-los-experimentos","title":"\u00bfPor qu\u00e9 realizar un seguimiento de los experimentos?\u00b6","text":"<p>Si s\u00f3lo est\u00e1 ejecutando un pu\u00f1ado de modelos (como lo hemos hecho hasta ahora), podr\u00eda estar bien simplemente realizar un seguimiento de sus resultados en impresiones y algunos diccionarios.</p> <p>Sin embargo, a medida que la cantidad de experimentos que realiza comienza a aumentar, esta forma ingenua de seguimiento podr\u00eda salirse de control.</p> <p>Entonces, si sigues el lema de los profesionales del aprendizaje autom\u00e1tico de \u00a1experimenta, experimenta, experimenta!, querr\u00e1s una forma de rastrearlos.</p> <p>Despu\u00e9s de crear algunos modelos y realizar un seguimiento de sus resultados, comenzar\u00e1s a notar lo r\u00e1pido que se puede salir de control.</p>"},{"location":"10-07_pytorch_experiment_tracking/#diferentes-formas-de-realizar-un-seguimiento-de-los-experimentos-de-aprendizaje-automatico","title":"Diferentes formas de realizar un seguimiento de los experimentos de aprendizaje autom\u00e1tico\u00b6","text":"<p>Hay tantas formas diferentes de realizar un seguimiento de los experimentos de aprendizaje autom\u00e1tico como experimentos para ejecutar.</p> <p>Esta tabla cubre algunos.</p> M\u00e9todo Configuraci\u00f3n Ventajas Desventajas Costo Diccionarios Python, archivos CSV, impresiones Ninguno F\u00e1cil de configurar, se ejecuta en Python puro Es dif\u00edcil realizar un seguimiento de un gran n\u00famero de experimentos Gratis TensorBoard M\u00ednimo, instale <code>tensorboard</code> Las extensiones integradas en PyTorch, ampliamente reconocidas y utilizadas, se escalan f\u00e1cilmente. La experiencia del usuario no es tan agradable como la de otras opciones. Gratis Seguimiento de experimentos de pesos y sesgos M\u00ednimo, instale <code>wandb</code>, cree una cuenta Incre\u00edble experiencia de usuario, hacer p\u00fablicos los experimentos, rastrear casi cualquier cosa. Requiere recursos externos fuera de PyTorch. Gratis para uso personal MLFlow M\u00ednimo, instale <code>mlflow</code> e inicie el seguimiento Gesti\u00f3n del ciclo de vida de MLOps totalmente de c\u00f3digo abierto, muchas integraciones. Es un poco m\u00e1s dif\u00edcil configurar un servidor de seguimiento remoto que otros servicios. Gratis <p>Varios lugares y t\u00e9cnicas que puede utilizar para realizar un seguimiento de sus experimentos de aprendizaje autom\u00e1tico. Nota: Hay varias otras opciones similares a Weights &amp; Biases y opciones de c\u00f3digo abierto similares a MLflow, pero las omit\u00ed por brevedad. Puede encontrar m\u00e1s informaci\u00f3n buscando \"seguimiento de experimentos de aprendizaje autom\u00e1tico\".</p>"},{"location":"10-07_pytorch_experiment_tracking/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>Realizaremos varios experimentos de modelado diferentes con varios niveles de datos, tama\u00f1o del modelo y tiempo de entrenamiento para intentar mejorar FoodVision Mini.</p> <p>Y debido a su estrecha integraci\u00f3n con PyTorch y su uso generalizado, este cuaderno se centra en el uso de TensorBoard para realizar un seguimiento de nuestros experimentos.</p> <p>Sin embargo, los principios que vamos a cubrir son similares en todas las dem\u00e1s herramientas para el seguimiento de experimentos.</p> Tema Contenido 0. Obteniendo configuraci\u00f3n Hemos escrito bastante c\u00f3digo \u00fatil en las \u00faltimas secciones, descargu\u00e9moslo y asegur\u00e9monos de poder usarlo nuevamente. 1. Obtener datos Obtengamos el conjunto de datos de clasificaci\u00f3n de im\u00e1genes de pizza, bistec y sushi que hemos estado usando para intentar mejorar los resultados de nuestro modelo FoodVision Mini. 2. Crear conjuntos de datos y cargadores de datos Usaremos el script <code>data_setup.py</code> que escribimos en el cap\u00edtulo 05. PyTorch se vuelve modular para configurar nuestros DataLoaders. 3. Obtenga y personalice un modelo previamente entrenado Al igual que en la \u00faltima secci\u00f3n, 06. PyTorch Transfer Learning, descargaremos un modelo previamente entrenado de <code>torchvision.models</code> y lo personalizaremos seg\u00fan nuestro propio problema. 4. Modelo de tren y resultados de v\u00eda Veamos c\u00f3mo es entrenar y rastrear los resultados del entrenamiento de un solo modelo usando TensorBoard. 5. Vea los resultados de nuestro modelo en TensorBoard Anteriormente visualizamos las curvas de p\u00e9rdida de nuestro modelo con una funci\u00f3n auxiliar, ahora veamos c\u00f3mo se ven en TensorBoard. 6. Creando una funci\u00f3n auxiliar para rastrear experimentos Si vamos a seguir el lema del practicante de aprendizaje autom\u00e1tico de \u00a1experimentar, experimentar, experimentar!, lo mejor ser\u00e1 que creemos una funci\u00f3n que nos ayude a guardar los resultados de nuestro experimento de modelado. 7. Configuraci\u00f3n de una serie de experimentos de modelado En lugar de ejecutar experimentos uno por uno, \u00bfqu\u00e9 tal si escribimos c\u00f3digo para ejecutar varios experimentos a la vez, con diferentes modelos, diferentes cantidades de datos y diferentes tiempos de entrenamiento? 8. Ver experimentos de modelado en TensorBoard En esta etapa habremos realizado ocho experimentos de modelado de una sola vez, bastante para realizar un seguimiento; veamos c\u00f3mo se ven sus resultados en TensorBoard. 9. Cargue el mejor modelo y haga predicciones con \u00e9l El objetivo del seguimiento del experimento es descubrir qu\u00e9 modelo funciona mejor, carguemos el modelo con mejor rendimiento y hagamos algunas predicciones con \u00e9l para \u00a1visualizar, visualizar, visualizar!."},{"location":"10-07_pytorch_experiment_tracking/#donde-puedes-obtener-ayuda","title":"\u00bfD\u00f3nde puedes obtener ayuda?\u00b6","text":"<p>Todos los materiales de este curso est\u00e1n disponibles en GitHub.</p> <p>Si tiene problemas, puede hacer una pregunta en el curso [p\u00e1gina de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).</p> <p>Y, por supuesto, est\u00e1 la documentaci\u00f3n de PyTorch y los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"10-07_pytorch_experiment_tracking/#0-configuracion","title":"0. Configuraci\u00f3n\u00b6","text":"<p>Comencemos descargando todos los m\u00f3dulos que necesitaremos para esta secci\u00f3n.</p> <p>Para ahorrarnos escribir c\u00f3digo adicional, aprovecharemos algunos de los scripts de Python (como <code>data_setup.py</code> y <code>engine.py</code>) que creamos en la secci\u00f3n 05. PyTorch se vuelve modular.</p> <p>Espec\u00edficamente, vamos a descargar el directorio <code>going_modular</code> del repositorio <code>pytorch-deep-learning</code> (si a\u00fan no lo tenemos).</p> <p>Tambi\u00e9n obtendremos el paquete <code>torchinfo</code> si no est\u00e1 disponible.</p> <p><code>torchinfo</code> nos ayudar\u00e1 m\u00e1s adelante a brindarnos res\u00famenes visuales de nuestro(s) modelo(s).</p> <p>Y dado que estamos usando una versi\u00f3n m\u00e1s nueva del paquete <code>torchvision</code> (v0.13 a partir de junio de 2022), nos aseguraremos de tener las \u00faltimas versiones.</p>"},{"location":"10-07_pytorch_experiment_tracking/#crear-una-funcion-auxiliar-para-establecer-semillas","title":"Crear una funci\u00f3n auxiliar para establecer semillas\u00b6","text":"<p>Dado que hemos estado configurando muchas semillas aleatorias en las secciones anteriores, \u00bfqu\u00e9 tal si las funcionalizamos?</p> <p>Creemos una funci\u00f3n para \"establecer las semillas\" llamada <code>set_seeds()</code>.</p> <p>Nota: Recuerde que una semilla aleatoria es una forma de darle sabor a la aleatoriedad generada por una computadora. No es necesario configurarlos siempre cuando se ejecuta c\u00f3digo de aprendizaje autom\u00e1tico; sin embargo, ayudan a garantizar que haya un elemento de reproducibilidad (los n\u00fameros que obtengo con mi c\u00f3digo son similares a los n\u00fameros que obtienes con tu c\u00f3digo). Fuera de un entorno educativo o experimental, generalmente no se requieren semillas aleatorias.</p>"},{"location":"10-07_pytorch_experiment_tracking/#1-obtener-datos","title":"1. Obtener datos\u00b6","text":"<p>Como siempre, antes de que podamos ejecutar experimentos de aprendizaje autom\u00e1tico, necesitaremos un conjunto de datos.</p> <p>Continuaremos intentando mejorar los resultados que hemos obtenido con FoodVision Mini.</p> <p>En el apartado anterior, 06. PyTorch Transfer Learning, vimos lo poderoso que puede ser el uso de un modelo previamente entrenado y el aprendizaje por transferencia al clasificar im\u00e1genes de pizza, bistec y sushi.</p> <p>Entonces, \u00bfqu\u00e9 tal si realizamos algunos experimentos e intentamos mejorar a\u00fan m\u00e1s nuestros resultados?</p> <p>Para hacerlo, usaremos un c\u00f3digo similar al de la secci\u00f3n anterior para descargar <code>pizza_steak_sushi.zip</code> (si los datos a\u00fan no existen) excepto que esta vez se ha funcionalizado.</p> <p>Esto nos permitir\u00e1 volver a utilizarlo m\u00e1s adelante.</p>"},{"location":"10-07_pytorch_experiment_tracking/#2-crear-conjuntos-de-datos-y-cargadores-de-datos","title":"2. Crear conjuntos de datos y cargadores de datos\u00b6","text":"<p>Ahora que tenemos algunos datos, convirt\u00e1moslos en PyTorch DataLoaders.</p> <p>Podemos hacerlo usando la funci\u00f3n <code>create_dataloaders()</code> que creamos en 05. PyTorch se vuelve modular, parte 2.</p> <p>Y dado que usaremos aprendizaje por transferencia y modelos espec\u00edficamente entrenados previamente de <code>torchvision.models</code>, crearemos una transformaci\u00f3n para preparar nuestras im\u00e1genes correctamente. .</p> <p>Para transformar nuestras im\u00e1genes en tensores, podemos usar:</p> <ol> <li>Transformaciones creadas manualmente usando <code>torchvision.transforms</code>.</li> <li>Transformaciones creadas autom\u00e1ticamente usando <code>torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()</code>.<ul> <li>Donde <code>MODEL_NAME</code> es una arquitectura espec\u00edfica de <code>torchvision.models</code>, <code>MODEL_WEIGHTS</code> es un conjunto espec\u00edfico de pesos previamente entrenados y <code>DEFAULT</code> significa los \"mejores pesos disponibles\".</li> </ul> </li> </ol> <p>Vimos un ejemplo de cada uno de estos en [06. Secci\u00f3n 2 del aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#2-create-datasets-and-dataloaders).</p> <p>Veamos primero un ejemplo de creaci\u00f3n manual de una canalizaci\u00f3n <code>torchvision.transforms</code> (crear una canalizaci\u00f3n de transformaciones de esta manera brinda la mayor personalizaci\u00f3n, pero puede resultar potencialmente en una degradaci\u00f3n del rendimiento si las transformaciones no coinciden con el modelo previamente entrenado).</p> <p>La principal transformaci\u00f3n manual de la que debemos estar seguros es que todas nuestras im\u00e1genes est\u00e9n normalizadas en formato ImageNet (esto se debe a que los <code>torchvision.models</code> previamente entrenados est\u00e1n todos preentrenados en [ImageNet] (https://www.image-net.org /)).</p> <p>Podemos hacer esto con:</p> <pre><code>pit\u00f3n\nnormalizar = transforma.Normalizar(media=[0.485, 0.456, 0.406],\n                                 est\u00e1ndar=[0,229, 0,224, 0,225])\n</code></pre>"},{"location":"10-07_pytorch_experiment_tracking/#21-crear-cargadores-de-datos-utilizando-transformaciones-creadas-manualmente","title":"2.1 Crear cargadores de datos utilizando transformaciones creadas manualmente\u00b6","text":""},{"location":"10-07_pytorch_experiment_tracking/#22-crear-cargadores-de-datos-utilizando-transformaciones-creadas-automaticamente","title":"2.2 Crear cargadores de datos utilizando transformaciones creadas autom\u00e1ticamente\u00b6","text":"<p>\u00a1Datos transformados y DataLoaders creados!</p> <p>Veamos ahora c\u00f3mo se ve el mismo proceso de transformaci\u00f3n, pero esta vez mediante transformaciones autom\u00e1ticas.</p> <p>Podemos hacer esto creando primero una instancia de un conjunto de pesos previamente entrenados (por ejemplo, <code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT</code>) que nos gustar\u00eda usar y llamando al m\u00e9todo <code>transforms()</code>.</p>"},{"location":"10-07_pytorch_experiment_tracking/#3-obtener-un-modelo-previamente-entrenado-congelar-las-capas-base-y-cambiar-el-cabezal-clasificador","title":"3. Obtener un modelo previamente entrenado, congelar las capas base y cambiar el cabezal clasificador\u00b6","text":"<p>Antes de ejecutar y realizar un seguimiento de varios experimentos de modelado, veamos c\u00f3mo es ejecutar y realizar un seguimiento de uno solo.</p> <p>Y como nuestros datos est\u00e1n listos, lo siguiente que necesitaremos es un modelo.</p> <p>Descarguemos los pesos previamente entrenados para un modelo <code>torchvision.models.ficientnet_b0()</code> y prepar\u00e9moslo para usarlo con nuestros propios datos.</p>"},{"location":"10-07_pytorch_experiment_tracking/#4-entrenar-el-modelo-y-realizar-un-seguimiento-de-los-resultados","title":"4. Entrenar el modelo y realizar un seguimiento de los resultados.\u00b6","text":"<p>\u00a1Modelo listo para funcionar!</p> <p>Prepar\u00e9monos para entrenarlo creando una funci\u00f3n de p\u00e9rdida y un optimizador.</p> <p>Como estamos trabajando con varias clases, usaremos <code>torch.nn.CrossEntropyLoss()</code> como p\u00e9rdida funci\u00f3n.</p> <p>Y nos quedaremos con <code>torch.optim.Adam()</code> con una tasa de aprendizaje de <code>0,001</code> para el optimizador.</p>"},{"location":"10-07_pytorch_experiment_tracking/#ajuste-la-funcion-train-para-realizar-un-seguimiento-de-los-resultados-con-summarywriter","title":"Ajuste la funci\u00f3n <code>train()</code> para realizar un seguimiento de los resultados con <code>SummaryWriter()</code>\u00b6","text":"<p>\u00a1Hermoso!</p> <p>Todas las piezas de nuestro c\u00f3digo de formaci\u00f3n est\u00e1n empezando a encajar.</p> <p>Agreguemos ahora la pieza final para realizar un seguimiento de nuestros experimentos.</p> <p>Anteriormente, hemos realizado un seguimiento de nuestros experimentos de modelado utilizando varios diccionarios de Python (uno para cada modelo).</p> <p>Pero puedes imaginar que esto podr\u00eda salirse de control si estuvi\u00e9ramos realizando algo m\u00e1s que unos pocos experimentos.</p> <p>\u00a1No te preocupes, hay una mejor opci\u00f3n!</p> <p>Podemos usar la clase <code>torch.utils.tensorboard.SummaryWriter()</code> de PyTorch para guardar varias partes del progreso del entrenamiento de nuestro modelo en un archivo.</p> <p>De forma predeterminada, la clase <code>SummaryWriter()</code> guarda diversa informaci\u00f3n sobre nuestro modelo en un archivo establecido por el par\u00e1metro <code>log_dir</code>.</p> <p>La ubicaci\u00f3n predeterminada para <code>log_dir</code> est\u00e1 en <code>runs/CURRENT_DATETIME_HOSTNAME</code>, donde <code>HOSTNAME</code> es el nombre de su computadora.</p> <p>Pero, por supuesto, puedes cambiar d\u00f3nde se realiza el seguimiento de tus experimentos (el nombre del archivo es tan personalizable como quieras).</p> <p>Las salidas de <code>SummaryWriter()</code> se guardan en [formato TensorBoard] (https://www.tensorflow.org/tensorboard/).</p> <p>TensorBoard es parte de la biblioteca de aprendizaje profundo de TensorFlow y es una excelente manera de visualizar diferentes partes de su modelo.</p> <p>Para comenzar a rastrear nuestros experimentos de modelado, creemos una instancia predeterminada <code>SummaryWriter()</code>.</p>"},{"location":"10-07_pytorch_experiment_tracking/#5-ver-los-resultados-de-nuestro-modelo-en-tensorboard","title":"5. Ver los resultados de nuestro modelo en TensorBoard\u00b6","text":"<p>La clase <code>SummaryWriter()</code> almacena los resultados de nuestro modelo en un directorio llamado <code>runs/</code> en formato TensorBoard de forma predeterminada.</p> <p>TensorBoard es un programa de visualizaci\u00f3n creado por el equipo de TensorFlow para ver e inspeccionar informaci\u00f3n sobre modelos y datos.</p> <p>\u00bfSabes lo que significa?</p> <p>Es hora de seguir el lema del visualizador de datos y \u00a1visualizar, visualizar, visualizar!</p> <p>Puedes ver TensorBoard de varias maneras:</p> Entorno de c\u00f3digo C\u00f3mo ver TensorBoard Recurso VS Code (cuadernos o scripts de Python) Presione <code>SHIFT + CMD + P</code> para abrir la paleta de comandos y busque el comando \"Python: Iniciar TensorBoard\". Gu\u00eda de c\u00f3digo VS en TensorBoard y PyTorch Cuadernos Jupyter y Colab Aseg\u00farese de que [TensorBoard est\u00e9 instalado] (https://pypi.org/project/tensorboard/), c\u00e1rguelo con <code>%load_ext tensorboard</code> y luego vea los resultados con <code>%tensorboard --logdir DIR_WITH_LOGS</code>. <code>torch.utils.tensorboard</code> y Comenzar con TensorBoard <p>Tambi\u00e9n puedes subir tus experimentos a tensorboard.dev para compartirlos p\u00fablicamente con otros.</p> <p>Al ejecutar el siguiente c\u00f3digo en Google Colab o Jupyter Notebook se iniciar\u00e1 una sesi\u00f3n interactiva de TensorBoard para ver los archivos de TensorBoard en el directorio <code>runs/</code>.</p> <pre><code>pit\u00f3n\n%load_ext tensorboard # l\u00ednea m\u00e1gica para cargar TensorBoard\n%tensorboard --logdir ejecuta # ejecuta la sesi\u00f3n de TensorBoard con el directorio \"ejecuta/\"\n</code></pre>"},{"location":"10-07_pytorch_experiment_tracking/#6-cree-una-funcion-auxiliar-para-crear-instancias-summarywriter","title":"6. Cree una funci\u00f3n auxiliar para crear instancias <code>SummaryWriter()</code>\u00b6","text":"<p>La clase <code>SummaryWriter()</code> registra diversa informaci\u00f3n en un directorio especificado por el par\u00e1metro <code>log_dir</code>.</p> <p>\u00bfQu\u00e9 tal si creamos una funci\u00f3n auxiliar para crear un directorio personalizado por experimento?</p> <p>En esencia, cada experimento tiene su propio directorio de registros.</p> <p>Por ejemplo, digamos que nos gustar\u00eda realizar un seguimiento de cosas como:</p> <ul> <li>Fecha/marca de tiempo del experimento: \u00bfcu\u00e1ndo tuvo lugar el experimento?</li> <li>Nombre del experimento: \u00bfhay algo que nos gustar\u00eda llamar al experimento?</li> <li>Nombre del modelo: \u00bfqu\u00e9 modelo se utiliz\u00f3?</li> <li>Extra: \u00bfse debe realizar un seguimiento de algo m\u00e1s?</li> </ul> <p>Puedes rastrear casi cualquier cosa aqu\u00ed y ser tan creativo como quieras, pero esto deber\u00eda ser suficiente para comenzar.</p> <p>Creemos una funci\u00f3n auxiliar llamada <code>create_writer()</code> que produzca un seguimiento de instancia <code>SummaryWriter()</code> a un <code>log_dir</code> personalizado.</p> <p>Idealmente, nos gustar\u00eda que <code>log_dir</code> fuera algo como:</p> <p><code>ejecuta/AAAA-MM-DD/nombre_experimento/nombre_modelo/extra</code></p> <p>Donde \"AAAA-MM-DD\" es la fecha en que se ejecut\u00f3 el experimento (tambi\u00e9n puede agregar la hora si lo desea).</p>"},{"location":"10-07_pytorch_experiment_tracking/#61-actualice-la-funcion-train-para-incluir-un-parametro-writer","title":"6.1 Actualice la funci\u00f3n <code>train()</code> para incluir un par\u00e1metro <code>writer</code>\u00b6","text":"<p>Nuestra funci\u00f3n <code>create_writer()</code> funciona fant\u00e1stico.</p> <p>\u00bfQu\u00e9 tal si le damos a nuestra funci\u00f3n <code>train()</code> la capacidad de aceptar un par\u00e1metro <code>writer</code> para que actualicemos activamente la instancia <code>SummaryWriter()</code> que estamos usando cada vez que llamamos a <code>train()</code>?</p> <p>Por ejemplo, digamos que estamos ejecutando una serie de experimentos, llamando a \"train()\" varias veces para m\u00faltiples modelos diferentes, ser\u00eda bueno si cada experimento usara un \"escritor\" diferente.</p> <p>Un \"escritor\" por experimento = un directorio de registros por experimento.</p> <p>Para ajustar la funci\u00f3n <code>train()</code> agregaremos un par\u00e1metro <code>writer</code> a la funci\u00f3n y luego agregaremos algo de c\u00f3digo para ver si hay un <code>writer</code> y, de ser as\u00ed, rastrearemos nuestra informaci\u00f3n all\u00ed.</p>"},{"location":"10-07_pytorch_experiment_tracking/#7-configuracion-de-una-serie-de-experimentos-de-modelado","title":"7. Configuraci\u00f3n de una serie de experimentos de modelado.\u00b6","text":"<p>Es para dar un paso m\u00e1s en las cosas.</p> <p>Anteriormente hemos realizado varios experimentos e inspeccionado los resultados uno por uno.</p> <p>Pero, \u00bfqu\u00e9 pasar\u00eda si pudi\u00e9ramos realizar varios experimentos y luego inspeccionar todos los resultados juntos?</p> <p>\u00bfTe unes?</p> <p>Vamos, v\u00e1monos.</p>"},{"location":"10-07_pytorch_experiment_tracking/#71-que-tipo-de-experimentos-deberia-realizar","title":"7.1 \u00bfQu\u00e9 tipo de experimentos deber\u00eda realizar?\u00b6","text":"<p>Esa es la pregunta del mill\u00f3n en el aprendizaje autom\u00e1tico.</p> <p>Porque realmente no hay l\u00edmite para los experimentos que puedes realizar.</p> <p>Esta libertad es la raz\u00f3n por la que el aprendizaje autom\u00e1tico es tan emocionante y aterrador al mismo tiempo.</p> <p>Aqu\u00ed es donde tendr\u00e1s que ponerte tu bata de cient\u00edfico y recordar el lema de los profesionales del aprendizaje autom\u00e1tico: \u00a1experimenta, experimenta, experimenta!</p> <p>Cada hiperpar\u00e1metro constituye un punto de partida para un experimento diferente:</p> <ul> <li>Cambiar el n\u00famero de \u00e9pocas.</li> <li>Cambiar el n\u00famero de capas/unidades ocultas.</li> <li>Cambiar la cantidad de datos.</li> <li>Cambiar la tasa de aprendizaje.</li> <li>Pruebe diferentes tipos de aumento de datos.</li> <li>Elija una arquitectura de modelo diferente.</li> </ul> <p>Con pr\u00e1ctica y realizando muchos experimentos diferentes, comenzar\u00e1s a desarrollar una intuici\u00f3n de lo que podr\u00eda ayudar a tu modelo.</p> <p>Digo podr\u00eda a prop\u00f3sito porque no hay garant\u00edas.</p> <p>Pero en general, a la luz de The Bitter Lesson (lo he mencionado dos veces porque es un ensayo importante en el mundo de la IA), En general, cuanto m\u00e1s grande sea su modelo (m\u00e1s par\u00e1metros que se pueden aprender) y m\u00e1s datos tenga (m\u00e1s oportunidades de aprender), mejor ser\u00e1 el rendimiento.</p> <p>Sin embargo, cuando se acerque por primera vez a un problema de aprendizaje autom\u00e1tico: comience poco a poco y, si algo funciona, ampl\u00edelo.</p> <p>Su primer lote de experimentos no deber\u00eda tardar m\u00e1s de unos segundos o unos minutos en ejecutarse.</p> <p>Cuanto m\u00e1s r\u00e1pido puedas experimentar, m\u00e1s r\u00e1pido podr\u00e1s descubrir lo que no funciona y, a su vez, m\u00e1s r\u00e1pido podr\u00e1s descubrir lo que s\u00ed funciona.</p>"},{"location":"10-07_pytorch_experiment_tracking/#72-que-experimentos-vamos-a-realizar","title":"7.2 \u00bfQu\u00e9 experimentos vamos a realizar?\u00b6","text":"<p>Nuestro objetivo es mejorar el modelo que impulsa FoodVision Mini sin que crezca demasiado.</p> <p>En esencia, nuestro modelo ideal logra un alto nivel de precisi\u00f3n del conjunto de pruebas (m\u00e1s del 90 %), pero no lleva demasiado tiempo entrenar/realizar inferencias (hacer predicciones).</p> <p>Tenemos muchas opciones, pero \u00bfqu\u00e9 tal si mantenemos las cosas simples?</p> <p>Probemos una combinaci\u00f3n de:</p> <ol> <li>Una cantidad de datos diferente (10 % de pizza, bistec y sushi frente a 20 %)</li> <li>Un modelo diferente (<code>torchvision.models.ficientnet_b0</code> vs. <code>torchvision. modelos.eficientenet_b2</code>)</li> <li>Un tiempo de entrenamiento diferente (5 \u00e9pocas frente a 10 \u00e9pocas)</li> </ol> <p>Desglos\u00e1ndolos obtenemos:</p> N\u00famero de experimento Conjunto de datos de entrenamiento Modelo (preentrenado en ImageNet) N\u00famero de \u00e9pocas 1 Pizza, bistec, sushi 10% por ciento EfficientNetB0 5 2 Pizza, bistec, sushi 10% por ciento EfficientNetB2 5 3 Pizza, bistec, sushi 10% por ciento EfficientNetB0 10 4 Pizza, bistec, sushi 10% por ciento EfficientNetB2 10 5 Pizza, bistec, sushi 20% por ciento EfficientNetB0 5 6 Pizza, bistec, sushi 20% por ciento EfficientNetB2 5 7 Pizza, bistec, sushi 20% por ciento EfficientNetB0 10 8 Pizza, bistec, sushi 20% por ciento EfficientNetB2 10 <p>Observe c\u00f3mo poco a poco estamos ampliando las cosas.</p> <p>Con cada experimento aumentamos lentamente la cantidad de datos, el tama\u00f1o del modelo y la duraci\u00f3n del entrenamiento.</p> <p>Al final, el experimento 8 utilizar\u00e1 el doble de datos, el doble del tama\u00f1o del modelo y el doble de duraci\u00f3n del entrenamiento en comparaci\u00f3n con el experimento 1.</p> <p>Nota: Quiero dejar claro que realmente no hay l\u00edmite para la cantidad de experimentos que puedes ejecutar. Lo que hemos dise\u00f1ado aqu\u00ed es s\u00f3lo un subconjunto muy peque\u00f1o de opciones. Sin embargo, no puedes probar todo as\u00ed que es mejor probar algunas cosas para empezar y luego seguir las que funcionan mejor.</p> <p>Y como recordatorio, los conjuntos de datos que estamos usando son un subconjunto del conjunto de datos Food101 (3 clases, pizza, bistec, sushi, en lugar de 101) y 10% y 20% de las im\u00e1genes en lugar de 100%. Si nuestros experimentos funcionan, podr\u00edamos comenzar a ejecutar m\u00e1s con m\u00e1s datos (aunque esto llevar\u00e1 m\u00e1s tiempo calcularlo). Puede ver c\u00f3mo se crearon los conjuntos de datos a trav\u00e9s del [cuaderno <code>04_custom_data_creation.ipynb</code>] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb).</p>"},{"location":"10-07_pytorch_experiment_tracking/#73-descargar-diferentes-conjuntos-de-datos","title":"7.3 Descargar diferentes conjuntos de datos\u00b6","text":"<p>Antes de comenzar a ejecutar nuestra serie de experimentos, debemos asegurarnos de que nuestros conjuntos de datos est\u00e9n listos.</p> <p>Necesitaremos dos formas de conjunto de entrenamiento:</p> <ol> <li>Un conjunto de entrenamiento con 10% de los datos de im\u00e1genes de pizza, bistec y sushi de Food101 (ya hemos creado esto arriba, pero lo haremos nuevamente para que est\u00e9 completo).</li> <li>Un conjunto de entrenamiento con 20% de los datos de im\u00e1genes de pizza, bistec y sushi de Food101.</li> </ol> <p>Para mantener la coherencia, todos los experimentos utilizar\u00e1n el mismo conjunto de datos de prueba (el de la divisi\u00f3n de datos del 10%).</p> <p>Comenzaremos descargando los diversos conjuntos de datos que necesitamos usando la funci\u00f3n <code>download_data()</code> que creamos anteriormente.</p> <p>Ambos conjuntos de datos est\u00e1n disponibles en el curso GitHub:</p> <ol> <li>Pizza, bistec, sushi 10% de datos de entrenamiento.</li> <li>Pizza, bistec, sushi 20% de datos de entrenamiento.</li> </ol>"},{"location":"10-07_pytorch_experiment_tracking/#74-transformar-conjuntos-de-datos-y-crear-cargadores-de-datos","title":"7.4 Transformar conjuntos de datos y crear cargadores de datos\u00b6","text":"<p>A continuaci\u00f3n, crearemos una serie de transformaciones para preparar nuestras im\u00e1genes para nuestro(s) modelo(s).</p> <p>Para mantener la coherencia, crearemos manualmente una transformaci\u00f3n (tal como lo hicimos anteriormente) y usaremos la misma transformaci\u00f3n en todos los conjuntos de datos.</p> <p>La transformaci\u00f3n:</p> <ol> <li>Cambie el tama\u00f1o de todas las im\u00e1genes (comenzaremos con 224, 224 pero esto podr\u00eda cambiarse).</li> <li>Convi\u00e9rtelos en tensores con valores entre 0 y 1.</li> <li>Normal\u00edcelos de manera que sus distribuciones est\u00e9n alineadas con el conjunto de datos de ImageNet (hacemos esto porque nuestros modelos de <code>torchvision.models</code> han sido entrenados previamente en ImageNet).</li> </ol>"},{"location":"10-07_pytorch_experiment_tracking/#75-crear-modelos-de-extraccion-de-caracteristicas","title":"7.5 Crear modelos de extracci\u00f3n de caracter\u00edsticas\u00b6","text":"<p>Es hora de empezar a construir nuestros modelos.</p> <p>Vamos a crear dos modelos de extractor de funciones:</p> <ol> <li><code>torchvision.models.ficientnet_b0()</code> columna vertebral previamente entrenada + cabezal clasificador personalizado (EffNetB0 para abreviar).</li> <li><code>torchvision.models.ficientnet_b2()</code> columna vertebral previamente entrenada + cabezal clasificador personalizado (EffNetB2 para abreviar).</li> </ol> <p>Para hacer esto, congelaremos las capas base (las capas de caracter\u00edsticas) y actualizaremos los cabezales clasificadores del modelo (capas de salida) para adaptarnos a nuestro problema tal como lo hicimos en [06. Secci\u00f3n 3.4 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs).</p> <p>Vimos en el cap\u00edtulo anterior que el par\u00e1metro <code>in_features</code> para el cabezal clasificador de EffNetB0 es <code>1280</code> (la columna vertebral convierte la imagen de entrada en un vector de caracter\u00edsticas de tama\u00f1o <code>1280</code>).</p> <p>Dado que EffNetB2 tiene un n\u00famero diferente de capas y par\u00e1metros, necesitaremos adaptarlo en consecuencia.</p> <p>Nota: Siempre que utilices un modelo diferente, una de las primeras cosas que debes inspeccionar son las formas de entrada y salida. De esa manera sabr\u00e1 c\u00f3mo tendr\u00e1 que preparar sus datos de entrada/actualizar el modelo para tener la forma de salida correcta.</p> <p>Podemos encontrar las formas de entrada y salida de EffNetB2 usando <code>torchinfo.summary()</code> y pasando <code>input_size=(32, 3, 224, 224)</code> El par\u00e1metro (<code>(32, 3, 224, 224)</code> es equivalente a <code>(batch_size, color_channels, height, width)</code>, es decir, pasamos un ejemplo de lo que ser\u00eda un solo lote de datos a nuestro modelo).</p> <p>Nota: Muchos modelos modernos pueden manejar im\u00e1genes de entrada de diferentes tama\u00f1os gracias a [<code>torch.nn.AdaptiveAvgPool2d()</code>](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d .html), esta capa ajusta de forma adaptativa el <code>output_size</code> de una entrada determinada seg\u00fan sea necesario. Puede probar esto pasando im\u00e1genes de entrada de diferentes tama\u00f1os a <code>torchinfo.summary()</code> o a sus propios modelos usando la capa.</p> <p>Para encontrar la forma de entrada requerida para la capa final de EffNetB2, hagamos lo siguiente:</p> <ol> <li>Cree una instancia de <code>torchvision.models.ficientnet_b2(pretrained=True)</code>.</li> <li>Vea las diversas formas de entrada y salida ejecutando <code>torchinfo.summary()</code>.</li> <li>Imprima el n\u00famero de <code>in_features</code> inspeccionando <code>state_dict()</code> de la parte del clasificador de EffNetB2 e imprimiendo la longitud de la matriz de peso.<ul> <li>Nota: Tambi\u00e9n puedes simplemente inspeccionar la salida de <code>effnetb2.classifier</code>.</li> </ul> </li> </ol>"},{"location":"10-07_pytorch_experiment_tracking/#76-crear-experimentos-y-configurar-codigo-de-entrenamiento","title":"7.6 Crear experimentos y configurar c\u00f3digo de entrenamiento\u00b6","text":"<p>Hemos preparado nuestros datos y nuestros modelos, \u00a1ha llegado el momento de configurar algunos experimentos!</p> <p>Comenzaremos creando dos listas y un diccionario:</p> <ol> <li>Una lista del n\u00famero de \u00e9pocas que nos gustar\u00eda probar (<code>[5, 10]</code>)</li> <li>Una lista de los modelos que nos gustar\u00eda probar (<code>[\"effnetb0\", \"effnetb2\"]</code>)</li> <li>Un diccionario de los diferentes DataLoaders de entrenamiento</li> </ol>"},{"location":"10-07_pytorch_experiment_tracking/#8-ver-experimentos-en-tensorboard","title":"8. Ver experimentos en TensorBoard\u00b6","text":"<p>\u00a1Ho, ho!</p> <p>\u00a1M\u00edranos ir!</p> <p>\u00bfEntrenar ocho modelos de una sola vez?</p> <p>\u00a1Eso s\u00ed que est\u00e1 a la altura del lema!</p> <p>\u00a1Experimenta, experimenta, experimenta!</p> <p>\u00bfQu\u00e9 tal si comprobamos los resultados en TensorBoard?</p>"},{"location":"10-07_pytorch_experiment_tracking/#9-cargue-el-mejor-modelo-y-haga-predicciones-con-el","title":"9. Cargue el mejor modelo y haga predicciones con \u00e9l.\u00b6","text":"<p>Al observar los registros de TensorBoard para nuestros ocho experimentos, parece que el experimento n\u00famero ocho logr\u00f3 los mejores resultados generales (mayor precisi\u00f3n de prueba, segunda p\u00e9rdida de prueba m\u00e1s baja).</p> <p>Este es el experimento que utiliz\u00f3:</p> <ul> <li>EffNetB2 (duplica los par\u00e1metros de EffNetB0)</li> <li>20% de datos de entrenamiento de pizza, bistec y sushi (el doble de los datos de entrenamiento originales)</li> <li>10 \u00e9pocas (el doble del tiempo de entrenamiento original)</li> </ul> <p>En esencia, nuestro modelo m\u00e1s grande logr\u00f3 los mejores resultados.</p> <p>Aunque no es que estos resultados fueran mucho mejores que los de otros modelos.</p> <p>El mismo modelo con los mismos datos logr\u00f3 resultados similares en la mitad del tiempo de entrenamiento (experimento n\u00famero 6).</p> <p>Esto sugiere que potencialmente las partes m\u00e1s influyentes de nuestros experimentos fueron la cantidad de par\u00e1metros y la cantidad de datos.</p> <p>Al inspeccionar m\u00e1s a fondo los resultados, parece que, en general, un modelo con m\u00e1s par\u00e1metros (EffNetB2) y m\u00e1s datos (20% de datos de entrenamiento de pizza, bistec y sushi) funciona mejor (menor p\u00e9rdida de prueba y mayor precisi\u00f3n de la prueba).</p> <p>Se podr\u00edan realizar m\u00e1s experimentos para probar esto m\u00e1s a fondo, pero por ahora, importemos nuestro modelo de mejor rendimiento del experimento ocho (guardado en: <code>models/07_effnetb2_data_20_percent_10_epochs.pth</code>, puede [descargar este modelo del curso GitHub](https:// github.com/mrdbourke/pytorch-deep-learning/blob/main/models/07_effnetb2_data_20_percent_10_epochs.pth)) y realice algunas evaluaciones cualitativas.</p> <p>En otras palabras, \u00a1visualicemos, visualicemos, visualicemos!*</p> <p>Podemos importar el mejor modelo guardado creando una nueva instancia de EffNetB2 usando la funci\u00f3n <code>create_effnetb2()</code> y luego cargar el <code>state_dict()</code> guardado con <code>torch.load()</code>.</p>"},{"location":"10-07_pytorch_experiment_tracking/#91-predecir-en-una-imagen-personalizada-con-el-mejor-modelo","title":"9.1 Predecir en una imagen personalizada con el mejor modelo\u00b6","text":"<p>Hacer predicciones sobre el conjunto de datos de prueba es genial, pero la verdadera magia del aprendizaje autom\u00e1tico es hacer predicciones sobre sus propias im\u00e1genes personalizadas.</p> <p>Entonces, importemos la confiable imagen del pap\u00e1 de la pizza (una foto de mi pap\u00e1 frente a una pizza) que hemos estado usando durante las \u00faltimas secciones y ver c\u00f3mo funciona nuestro modelo en ella.</p>"},{"location":"10-07_pytorch_experiment_tracking/#principales-conclusiones","title":"Principales conclusiones\u00b6","text":"<p>Ahora hemos completado el c\u00edrculo en el flujo de trabajo de PyTorch introducido en [01. Fundamentos del flujo de trabajo de PyTorch] (https://www.learnpytorch.io/01_pytorch_workflow/), preparamos los datos, creamos y elegimos un modelo previamente entrenado, utilizamos nuestras diversas funciones auxiliares para entrenar y evaluar el modelo. y en este cuaderno hemos mejorado nuestro modelo FoodVision Mini ejecutando y rastreando una serie de experimentos.</p> <p></p> <p>Deber\u00edas estar orgulloso de ti mismo, \u00a1esto no es poca cosa!</p> <p>Las ideas principales que debes extraer de este Proyecto Hito 1 son:</p> <ul> <li>El lema del profesional del aprendizaje autom\u00e1tico: \u00a1experimenta, experimenta, experimenta! (aunque ya hemos estado haciendo mucho de esto).</li> <li>Al principio, mantenga sus experimentos peque\u00f1os para que pueda trabajar r\u00e1pido; sus primeros experimentos no deber\u00edan tardar m\u00e1s de unos segundos o unos minutos en ejecutarse.</li> <li>Cuantos m\u00e1s experimentos hagas, m\u00e1s r\u00e1pido podr\u00e1s descubrir qu\u00e9 no funciona.</li> <li>Aumente la escala cuando encuentre algo que funcione. Por ejemplo, dado que hemos encontrado un modelo con un rendimiento bastante bueno con EffNetB2 como extractor de funciones, tal vez ahora le gustar\u00eda ver qu\u00e9 sucede cuando lo ampl\u00eda a todo el [conjunto de datos de Food101](https://pytorch.org /vision/main/generated/torchvision.datasets.Food101.html) de <code>torchvision.datasets</code>.</li> <li>El seguimiento program\u00e1tico de sus experimentos requiere algunos pasos para configurarlo, pero a la larga vale la pena para que pueda descubrir qu\u00e9 funciona y qu\u00e9 no.<ul> <li>Existen muchos rastreadores de experimentos de aprendizaje autom\u00e1tico diferentes, as\u00ed que explore algunos y pru\u00e9belos.</li> </ul> </li> </ul>"},{"location":"10-07_pytorch_experiment_tracking/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Nota: Estos ejercicios requieren el uso de <code>torchvision</code> v0.13+ (lanzado en julio de 2022); las versiones anteriores pueden funcionar, pero probablemente tendr\u00e1n errores.</p> <p>Todos los ejercicios se centran en practicar el c\u00f3digo anterior.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Todos los ejercicios deben completarse utilizando c\u00f3digo independiente del dispositivo.</p> <p>Recursos:</p> <ul> <li>Cuaderno de plantilla de ejercicios para 07</li> <li>Cuaderno de soluciones de ejemplo para 07 (pruebe los ejercicios antes de mirar esto)<ul> <li>Vea un [video tutorial de las soluciones en YouTube] en vivo (https://youtu.be/cO_r2FYcAjU) (errores y todo)</li> </ul> </li> </ul> <ol> <li>Elija un modelo m\u00e1s grande de <code>torchvision.models</code> para agregarlo a la lista de experimentos (por ejemplo, EffNetB3 o superior).<ul> <li>\u00bfC\u00f3mo funciona en comparaci\u00f3n con nuestros modelos existentes?</li> </ul> </li> <li>Introduzca el aumento de datos en la lista de experimentos que utilizan conjuntos de datos de prueba y entrenamiento de pizza, bistec y sushi al 20%. \u00bfEsto cambia algo?<ul> <li>Por ejemplo, podr\u00eda tener un DataLoader de entrenamiento que utilice aumento de datos (por ejemplo, <code>train_dataloader_20_percent_aug</code> y <code>train_dataloader_20_percent_no_aug</code>) y luego comparar los resultados de dos tipos de modelos iguales entrenando en estos dos DataLoaders.</li> <li>Nota: Es posible que necesite modificar la funci\u00f3n <code>create_dataloaders()</code> para poder realizar una transformaci\u00f3n para los datos de entrenamiento y los datos de prueba (porque no necesita realizar un aumento de datos en los datos de prueba) . Ver [04. Secci\u00f3n 6 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) para ver ejemplos de uso del aumento de datos o el siguiente script para ver un ejemplo:</li> </ul> </li> </ol> <pre><code>pit\u00f3n\n# Nota: Una transformaci\u00f3n de aumento de datos como esta solo debe realizarse en datos de entrenamiento\ntrain_transform_data_aug = transforma.Compose([\n    transforma.Resize((224, 224)),\n    transforma.TrivialAugmentWide(),\n    transforma.ToTensor(),\n    normalizar\n])\n\n# Funci\u00f3n auxiliar para ver im\u00e1genes en un DataLoader (funciona con transformaciones de aumento de datos o no) \ndef view_dataloader_images(cargador de datos, n=10):\n    si norte &gt; 10:\n        print(f\"Tener n mayor que 10 crear\u00e1 gr\u00e1ficos desordenados, reduci\u00e9ndolos a 10.\")\n        norte = 10\n    imgs, etiquetas = siguiente (iter (cargador de datos))\n    plt.figura(tama\u00f1o de figura=(16, 8))\n    para i en el rango (n):\n        # Escala m\u00ednima m\u00e1xima de la imagen para fines de visualizaci\u00f3n\n        targ_image = im\u00e1genes[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # Trazar im\u00e1genes con informaci\u00f3n de ejes apropiada\n        plt.subtrama(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # cambiar el tama\u00f1o para los requisitos de Matplotlib\n        plt.title(nombres_clase[etiquetas[i]])\n        eje.plt(Falso)\n\n# Tenemos que actualizar `create_dataloaders()` para manejar diferentes aumentos\nimportar sistema operativo\ndesde torch.utils.data importar DataLoader\ndesde conjuntos de datos de importaci\u00f3n de torchvision\n\nNUM_WORKERS = os.cpu_count() # usa el n\u00famero m\u00e1ximo de CPU para que los trabajadores carguen datos \n\n# Nota: esta es una versi\u00f3n actualizada de data_setup.create_dataloaders para manejar\n# diferentes transformaciones de tren y prueba.\ndef crear_cargadores de datos (\n    tren_dir, \n    dir_prueba, \n    train_transform, # agrega par\u00e1metro para la transformaci\u00f3n del tren (transformaciones en el conjunto de datos del tren)\n    test_transform, # agrega par\u00e1metro para la transformaci\u00f3n de prueba (transformaciones en el conjunto de datos de prueba)\n    tama\u00f1o_lote=32, num_trabajadores=NUM_TRABAJADORES\n):\n    # Utilice ImageFolder para crear conjuntos de datos\n    train_data = conjuntos de datos.ImageFolder(train_dir, transform=train_transform)\n    test_data = conjuntos de datos.ImageFolder(test_dir, transform=test_transform)\n\n    # Obtener nombres de clases\n    nombres_clase = datos_tren.clases\n\n    # Convertir im\u00e1genes en cargadores de datos\n    train_dataloader = Cargador de datos(\n        datos_tren,\n        tama\u00f1o_lote = tama\u00f1o_lote,\n        barajar = Verdadero,\n        num_trabajadores=num_trabajadores,\n        pin_memory=Verdadero,\n    )\n    test_dataloader = Cargador de datos(\n        datos de prueba,\n        tama\u00f1o_lote = tama\u00f1o_lote,\n        barajar = Verdadero,\n        num_trabajadores=num_trabajadores,\n        pin_memory=Verdadero,\n    )\n\n    devolver train_dataloader, test_dataloader, class_names\n</code></pre> <ol> <li>Ampl\u00ede el conjunto de datos para convertir FoodVision Mini en FoodVision Big utilizando todo el [conjunto de datos Food101 de <code>torchvision.models</code>](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision .conjuntos de datos.Food101)<ul> <li>Podr\u00eda tomar el modelo de mejor rendimiento de sus diversos experimentos o incluso el extractor de funciones EffNetB2 que creamos en este cuaderno y ver c\u00f3mo se adapta durante 5 \u00e9pocas en todo Food101.</li> <li>Si prueba m\u00e1s de un modelo, ser\u00eda bueno realizar un seguimiento de los resultados del modelo.</li> <li>Si carga el conjunto de datos Food101 desde <code>torchvision.models</code>, deber\u00e1 crear PyTorch DataLoaders para usarlo en el entrenamiento.</li> <li>Nota: Debido a la mayor cantidad de datos en Food101 en comparaci\u00f3n con nuestro conjunto de datos de pizza, bistec y sushi, este modelo tardar\u00e1 m\u00e1s en entrenarse.</li> </ul> </li> </ol>"},{"location":"10-07_pytorch_experiment_tracking/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Lea la publicaci\u00f3n del blog The Bitter Lesson de Richard Sutton para tener una idea de cu\u00e1ntos de los \u00faltimos avances en IA provienen de una mayor escala (conjuntos de datos m\u00e1s grandes). y modelos m\u00e1s grandes) y m\u00e9todos m\u00e1s generales (menos meticulosamente elaborados).</li> <li>Consulte el tutorial de c\u00f3digo/YouTube de PyTorch para TensorBoard durante 20 minutos y vea c\u00f3mo se compara con el c\u00f3digo que hemos escrito en este computadora port\u00e1til.</li> <li>Quiz\u00e1s quieras ver y reorganizar los registros de TensorBoard de tu modelo con un DataFrame (para que puedas ordenar los resultados por menor p\u00e9rdida o mayor precisi\u00f3n), hay una gu\u00eda para esto [en la documentaci\u00f3n de TensorBoard] (https://www.tensorflow .org/tensorboard/dataframe_api).</li> <li>Si desea usar VSCode para el desarrollo usando scripts o cuadernos (VSCode ahora puede usar Jupyter Notebooks de forma nativa), puede configurar TensorBoard directamente dentro de VSCode usando la [Gu\u00eda de desarrollo de PyTorch en VSCode] (https://code.visualstudio.com/ documentos/datascience/pytorch-support).</li> <li>Para ir m\u00e1s all\u00e1 con el seguimiento de experimentos y ver c\u00f3mo se est\u00e1 desempe\u00f1ando su modelo PyTorch desde una perspectiva de velocidad (\u00bfhay alg\u00fan cuello de botella que podr\u00eda mejorarse para acelerar el entrenamiento?), consulte la [documentaci\u00f3n de PyTorch para el generador de perfiles de PyTorch](https:// pytorch.org/blog/introduciendo-pytorch-profiler-the-new-and-improved-rendimiento-herramienta/).</li> <li>Made With ML es un excelente recurso para todo lo relacionado con el aprendizaje autom\u00e1tico de Goku Mohandas y su gu\u00eda sobre seguimiento de experimentos contiene una fant\u00e1stica introducci\u00f3n al seguimiento del aprendizaje autom\u00e1tico. experimentos con MLflow.</li> </ul>"},{"location":"11-06_pytorch_transfer_learning/","title":"06. Aprendizaje por transferencia de PyTorch","text":"<p>Ver c\u00f3digo fuente | [Ver diapositivas] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/06_pytorch_transfer_learning.pdf)</p> In\u00a0[\u00a0]: Copied! <pre># Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+. try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") In\u00a0[\u00a0]: Copied! <pre># Continuar con las importaciones regulares\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Intente obtener torchinfo, inst\u00e1lelo si no funciona\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continuar con las importaciones regulares import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Intente obtener torchinfo, inst\u00e1lelo si no funciona try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Ahora configuremos el c\u00f3digo independiente del dispositivo.</p> <p>Nota: Si est\u00e1s usando Google Colab y a\u00fan no tienes una GPU activada, ahora es el momento de activar una a trav\u00e9s de <code>Runtime -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Acelerador de hardware -&gt; GPU</code> .</p> In\u00a0[\u00a0]: Copied! <pre># Configurar c\u00f3digo independiente del dispositivo\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Configurar c\u00f3digo independiente del dispositivo device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Ruta de configuraci\u00f3n a la carpeta de datos\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# Si la carpeta de im\u00e1genes no existe, desc\u00e1rgala y prep\u00e1rala...\nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n</pre> import os import zipfile  from pathlib import Path  import requests  # Ruta de configuraci\u00f3n a la carpeta de datos data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # Si la carpeta de im\u00e1genes no existe, desc\u00e1rgala y prep\u00e1rala... if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path)      # Remove .zip file     os.remove(data_path / \"pizza_steak_sushi.zip\") <p>\u00a1Excelente!</p> <p>Ahora tenemos el mismo conjunto de datos que hemos estado usando anteriormente, una serie de im\u00e1genes de pizza, bistec y sushi en formato de clasificaci\u00f3n de im\u00e1genes est\u00e1ndar.</p> <p>Ahora creemos rutas a nuestros directorios de capacitaci\u00f3n y pruebas.</p> In\u00a0[\u00a0]: Copied! <pre># Directorios de configuraci\u00f3n\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Directorios de configuraci\u00f3n train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[\u00a0]: Copied! <pre># Cree una canalizaci\u00f3n de transformaciones manualmente (requerido para torchvision &lt;0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n</pre> # Cree una canalizaci\u00f3n de transformaciones manualmente (requerido para torchvision &lt;0.13) manual_transforms = transforms.Compose([     transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)     transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1      transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)                          std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel), ]) <p>\u00a1Maravilloso!</p> <p>Ahora que tenemos una serie de transformaciones creadas manualmente lista para preparar nuestras im\u00e1genes, creemos DataLoaders de entrenamiento y prueba.</p> <p>Podemos crearlos usando la funci\u00f3n <code>create_dataloaders</code> desde el script <code>data_setup.py</code> creado en 05. PyTorch se vuelve modular, parte 2.</p> <p>Estableceremos <code>batch_size=32</code> para que nuestro modelo vea minilotes de 32 muestras a la vez.</p> <p>Y podemos transformar nuestras im\u00e1genes usando el canal de transformaci\u00f3n que creamos anteriormente configurando <code>transform=manual_transforms</code>.</p> <p>Nota: He incluido esta creaci\u00f3n manual de transformaciones en este cuaderno porque es posible que encuentres recursos que utilicen este estilo. Tambi\u00e9n es importante tener en cuenta que debido a que estas transformaciones se crean manualmente, tambi\u00e9n son infinitamente personalizables. Entonces, si quisiera incluir t\u00e9cnicas de aumento de datos en su proceso de transformaci\u00f3n, podr\u00eda hacerlo.</p> In\u00a0[\u00a0]: Copied! <pre># Cree cargadores de datos de entrenamiento y prueba y obtenga una lista de nombres de clases\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Cree cargadores de datos de entrenamiento y prueba y obtenga una lista de nombres de clases train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names In\u00a0[\u00a0]: Copied! <pre># Obtenga un conjunto de pesos de modelo previamente entrenados\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n</pre> # Obtenga un conjunto de pesos de modelo previamente entrenados weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet weights <p>Y ahora para acceder a las transformaciones asociadas con nuestros <code>pesos</code>, podemos usar el m\u00e9todo <code>transforms()</code>.</p> <p>B\u00e1sicamente, esto significa \"obtener las transformaciones de datos que se utilizaron para entrenar <code>EfficientNet_B0_Weights</code> en ImageNet\".</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga las transformaciones utilizadas para crear nuestros pesos previamente entrenados.\nauto_transforms = weights.transforms()\nauto_transforms\n</pre> # Obtenga las transformaciones utilizadas para crear nuestros pesos previamente entrenados. auto_transforms = weights.transforms() auto_transforms <p>Observe c\u00f3mo <code>auto_transforms</code> es muy similar a <code>manual_transforms</code>, la \u00fanica diferencia es que <code>auto_transforms</code> vino con la arquitectura del modelo que elegimos, mientras que tuvimos que crear <code>manual_transforms</code> a mano.</p> <p>El beneficio de crear autom\u00e1ticamente una transformaci\u00f3n a trav\u00e9s de <code>weights.transforms()</code> es que garantiza que est\u00e1 utilizando la misma transformaci\u00f3n de datos que el modelo previamente entrenado que se us\u00f3 cuando se entren\u00f3.</p> <p>Sin embargo, la desventaja de utilizar transformaciones creadas autom\u00e1ticamente es la falta de personalizaci\u00f3n.</p> <p>Podemos usar <code>auto_transforms</code> para crear DataLoaders con <code>create_dataloaders()</code> tal como antes.</p> In\u00a0[\u00a0]: Copied! <pre># Cree cargadores de datos de entrenamiento y prueba y obtenga una lista de nombres de clases\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Cree cargadores de datos de entrenamiento y prueba y obtenga una lista de nombres de clases train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=auto_transforms, # perform same data transforms on our own data as the pretrained model                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names In\u00a0[\u00a0]: Copied! <pre># ANTIGUO: configure el modelo con pesos previamente entrenados y env\u00edelo al dispositivo de destino (esto era antes de torchvision v0.13)\n# model = torchvision.models.ficientnet_b0(pretrained=True).to(device) # M\u00e9todo ANTIGUO (con pretrained=True)\n\n# NUEVO: Configure el modelo con pesas previamente entrenadas y env\u00edelo al dispositivo de destino (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# modelo # descomentar en la salida (es muy largo)\n</pre> # ANTIGUO: configure el modelo con pesos previamente entrenados y env\u00edelo al dispositivo de destino (esto era antes de torchvision v0.13) # model = torchvision.models.ficientnet_b0(pretrained=True).to(device) # M\u00e9todo ANTIGUO (con pretrained=True)  # NUEVO: Configure el modelo con pesas previamente entrenadas y env\u00edelo al dispositivo de destino (torchvision v0.13+) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights  model = torchvision.models.efficientnet_b0(weights=weights).to(device)  # modelo # descomentar en la salida (es muy largo) <p>Nota: En versiones anteriores de <code>torchvision</code>, se creaba un modelo previamente entrenado con c\u00f3digo como:</p> <p><code>modelo = torchvision.models.ficientnet_b0(preentrenado=True).to(dispositivo)</code></p> <p>Sin embargo, ejecutar esto usando <code>torchvision</code> v0.13+ resultar\u00e1 en errores como los siguientes:</p> <p><code>Advertencia de usuario: el par\u00e1metro 'preentrenado' est\u00e1 obsoleto desde 0.13 y se eliminar\u00e1 en 0.15; utilice 'pesos' en su lugar.</code></p> <p>Y...</p> <p>`Advertencia de usuario: Los argumentos distintos de una enumeraci\u00f3n de peso o Ninguno para los pesos est\u00e1n obsoletos desde 0.13 y se eliminar\u00e1n en 0.15. El comportamiento actual es equivalente a pasar pesos=EfficientNet_B0_Weights.IMAGENET1K_V1. Tambi\u00e9n puede utilizar Weights=EfficientNet_B0_Weights.DEFAULT para obtener los pesos m\u00e1s actualizados.</p> <p>Si imprimimos el modelo, obtenemos algo similar a lo siguiente:</p> <p>Montones, montones, montones de capas.</p> <p>Este es uno de los beneficios del aprendizaje por transferencia: tomar un modelo existente, que ha sido elaborado por algunos de los mejores ingenieros del mundo y aplicarlo a su propio problema.</p> <p>Nuestro <code>ficientnet_b0</code> se compone de tres partes principales:</p> <ol> <li><code>caracter\u00edsticas</code>: una colecci\u00f3n de capas convolucionales y otras capas de activaci\u00f3n para aprender una representaci\u00f3n base de los datos de visi\u00f3n (esta representaci\u00f3n/colecci\u00f3n base de capas a menudo se denomina caracter\u00edsticas o extractor de caracter\u00edsticas, \"las capas base del modelo aprenden las diferentes caracter\u00edsticas de las im\u00e1genes\").</li> <li><code>avgpool</code>: toma el promedio de la salida de las capas de <code>caracter\u00edsticas</code> y lo convierte en un vector de caracter\u00edsticas.</li> <li><code>classifier</code>: convierte el vector de caracter\u00edsticas en un vector con la misma dimensionalidad que el n\u00famero de clases de salida requeridas (ya que <code>ficientnet_b0</code> est\u00e1 preentrenado en ImageNet y debido a que ImageNet tiene 1000 clases, <code>out_features=1000</code> es el valor por defecto).</li> </ol> In\u00a0[\u00a0]: Copied! <pre># Imprima un resumen usando torchinfo (descomente el resultado real)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # Imprima un resumen usando torchinfo (descomente el resultado real) summary(model=model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] )  <p>\u00a1Guau!</p> <p>\u00a1Ese s\u00ed que es un gran modelo!</p> <p>Desde el resultado del resumen, podemos ver todos los diversos cambios de forma de entrada y salida a medida que los datos de nuestra imagen pasan por el modelo.</p> <p>Y hay muchos m\u00e1s par\u00e1metros totales (pesos previamente entrenados) para reconocer diferentes patrones en nuestros datos.</p> <p>Como referencia, nuestro modelo de secciones anteriores, TinyVGG, ten\u00eda 8.083 par\u00e1metros frente a 5.288.548 par\u00e1metros para <code>ficientnet_b0</code>, \u00a1un aumento de ~654x!</p> <p>\u00bfQu\u00e9 opinas? \u00bfEsto significar\u00e1 un mejor rendimiento?</p> In\u00a0[\u00a0]: Copied! <pre># Congele todas las capas base en la secci\u00f3n \"caracter\u00edsticas\" del modelo (el extractor de caracter\u00edsticas) configurando require_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Congele todas las capas base en la secci\u00f3n \"caracter\u00edsticas\" del modelo (el extractor de caracter\u00edsticas) configurando require_grad=False for param in model.features.parameters():     param.requires_grad = False <p>\u00a1Caracter\u00edsticas capas extractoras congeladas!</p> <p>Ahora ajustemos la capa de salida o la parte del \"clasificador\" de nuestro modelo previamente entrenado a nuestras necesidades.</p> <p>En este momento, nuestro modelo previamente entrenado tiene <code>out_features=1000</code> porque hay 1000 clases en ImageNet.</p> <p>Sin embargo, no tenemos 1000 clases, solo tenemos tres: pizza, bistec y sushi.</p> <p>Podemos cambiar la parte \"clasificador\" de nuestro modelo creando una nueva serie de capas.</p> <p>El \"clasificador\" actual consta de:</p> <pre><code>(clasificador): Secuencial(\n    (0): Abandono(p=0,2, in situ=Verdadero)\n    (1): Lineal (in_features=1280, out_features=1000, sesgo=Verdadero)\n</code></pre> <p>Mantendremos la capa <code>Dropout</code> igual usando [<code>torch.nn.Dropout(p=0.2, inplace=True)</code>](https://pytorch.org/docs/stable/generated/torch.nn.Dropout .html).</p> <p>Nota: Capas de abandono elimina aleatoriamente conexiones entre dos capas de redes neuronales con una probabilidad de \"p\". Por ejemplo, si <code>p=0.2</code>, el 20% de las conexiones entre capas de la red neuronal se eliminar\u00e1n aleatoriamente en cada pasada. Esta pr\u00e1ctica est\u00e1 destinada a ayudar a regularizar (evitar el sobreajuste) un modelo asegur\u00e1ndose de que las conexiones que quedan aprendan caracter\u00edsticas para compensar la eliminaci\u00f3n de las otras conexiones (con suerte, estas caracter\u00edsticas restantes son m\u00e1s generales).</p> <p>Y mantendremos <code>in_features=1280</code> para nuestra capa de salida <code>Lineal</code> pero cambiaremos el valor <code>out_features</code> a la longitud de nuestros <code>class_names</code> (<code>len(['pizza', 'steak', 'sushi ']) = 3</code>).</p> <p>Nuestra nueva capa \"clasificador\" deber\u00eda estar en el mismo dispositivo que nuestro \"modelo\".</p> In\u00a0[\u00a0]: Copied! <pre># Colocar las semillas manuales.\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Obtenga la longitud de class_names (una unidad de salida para cada clase)\noutput_shape = len(class_names)\n\n# Vuelva a crear la capa del clasificador y si\u00e9mbrela en el dispositivo de destino.\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n</pre> # Colocar las semillas manuales. torch.manual_seed(42) torch.cuda.manual_seed(42)  # Obtenga la longitud de class_names (una unidad de salida para cada clase) output_shape = len(class_names)  # Vuelva a crear la capa del clasificador y si\u00e9mbrela en el dispositivo de destino. model.classifier = torch.nn.Sequential(     torch.nn.Dropout(p=0.2, inplace=True),      torch.nn.Linear(in_features=1280,                      out_features=output_shape, # same number of output units as our number of classes                     bias=True)).to(device) <p>\u00a1Lindo!</p> <p>Capa de salida actualizada, obtengamos otro resumen de nuestro modelo y veamos qu\u00e9 ha cambiado.</p> In\u00a0[\u00a0]: Copied! <pre># # Hacer un resumen *despu\u00e9s* de congelar las caracter\u00edsticas y cambiar la capa del clasificador de salida (descomentar para la salida real)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # # Hacer un resumen *despu\u00e9s* de congelar las caracter\u00edsticas y cambiar la capa del clasificador de salida (descomentar para la salida real) summary(model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)         verbose=0,         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) <p>\u00a1Ho, ho! \u00a1Hay algunos cambios aqu\u00ed!</p> <p>Repas\u00e9moslos:</p> <ul> <li>Columna entrenable: ver\u00e1 que muchas de las capas base (las que est\u00e1n en la parte \"caracter\u00edsticas\") tienen su valor entrenable como \"Falso\". Esto se debe a que configuramos su atributo <code>requires_grad=False</code>. A menos que cambiemos esto, estas capas no se actualizar\u00e1n durante el entrenamiento futuro.</li> <li>Forma de salida del <code>clasificador</code>: la parte del <code>clasificador</code> del modelo ahora tiene un valor de Forma de salida de <code>[32, 3]</code> en lugar de <code>[32, 1000]</code>. Su valor entrenable tambi\u00e9n es \"Verdadero\". Esto significa que sus par\u00e1metros se actualizar\u00e1n durante el entrenamiento. En esencia, estamos usando la parte de \"caracter\u00edsticas\" para alimentar a nuestra parte de \"clasificador\" con una representaci\u00f3n base de una imagen y luego nuestra capa de \"clasificador\" aprender\u00e1 c\u00f3mo alinear la representaci\u00f3n base con nuestro problema.</li> <li>Menos par\u00e1metros entrenables: anteriormente hab\u00eda 5.288.548 par\u00e1metros entrenables. Pero como congelamos muchas de las capas del modelo y solo dejamos el \"clasificador\" como entrenable, ahora solo hay 3843 par\u00e1metros entrenables (incluso menos que nuestro modelo TinyVGG). Aunque tambi\u00e9n hay 4.007.548 par\u00e1metros no entrenables, estos crear\u00e1n una representaci\u00f3n base de nuestras im\u00e1genes de entrada para alimentar nuestra capa \"clasificadora\".</li> </ul> <p>Nota: Cuantos m\u00e1s par\u00e1metros entrenables tenga un modelo, m\u00e1s potencia de c\u00e1lculo y m\u00e1s tiempo llevar\u00e1 entrenar. Congelar las capas base de nuestro modelo y dejarlo con par\u00e1metros menos entrenables significa que nuestro modelo deber\u00eda entrenarse con bastante rapidez. Este es un gran beneficio del aprendizaje por transferencia: tomar los par\u00e1metros ya aprendidos de un modelo entrenado en un problema similar al suyo y ajustar solo ligeramente los resultados para adaptarlos a su problema.</p> In\u00a0[\u00a0]: Copied! <pre># Definir p\u00e9rdida y optimizador\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Definir p\u00e9rdida y optimizador loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) <p>\u00a1Maravilloso!</p> <p>Para entrenar nuestro modelo, podemos usar la funci\u00f3n <code>train()</code> que definimos en 05. PyTorch Going Modular secci\u00f3n 04.</p> <p>La funci\u00f3n <code>train()</code> est\u00e1 en el script <code>engine.py</code> dentro del [ Directorio <code>going_modular</code>] (https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular/going_modular).</p> <p>Veamos cu\u00e1nto tiempo lleva entrenar nuestro modelo durante 5 \u00e9pocas.</p> <p>Nota: Aqu\u00ed solo entrenaremos los par\u00e1metros <code>clasificador</code> ya que todos los dem\u00e1s par\u00e1metros de nuestro modelo se han congelado.</p> In\u00a0[\u00a0]: Copied! <pre># Establecer las semillas aleatorias\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# iniciar el cron\u00f3metro\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Configurar el entrenamiento y guardar los resultados.\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# Finalice el cron\u00f3metro e imprima cu\u00e1nto tiempo tard\u00f3\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Establecer las semillas aleatorias torch.manual_seed(42) torch.cuda.manual_seed(42)  # iniciar el cron\u00f3metro from timeit import default_timer as timer  start_time = timer()  # Configurar el entrenamiento y guardar los resultados. results = engine.train(model=model,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=5,                        device=device)  # Finalice el cron\u00f3metro e imprima cu\u00e1nto tiempo tard\u00f3 end_time = timer() print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\") <p>\u00a1Guau!</p> <p>Nuestro modelo se entren\u00f3 bastante r\u00e1pido (~5 segundos en mi m\u00e1quina local con una GPU NVIDIA TITAN RTX/ unos 15 segundos en Google Colab con una GPU NVIDIA P100).</p> <p>\u00a1Y parece que arras\u00f3 con los resultados de nuestro modelo anterior!</p> <p>Con una columna vertebral <code>ficientnet_b0</code>, nuestro modelo logra una precisi\u00f3n de casi el 85%+ en el conjunto de datos de prueba, casi el doble de lo que pudimos lograr con TinyVGG.</p> <p>Nada mal para un modelo que descargamos con unas pocas l\u00edneas de c\u00f3digo.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga la funci\u00f3n plot_loss_curves() de helper_functions.py, descargue el archivo si no lo tenemos\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Trazar las curvas de p\u00e9rdidas de nuestro modelo.\nplot_loss_curves(results)\n</pre> # Obtenga la funci\u00f3n plot_loss_curves() de helper_functions.py, descargue el archivo si no lo tenemos try:     from helper_functions import plot_loss_curves except:     print(\"[INFO] Couldn't find helper_functions.py, downloading...\")     with open(\"helper_functions.py\", \"wb\") as f:         import requests         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")         f.write(request.content)     from helper_functions import plot_loss_curves  # Trazar las curvas de p\u00e9rdidas de nuestro modelo. plot_loss_curves(results) <p>\u00a1Esas son algunas curvas de p\u00e9rdidas de excelente apariencia!</p> <p>Parece que la p\u00e9rdida de ambos conjuntos de datos (entrenamiento y prueba) va en la direcci\u00f3n correcta.</p> <p>Lo mismo ocurre con los valores de precisi\u00f3n, con tendencia al alza.</p> <p>Esto demuestra el poder de la transferencia de aprendizaje. El uso de un modelo previamente entrenado a menudo genera resultados bastante buenos con una peque\u00f1a cantidad de datos en menos tiempo.</p> <p>Me pregunto qu\u00e9 pasar\u00eda si intentaras entrenar al modelo por m\u00e1s tiempo. \u00bfO si agregamos m\u00e1s datos?</p> <p>Pregunta: Al observar las curvas de p\u00e9rdida, \u00bfnuestro modelo parece estar sobreajustado o insuficientemente ajustado? \u00bfO tal vez ninguno de los dos? Pista: consulte el cuaderno 04. Conjuntos de datos personalizados de PyTorch, parte 8. \u00bfC\u00f3mo deber\u00eda ser una curva de p\u00e9rdida ideal? para obtener ideas .</p> In\u00a0[\u00a0]: Copied! <pre>from typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Tome un modelo entrenado, nombres de clases, ruta de imagen, tama\u00f1o de imagen, una transformaci\u00f3n y un dispositivo de destino.\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n</pre> from typing import List, Tuple  from PIL import Image  # 1. Tome un modelo entrenado, nombres de clases, ruta de imagen, tama\u00f1o de imagen, una transformaci\u00f3n y un dispositivo de destino. def pred_and_plot_image(model: torch.nn.Module,                         image_path: str,                          class_names: List[str],                         image_size: Tuple[int, int] = (224, 224),                         transform: torchvision.transforms = None,                         device: torch.device=device):               # 2. Open image     img = Image.open(image_path)      # 3. Create transformation for image (if one doesn't exist)     if transform is not None:         image_transform = transform     else:         image_transform = transforms.Compose([             transforms.Resize(image_size),             transforms.ToTensor(),             transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225]),         ])      ### Predict on image ###       # 4. Make sure the model is on the target device     model.to(device)      # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():       # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])       transformed_image = image_transform(img).unsqueeze(dim=0)        # 7. Make a prediction on image with an extra dimension and send it to the target device       target_image_pred = model(transformed_image.to(device))      # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 9. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)      # 10. Plot image with predicted label and probability      plt.figure()     plt.imshow(img)     plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")     plt.axis(False); <p>\u00a1Qu\u00e9 funci\u00f3n tan atractiva!</p> <p>Prob\u00e9moslo haciendo predicciones sobre algunas im\u00e1genes aleatorias del conjunto de prueba.</p> <p>Podemos obtener una lista de todas las rutas de las im\u00e1genes de prueba usando <code>list(Path(test_dir).glob(\"*/*.jpg\"))</code>, las estrellas en el m\u00e9todo <code>glob()</code> dicen \"cualquier archivo que coincida con este patr\u00f3n \", es decir, cualquier archivo que termine en <code>.jpg</code> (todas nuestras im\u00e1genes).</p> <p>Y luego podemos muestrear aleatoriamente varios de estos usando <code>random.sample(populuation, k)</code> de Python donde <code>poblaci\u00f3n </code> es la secuencia a muestrear y <code>k</code> es el n\u00famero de muestras a recuperar.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga una lista aleatoria de rutas de im\u00e1genes del conjunto de prueba\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Hacer predicciones y trazar las im\u00e1genes.\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n</pre> # Obtenga una lista aleatoria de rutas de im\u00e1genes del conjunto de prueba import random num_images_to_plot = 3 test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data  test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths                                        k=num_images_to_plot) # randomly select 'k' image paths to pred and plot  # Hacer predicciones y trazar las im\u00e1genes. for image_path in test_image_path_sample:     pred_and_plot_image(model=model,                          image_path=image_path,                         class_names=class_names,                         # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights                         image_size=(224, 224)) <p>\u00a1Guau!</p> <p>Esas predicciones parecen mucho mejores que las que nuestro modelo TinyVGG hac\u00eda anteriormente.</p> In\u00a0[\u00a0]: Copied! <pre># Descargar imagen personalizada\nimport requests\n\n# Configurar ruta de imagen personalizada\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Descarga la imagen si a\u00fan no existe\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predecir en imagen personalizada\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Descargar imagen personalizada import requests  # Configurar ruta de imagen personalizada custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Descarga la imagen si a\u00fan no existe if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predecir en imagen personalizada pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <p>\u00a1Dos pulgares arriba!</p> <p>\u00a1Parece que nuestro modelo volvi\u00f3 a acertar!</p> <p>Pero esta vez la probabilidad de predicci\u00f3n es mayor que la de TinyVGG (<code>0.373</code>) en [04. Secci\u00f3n 11.3 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function).</p> <p>Esto indica que nuestro modelo <code>ficientnet_b0</code> tiene m\u00e1s confianza en su predicci\u00f3n, mientras que nuestro modelo TinyVGG era equivalente a solo adivinar.</p>"},{"location":"11-06_pytorch_transfer_learning/#06-aprendizaje-por-transferencia-de-pytorch","title":"06. Aprendizaje por transferencia de PyTorch\u00b6","text":"<p>Nota: Este cuaderno utiliza la nueva [API de soporte multipeso de <code>torchvision</code> (disponible en <code>torchvision</code> v0.13+)](https://pytorch.org/blog/introtaining-torchvision-new -api-soporte-multi-peso/).</p> <p>Hasta ahora hemos construido algunos modelos a mano.</p> <p>Pero su desempe\u00f1o ha sido pobre.</p> <p>Quiz\u00e1s est\u00e9 pensando: \u00bfExiste ya un modelo de buen rendimiento para nuestro problema?</p> <p>Y en el mundo del aprendizaje profundo, la respuesta suele ser s\u00ed.</p> <p>Veremos c\u00f3mo utilizar una poderosa t\u00e9cnica llamada transferir aprendizaje.</p>"},{"location":"11-06_pytorch_transfer_learning/#que-es-el-aprendizaje-por-transferencia","title":"\u00bfQu\u00e9 es el aprendizaje por transferencia?\u00b6","text":"<p>El aprendizaje por transferencia nos permite tomar los patrones (tambi\u00e9n llamados pesos) que otro modelo ha aprendido de otro problema y usarlos para nuestro propio problema.</p> <p>Por ejemplo, podemos tomar los patrones que un modelo de visi\u00f3n por computadora ha aprendido de conjuntos de datos como ImageNet (millones de im\u00e1genes de diferentes objetos) y usarlos para impulsar nuestro FoodVision. Modelo mini.</p> <p>O podr\u00edamos tomar los patrones de un modelo de lenguaje (un modelo que ha analizado grandes cantidades de texto para aprender una representaci\u00f3n de lenguaje) y utilizarlos como base de un modelo para clasificar diferentes muestras de texto.</p> <p>La premisa sigue siendo: encuentre un modelo existente que funcione bien y apl\u00edquelo a su propio problema.</p> <p>Ejemplo de aprendizaje por transferencia aplicado a la visi\u00f3n por computadora y al procesamiento del lenguaje natural (PLN). En el caso de la visi\u00f3n por computadora, un modelo de visi\u00f3n por computadora podr\u00eda aprender patrones en millones de im\u00e1genes en ImageNet y luego usar esos patrones para inferir otro problema. Y para la PNL, un modelo de lenguaje puede aprender la estructura del lenguaje leyendo toda Wikipedia (y quiz\u00e1s m\u00e1s) y luego aplicar ese conocimiento a un problema diferente.</p>"},{"location":"11-06_pytorch_transfer_learning/#por-que-utilizar-el-aprendizaje-por-transferencia","title":"\u00bfPor qu\u00e9 utilizar el aprendizaje por transferencia?\u00b6","text":"<p>Hay dos beneficios principales al utilizar el aprendizaje por transferencia:</p> <ol> <li>Puede aprovechar un modelo existente (generalmente una arquitectura de red neuronal) que ha demostrado funcionar en problemas similares al nuestro.</li> <li>Puede aprovechar un modelo funcional que ya ha aprendido patrones sobre datos similares a los nuestros. Esto a menudo da como resultado excelentes resultados con menos datos personalizados.</li> </ol> <p>Los pondremos a prueba para nuestro problema FoodVision Mini, tomaremos un modelo de visi\u00f3n por computadora previamente entrenado en ImageNet e intentaremos aprovechar sus representaciones aprendidas subyacentes para clasificar im\u00e1genes de pizza, bistec y sushi.</p> <p>Tanto la investigaci\u00f3n como la pr\u00e1ctica tambi\u00e9n respaldan el uso del aprendizaje por transferencia.</p> <p>Un hallazgo de un art\u00edculo de investigaci\u00f3n reciente sobre aprendizaje autom\u00e1tico recomend\u00f3 que los profesionales utilicen el aprendizaje por transferencia siempre que sea posible.</p> <p></p> <p>Un estudio sobre los efectos de si entrenar desde cero o utilizar el aprendizaje por transferencia era mejor desde el punto de vista de un profesional, encontr\u00f3 que el aprendizaje por transferencia era mucho m\u00e1s beneficioso en t\u00e9rminos de costo y tiempo. Fuente: \u00bfC\u00f3mo entrenar tu ViT? Datos, aumento y regularizaci\u00f3n en Vision Transformers secci\u00f3n 6 del art\u00edculo (conclusi\u00f3n).</p> <p>Y Jeremy Howard (fundador de fastai) es un gran defensor del aprendizaje por transferencia.</p> <p>Las cosas que realmente marcan la diferencia (aprendizaje por transferencia), si podemos hacerlo mejor en el aprendizaje por transferencia, es algo que cambiar\u00e1 el mundo. De repente, mucha m\u00e1s gente puede realizar un trabajo de primer nivel con menos recursos y menos datos. \u2014 [Jeremy Howard en el podcast de Lex Fridman] (https://youtu.be/Bi7f1JSSlh8?t=72)</p>"},{"location":"11-06_pytorch_transfer_learning/#donde-encontrar-modelos-previamente-entrenados","title":"D\u00f3nde encontrar modelos previamente entrenados\u00b6","text":"<p>El mundo del aprendizaje profundo es un lugar asombroso.</p> <p>Es tan sorprendente que muchas personas alrededor del mundo comparten su trabajo.</p> <p>A menudo, el c\u00f3digo y los modelos previamente entrenados para las \u00faltimas investigaciones de vanguardia se publican a los pocos d\u00edas de su publicaci\u00f3n.</p> <p>Y hay varios lugares donde puede encontrar modelos previamente entrenados para utilizarlos en sus propios problemas.</p> Ubicaci\u00f3n \u00bfQu\u00e9 hay ah\u00ed? Enlace(s) Bibliotecas de dominio PyTorch Cada una de las bibliotecas de dominio de PyTorch (<code>torchvision</code>, <code>torchtext</code>) viene con modelos previamente entrenados de alg\u00fan tipo. Los modelos all\u00ed funcionan directamente dentro de PyTorch. <code>torchvision.models</code>, <code>torchtext.models</code>, <code>torchaudio.models</code>, <code>torchrec.models</code> HuggingFace Hub Una serie de modelos previamente entrenados en muchos dominios diferentes (visi\u00f3n, texto, audio y m\u00e1s) de organizaciones de todo el mundo. Tambi\u00e9n hay muchos conjuntos de datos diferentes. https://huggingface.co/models, https://huggingface.co/datasets Biblioteca <code>timm</code> (modelos de im\u00e1genes PyTorch) Casi todos los modelos de visi\u00f3n por computadora m\u00e1s recientes y mejores en c\u00f3digo PyTorch, as\u00ed como muchas otras funciones \u00fatiles de visi\u00f3n por computadora. https://github.com/rwightman/pytorch-image-models Papelesconc\u00f3digo Una colecci\u00f3n de los \u00faltimos art\u00edculos sobre aprendizaje autom\u00e1tico con implementaciones de c\u00f3digo adjuntas. Tambi\u00e9n puede encontrar aqu\u00ed puntos de referencia del rendimiento del modelo en diferentes tareas. https://paperswithcode.com/ <p>Con acceso a recursos de alta calidad como los anteriores, deber\u00eda ser una pr\u00e1ctica com\u00fan al comienzo de cada problema de aprendizaje profundo que asuma preguntar: \"\u00bfExiste un modelo previamente entrenado para mi problema?\"</p> <p>Ejercicio: Dedique 5 minutos a revisar <code>torchvision.models</code>, as\u00ed como a la [p\u00e1gina de modelos de HuggingFace Hub](https: //huggingface.co/models), \u00bfqu\u00e9 encuentras? (aqu\u00ed no hay respuestas correctas, es solo para practicar la exploraci\u00f3n)</p>"},{"location":"11-06_pytorch_transfer_learning/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>Tomaremos un modelo previamente entrenado de <code>torchvision.models</code> y lo personalizaremos para que funcione (y con suerte mejore) nuestro problema FoodVision Mini.</p> Tema Contenido 0. Obteniendo configuraci\u00f3n Hemos escrito bastante c\u00f3digo \u00fatil en las \u00faltimas secciones, descargu\u00e9moslo y asegur\u00e9monos de poder usarlo nuevamente. 1. Obtener datos Obtengamos el conjunto de datos de clasificaci\u00f3n de im\u00e1genes de pizza, bistec y sushi que hemos estado usando para intentar mejorar los resultados de nuestro modelo. 2. Crear conjuntos de datos y cargadores de datos Usaremos el script <code>data_setup.py</code> que escribimos en el cap\u00edtulo 05. PyTorch se vuelve modular para configurar nuestros DataLoaders. 3. Obtenga y personalice un modelo previamente entrenado Aqu\u00ed descargaremos un modelo previamente entrenado desde <code>torchvision.models</code> y lo personalizaremos seg\u00fan nuestro propio problema. 4. Modelo de tren Veamos c\u00f3mo funciona el nuevo modelo previamente entrenado en nuestro conjunto de datos de pizza, bistec y sushi. Usaremos las funciones de entrenamiento que creamos en el cap\u00edtulo anterior. 5. Eval\u00fae el modelo trazando curvas de p\u00e9rdidas \u00bfC\u00f3mo fue nuestro primer modelo de aprendizaje por transferencia? \u00bfSe ajustaba demasiado o no? 6. Haga predicciones sobre im\u00e1genes del conjunto de prueba Una cosa es comprobar las m\u00e9tricas de evaluaci\u00f3n de un modelo, pero otra cosa es ver sus predicciones en muestras de prueba. \u00a1visualicemos, visualicemos, visualicemos!"},{"location":"11-06_pytorch_transfer_learning/#donde-puedes-obtener-ayuda","title":"\u00bfD\u00f3nde puedes obtener ayuda?\u00b6","text":"<p>Todos los materiales de este curso est\u00e1n disponibles en GitHub.</p> <p>Si tiene problemas, puede hacer una pregunta en el curso [p\u00e1gina de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).</p> <p>Y, por supuesto, est\u00e1 la documentaci\u00f3n de PyTorch y los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"11-06_pytorch_transfer_learning/#0-configuracion","title":"0. Configuraci\u00f3n\u00b6","text":"<p>Comencemos importando/descargando los m\u00f3dulos necesarios para esta secci\u00f3n.</p> <p>Para ahorrarnos escribir c\u00f3digo adicional, aprovecharemos algunos de los scripts de Python (como <code>data_setup.py</code> y <code>engine.py</code>) que creamos en la secci\u00f3n anterior, 05. PyTorch se vuelve modular.</p> <p>Espec\u00edficamente, vamos a descargar el directorio <code>going_modular</code> del repositorio <code>pytorch-deep-learning</code> (si a\u00fan no lo tenemos).</p> <p>Tambi\u00e9n obtendremos el paquete <code>torchinfo</code> si no est\u00e1 disponible.</p> <p><code>torchinfo</code> nos ayudar\u00e1 m\u00e1s adelante a darnos una representaci\u00f3n visual de nuestro modelo.</p> <p>Nota: A partir de junio de 2022, este cuaderno utiliza las versiones nocturnas de <code>torch</code> y <code>torchvision</code>, ya que se requiere <code>torchvision</code> v0.13+ para usar la API de pesos m\u00faltiples actualizada. Puede instalarlos usando el siguiente comando.</p>"},{"location":"11-06_pytorch_transfer_learning/#1-obtener-datos","title":"1. Obtener datos\u00b6","text":"<p>Antes de que podamos comenzar a utilizar transferencia de aprendizaje, necesitaremos un conjunto de datos.</p> <p>Para ver c\u00f3mo se compara el aprendizaje por transferencia con nuestros intentos anteriores de creaci\u00f3n de modelos, descargaremos el mismo conjunto de datos que hemos estado usando para FoodVision Mini.</p> <p>Escribamos un c\u00f3digo para descargar el conjunto de datos <code>pizza_steak_sushi.zip</code> del curso GitHub y luego descompr\u00edmalo. .</p> <p>Tambi\u00e9n podemos asegurarnos de que si ya tenemos los datos, no se vuelvan a descargar.</p>"},{"location":"11-06_pytorch_transfer_learning/#2-crear-conjuntos-de-datos-y-cargadores-de-datos","title":"2. Crear conjuntos de datos y cargadores de datos\u00b6","text":"<p>Como hemos descargado el directorio <code>going_modular</code>, podemos usar <code>data_setup.py</code> script que creamos en la secci\u00f3n 05. PyTorch Going Modular para preparar y configurar nuestros DataLoaders.</p> <p>Pero como usaremos un modelo previamente entrenado de <code>torchvision.models</code>, hay una transformaci\u00f3n espec\u00edfica que necesitamos para preparar nuestras im\u00e1genes primero.</p>"},{"location":"11-06_pytorch_transfer_learning/#21-creando-una-transformacion-para-torchvisionmodels-creacion-manual","title":"2.1 Creando una transformaci\u00f3n para <code>torchvision.models</code> (creaci\u00f3n manual)\u00b6","text":"<p>Nota: A partir de <code>torchvision</code> v0.13+, hay una actualizaci\u00f3n sobre c\u00f3mo se pueden crear transformaciones de datos usando <code>torchvision.models</code>. Llam\u00e9 al m\u00e9todo anterior \"creaci\u00f3n manual\" y al nuevo m\u00e9todo \"creaci\u00f3n autom\u00e1tica\". Este cuaderno muestra ambos.</p> <p>Cuando se utiliza un modelo previamente entrenado, es importante que los datos personalizados que se incluyen en el modelo se preparen de la misma manera que los datos de entrenamiento originales que se incluyeron en el modelo.</p> <p>Antes de <code>torchvision</code> v0.13+, para crear una transformaci\u00f3n para un modelo previamente entrenado en <code>torchvision.models</code>, la documentaci\u00f3n dec\u00eda:</p> <p>Todos los modelos previamente entrenados esperan im\u00e1genes de entrada normalizadas de la misma manera, es decir, minilotes de im\u00e1genes de forma RGB de 3 canales (3 x H x W), donde se espera que H y W sean al menos 224.</p> <p>Las im\u00e1genes deben cargarse en un rango de <code>[0, 1]</code> y luego normalizarse usando <code>mean = [0.485, 0.456, 0.406]</code> y <code>std = [0.229, 0.224, 0.225]</code>.</p> <p>Puedes usar la siguiente transformaci\u00f3n para normalizar:</p> <pre><code>normalizar = transforma.Normalizar(media=[0.485, 0.456, 0.406],\nest\u00e1ndar=[0,229, 0,224, 0,225])\n</code></pre> <p>La buena noticia es que podemos lograr las transformaciones anteriores con una combinaci\u00f3n de:</p> N\u00famero de transformaci\u00f3n Se requiere transformaci\u00f3n C\u00f3digo para realizar la transformaci\u00f3n 1 Minilotes de tama\u00f1o <code>[batch_size, 3, height, width]</code> donde la altura y el ancho son al menos 224x224^. <code>torchvision.transforms.Resize()</code> para cambiar el tama\u00f1o de las im\u00e1genes a <code>[3, 224, 224]</code>^ y <code>torch.utils.data.DataLoader()</code> para crear lotes de im\u00e1genes. 2 Valores entre 0 y 1. <code>torchvision.transforms.ToTensor()</code> 3 Una media de <code>[0,485, 0,456, 0,406]</code> (valores en cada canal de color). <code>torchvision.transforms.Normalize(mean=...)</code> para ajustar la media de nuestras im\u00e1genes. 4 Una desviaci\u00f3n est\u00e1ndar de \"[0,229, 0,224, 0,225]\" (valores en cada canal de color). <code>torchvision.transforms.Normalize(std=...)</code> para ajustar la desviaci\u00f3n est\u00e1ndar de nuestras im\u00e1genes. <p>Nota: ^algunos modelos previamente entrenados desde <code>torchvision.models</code> en diferentes tama\u00f1os hasta <code>[3, 224, 224]</code>, por ejemplo, algunos podr\u00edan tomarlos en <code>[3, 240, 240]</code>. Para tama\u00f1os de imagen de entrada espec\u00edficos, consulte la documentaci\u00f3n.</p> <p>Pregunta: \u00bfDe d\u00f3nde provienen los valores de media y desviaci\u00f3n est\u00e1ndar? \u00bfPor qu\u00e9 necesitamos hacer esto?</p> <p>Estos fueron calculados a partir de los datos. Espec\u00edficamente, el conjunto de datos ImageNet toma las medias y las desviaciones est\u00e1ndar de un subconjunto de im\u00e1genes.</p> <p>Tampoco necesitamos hacer esto. Las redes neuronales suelen ser bastante capaces de determinar distribuciones de datos apropiadas (calcular\u00e1n por s\u00ed mismas d\u00f3nde deben estar la media y las desviaciones est\u00e1ndar), pero establecerlas desde el principio puede ayudar a nuestras redes a lograr un mejor rendimiento m\u00e1s r\u00e1pido.</p> <p>Compongamos una serie de <code>torchvision.transforms</code> para realizar los pasos anteriores.</p>"},{"location":"11-06_pytorch_transfer_learning/#22-creando-una-transformacion-para-torchvisionmodels-creacion-automatica","title":"2.2 Creando una transformaci\u00f3n para <code>torchvision.models</code> (creaci\u00f3n autom\u00e1tica)\u00b6","text":"<p>Como se indic\u00f3 anteriormente, cuando se utiliza un modelo previamente entrenado, es importante que los datos personalizados que se ingresan en el modelo se preparen de la misma manera que los datos de entrenamiento originales que se ingresaron en el modelo.</p> <p>Arriba vimos c\u00f3mo crear manualmente una transformaci\u00f3n para un modelo previamente entrenado.</p> <p>Pero a partir de <code>torchvision</code> v0.13+, se agreg\u00f3 una funci\u00f3n de creaci\u00f3n de transformaci\u00f3n autom\u00e1tica.</p> <p>Cuando configura un modelo desde <code>torchvision.models</code> y selecciona los pesos del modelo previamente entrenado que le gustar\u00eda usar, por ejemplo, digamos que nos gustar\u00eda usar:</p> <pre><code>pit\u00f3n\npesos = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n</code></pre> <p>D\u00f3nde,</p> <ul> <li><code>EfficientNet_B0_Weights</code> son los pesos de la arquitectura del modelo que nos gustar\u00eda usar (hay muchas opciones diferentes de arquitectura de modelo en <code>torchvision.models</code>).</li> <li><code>DEFAULT</code> significa los mejores pesos disponibles (el mejor rendimiento en ImageNet).<ul> <li>Nota: Dependiendo de la arquitectura del modelo que elija, tambi\u00e9n puede ver otras opciones como <code>IMAGENET_V1</code> e <code>IMAGENET_V2</code>, donde generalmente cuanto mayor sea el n\u00famero de versi\u00f3n, mejor. Aunque si desea lo mejor disponible, \"DEFAULT\" es la opci\u00f3n m\u00e1s sencilla. Consulte la documentaci\u00f3n <code>torchvision.models</code> para obtener m\u00e1s informaci\u00f3n.</li> </ul> </li> </ul> <p>Prob\u00e9moslo.</p>"},{"location":"11-06_pytorch_transfer_learning/#3-obtener-un-modelo-previamente-entrenado","title":"3. Obtener un modelo previamente entrenado\u00b6","text":"<p>Muy bien, \u00a1aqu\u00ed viene la parte divertida!</p> <p>En los \u00faltimos cuadernos hemos estado construyendo redes neuronales PyTorch desde cero.</p> <p>Y si bien es una buena habilidad, nuestros modelos no han funcionado tan bien como nos gustar\u00eda.</p> <p>Ah\u00ed es donde entra en juego la transferencia de aprendizaje.</p> <p>La idea general del aprendizaje por transferencia es tomar un modelo que ya funciona bien en un espacio de problemas similar al suyo y luego personalizarlo seg\u00fan su caso de uso.</p> <p>Dado que estamos trabajando en un problema de visi\u00f3n por computadora (clasificaci\u00f3n de im\u00e1genes con FoodVision Mini), podemos encontrar modelos de clasificaci\u00f3n previamente entrenados en <code>torchvision.models</code>.</p> <p>Al explorar la documentaci\u00f3n, encontrar\u00e1 muchos pilares de arquitectura de visi\u00f3n por computadora comunes, como:</p> La columna vertebral de la arquitectura C\u00f3digo ResNet's <code>torchvision.models.resnet18()</code>, <code>torchvision.models.resnet50()</code>... VGG (similar a lo que usamos para TinyVGG) <code>torchvision.models.vgg16()</code> EfficientNet's <code>torchvision.models.ficientnet_b0()</code>, <code>torchvision.models.ficientnet_b1()</code>... VisionTransformer (ViT) <code>torchvision.models.vit_b_16()</code>, <code>torchvision.models.vit_b_32()</code>... ConvNeXt <code>torchvision.models.convnext_tiny()</code>, <code>torchvision.models.convnext_small()</code>... M\u00e1s disponible en <code>torchvision.models</code> <code>modelos.torchvision...</code>"},{"location":"11-06_pytorch_transfer_learning/#31-que-modelo-previamente-entrenado-deberia-utilizar","title":"3.1 \u00bfQu\u00e9 modelo previamente entrenado deber\u00eda utilizar?\u00b6","text":"<p>Depende de su problema/del dispositivo con el que est\u00e9 trabajando.</p> <p>Generalmente, el n\u00famero m\u00e1s alto en el nombre del modelo (por ejemplo, <code>ficientnet_b0()</code> -&gt; <code>ficientnet_b1()</code> -&gt; <code>ficientnet_b7()</code>) significa mejor rendimiento pero un modelo m\u00e1s grande.</p> <p>Se podr\u00eda pensar que un mejor rendimiento es siempre mejor, \u00bfverdad?</p> <p>Eso es cierto, pero algunos modelos de mejor rendimiento son demasiado grandes para algunos dispositivos.</p> <p>Por ejemplo, supongamos que desea ejecutar su modelo en un dispositivo m\u00f3vil, tendr\u00e1 que tener en cuenta los recursos inform\u00e1ticos limitados del dispositivo, por lo que buscar\u00e1 un modelo m\u00e1s peque\u00f1o.</p> <p>Pero si tienes un poder de c\u00f3mputo ilimitado, como afirma The Bitter Lesson, probablemente elegir\u00e1s el modelo m\u00e1s grande y con mayor necesidad de c\u00f3mputo que puedas. poder.</p> <p>Comprender esta compensaci\u00f3n entre rendimiento, velocidad y tama\u00f1o llegar\u00e1 con el tiempo y la pr\u00e1ctica.</p> <p>Para m\u00ed, he encontrado un buen equilibrio en los modelos <code>ficientnet_bX</code>.</p> <p>A partir de mayo de 2022, Nutrify (la aplicaci\u00f3n basada en aprendizaje autom\u00e1tico en la que estoy trabajando) funciona con un <code>ficientnet_b0</code>.</p> <p>Comma.ai (una empresa que fabrica software de c\u00f3digo abierto para veh\u00edculos aut\u00f3nomos) [utiliza un <code>ficientnet_b2</code>](https://geohot.github.io/blog/jekyll/ update/2021/10/29/an-architecture-for-life.html) para conocer una representaci\u00f3n de la carretera.</p> <p>Nota: Aunque estamos usando <code>ficientnet_bX</code>, es importante no apegarse demasiado a ninguna arquitectura en particular, ya que siempre cambian a medida que se publican nuevas investigaciones. Lo mejor es experimentar, experimentar, experimentar y ver qu\u00e9 funciona para su problema.</p>"},{"location":"11-06_pytorch_transfer_learning/#32-configurar-un-modelo-previamente-entrenado","title":"3.2 Configurar un modelo previamente entrenado\u00b6","text":"<p>El modelo previamente entrenado que usaremos es <code>torchvision.models.ficientnet_b0()</code>.</p> <p>La arquitectura es del art\u00edculo EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.</p> <p>Ejemplo de lo que vamos a crear, un modelo <code>EfficientNet_B0</code> previamente entrenado de <code>torchvision.models</code> con la capa de salida ajustada para nuestro caso de uso de clasificaci\u00f3n de im\u00e1genes de pizza, bistec y sushi.</p> <p>Podemos configurar los pesos de ImageNet previamente entrenados en <code>EfficientNet_B0</code> usando el mismo c\u00f3digo que usamos para crear las transformaciones.</p> <pre><code>pit\u00f3n\npesos = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = mejores pesos disponibles para ImageNet\n</code></pre> <p>Esto significa que el modelo ya ha sido entrenado en millones de im\u00e1genes y tiene una buena representaci\u00f3n base de los datos de las im\u00e1genes.</p> <p>La versi\u00f3n PyTorch de este modelo previamente entrenado es capaz de lograr una precisi\u00f3n de ~77,7% en las 1000 clases de ImageNet.</p> <p>Tambi\u00e9n lo enviaremos al dispositivo de destino.</p>"},{"location":"11-06_pytorch_transfer_learning/#33-obteniendo-un-resumen-de-nuestro-modelo-con-torchinfosummary","title":"3.3 Obteniendo un resumen de nuestro modelo con <code>torchinfo.summary()</code>\u00b6","text":"<p>Para obtener m\u00e1s informaci\u00f3n sobre nuestro modelo, usemos el m\u00e9todo [<code>summary()</code>] de <code>torchinfo</code> (https://github.com/TylerYep/torchinfo#documentation).</p> <p>Para hacerlo, pasaremos:</p> <ul> <li><code>model</code>: el modelo del que nos gustar\u00eda obtener un resumen.</li> <li><code>input_size</code> - la forma de los datos que nos gustar\u00eda pasar a nuestro modelo, para el caso de <code>ficientnet_b0</code>, el tama\u00f1o de entrada es <code>(batch_size, 3, 224, 224)</code>, aunque otras variantes de <code> eficientenet_bX</code> tiene diferentes tama\u00f1os de entrada.<ul> <li>Nota: Muchos modelos modernos pueden manejar im\u00e1genes de entrada de diferentes tama\u00f1os gracias a [<code>torch.nn.AdaptiveAvgPool2d()</code>](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d .html), esta capa ajusta de forma adaptativa el <code>output_size</code> de una entrada determinada seg\u00fan sea necesario. Puede probar esto pasando im\u00e1genes de entrada de diferentes tama\u00f1os a <code>summary()</code> o a sus modelos.</li> </ul> </li> <li><code>col_names</code>: las diversas columnas de informaci\u00f3n que nos gustar\u00eda ver sobre nuestro modelo.</li> <li><code>col_width</code>: qu\u00e9 ancho deben tener las columnas para el resumen.</li> <li><code>row_settings</code>: qu\u00e9 funciones mostrar en una fila.</li> </ul>"},{"location":"11-06_pytorch_transfer_learning/#34-congelar-el-modelo-base-y-cambiar-la-capa-de-salida-para-adaptarla-a-nuestras-necesidades","title":"3.4 Congelar el modelo base y cambiar la capa de salida para adaptarla a nuestras necesidades\u00b6","text":"<p>El proceso de aprendizaje por transferencia suele ser el siguiente: congelar algunas capas base de un modelo previamente entrenado (normalmente la secci\u00f3n \"caracter\u00edsticas\") y luego ajustar las capas de salida (tambi\u00e9n llamadas capas principales/clasificadoras) para satisfacer sus necesidades.</p> <p>Puede personalizar las salidas de un modelo previamente entrenado cambiando las capas de salida para adaptarlas a su problema. El <code>torchvision.models.ficientnet_b0()</code> original viene con <code>out_features=1000</code> porque hay 1000 clases en ImageNet, el conjunto de datos en el que se entren\u00f3. Sin embargo, para nuestro problema de clasificar im\u00e1genes de pizza, bistec y sushi solo necesitamos <code>out_features=3</code>.</p> <p>Congelemos todas las capas/par\u00e1metros en la secci\u00f3n \"caracter\u00edsticas\" de nuestro modelo \"ficientnet_b0\".</p> <p>Nota: Congelar capas significa mantenerlas como est\u00e1n durante el entrenamiento. Por ejemplo, si su modelo tiene capas previamente entrenadas, congelarlas ser\u00eda decir: \"no cambie ninguno de los patrones en estas capas durante el entrenamiento, mant\u00e9ngalos como est\u00e1n\". En esencia, nos gustar\u00eda mantener los pesos/patrones previamente entrenados que nuestro modelo ha aprendido de ImageNet como columna vertebral y luego solo cambiar las capas de salida.</p> <p>Podemos congelar todas las capas/par\u00e1metros en la secci\u00f3n \"caracter\u00edsticas\" configurando el atributo \"requires_grad=False\".</p> <p>Para los par\u00e1metros con <code>requires_grad=False</code>, PyTorch no realiza un seguimiento de las actualizaciones de gradiente y, a su vez, nuestro optimizador no cambiar\u00e1 estos par\u00e1metros durante el entrenamiento.</p> <p>En esencia, un par\u00e1metro con <code>requires_grad=False</code> es \"no entrenable\" o \"congelado\" en su lugar.</p>"},{"location":"11-06_pytorch_transfer_learning/#4-modelo-de-tren","title":"4. Modelo de tren\u00b6","text":"<p>Ahora que tenemos un modelo previamente entrenado que est\u00e1 semicongelado y tiene un \"clasificador\" personalizado, \u00bfqu\u00e9 tal si vemos el aprendizaje por transferencia en acci\u00f3n?</p> <p>Para comenzar a entrenar, creemos una funci\u00f3n de p\u00e9rdida y un optimizador.</p> <p>Como todav\u00eda estamos trabajando con clasificaci\u00f3n de clases m\u00faltiples, usaremos <code>nn.CrossEntropyLoss()</code> para la funci\u00f3n de p\u00e9rdida.</p> <p>Y nos quedaremos con <code>torch.optim.Adam()</code> como nuestro optimizador con <code>lr=0.001</code>.</p>"},{"location":"11-06_pytorch_transfer_learning/#5-evaluar-el-modelo-trazando-curvas-de-perdida","title":"5. Evaluar el modelo trazando curvas de p\u00e9rdida\u00b6","text":"<p>Nuestro modelo parece estar funcionando bastante bien.</p> <p>Tracemos sus curvas de p\u00e9rdida para ver c\u00f3mo se ve el entrenamiento a lo largo del tiempo.</p> <p>Podemos trazar las curvas de p\u00e9rdida usando la funci\u00f3n <code>plot_loss_curves()</code> que creamos en [04. Secci\u00f3n 7.8 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0).</p> <p>La funci\u00f3n est\u00e1 almacenada en el script <code>helper_functions.py</code>, por lo que intentaremos importarla y descargarla. script si no lo tenemos.</p>"},{"location":"11-06_pytorch_transfer_learning/#6-haga-predicciones-sobre-imagenes-del-conjunto-de-prueba","title":"6. Haga predicciones sobre im\u00e1genes del conjunto de prueba.\u00b6","text":"<p>Parece que nuestro modelo funciona bien cuantitativamente pero \u00bfqu\u00e9 tal cualitativamente?</p> <p>Averig\u00fcemos haciendo algunas predicciones con nuestro modelo en im\u00e1genes del conjunto de prueba (\u00e9stas no se ven durante el entrenamiento) y grafiqu\u00e9moslas.</p> <p>\u00a1Visualiza, visualiza, visualiza!</p> <p>Una cosa que tendremos que recordar es que para que nuestro modelo haga predicciones sobre una imagen, la imagen debe tener el mismo formato que las im\u00e1genes en las que se entren\u00f3 nuestro modelo.</p> <p>Esto significa que necesitaremos asegurarnos de que nuestras im\u00e1genes tengan:</p> <ul> <li>Misma forma: si nuestras im\u00e1genes tienen formas diferentes a las que se entren\u00f3 nuestro modelo, obtendremos errores de forma.</li> <li>Mismo tipo de datos: si nuestras im\u00e1genes tienen un tipo de datos diferente (por ejemplo, <code>torch.int8</code> frente a <code>torch.float32</code>), obtendremos errores de tipo de datos.</li> <li>Mismo dispositivo: si nuestras im\u00e1genes est\u00e1n en un dispositivo diferente a nuestro modelo, obtendremos errores de dispositivo.</li> <li>Mismas transformaciones: si nuestro modelo se entrena con im\u00e1genes que se han transformado de cierta manera (por ejemplo, normalizadas con una media y una desviaci\u00f3n est\u00e1ndar espec\u00edficas) e intentamos hacer predicciones sobre im\u00e1genes transformadas de una manera diferente, estas predicciones pueden me voy.</li> </ul> <p>Nota: Estos requisitos se aplican a todo tipo de datos si intentas hacer predicciones con un modelo entrenado. Los datos que desea predecir deben estar en el mismo formato en el que se entren\u00f3 su modelo.</p> <p>Para hacer todo esto, crearemos una funci\u00f3n <code>pred_and_plot_image()</code> para:</p> <ol> <li>Tome un modelo entrenado, una lista de nombres de clases, una ruta de archivo a una imagen de destino, un tama\u00f1o de imagen, una transformaci\u00f3n y un dispositivo de destino.</li> <li>Abra una imagen con <code>PIL.Image.open()</code>.</li> <li>Cree una transformaci\u00f3n para la imagen (por defecto ser\u00e1 <code>manual_transforms</code> que creamos anteriormente o podr\u00eda usar una transformaci\u00f3n generada a partir de <code>weights.transforms()</code>).</li> <li>Aseg\u00farese de que el modelo est\u00e9 en el dispositivo de destino.</li> <li>Active el modo de evaluaci\u00f3n del modelo con <code>model.eval()</code> (esto desactiva capas como <code>nn.Dropout()</code>, por lo que no se usan para la inferencia) y el administrador de contexto del modo de inferencia.</li> <li>Transforme la imagen de destino con la transformaci\u00f3n realizada en el paso 3 y agregue una dimensi\u00f3n de lote adicional con <code>torch.unsqueeze(dim=0)</code> para que nuestra imagen de entrada tenga la forma <code>[batch_size, color_channels, height, width]</code>.</li> <li>Haga una predicci\u00f3n sobre la imagen pas\u00e1ndola al modelo asegur\u00e1ndose de que est\u00e9 en el dispositivo de destino.</li> <li>Convierta los logits de salida del modelo en probabilidades de predicci\u00f3n con <code>torch.softmax()</code>.</li> <li>Convierta las probabilidades de predicci\u00f3n del modelo en etiquetas de predicci\u00f3n con <code>torch.argmax()</code>.</li> <li>Trace la imagen con <code>matplotlib</code> y establezca el t\u00edtulo en la etiqueta de predicci\u00f3n del paso 9 y la probabilidad de predicci\u00f3n del paso 8.</li> </ol> <p>Nota: Esta es una funci\u00f3n similar a [04. Secci\u00f3n 11.3 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function) <code>pred_and_plot_image()</code> con algunos pasos modificados .</p>"},{"location":"11-06_pytorch_transfer_learning/#61-hacer-predicciones-sobre-una-imagen-personalizada","title":"6.1 Hacer predicciones sobre una imagen personalizada\u00b6","text":"<p>Parece que nuestro modelo obtiene buenos resultados cualitativos con los datos del conjunto de prueba.</p> <p>Pero \u00bfqu\u00e9 tal nuestra propia imagen personalizada?</p> <p>\u00a1Ah\u00ed es donde est\u00e1 la verdadera diversi\u00f3n del aprendizaje autom\u00e1tico!</p> <p>Predecir sobre sus propios datos personalizados, fuera de cualquier conjunto de entrenamiento o prueba.</p> <p>Para probar nuestro modelo en una imagen personalizada, importemos la antigua y fiel imagen <code>pizza-dad.jpeg</code> (una imagen de mi pap\u00e1 comiendo pizza).</p> <p>Luego lo pasaremos a la funci\u00f3n <code>pred_and_plot_image()</code> que creamos anteriormente y veremos qu\u00e9 sucede.</p>"},{"location":"11-06_pytorch_transfer_learning/#principales-conclusiones","title":"Principales conclusiones\u00b6","text":"<ul> <li>El aprendizaje por transferencia a menudo le permite obtener buenos resultados con una cantidad relativamente peque\u00f1a de datos personalizados.</li> <li>Conociendo el poder del aprendizaje por transferencia, es una buena idea preguntar al comienzo de cada problema: \"\u00bfExiste un modelo de buen rendimiento para mi problema?\"</li> <li>Cuando utilice un modelo previamente entrenado, es importante que sus datos personalizados est\u00e9n formateados/preprocesados \u200b\u200bde la misma manera que se entren\u00f3 el modelo original; de lo contrario, es posible que se degrade el rendimiento.</li> <li>Lo mismo ocurre con la predicci\u00f3n de datos personalizados; aseg\u00farese de que sus datos personalizados est\u00e9n en el mismo formato que los datos con los que se entren\u00f3 su modelo.</li> <li>Hay varios lugares diferentes para encontrar modelos previamente entrenados de las bibliotecas del dominio PyTorch, HuggingFace Hub y bibliotecas como <code>timm </code> (Modelos de imagen de PyTorch).</li> </ul>"},{"location":"11-06_pytorch_transfer_learning/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Todos los ejercicios se centran en practicar el c\u00f3digo anterior.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Todos los ejercicios deben completarse utilizando c\u00f3digo independiente del dispositivo.</p> <p>Recursos:</p> <ul> <li>Cuaderno de plantilla de ejercicios para 06</li> <li>Cuaderno de soluciones de ejemplo para 06 (pruebe los ejercicios antes de mirar esto)<ul> <li>Vea un video tutorial de las soluciones en vivo en YouTube (errores y todo)</li> </ul> </li> </ul> <ol> <li>Haga predicciones sobre todo el conjunto de datos de prueba y trace una matriz de confusi\u00f3n para los resultados de nuestro modelo en comparaci\u00f3n con las etiquetas de verdad. Consulte 03. Secci\u00f3n 10 de PyTorch Computer Vision para obtener ideas.</li> <li>Obtenga las predicciones \"m\u00e1s incorrectas\" en el conjunto de datos de prueba y trace las 5 im\u00e1genes \"m\u00e1s incorrectas\". Puedes hacer esto mediante:<ul> <li>Predecir en todo el conjunto de datos de prueba, almacenar las etiquetas y las probabilidades predichas.</li> <li>Ordene las predicciones por predicci\u00f3n incorrecta y luego probabilidades predichas descendentes, esto le dar\u00e1 las predicciones incorrectas con las probabilidades de predicci\u00f3n m\u00e1s altas, en otras palabras, las \"m\u00e1s incorrectas\".</li> <li>Traza las 5 im\u00e1genes \"m\u00e1s incorrectas\", \u00bfpor qu\u00e9 crees que el modelo se equivoc\u00f3?</li> </ul> </li> <li>Predice tu propia imagen de pizza/filete/sushi: \u00bfc\u00f3mo va el modelo? \u00bfQu\u00e9 sucede si predices en una imagen que no es pizza/filete/sushi?</li> <li>Entrene el modelo de la secci\u00f3n 4 anterior por m\u00e1s tiempo (10 \u00e9pocas deber\u00edan ser suficientes), \u00bfqu\u00e9 sucede con el rendimiento?</li> <li>Entrene el modelo de la secci\u00f3n 4 anterior con m\u00e1s datos, digamos el 20% de las im\u00e1genes de Food101 de pizza, bistec y sushi.<ul> <li>Puede encontrar el conjunto de datos 20% de pizza, bistec y sushi en el curso GitHub. Fue creado con el cuaderno <code>extras/04_custom_data_creation.ipynb</code>.</li> </ul> </li> <li>Pruebe un modelo diferente de <code>torchvision.models</code> en los datos de pizza, bistec y sushi. \u00bfC\u00f3mo funciona este modelo?<ul> <li>Tendr\u00e1s que cambiar el tama\u00f1o de la capa clasificadora para adaptarla a nuestro problema.</li> <li>Es posible que desee probar un EfficientNet con un n\u00famero mayor que nuestro B0, \u00bftal vez <code>torchvision.models.ficientnet_b2()</code>?</li> </ul> </li> </ol>"},{"location":"11-06_pytorch_transfer_learning/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Busque qu\u00e9 es el \"ajuste de modelo\" y dedique 30 minutos a investigar diferentes m\u00e9todos para realizarlo con PyTorch. \u00bfC\u00f3mo cambiar\u00edamos nuestro c\u00f3digo para perfeccionarlo? Consejo: el ajuste fino generalmente funciona mejor si tiene muchos datos personalizados, mientras que la extracci\u00f3n de caracter\u00edsticas suele ser mejor si tiene menos datos personalizados.</li> <li>Consulte la nueva/pr\u00f3xima [API de pesos m\u00faltiples de PyTorch] (https://pytorch.org/blog/introtaining-torchvision-new-multi-weight-support-api/) (a\u00fan en versi\u00f3n beta al momento de escribir este art\u00edculo, mayo 2022), es una nueva forma de realizar aprendizaje por transferencia en PyTorch. \u00bfQu\u00e9 cambios ser\u00eda necesario realizar en nuestro c\u00f3digo para utilizar la nueva API?</li> <li>Intente crear su propio clasificador en dos clases de im\u00e1genes; por ejemplo, podr\u00eda recopilar 10 fotos de su perro y el perro de sus amigos y entrenar un modelo para clasificar a los dos perros. Esta ser\u00eda una buena manera de practicar la creaci\u00f3n de un conjunto de datos y la construcci\u00f3n de un modelo a partir de ese conjunto de datos.</li> </ul>"},{"location":"12-08_pytorch_paper_replicating/","title":"08. Replicaci\u00f3n de papel PyTorch","text":"<p>Ver c\u00f3digo fuente | [Ver diapositivas] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/08_pytorch_paper_replicating.pdf)</p> In\u00a0[\u00a0]: Copied! <pre># Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+. try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <p>Nota: Si est\u00e1 utilizando Google Colab y la celda de arriba comienza a instalar varios paquetes de software, es posible que deba reiniciar su tiempo de ejecuci\u00f3n despu\u00e9s de ejecutar la celda de arriba. Despu\u00e9s de reiniciar, puede ejecutar la celda nuevamente y verificar que tenga las versiones correctas de <code>torch</code> y <code>torchvision</code>.</p> <p>Ahora continuaremos con las importaciones regulares, configurando el c\u00f3digo independiente del dispositivo y esta vez tambi\u00e9n obtendremos [<code>helper_functions.py</code>](https://github.com/mrdbourke/pytorch-deep-learning/blob/ main/helper_functions.py) script de GitHub.</p> <p>El script <code>helper_functions.py</code> contiene varias funciones que creamos en secciones anteriores:</p> <ul> <li><code>set_seeds()</code> para configurar las semillas aleatorias (creadas en 07. Secci\u00f3n 0 de seguimiento de experimentos de PyTorch).</li> <li><code>download_data()</code> para descargar una fuente de datos mediante un enlace (creado en [07. Secci\u00f3n 1 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).</li> <li><code>plot_loss_curves()</code> para inspeccionar los resultados del entrenamiento de nuestro modelo (creado en [04. PyTorch Custom Datasets secci\u00f3n 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of- modelo-0))</li> </ul> <p>Nota: Puede ser una mejor idea que muchas de las funciones en el script <code>helper_functions.py</code> se fusionen en <code>going_modular/going_modular/utils.py</code>, tal vez sea una extensi\u00f3n que le gustar\u00eda probar .</p> In\u00a0[\u00a0]: Copied! <pre># Continuar con las importaciones regulares\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Intente obtener torchinfo, inst\u00e1lelo si no funciona\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continuar con las importaciones regulares import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Intente obtener torchinfo, inst\u00e1lelo si no funciona try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <p>Nota: Si est\u00e1s usando Google Colab y a\u00fan no tienes una GPU activada, ahora es el momento de activar una a trav\u00e9s de <code>Runtime -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Acelerador de hardware -&gt; GPU</code> .</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre># Descargue im\u00e1genes de pizza, bistec y sushi desde GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> # Descargue im\u00e1genes de pizza, bistec y sushi desde GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <p>\u00a1Hermoso! Datos descargados, configuremos los directorios de capacitaci\u00f3n y prueba.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar rutas de directorio para entrenar y probar im\u00e1genes\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Configurar rutas de directorio para entrenar y probar im\u00e1genes train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[\u00a0]: Copied! <pre># Crear tama\u00f1o de imagen (de la Tabla 3 en el art\u00edculo de ViT)\nIMG_SIZE = 224\n\n# Crear canalizaci\u00f3n de transformaci\u00f3n manualmente\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])\nprint(f\"Manually created transforms: {manual_transforms}\")\n</pre> # Crear tama\u00f1o de imagen (de la Tabla 3 en el art\u00edculo de ViT) IMG_SIZE = 224  # Crear canalizaci\u00f3n de transformaci\u00f3n manualmente manual_transforms = transforms.Compose([     transforms.Resize((IMG_SIZE, IMG_SIZE)),     transforms.ToTensor(), ]) print(f\"Manually created transforms: {manual_transforms}\") In\u00a0[\u00a0]: Copied! <pre># Establecer el tama\u00f1o del lote\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Crear cargadores de datos\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Establecer el tama\u00f1o del lote BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small  # Crear cargadores de datos train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=BATCH_SIZE )  train_dataloader, test_dataloader, class_names In\u00a0[\u00a0]: Copied! <pre># Obtener un lote de im\u00e1genes\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Obtenga una sola imagen del lote\nimage, label = image_batch[0], label_batch[0]\n\n# Ver las formas del lote\nimage.shape, label\n</pre> # Obtener un lote de im\u00e1genes image_batch, label_batch = next(iter(train_dataloader))  # Obtenga una sola imagen del lote image, label = image_batch[0], label_batch[0]  # Ver las formas del lote image.shape, label <p>\u00a1Maravilloso!</p> <p>Ahora tracemos la imagen y su etiqueta con <code>matplotlib</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Trazar imagen con matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Trazar imagen con matplotlib plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels] plt.title(class_names[label]) plt.axis(False); <p>\u00a1Lindo!</p> <p>Parece que nuestras im\u00e1genes se est\u00e1n importando correctamente, continuemos con la replicaci\u00f3n en papel.</p> In\u00a0[\u00a0]: Copied! <pre># Crear valores de ejemplo\nheight = 224 # H (\"The training resolution is 224.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# Calcular N (n\u00famero de parches)\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n</pre> # Crear valores de ejemplo height = 224 # H (\"The training resolution is 224.\") width = 224 # W color_channels = 3 # C patch_size = 16 # P  # Calcular N (n\u00famero de parches) number_of_patches = int((height * width) / patch_size**2) print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\") <p>Tenemos la cantidad de parches, \u00bfqu\u00e9 tal si creamos tambi\u00e9n el tama\u00f1o de salida de la imagen?</p> <p>Mejor a\u00fan, repliquemos las formas de entrada y salida de la capa de incrustaci\u00f3n del parche.</p> <p>Recordar:</p> <ul> <li>Entrada: La imagen comienza como 2D con tama\u00f1o ${H \\times W \\times C}$.</li> <li>Salida: La imagen se convierte en una secuencia de parches 2D aplanados con tama\u00f1o ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Forma de entrada (este es el tama\u00f1o de una sola imagen)\nembedding_layer_input_shape = (height, width, color_channels)\n\n# Forma de salida\nembedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\nprint(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")\n</pre> # Forma de entrada (este es el tama\u00f1o de una sola imagen) embedding_layer_input_shape = (height, width, color_channels)  # Forma de salida embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)  print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\") print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\") <p>\u00a1Formas de entrada y salida adquiridas!</p> In\u00a0[\u00a0]: Copied! <pre># Ver una sola imagen\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Ver una sola imagen plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); <p>Queremos convertir esta imagen en parches de s\u00ed misma en l\u00ednea con la Figura 1 del art\u00edculo de ViT.</p> <p>\u00bfQu\u00e9 tal si comenzamos visualizando simplemente la fila superior de p\u00edxeles parcheados?</p> <p>Podemos hacer esto indexando las diferentes dimensiones de la imagen.</p> In\u00a0[\u00a0]: Copied! <pre># Cambie la forma de la imagen para que sea compatible con matplotlib (color_channels, alto, ancho) -&gt; (alto, ancho, color_channels)\nimage_permuted = image.permute(1, 2, 0)\n\n# \u00cdndice para trazar la fila superior de p\u00edxeles parcheados\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n</pre> # Cambie la forma de la imagen para que sea compatible con matplotlib (color_channels, alto, ancho) -&gt; (alto, ancho, color_channels) image_permuted = image.permute(1, 2, 0)  # \u00cdndice para trazar la fila superior de p\u00edxeles parcheados patch_size = 16 plt.figure(figsize=(patch_size, patch_size)) plt.imshow(image_permuted[:patch_size, :, :]); <p>Ahora que tenemos la fila superior, convirt\u00e1mosla en parches.</p> <p>Podemos hacer esto repitiendo la cantidad de parches que habr\u00eda en la fila superior.</p> In\u00a0[\u00a0]: Copied! <pre># Configure los hiperpar\u00e1metros y aseg\u00farese de que img_size y patch_size sean compatibles\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size\nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\nprint(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Crea una serie de subtramas.\nfig, axs = plt.subplots(nrows=1,\n                        ncols=img_size // patch_size, # one column for each patch\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Iterar a trav\u00e9s del n\u00famero de parches en la fila superior\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n    axs[i].set_xlabel(i+1) # set the label\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n</pre> # Configure los hiperpar\u00e1metros y aseg\u00farese de que img_size y patch_size sean compatibles img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Crea una serie de subtramas. fig, axs = plt.subplots(nrows=1,                         ncols=img_size // patch_size, # one column for each patch                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Iterar a trav\u00e9s del n\u00famero de parches en la fila superior for i, patch in enumerate(range(0, img_size, patch_size)):     axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index     axs[i].set_xlabel(i+1) # set the label     axs[i].set_xticks([])     axs[i].set_yticks([]) <p>\u00a1Esos son unos parches muy bonitos!</p> <p>\u00bfQu\u00e9 tal si lo hacemos para toda la imagen?</p> <p>Esta vez recorreremos los \u00edndices de alto y ancho y trazaremos cada parche como su propia trama secundaria.</p> In\u00a0[\u00a0]: Copied! <pre># Configure los hiperpar\u00e1metros y aseg\u00farese de que img_size y patch_size sean compatibles\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size\nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\nprint(f\"Number of patches per row: {num_patches}\\\n        \\nNumber of patches per column: {num_patches}\\\n        \\nTotal patches: {num_patches*num_patches}\\\n        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Crea una serie de subtramas.\nfig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n                        ncols=img_size // patch_size,\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Recorrer el alto y el ancho de la imagen\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n\n        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n                                        patch_width:patch_width+patch_size, # iterate through width\n                                        :]) # get all color channels\n\n        # Set up label information, remove the ticks for clarity and set labels to outside\n        axs[i, j].set_ylabel(i+1,\n                             rotation=\"horizontal\",\n                             horizontalalignment=\"right\",\n                             verticalalignment=\"center\")\n        axs[i, j].set_xlabel(j+1)\n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# Establecer un s\u00faper t\u00edtulo\nfig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16)\nplt.show()\n</pre> # Configure los hiperpar\u00e1metros y aseg\u00farese de que img_size y patch_size sean compatibles img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\         \\nNumber of patches per column: {num_patches}\\         \\nTotal patches: {num_patches*num_patches}\\         \\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Crea una serie de subtramas. fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float                         ncols=img_size // patch_size,                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Recorrer el alto y el ancho de la imagen for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height     for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width          # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))         axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height                                         patch_width:patch_width+patch_size, # iterate through width                                         :]) # get all color channels          # Set up label information, remove the ticks for clarity and set labels to outside         axs[i, j].set_ylabel(i+1,                              rotation=\"horizontal\",                              horizontalalignment=\"right\",                              verticalalignment=\"center\")         axs[i, j].set_xlabel(j+1)         axs[i, j].set_xticks([])         axs[i, j].set_yticks([])         axs[i, j].label_outer()  # Establecer un s\u00faper t\u00edtulo fig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16) plt.show() <p>Imagen parcheada!</p> <p>Vaya, eso se ve genial.</p> <p>Ahora, \u00bfc\u00f3mo convertimos cada uno de estos parches en una incrustaci\u00f3n y los convertimos en una secuencia?</p> <p>Sugerencia: podemos usar capas de PyTorch. \u00bfPuedes adivinar cu\u00e1l?</p> In\u00a0[\u00a0]: Copied! <pre>from torch import nn\n\n# Establecer el tama\u00f1o del parche\npatch_size=16\n\n# Cree la capa Conv2d con hiperpar\u00e1metros del documento ViT\nconv2d = nn.Conv2d(in_channels=3, # number of color channels\n                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n                   stride=patch_size,\n                   padding=0)\n</pre> from torch import nn  # Establecer el tama\u00f1o del parche patch_size=16  # Cree la capa Conv2d con hiperpar\u00e1metros del documento ViT conv2d = nn.Conv2d(in_channels=3, # number of color channels                    out_channels=768, # from Table 1: Hidden size D, this is the embedding size                    kernel_size=patch_size, # could also use (patch_size, patch_size)                    stride=patch_size,                    padding=0) <p>Ahora que tenemos una capa convolucional, veamos qu\u00e9 sucede cuando pasamos una sola imagen a trav\u00e9s de ella.</p> In\u00a0[\u00a0]: Copied! <pre># Ver una sola imagen\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Ver una sola imagen plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); In\u00a0[\u00a0]: Copied! <pre># Pasar la imagen a trav\u00e9s de la capa convolucional.\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)\nprint(image_out_of_conv.shape)\n</pre> # Pasar la imagen a trav\u00e9s de la capa convolucional. image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels) print(image_out_of_conv.shape) <p>Pasar nuestra imagen a trav\u00e9s de la capa convolucional la convierte en una serie de 768 (este es el tama\u00f1o de incrustaci\u00f3n o $D$) mapas de caracter\u00edsticas/activaci\u00f3n.</p> <p>Entonces su forma de salida se puede leer como:</p> <pre><code>pit\u00f3n\ntorch.Size([1, 768, 14, 14]) -&gt; [batch_size, embedding_dim, feature_map_height, feature_map_width]\n</code></pre> <p>Visualicemos cinco mapas de caracter\u00edsticas aleatorias y veamos c\u00f3mo se ven.</p> In\u00a0[\u00a0]: Copied! <pre># Trazar 5 mapas de caracter\u00edsticas convolucionales aleatorios\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\nprint(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n\n# Crear trama\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# Trazar mapas de caracter\u00edsticas de im\u00e1genes aleatorias\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n</pre> # Trazar 5 mapas de caracter\u00edsticas convolucionales aleatorios import random random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")  # Crear trama fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))  # Trazar mapas de caracter\u00edsticas de im\u00e1genes aleatorias for i, idx in enumerate(random_indexes):     image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer     axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())     axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]); <p>Observe c\u00f3mo todos los mapas de caracter\u00edsticas representan la imagen original; despu\u00e9s de visualizar algunos m\u00e1s, puede comenzar a ver los diferentes contornos principales y algunas caracter\u00edsticas principales.</p> <p>Lo importante a tener en cuenta es que estas caracter\u00edsticas pueden cambiar con el tiempo a medida que la red neuronal aprende.</p> <p>Y debido a esto, estos mapas de caracter\u00edsticas pueden considerarse una incrustaci\u00f3n que se puede aprender de nuestra imagen.</p> <p>Veamos uno en forma num\u00e9rica.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga un mapa de caracter\u00edsticas \u00fanicas en forma tensorial\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n</pre> # Obtenga un mapa de caracter\u00edsticas \u00fanicas en forma tensorial single_feature_map = image_out_of_conv[:, 0, :, :] single_feature_map, single_feature_map.requires_grad <p>La salida <code>grad_fn</code> de <code>single_feature_map</code> y el atributo <code>requires_grad=True</code> significa que PyTorch est\u00e1 rastreando los gradientes de este mapa de caracter\u00edsticas y se actualizar\u00e1 mediante el descenso de gradiente durante el entrenamiento.</p> In\u00a0[\u00a0]: Copied! <pre># Forma del tensor actual\nprint(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\")\n</pre> # Forma del tensor actual print(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\") <p>Bueno, tenemos la parte 768 ( $(P^{2} \\cdot C)$ ) pero a\u00fan necesitamos la cantidad de parches ($N$).</p> <p>Volviendo a leer la secci\u00f3n 3.1 del documento de ViT, dice (negrita m\u00eda):</p> <p>Como caso especial, los parches pueden tener un tama\u00f1o espacial $1 \\times 1$, lo que significa que la secuencia de entrada se obtiene simplemente aplanando las dimensiones espaciales del mapa de caracter\u00edsticas y proyect\u00e1ndolas a la dimensi\u00f3n del Transformador.</p> <p>Aplanando las dimensiones espaciales del mapa de caracter\u00edsticas, \u00bfeh?</p> <p>\u00bfQu\u00e9 capa tenemos en PyTorch que se pueda aplanar?</p> <p>\u00bfQu\u00e9 tal <code>torch.nn.Flatten()</code>?</p> <p>Pero no queremos aplanar todo el tensor, s\u00f3lo queremos aplanar las \"dimensiones espaciales del mapa de caracter\u00edsticas\".</p> <p>Que en nuestro caso son las dimensiones <code>feature_map_height</code> y <code>feature_map_width</code> de <code>image_out_of_conv</code>.</p> <p>Entonces, \u00bfqu\u00e9 tal si creamos una capa <code>torch.nn.Flatten()</code> para aplanar solo esas dimensiones? \u00bfPodemos usar los par\u00e1metros <code>start_dim</code> y <code>end_dim</code> para configurar eso?</p> In\u00a0[\u00a0]: Copied! <pre># Crear capa aplanada\nflatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n                     end_dim=3) # flatten feature_map_width (dimension 3)\n</pre> # Crear capa aplanada flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)                      end_dim=3) # flatten feature_map_width (dimension 3) <p>\u00a1Lindo! \u00a1Ahora juntemos todo!</p> <p>Bien:</p> <ol> <li>Tome una sola imagen.</li> <li>Introd\u00fazcalo a trav\u00e9s de la capa convolucional (<code>conv2d</code>) para convertir la imagen en mapas de caracter\u00edsticas 2D (incrustaciones de parches).</li> <li>Aplana el mapa de caracter\u00edsticas 2D en una sola secuencia.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># 1. Ver una sola imagen\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"Original image shape: {image.shape}\")\n\n# 2. Convierta la imagen en mapas de caracter\u00edsticas\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\nprint(f\"Image feature map shape: {image_out_of_conv.shape}\")\n\n# 3. Aplanar los mapas de caracter\u00edsticas.\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")\n</pre> # 1. Ver una sola imagen plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); print(f\"Original image shape: {image.shape}\")  # 2. Convierta la imagen en mapas de caracter\u00edsticas image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors print(f\"Image feature map shape: {image_out_of_conv.shape}\")  # 3. Aplanar los mapas de caracter\u00edsticas. image_out_of_conv_flattened = flatten(image_out_of_conv) print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\") <p>\u00a1Guau! Parece que nuestra forma <code>image_out_of_conv_flattened</code> est\u00e1 muy cerca de nuestra forma de salida deseada:</p> <ul> <li>Salida deseada (parches 2D aplanados): (196, 768) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> <li>Forma actual: (1, 768, 196)</li> </ul> <p>La \u00fanica diferencia es que nuestra forma actual tiene un tama\u00f1o de lote y las dimensiones est\u00e1n en un orden diferente al resultado deseado.</p> <p>\u00bfC\u00f3mo podr\u00edamos solucionar esto?</p> <p>Bueno, \u00bfqu\u00e9 tal si reorganizamos las dimensiones?</p> <p>Podemos hacerlo con <code>torch.Tensor.permute()</code> tal como lo hacemos cuando reorganizamos los tensores de im\u00e1genes para trazarlos con matplotlib.</p> <p>Intentemos.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga incrustaciones de parches de im\u00e1genes aplanadas en la forma correcta\nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\nprint(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\")\n</pre> # Obtenga incrustaciones de parches de im\u00e1genes aplanadas en la forma correcta image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\") <p>\u00a1\u00a1\u00a1S\u00ed!!!</p> <p>Ahora hemos hecho coincidir las formas de entrada y salida deseadas para la capa de incrustaci\u00f3n de parches de la arquitectura ViT usando un par de capas de PyTorch.</p> <p>\u00bfQu\u00e9 tal si visualizamos uno de los mapas de caracter\u00edsticas aplanados?</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga un \u00fanico mapa de caracter\u00edsticas aplanado\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n\n# Trazar visualmente el mapa de caracter\u00edsticas aplanado\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n</pre> # Obtenga un \u00fanico mapa de caracter\u00edsticas aplanado single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)  # Trazar visualmente el mapa de caracter\u00edsticas aplanado plt.figure(figsize=(22, 22)) plt.imshow(single_flattened_feature_map.detach().numpy()) plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\") plt.axis(False); <p>Hmm, el mapa de caracter\u00edsticas aplanado no parece gran cosa visualmente, pero eso no es lo que nos preocupa, esto es lo que ser\u00e1 la salida de la capa de incrustaci\u00f3n de parches y la entrada al resto de la arquitectura ViT.</p> <p>Nota: La arquitectura Transformer original fue dise\u00f1ada para funcionar con texto. La arquitectura Vision Transformer (ViT) ten\u00eda el objetivo de utilizar el Transformer original para im\u00e1genes. Es por eso que la entrada a la arquitectura ViT se procesa de la forma en que est\u00e1. B\u00e1sicamente, tomamos una imagen 2D y la formateamos para que aparezca como una secuencia de texto 1D.</p> <p>\u00bfQu\u00e9 tal si vemos el mapa de caracter\u00edsticas aplanado en forma tensorial?</p> In\u00a0[\u00a0]: Copied! <pre># Vea el mapa de caracter\u00edsticas aplanado como un tensor\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n</pre> # Vea el mapa de caracter\u00edsticas aplanado como un tensor single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape <p>\u00a1Hermoso!</p> <p>Hemos convertido nuestra \u00fanica imagen 2D en un vector de incrustaci\u00f3n 1D que se puede aprender (o \"Proyecci\u00f3n lineal de parches aplanados\" en la Figura 1 del art\u00edculo de ViT).</p> In\u00a0[\u00a0]: Copied! <pre># 1. Cree una clase que subclase nn.Module\nclass PatchEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n\n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\"\n    # 2. Initialize the class with appropriate variables\n    def __init__(self,\n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n\n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method\n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n\n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched)\n        # 6. Make sure the output shape has the right order\n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\n</pre> # 1. Cree una clase que subclase nn.Module class PatchEmbedding(nn.Module):     \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.      Args:         in_channels (int): Number of color channels for the input images. Defaults to 3.         patch_size (int): Size of patches to convert input image into. Defaults to 16.         embedding_dim (int): Size of embedding to turn image into. Defaults to 768.     \"\"\"     # 2. Initialize the class with appropriate variables     def __init__(self,                  in_channels:int=3,                  patch_size:int=16,                  embedding_dim:int=768):         super().__init__()          # 3. Create a layer to turn an image into patches         self.patcher = nn.Conv2d(in_channels=in_channels,                                  out_channels=embedding_dim,                                  kernel_size=patch_size,                                  stride=patch_size,                                  padding=0)          # 4. Create a layer to flatten the patch feature maps into a single dimension         self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector                                   end_dim=3)      # 5. Define the forward method     def forward(self, x):         # Create assertion to check that inputs are the correct shape         image_resolution = x.shape[-1]         assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"          # Perform the forward pass         x_patched = self.patcher(x)         x_flattened = self.flatten(x_patched)         # 6. Make sure the output shape has the right order         return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] <p>\u00a1Capa <code>PatchEmbedding</code> creada!</p> <p>Prob\u00e9moslo en una sola imagen.</p> In\u00a0[\u00a0]: Copied! <pre>set_seeds()\n\n# Crear una instancia de capa de incrustaci\u00f3n de parches\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# Pasar una sola imagen a trav\u00e9s\nprint(f\"Input image shape: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\nprint(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n</pre> set_seeds()  # Crear una instancia de capa de incrustaci\u00f3n de parches patchify = PatchEmbedding(in_channels=3,                           patch_size=16,                           embedding_dim=768)  # Pasar una sola imagen a trav\u00e9s print(f\"Input image shape: {image.unsqueeze(0).shape}\") patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error print(f\"Output patch embedding shape: {patch_embedded_image.shape}\") <p>\u00a1Hermoso!</p> <p>La forma de salida coincide con las formas de entrada y salida ideales que nos gustar\u00eda ver en la capa de incrustaci\u00f3n del parche:</p> <ul> <li>Entrada: La imagen comienza como 2D con tama\u00f1o ${H \\times W \\times C}$.</li> <li>Salida: La imagen se convierte en una secuencia 1D de parches 2D aplanados con tama\u00f1o ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>D\u00f3nde:</p> <ul> <li>$(H, W)$ es la resoluci\u00f3n de la imagen original.</li> <li>$C$ es el n\u00famero de canales.</li> <li>$(P, P)$ es la resoluci\u00f3n de cada parche de imagen (tama\u00f1o del parche).</li> <li>$N=H W / P^{2}$ es el n\u00famero resultante de parches, que tambi\u00e9n sirve como longitud efectiva de la secuencia de entrada para el Transformer.</li> </ul> <p>Ahora hemos replicado la incorporaci\u00f3n del parche para la ecuaci\u00f3n 1, pero no la incorporaci\u00f3n del token de clase/posici\u00f3n.</p> <p>Llegaremos a esto m\u00e1s adelante.</p> <p></p> <p>Nuestra clase <code>PatchEmbedding</code> (derecha) replica la incorporaci\u00f3n de parches de la arquitectura ViT de la Figura 1 y la Ecuaci\u00f3n 1 del art\u00edculo de ViT (izquierda). Sin embargo, las incrustaciones de clases y posiciones que se pueden aprender a\u00fan no se han creado. Estos llegar\u00e1n pronto.</p> <p>Ahora obtengamos un resumen de nuestra capa <code>PatchEmbedding</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Crear tama\u00f1os de entrada aleatorios\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# # Obtener un resumen de las entradas y salidas de PatchEmbedding (descomentar para obtener el resultado completo)\n# resumen(PatchEmbedding(),\n# input_size=random_input_image, # intenta cambiar esto por \"random_input_image_error\"\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"])\n</pre> # Crear tama\u00f1os de entrada aleatorios random_input_image = (1, 3, 224, 224) random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size  # # Obtener un resumen de las entradas y salidas de PatchEmbedding (descomentar para obtener el resultado completo) # resumen(PatchEmbedding(), # input_size=random_input_image, # intenta cambiar esto por \"random_input_image_error\" # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"]) In\u00a0[\u00a0]: Copied! <pre># Ver la incrustaci\u00f3n del parche y la forma de la incrustaci\u00f3n del parche\nprint(patch_embedded_image)\nprint(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Ver la incrustaci\u00f3n del parche y la forma de la incrustaci\u00f3n del parche print(patch_embedded_image) print(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <p>Para \"anteponer una incrustaci\u00f3n que se puede aprender a la secuencia de parches incrustados\", necesitamos crear una incrustaci\u00f3n que se puede aprender en la forma de <code>embedding_dimension</code> ($D$) y luego agregarla a la dimensi\u00f3n <code>number_of_patches</code>.</p> <p>O en pseudoc\u00f3digo:</p> <pre><code>pit\u00f3n\npatch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = incrustaci\u00f3n_aprendible\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n</code></pre> <p>Observe que la concatenaci\u00f3n (<code>torch.cat()</code>) ocurre en <code>dim=1</code> (la dimensi\u00f3n <code>number_of_patches</code>).</p> <p>Creemos una incrustaci\u00f3n que se pueda aprender para el token de clase.</p> <p>Para hacerlo, obtendremos el tama\u00f1o del lote y la forma de la dimensi\u00f3n de incrustaci\u00f3n y luego crearemos un tensor <code>torch.ones()</code> con la forma <code>[batch_size, 1, embedding_dimension]</code>.</p> <p>Y haremos que el tensor se pueda aprender pas\u00e1ndolo a <code>nn.Parameter()</code> con <code>requires_grad=True</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga el tama\u00f1o del lote y la dimensi\u00f3n de incrustaci\u00f3n\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# Cree la incrustaci\u00f3n del token de clase como un par\u00e1metro que se puede aprender y que comparte el mismo tama\u00f1o que la dimensi\u00f3n de incrustaci\u00f3n (D)\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n                           requires_grad=True) # make sure the embedding is learnable\n\n# Muestra los primeros 10 ejemplos de class_token\nprint(class_token[:, :, :10])\n\n# Imprime la forma class_token\nprint(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\")\n</pre> # Obtenga el tama\u00f1o del lote y la dimensi\u00f3n de incrustaci\u00f3n batch_size = patch_embedded_image.shape[0] embedding_dimension = patch_embedded_image.shape[-1]  # Cree la incrustaci\u00f3n del token de clase como un par\u00e1metro que se puede aprender y que comparte el mismo tama\u00f1o que la dimensi\u00f3n de incrustaci\u00f3n (D) class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]                            requires_grad=True) # make sure the embedding is learnable  # Muestra los primeros 10 ejemplos de class_token print(class_token[:, :, :10])  # Imprime la forma class_token print(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\") <p>Nota: Aqu\u00ed solo estamos creando el token de clase incrustado como <code>torch.ones()</code> para demostraci\u00f3n prop\u00f3sitos, en realidad, probablemente crear\u00edas el token de clase incrustado con <code>torch.randn()</code> (ya que el aprendizaje autom\u00e1tico es todo se trata de aprovechar el poder de la aleatoriedad controlada, generalmente se comienza con un n\u00famero aleatorio y se mejora con el tiempo).</p> <p>Vea c\u00f3mo la dimensi\u00f3n <code>number_of_tokens</code> de <code>class_token</code> es <code>1</code> ya que solo queremos anteponer un valor de token de clase al inicio de la secuencia de incorporaci\u00f3n del parche.</p> <p>Ahora que tenemos el token de clase incrustado, antepongamoslo a nuestra secuencia de parches de im\u00e1genes, <code>patch_embedded_image</code>.</p> <p>Podemos hacerlo usando <code>torch.cat()</code> y establecer <code>dim=1</code> (por lo que <code>class_token</code>' La dimensi\u00f3n number_of_tokens<code>est\u00e1 preadaptada a la dimensi\u00f3n</code>number_of_patches<code>de</code>patch_embedded_image`).</p> In\u00a0[\u00a0]: Copied! <pre># Agregue la incrustaci\u00f3n del token de clase al frente de la incrustaci\u00f3n del parche\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n                                                      dim=1) # concat on first dimension\n\n# Imprima la secuencia de incrustaciones de parches con la incrustaci\u00f3n de token de clase antepuesta\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Agregue la incrustaci\u00f3n del token de clase al frente de la incrustaci\u00f3n del parche patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),                                                       dim=1) # concat on first dimension  # Imprima la secuencia de incrustaciones de parches con la incrustaci\u00f3n de token de clase antepuesta print(patch_embedded_image_with_class_embedding) print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <p>\u00a1Lindo! \u00a1Token de clase aprendible antepuesto!</p> <p></p> <p>Al revisar lo que hemos hecho para crear el token de clase que se puede aprender, comenzamos con una secuencia de incrustaciones de parches de im\u00e1genes creadas por <code>PatchEmbedding()</code> en una sola imagen, luego creamos un token de clase que se puede aprender con un valor para cada una de las dimensiones de incrustaci\u00f3n. y luego lo antepuso a la secuencia original de incrustaciones de parches. Nota: El uso de <code>torch.ones()</code> para crear el token de clase que se puede aprender es principalmente solo para fines de demostraci\u00f3n; en la pr\u00e1ctica, probablemente lo crear\u00edas con <code>torch.randn()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Ver la secuencia de incrustaciones de parches con la incrustaci\u00f3n de clases antepuesta\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n</pre> # Ver la secuencia de incrustaciones de parches con la incrustaci\u00f3n de clases antepuesta patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape <p>La ecuaci\u00f3n 1 establece que las incrustaciones de posici\u00f3n ($\\mathbf{E}_{\\text {pos }}$) deben tener la forma $(N + 1) \\times D$:</p> <p>$$\\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}$$</p> <p>D\u00f3nde:</p> <ul> <li>$N=H W / P^{2}$ es el n\u00famero resultante de parches, que tambi\u00e9n sirve como longitud efectiva de la secuencia de entrada para el Transformer (n\u00famero de parches).</li> <li>$D$ es el tama\u00f1o de las incrustaciones de parches; se pueden encontrar diferentes valores para $D$ en la Tabla 1 (dimensi\u00f3n de incrustaci\u00f3n).</li> </ul> <p>Afortunadamente, ya tenemos ambos valores.</p> <p>Entonces, hagamos una incrustaci\u00f3n 1D que se pueda aprender con <code>torch.ones()</code> para crear $\\mathbf{E}_{\\text {pos }}$.</p> In\u00a0[\u00a0]: Copied! <pre># Calcular N (n\u00famero de parches)\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# Obtener dimensi\u00f3n de incrustaci\u00f3n\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# Cree la incrustaci\u00f3n de posici\u00f3n 1D que se puede aprender\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1,\n                                             embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# Muestre las primeras 10 secuencias y los 10 valores de incrustaci\u00f3n de posici\u00f3n y verifique la forma de la incrustaci\u00f3n de posici\u00f3n\nprint(position_embedding[:, :10, :10])\nprint(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Calcular N (n\u00famero de parches) number_of_patches = int((height * width) / patch_size**2)  # Obtener dimensi\u00f3n de incrustaci\u00f3n embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]  # Cree la incrustaci\u00f3n de posici\u00f3n 1D que se puede aprender position_embedding = nn.Parameter(torch.ones(1,                                              number_of_patches+1,                                              embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # Muestre las primeras 10 secuencias y los 10 valores de incrustaci\u00f3n de posici\u00f3n y verifique la forma de la incrustaci\u00f3n de posici\u00f3n print(position_embedding[:, :10, :10]) print(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <p>Nota: Al crear la posici\u00f3n incrustada como <code>torch.ones()</code> solo para fines de demostraci\u00f3n, en realidad, probablemente crear\u00edas la posici\u00f3n incrustada con <code>torch.randn()</code> (comience con un n\u00famero aleatorio y mejorar mediante descenso de gradiente).</p> <p>\u00a1Incrustaciones de posici\u00f3n creadas!</p> <p>Agregu\u00e9moslos a nuestra secuencia de incrustaciones de parches con un token de clase antepuesto.</p> In\u00a0[\u00a0]: Copied! <pre># Agregue la incrustaci\u00f3n de posici\u00f3n a la incrustaci\u00f3n de parche y token de clase\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Agregue la incrustaci\u00f3n de posici\u00f3n a la incrustaci\u00f3n de parche y token de clase patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding print(patch_and_position_embedding) print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <p>Observe c\u00f3mo los valores de cada uno de los elementos en el tensor de incrustaci\u00f3n aumentan en 1 (esto se debe a que las incrustaciones de posici\u00f3n se crean con <code>torch.ones()</code>).</p> <p>Nota: Podr\u00edamos colocar tanto la incrustaci\u00f3n del token de clase como la incrustaci\u00f3n de posici\u00f3n en su propia capa si quisi\u00e9ramos. Pero veremos m\u00e1s adelante en la secci\u00f3n 8 c\u00f3mo se pueden incorporar al m\u00e9todo <code>forward()</code> de la arquitectura ViT general.</p> <p></p> <p>El flujo de trabajo que hemos utilizado para agregar las incorporaciones de posici\u00f3n a la secuencia de incorporaciones de parches y tokens de clase. Nota: <code>torch.ones()</code> solo se usa para crear incrustaciones con fines ilustrativos; en la pr\u00e1ctica, probablemente usar\u00edas <code>torch.randn()</code> para comenzar con un n\u00famero aleatorio.</p> In\u00a0[\u00a0]: Copied! <pre>set_seeds()\n\n# 1. Establecer el tama\u00f1o del parche\npatch_size = 16\n\n# 2. Imprima la forma del tensor de imagen original y obtenga las dimensiones de la imagen.\nprint(f\"Image tensor shape: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. Obtenga el tensor de imagen y agregue la dimensi\u00f3n del lote\nx = image.unsqueeze(0)\nprint(f\"Input image with batch dimension shape: {x.shape}\")\n\n# 4. Crear una capa de incrustaci\u00f3n de parches\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. Pase la imagen a trav\u00e9s de la capa de incrustaci\u00f3n del parche.\npatch_embedding = patch_embedding_layer(x)\nprint(f\"Patching embedding shape: {patch_embedding.shape}\")\n\n# 6. Cree una incrustaci\u00f3n de token de clase\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # make sure it's learnable\nprint(f\"Class token embedding shape: {class_token.shape}\")\n\n# 7. Anteponer la incrustaci\u00f3n de token de clase a la incrustaci\u00f3n de parches\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n\n# 8. Crear posici\u00f3n incrustada\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# 9. Agregue incrustaci\u00f3n de posici\u00f3n a la incrustaci\u00f3n de parches con token de clase\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n</pre> set_seeds()  # 1. Establecer el tama\u00f1o del parche patch_size = 16  # 2. Imprima la forma del tensor de imagen original y obtenga las dimensiones de la imagen. print(f\"Image tensor shape: {image.shape}\") height, width = image.shape[1], image.shape[2]  # 3. Obtenga el tensor de imagen y agregue la dimensi\u00f3n del lote x = image.unsqueeze(0) print(f\"Input image with batch dimension shape: {x.shape}\")  # 4. Crear una capa de incrustaci\u00f3n de parches patch_embedding_layer = PatchEmbedding(in_channels=3,                                        patch_size=patch_size,                                        embedding_dim=768)  # 5. Pase la imagen a trav\u00e9s de la capa de incrustaci\u00f3n del parche. patch_embedding = patch_embedding_layer(x) print(f\"Patching embedding shape: {patch_embedding.shape}\")  # 6. Cree una incrustaci\u00f3n de token de clase batch_size = patch_embedding.shape[0] embedding_dimension = patch_embedding.shape[-1] class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),                            requires_grad=True) # make sure it's learnable print(f\"Class token embedding shape: {class_token.shape}\")  # 7. Anteponer la incrustaci\u00f3n de token de clase a la incrustaci\u00f3n de parches patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1) print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")  # 8. Crear posici\u00f3n incrustada number_of_patches = int((height * width) / patch_size**2) position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # 9. Agregue incrustaci\u00f3n de posici\u00f3n a la incrustaci\u00f3n de parches con token de clase patch_and_position_embedding = patch_embedding_class_token + position_embedding print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\") <p>\u00a1Guau!</p> <p>Desde una sola imagen hasta parchear y posicionar incrustaciones en una sola celda de c\u00f3digo.</p> <p></p> <p>Asignaci\u00f3n de la ecuaci\u00f3n 1 del art\u00edculo de ViT a nuestro c\u00f3digo PyTorch. Esta es la esencia de la replicaci\u00f3n en papel: tomar un trabajo de investigaci\u00f3n y convertirlo en c\u00f3digo utilizable.</p> <p>Ahora tenemos una manera de codificar nuestras im\u00e1genes y pasarlas al codificador Transformer en la Figura 1 del art\u00edculo de ViT.</p> <p>&lt;img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-architecture-animation-full-architecture.gif\" alt=\"Animaci\u00f3n de la arquitectura del transformador de visi\u00f3n , partiendo de una sola imagen y pas\u00e1ndola a trav\u00e9s de una capa de incrustaci\u00f3n de parches y luego pas\u00e1ndola a trav\u00e9s del codificador del transformador\". ancho=900/&gt;</p> <p>Animando todo el flujo de trabajo de ViT: desde la incorporaci\u00f3n de parches hasta el codificador transformador y el cabezal MLP.</p> <p>Desde una perspectiva de c\u00f3digo, la creaci\u00f3n de la incrustaci\u00f3n del parche es probablemente la secci\u00f3n m\u00e1s grande de la replicaci\u00f3n del documento ViT.</p> <p>Muchas de las otras partes del documento ViT, como las capas Multi-Head Attention y Norm, se pueden crear utilizando capas de PyTorch existentes.</p> <p>\u00a1Adelante!</p> In\u00a0[\u00a0]: Copied! <pre># 1. Crea una clase que hereda de nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n\n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=False) # do we need the weights or just the layer outputs?\n        return attn_output\n</pre> # 1. Crea una clase que hereda de nn.Module class MultiheadSelfAttentionBlock(nn.Module):     \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).     \"\"\"     # 2. Initialize the class with hyperparameters from Table 1     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks         super().__init__()          # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)          # 4. Create the Multi-Head Attention (MSA) layer         self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,                                                     num_heads=num_heads,                                                     dropout=attn_dropout,                                                     batch_first=True) # does our batch dimension come first?      # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         attn_output, _ = self.multihead_attn(query=x, # query embeddings                                              key=x, # key embeddings                                              value=x, # value embeddings                                              need_weights=False) # do we need the weights or just the layer outputs?         return attn_output <p>Nota: A diferencia de la Figura 1, nuestro <code>MultiheadSelfAttentionBlock</code> no incluye una conexi\u00f3n de omisi\u00f3n o residual (\"$+\\mathbf{z}_{\\ell-1}$\" en la ecuaci\u00f3n 2), Incluya esto cuando creemos el codificador de transformador completo m\u00e1s adelante en la secci\u00f3n 7.1.</p> <p>\u00a1MSABlock creado!</p> <p>Prob\u00e9moslo creando una instancia de nuestro <code>MultiheadSelfAttentionBlock</code> y pasando por la variable <code>patch_and_position_embedding</code> que creamos en la secci\u00f3n 4.8.</p> In\u00a0[\u00a0]: Copied! <pre># Crear una instancia de MSABlock\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n                                                             num_heads=12) # from Table 1\n\n# Pase el parche y coloque la imagen incrustada a trav\u00e9s de MSABlock\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\nprint(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")\n</pre> # Crear una instancia de MSABlock multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1                                                              num_heads=12) # from Table 1  # Pase el parche y coloque la imagen incrustada a trav\u00e9s de MSABlock patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding) print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\") print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\") <p>Observe c\u00f3mo la forma de entrada y salida de nuestros datos permanece igual cuando pasan por el bloque MSA.</p> <p>Esto no significa que los datos no cambien a medida que avanzan.</p> <p>Puede intentar imprimir el tensor de entrada y salida para ver c\u00f3mo cambia (aunque este cambio se producir\u00e1 en valores <code>1 * 197 * 768</code> y podr\u00eda ser dif\u00edcil de visualizar).</p> <p></p> <p>***Izquierda:** Arquitectura Vision Transformer de la Figura 1 con las capas Multi-Head Attention y LayerNorm resaltadas; estas capas constituyen la ecuaci\u00f3n 2 de la secci\u00f3n 3.1 del documento. Derecha: Replicando la ecuaci\u00f3n 2 (sin la conexi\u00f3n de salto al final) usando capas de PyTorch.*</p> <p>\u00a1Ahora hemos replicado oficialmente la ecuaci\u00f3n 2 (excepto por la conexi\u00f3n residual al final, pero llegaremos a esto en la secci\u00f3n 7.1)!</p> <p>\u00a1A la siguiente!</p> In\u00a0[\u00a0]: Copied! <pre># 1. Crea una clase que hereda de nn.Module\nclass MLPBlock(nn.Module):\n    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multilayer perceptron (MLP) layer(s)\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n        )\n\n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n</pre> # 1. Crea una clase que hereda de nn.Module class MLPBlock(nn.Module):     \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  dropout:float=0.1): # Dropout from Table 3 for ViT-Base         super().__init__()          # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)          # 4. Create the Multilayer perceptron (MLP) layer(s)         self.mlp = nn.Sequential(             nn.Linear(in_features=embedding_dim,                       out_features=mlp_size),             nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"             nn.Dropout(p=dropout),             nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above                       out_features=embedding_dim), # take back to embedding_dim             nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"         )      # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         x = self.mlp(x)         return x <p>Nota: A diferencia de la Figura 1, nuestro <code>MLPBlock()</code> no incluye una conexi\u00f3n de omisi\u00f3n o residual (\"$+\\mathbf{z}_{\\ell}^{\\prime}$\" en la ecuaci\u00f3n 3 ), incluiremos esto cuando creemos el codificador Transformer completo m\u00e1s adelante.</p> <p>\u00a1Clase MLPBlock creada!</p> <p>Prob\u00e9moslo creando una instancia de nuestro <code>MLPBlock</code> y pasando por la variable <code>patched_image_through_msa_block</code> que creamos en la secci\u00f3n 5.3.</p> In\u00a0[\u00a0]: Copied! <pre># Crear una instancia de MLPBlock\nmlp_block = MLPBlock(embedding_dim=768, # from Table 1\n                     mlp_size=3072, # from Table 1\n                     dropout=0.1) # from Table 3\n\n# Pasar la salida de MSABlock a trav\u00e9s de MLPBlock\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\nprint(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\nprint(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")\n</pre> # Crear una instancia de MLPBlock mlp_block = MLPBlock(embedding_dim=768, # from Table 1                      mlp_size=3072, # from Table 1                      dropout=0.1) # from Table 3  # Pasar la salida de MSABlock a trav\u00e9s de MLPBlock patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block) print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\") print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\") <p>Observe c\u00f3mo la forma de entrada y salida de nuestros datos nuevamente permanece igual cuando entran y salen del bloque MLP.</p> <p>Sin embargo, la forma cambia cuando los datos pasan a trav\u00e9s de las capas <code>nn.Linear()</code> dentro del bloque MLP (expandido al tama\u00f1o MLP de la Tabla 1 y luego comprimido nuevamente al tama\u00f1o Oculto $D$ de la Tabla 1).</p> <p></p> <p>Izquierda: Arquitectura de Vision Transformer de la Figura 1 con las capas MLP y Norm resaltadas; estas capas constituyen la ecuaci\u00f3n 3 de la secci\u00f3n 3.1 del documento. Derecha: Replicando la ecuaci\u00f3n 3 (sin la conexi\u00f3n de salto al final) usando capas de PyTorch.</p> <p>\u00a1Ho, ho!</p> <p>\u00a1La ecuaci\u00f3n 3 se replic\u00f3 (excepto por la conexi\u00f3n residual al final, pero llegaremos a esto en la secci\u00f3n 7.1)!</p> <p>Ahora que tenemos las ecuaciones 2 y 3 en el c\u00f3digo PyTorch, junt\u00e9moslas para crear el codificador Transformer.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Crea una clase que hereda de nn.Module\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"Creates a Transformer Encoder block.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                 attn_dropout:float=0): # Amount of dropout for attention layers\n        super().__init__()\n\n        # 3. Create MSA block (equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n\n        # 4. Create MLP block (equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n\n    # 5. Create a forward() method\n    def forward(self, x):\n\n        # 6. Create residual connection for MSA block (add the input to the output)\n        x =  self.msa_block(x) + x\n\n        # 7. Create residual connection for MLP block (add the input to the output)\n        x = self.mlp_block(x) + x\n\n        return x\n</pre> # 1. Crea una clase que hereda de nn.Module class TransformerEncoderBlock(nn.Module):     \"\"\"Creates a Transformer Encoder block.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                  attn_dropout:float=0): # Amount of dropout for attention layers         super().__init__()          # 3. Create MSA block (equation 2)         self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,                                                      num_heads=num_heads,                                                      attn_dropout=attn_dropout)          # 4. Create MLP block (equation 3)         self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,                                    mlp_size=mlp_size,                                    dropout=mlp_dropout)      # 5. Create a forward() method     def forward(self, x):          # 6. Create residual connection for MSA block (add the input to the output)         x =  self.msa_block(x) + x          # 7. Create residual connection for MLP block (add the input to the output)         x = self.mlp_block(x) + x          return x <p>\u00a1Hermoso!</p> <p>\u00a1Bloque codificador de transformador creado!</p> <p></p> <p>***Izquierda:** Figura 1 del art\u00edculo de ViT con el codificador transformador de la arquitectura ViT resaltado. Derecha: Transformer Encoder asignado a las ecuaciones 2 y 3 del documento ViT, el Transformer Encoder se compone de bloques alternos de la ecuaci\u00f3n 2 (Atenci\u00f3n de m\u00faltiples cabezales) y la ecuaci\u00f3n 3 (perceptr\u00f3n multicapa).*</p> <p>Vea c\u00f3mo estamos empezando a reconstruir la arquitectura general como si fueran legos, codificando un ladrillo (o ecuaci\u00f3n) a la vez.</p> <p></p> <p>Asignaci\u00f3n del codificador ViT Transformer al c\u00f3digo.</p> <p>Quiz\u00e1s hayas notado que la Tabla 1 del art\u00edculo de ViT tiene una columna Capas. Esto se refiere a la cantidad de bloques Transformer Encoder en la arquitectura ViT espec\u00edfica.</p> <p>En nuestro caso, para ViT-Base, apilaremos 12 de estos bloques Transformer Encoder para formar la columna vertebral de nuestra arquitectura (llegaremos a esto en la secci\u00f3n 8).</p> <p>Obtengamos un <code>torchinfo.summary()</code> de pasar una entrada de forma <code>(1, 197, 768) -&gt; (batch_size, num_patches, embedding_dimension)</code> a nuestro bloque Transformer Encoder.</p> In\u00a0[\u00a0]: Copied! <pre># Crea una instancia de TransformerEncoderBlock\ntransformer_encoder_block = TransformerEncoderBlock()\n\n# # Imprima un resumen de entrada y salida de nuestro codificador de transformador (descomentar para obtener un resultado completo)\n# resumen(modelo=transformer_encoder_block,\n# tama\u00f1o_entrada=(1, 197, 768), # (tama\u00f1o_lote, n\u00fam_parches, dimensi\u00f3n_incrustaci\u00f3n)\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"])\n</pre> # Crea una instancia de TransformerEncoderBlock transformer_encoder_block = TransformerEncoderBlock()  # # Imprima un resumen de entrada y salida de nuestro codificador de transformador (descomentar para obtener un resultado completo) # resumen(modelo=transformer_encoder_block, # tama\u00f1o_entrada=(1, 197, 768), # (tama\u00f1o_lote, n\u00fam_parches, dimensi\u00f3n_incrustaci\u00f3n) # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"]) <p>\u00a1Guau! \u00a1Mira todos esos par\u00e1metros!</p> <p>Puede ver nuestra entrada cambiando de forma a medida que se mueve a trav\u00e9s de las distintas capas en el bloque MSA y el bloque MLP del bloque Transformer Encoder antes de finalmente regresar a su forma original al final.</p> <p>Nota: El hecho de que nuestra entrada al bloque Transformer Encoder tenga la misma forma en la salida del bloque no significa que los valores no hayan sido manipulados, el objetivo general del bloque Transformer Encoder (y apilarlos juntos ) es aprender una representaci\u00f3n profunda de la entrada utilizando las distintas capas intermedias.</p> In\u00a0[\u00a0]: Copied! <pre># Crea lo mismo que arriba con torch.nn.TransformerEncoderLayer()\ntorch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n                                                             nhead=12, # Heads from Table 1 for ViT-Base\n                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                                                             activation=\"gelu\", # GELU non-linear activation\n                                                             batch_first=True, # Do our batches come first?\n                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n\ntorch_transformer_encoder_layer\n</pre> # Crea lo mismo que arriba con torch.nn.TransformerEncoderLayer() torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base                                                              nhead=12, # Heads from Table 1 for ViT-Base                                                              dim_feedforward=3072, # MLP size from Table 1 for ViT-Base                                                              dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                                                              activation=\"gelu\", # GELU non-linear activation                                                              batch_first=True, # Do our batches come first?                                                              norm_first=True) # Normalize first or after MSA/MLP layers?  torch_transformer_encoder_layer <p>Para inspeccionarlo m\u00e1s a fondo, obtengamos un resumen con <code>torchinfo.summary()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># # Obtener el resultado de la versi\u00f3n de PyTorch del Transformer Encoder (descomentar para obtener el resultado completo)\n# resumen(modelo=torch_transformer_encoder_layer,\n# tama\u00f1o_entrada=(1, 197, 768), # (tama\u00f1o_lote, n\u00fam_parches, dimensi\u00f3n_incrustaci\u00f3n)\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"])\n</pre> # # Obtener el resultado de la versi\u00f3n de PyTorch del Transformer Encoder (descomentar para obtener el resultado completo) # resumen(modelo=torch_transformer_encoder_layer, # tama\u00f1o_entrada=(1, 197, 768), # (tama\u00f1o_lote, n\u00fam_parches, dimensi\u00f3n_incrustaci\u00f3n) # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"]) <p>El resultado del resumen es ligeramente diferente al nuestro debido a c\u00f3mo <code>torch.nn.TransformerEncoderLayer()</code> construye su capa.</p> <p>Pero las capas que utiliza, la cantidad de par\u00e1metros y las formas de entrada y salida son las mismas.</p> <p>Quiz\u00e1s est\u00e9 pensando: \"si pudimos crear Transformer Encoder tan r\u00e1pidamente con capas de PyTorch, \u00bfpor qu\u00e9 nos molestamos en reproducir las ecuaciones 2 y 3?\"</p> <p>La respuesta es: practicar.</p> <p>Ahora que hemos replicado una serie de ecuaciones y capas de un papel, si necesitas cambiar las capas y probar algo diferente, puedes hacerlo.</p> <p>Pero existen ventajas al utilizar las capas predise\u00f1adas de PyTorch, como por ejemplo:</p> <ul> <li>Menos propenso a errores - Generalmente, si una capa ingresa a la biblioteca est\u00e1ndar de PyTorch, se ha probado y se ha intentado que funcione.</li> <li>Rendimiento potencialmente mejor: a partir de julio de 2022 y PyTorch 1.12, la versi\u00f3n implementada por PyTorch de <code>torch.nn.TransformerEncoderLayer()</code> puede ver [una aceleraci\u00f3n de m\u00e1s del doble en muchas cargas de trabajo comunes](https:// pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/).</li> </ul> <p>Finalmente, dado que la arquitectura ViT utiliza varias capas de transformador apiladas encima de cada una para la arquitectura completa (la Tabla 1 muestra 12 capas en el caso de ViT-Base), puede hacer esto con <code>torch.nn.TransformerEncoder(encoder_layer, num_layers )</code> donde:</p> <ul> <li><code>encoder_layer</code>: la capa de Transformer Encoder de destino creada con <code>torch.nn.TransformerEncoderLayer()</code>.</li> <li><code>num_layers</code>: el n\u00famero de capas de Transformer Encoder que se apilar\u00e1n juntas.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># 1. Cree una clase ViT que herede de nn.Module\nclass ViT(nn.Module):\n    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n                 in_channels:int=3, # Number of channels in input image\n                 patch_size:int=16, # Patch size\n                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0, # Dropout for attention projection\n                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n                 num_classes:int=1000): # Default for ImageNet but can customize this\n        super().__init__() # don't forget the super().__init__()!\n\n        # 3. Make the image size is divisble by the patch size\n        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n\n        # 4. Calculate number of patches (height * width/patch^2)\n        self.num_patches = (img_size * img_size) // patch_size**2\n\n        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n\n        # 6. Create learnable position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n\n        # 7. Create embedding dropout value\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n\n        # 8. Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n\n        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n        # Note: The \"*\" means \"all\"\n        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n\n        # 10. Create classifier head\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n\n    # 11. Create a forward() method\n    def forward(self, x):\n\n        # 12. Get batch size\n        batch_size = x.shape[0]\n\n        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n\n        # 14. Create patch embedding (equation 1)\n        x = self.patch_embedding(x)\n\n        # 15. Concat class embedding and patch embedding (equation 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # 16. Add position embedding to patch embedding (equation 1)\n        x = self.position_embedding + x\n\n        # 17. Run embedding dropout (Appendix B.1)\n        x = self.embedding_dropout(x)\n\n        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)\n        x = self.transformer_encoder(x)\n\n        # 19. Put 0 index logit through classifier (equation 4)\n        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n\n        return x\n</pre> # 1. Cree una clase ViT que herede de nn.Module class ViT(nn.Module):     \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  img_size:int=224, # Training resolution from Table 3 in ViT paper                  in_channels:int=3, # Number of channels in input image                  patch_size:int=16, # Patch size                  num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0, # Dropout for attention projection                  mlp_dropout:float=0.1, # Dropout for dense/MLP layers                  embedding_dropout:float=0.1, # Dropout for patch and position embeddings                  num_classes:int=1000): # Default for ImageNet but can customize this         super().__init__() # don't forget the super().__init__()!          # 3. Make the image size is divisble by the patch size         assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"          # 4. Calculate number of patches (height * width/patch^2)         self.num_patches = (img_size * img_size) // patch_size**2          # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)         self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),                                             requires_grad=True)          # 6. Create learnable position embedding         self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),                                                requires_grad=True)          # 7. Create embedding dropout value         self.embedding_dropout = nn.Dropout(p=embedding_dropout)          # 8. Create patch embedding layer         self.patch_embedding = PatchEmbedding(in_channels=in_channels,                                               patch_size=patch_size,                                               embedding_dim=embedding_dim)          # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())         # Note: The \"*\" means \"all\"         self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,                                                                             num_heads=num_heads,                                                                             mlp_size=mlp_size,                                                                             mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])          # 10. Create classifier head         self.classifier = nn.Sequential(             nn.LayerNorm(normalized_shape=embedding_dim),             nn.Linear(in_features=embedding_dim,                       out_features=num_classes)         )      # 11. Create a forward() method     def forward(self, x):          # 12. Get batch size         batch_size = x.shape[0]          # 13. Create class token embedding and expand it to match the batch size (equation 1)         class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)          # 14. Create patch embedding (equation 1)         x = self.patch_embedding(x)          # 15. Concat class embedding and patch embedding (equation 1)         x = torch.cat((class_token, x), dim=1)          # 16. Add position embedding to patch embedding (equation 1)         x = self.position_embedding + x          # 17. Run embedding dropout (Appendix B.1)         x = self.embedding_dropout(x)          # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)         x = self.transformer_encoder(x)          # 19. Put 0 index logit through classifier (equation 4)         x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index          return x <ol> <li>\ud83d\udd7a\ud83d\udc83\ud83e\udd73 \u00a1\u00a1\u00a1Guau!!! \u00a1Acabamos de construir un transformador de visi\u00f3n!</li> </ol> <p>\u00a1Qu\u00e9 esfuerzo!</p> <p>\u00a1Lento pero seguro creamos capas y bloques, entradas y salidas y los juntamos todos para construir nuestro propio ViT!</p> <p>Creemos una demostraci\u00f3n r\u00e1pida para mostrar lo que sucede con la incorporaci\u00f3n del token de clase que se expande en las dimensiones del lote.</p> In\u00a0[\u00a0]: Copied! <pre># Ejemplo de creaci\u00f3n de la clase incrustada y expandida en una dimensi\u00f3n por lotes\nbatch_size = 32\nclass_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\nclass_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n\n# Imprime el cambio de formas.\nprint(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\nprint(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")\n</pre> # Ejemplo de creaci\u00f3n de la clase incrustada y expandida en una dimensi\u00f3n por lotes batch_size = 32 class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"  # Imprime el cambio de formas. print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\") print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\") <p>Observe c\u00f3mo la primera dimensi\u00f3n se expande al tama\u00f1o del lote y las otras dimensiones permanecen iguales (porque se deducen de las dimensiones \"<code>-1</code>\" en <code>.expand(batch_size, -1, -1)</code>).</p> <p>Muy bien, es hora de probar la clase <code>ViT()</code>.</p> <p>Creemos un tensor aleatorio con la misma forma que una sola imagen, pasemos a una instancia de \"ViT\" y veamos qu\u00e9 sucede.</p> In\u00a0[\u00a0]: Copied! <pre>set_seeds()\n\n# Crea un tensor aleatorio con la misma forma que una sola imagen.\nrandom_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n\n# Crear una instancia de ViT con la cantidad de clases con las que estamos trabajando (pizza, bistec, sushi)\nvit = ViT(num_classes=len(class_names))\n\n# Pasar el tensor de imagen aleatorio a nuestra instancia de ViT\nvit(random_image_tensor)\n</pre> set_seeds()  # Crea un tensor aleatorio con la misma forma que una sola imagen. random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)  # Crear una instancia de ViT con la cantidad de clases con las que estamos trabajando (pizza, bistec, sushi) vit = ViT(num_classes=len(class_names))  # Pasar el tensor de imagen aleatorio a nuestra instancia de ViT vit(random_image_tensor) <p>\u00a1Pendiente!</p> <p>Parece que nuestro tensor de im\u00e1genes aleatorias lleg\u00f3 hasta nuestra arquitectura ViT y est\u00e1 generando tres valores logit (uno para cada clase).</p> <p>Y debido a que nuestra clase <code>ViT</code> tiene muchos par\u00e1metros, podr\u00edamos personalizar <code>img_size</code>, <code>patch_size</code> o <code>num_classes</code> si quisi\u00e9ramos.</p> In\u00a0[\u00a0]: Copied! <pre>from torchinfo import summary\n\n# # Imprima un resumen de nuestro modelo ViT personalizado usando torchinfo (descomente el resultado real)\n# resumen(modelo=vit,\n# tama\u00f1o_entrada=(32, 3, 224, 224), # (tama\u00f1o_lote, canales_color, alto, ancho)\n# # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Imprima un resumen de nuestro modelo ViT personalizado usando torchinfo (descomente el resultado real) # resumen(modelo=vit, # tama\u00f1o_entrada=(32, 3, 224, 224), # (tama\u00f1o_lote, canales_color, alto, ancho) # # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"] # ) <p>\u00a1Esas son algunas capas bonitas!</p> <p>Consulte tambi\u00e9n el n\u00famero total de par\u00e1metros, 85.800.963, \u00a1nuestro modelo m\u00e1s grande hasta el momento!</p> <p>El n\u00famero est\u00e1 muy cerca del ViT-Base previamente entrenado de PyTorch con tama\u00f1o de parche 16 en [<code>torch.vision.models.vit_b_16()</code>](https://pytorch.org/vision/main/models/generated/torchvision.models. vit_b_16.html#torchvision.models.vit_b_16) con 86,567,656 par\u00e1metros totales (aunque este n\u00famero de par\u00e1metros es para las 1000 clases en ImageNet).</p> <p>Ejercicio: Intente cambiar el par\u00e1metro <code>num_classes</code> de nuestro modelo <code>ViT()</code> a 1000 y luego cree otro resumen con <code>torchinfo.summary()</code> y vea si el n\u00famero de par\u00e1metros se alinea entre nuestro c\u00f3digo y <code>torchvision.models.vit_b_16()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Configure el optimizador para optimizar los par\u00e1metros de nuestro modelo ViT utilizando hiperpar\u00e1metros del art\u00edculo de ViT.\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)\n                             weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n\n# Configure la funci\u00f3n de p\u00e9rdida para la clasificaci\u00f3n de clases m\u00faltiples\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Establecer las semillas\nset_seeds()\n\n# Entrene el modelo y guarde los resultados del entrenamiento en un diccionario.\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)\n</pre> from going_modular.going_modular import engine  # Configure el optimizador para optimizar los par\u00e1metros de nuestro modelo ViT utilizando hiperpar\u00e1metros del art\u00edculo de ViT. optimizer = torch.optim.Adam(params=vit.parameters(),                              lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k                              betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)                              weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k  # Configure la funci\u00f3n de p\u00e9rdida para la clasificaci\u00f3n de clases m\u00faltiples loss_fn = torch.nn.CrossEntropyLoss()  # Establecer las semillas set_seeds()  # Entrene el modelo y guarde los resultados del entrenamiento en un diccionario. results = engine.train(model=vit,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=10,                        device=device) <p>\u00a1Maravilloso!</p> <p>\u00a1Nuestro modelo ViT ha cobrado vida!</p> <p>Aunque los resultados de nuestro conjunto de datos sobre pizza, bistec y sushi no parecen muy buenos.</p> <p>\u00bfQuiz\u00e1s sea porque nos faltan algunas cosas?</p> In\u00a0[\u00a0]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Trazar las curvas de p\u00e9rdida de nuestro modelo ViT.\nplot_loss_curves(results)\n</pre> from helper_functions import plot_loss_curves  # Trazar las curvas de p\u00e9rdida de nuestro modelo ViT. plot_loss_curves(results) <p>Hmm, parece que las curvas de p\u00e9rdida de nuestro modelo est\u00e1n por todos lados.</p> <p>Al menos la p\u00e9rdida parece ir en la direcci\u00f3n correcta, pero las curvas de precisi\u00f3n no son muy prometedoras.</p> <p>Es probable que estos resultados se deban a la diferencia en los recursos de datos y el r\u00e9gimen de entrenamiento de nuestro modelo ViT frente al documento ViT.</p> <p>Parece que nuestro modelo est\u00e1 muy insuficientemente adaptado (no logra los resultados que nos gustar\u00eda).</p> <p>\u00bfQu\u00e9 tal si vemos si podemos solucionarlo incorporando un modelo ViT previamente entrenado?</p> In\u00a0[\u00a0]: Copied! <pre># Lo siguiente requiere torch v0.12+ y torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__)\nprint(torchvision.__version__)\n</pre> # Lo siguiente requiere torch v0.12+ y torchvision v0.13+ import torch import torchvision print(torch.__version__) print(torchvision.__version__) <p>Luego configuraremos el c\u00f3digo agonista del dispositivo.</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device <p>Finalmente, obtendremos ViT-Base previamente entrenado con tama\u00f1o de parche 16 de <code>torchvision.models</code> y lo prepararemos para nuestro caso de uso de FoodVision Mini convirti\u00e9ndolo en un modelo de aprendizaje de transferencia de extractor de funciones.</p> <p>Espec\u00edficamente, haremos:</p> <ol> <li>Obtenga los pesos previamente entrenados para ViT-Base entrenados en ImageNet-1k desde [<code>torchvision.models.ViT_B_16_Weights.DEFAULT</code>](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16. html#torchvision.models.ViT_B_16_Weights) (<code>DEFAULT</code> significa mejor disponible).</li> <li>Configure una instancia de modelo ViT a trav\u00e9s de <code>torchvision.models.vit_b_16</code>, p\u00e1sele los pesos previamente entrenados paso 1 y env\u00edelo al dispositivo de destino.</li> <li>Congele todos los par\u00e1metros en el modelo ViT base creado en el paso 2 estableciendo su atributo <code>requires_grad</code> en <code>False</code>.</li> <li>Actualice el clasificador principal del modelo ViT creado en el paso 2 para adaptarlo a nuestro propio problema cambiando el n\u00famero de <code>out_features</code> a nuestro n\u00famero de clases (pizza, bistec, sushi).</li> </ol> <p>Cubrimos pasos como este en 06. Aprendizaje por transferencia de PyTorch [secci\u00f3n 3.2: Configuraci\u00f3n de un modelo previamente entrenado] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model) y [secci\u00f3n 3.4: Congelar el modelo base y cambiar la capa de salida para adaptarla a nuestras necesidades](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer- a-nuestras-necesidades).</p> In\u00a0[\u00a0]: Copied! <pre># 1. Obtenga pesas previamente entrenadas para ViT-Base\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available\n\n# 2. Configurar una instancia de modelo ViT con pesos previamente entrenados\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# 3. Congelar los par\u00e1metros base.\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n\n# 4. Cambie el cabezal clasificador (configure las semillas para garantizar la misma inicializaci\u00f3n con el cabezal lineal)\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n# pretrained_vit # descomentar para la salida del modelo\n</pre> # 1. Obtenga pesas previamente entrenadas para ViT-Base pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available  # 2. Configurar una instancia de modelo ViT con pesos previamente entrenados pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)  # 3. Congelar los par\u00e1metros base. for parameter in pretrained_vit.parameters():     parameter.requires_grad = False  # 4. Cambie el cabezal clasificador (configure las semillas para garantizar la misma inicializaci\u00f3n con el cabezal lineal) set_seeds() pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device) # pretrained_vit # descomentar para la salida del modelo <p>\u00a1Se ha creado un modelo de extractor de funciones ViT previamente entrenado!</p> <p>Comprob\u00e9moslo ahora imprimiendo un <code>torchinfo.summary()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># # Imprimir un resumen usando torchinfo (descomentar para el resultado real)\n# resumen (modelo = vit preentrenado,\n# tama\u00f1o_entrada=(32, 3, 224, 224), # (tama\u00f1o_lote, canales_color, alto, ancho)\n# # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"]\n# )\n</pre> # # Imprimir un resumen usando torchinfo (descomentar para el resultado real) # resumen (modelo = vit preentrenado, # tama\u00f1o_entrada=(32, 3, 224, 224), # (tama\u00f1o_lote, canales_color, alto, ancho) # # col_names=[\"input_size\"], # descomentar para resultados m\u00e1s peque\u00f1os # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"] # ) <p></p> <p>\u00a1Guau!</p> <p>Observe c\u00f3mo solo la capa de salida es entrenable, mientras que el resto de las capas no se pueden entrenar (congeladas).</p> <p>Y el n\u00famero total de par\u00e1metros, 85.800.963, es el mismo que el de nuestro modelo ViT personalizado anterior.</p> <p>Pero el n\u00famero de par\u00e1metros entrenables para <code>pretrained_vit</code> es mucho, mucho menor que nuestro <code>vit</code> personalizado: solo 2,307 en comparaci\u00f3n con 85,800,963 (en nuestro <code>vit</code> personalizado, dado que estamos entrenando desde cero, todos los par\u00e1metros se pueden entrenar).</p> <p>Esto significa que el modelo previamente entrenado deber\u00eda entrenarse mucho m\u00e1s r\u00e1pido; potencialmente podr\u00edamos incluso usar un tama\u00f1o de lote mayor, ya que menos actualizaciones de par\u00e1metros consumir\u00e1n memoria.</p> In\u00a0[\u00a0]: Copied! <pre>from helper_functions import download_data\n\n# Descargue im\u00e1genes de pizza, bistec y sushi desde GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> from helper_functions import download_data  # Descargue im\u00e1genes de pizza, bistec y sushi desde GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <p>Y ahora configuraremos las rutas del directorio de entrenamiento y prueba.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar rutas de directorio de tren y prueba\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\ntrain_dir, test_dir\n</pre> # Configurar rutas de directorio de tren y prueba train_dir = image_path / \"train\" test_dir = image_path / \"test\" train_dir, test_dir <p>Finalmente, transformaremos nuestras im\u00e1genes en tensores y convertiremos los tensores en DataLoaders.</p> <p>Dado que estamos usando un modelo previamente entrenado en forma de <code>torchvision.models</code>, podemos llamar al m\u00e9todo <code>transforms()</code> para obtener las transformaciones requeridas.</p> <p>Recuerde, si va a utilizar un modelo previamente entrenado, generalmente es importante asegurarse de que sus propios datos personalizados se transformen/formateen de la misma manera que los datos en los que se entren\u00f3 el modelo original.</p> <p>Cubrimos este m\u00e9todo de creaci\u00f3n de transformaciones \"autom\u00e1ticas\" en [06. Secci\u00f3n 2.2 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation).</p> In\u00a0[\u00a0]: Copied! <pre># Obtenga transformaciones autom\u00e1ticas a partir de pesas ViT previamente entrenadas\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n</pre> # Obtenga transformaciones autom\u00e1ticas a partir de pesas ViT previamente entrenadas pretrained_vit_transforms = pretrained_vit_weights.transforms() print(pretrained_vit_transforms) <p>Y ahora que tenemos las transformaciones listas, podemos convertir nuestras im\u00e1genes en DataLoaders usando el m\u00e9todo <code>data_setup.create_dataloaders()</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 2.</p> <p>Dado que estamos usando un modelo de extracci\u00f3n de caracter\u00edsticas (par\u00e1metros menos entrenables), podr\u00edamos aumentar el tama\u00f1o del lote a un valor m\u00e1s alto (si lo configuramos en 1024, estar\u00edamos imitando una mejora encontrada en Mejores l\u00edneas base de ViT simples para ImageNet -1k, un art\u00edculo que mejora el art\u00edculo original de ViT y sugiere lecturas adicionales). Pero como solo tenemos ~200 muestras de entrenamiento en total, nos quedaremos con 32.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar cargadores de datos\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n</pre> # Configurar cargadores de datos train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                      test_dir=test_dir,                                                                                                      transform=pretrained_vit_transforms,                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...) In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Crear optimizador y funci\u00f3n de p\u00e9rdida.\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Entrene el cabezal clasificador del modelo de extracci\u00f3n de caracter\u00edsticas ViT previamente entrenado\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n</pre> from going_modular.going_modular import engine  # Crear optimizador y funci\u00f3n de p\u00e9rdida. optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),                              lr=1e-3) loss_fn = torch.nn.CrossEntropyLoss()  # Entrene el cabezal clasificador del modelo de extracci\u00f3n de caracter\u00edsticas ViT previamente entrenado set_seeds() pretrained_vit_results = engine.train(model=pretrained_vit,                                       train_dataloader=train_dataloader_pretrained,                                       test_dataloader=test_dataloader_pretrained,                                       optimizer=optimizer,                                       loss_fn=loss_fn,                                       epochs=10,                                       device=device) <p>\u00a1Santa vaca!</p> <p>Parece que nuestro extractor de funciones ViT previamente entrenado funcion\u00f3 mucho mejor que nuestro modelo ViT personalizado entrenado desde cero (en la misma cantidad de tiempo).</p> <p>Seamos visuales.</p> In\u00a0[\u00a0]: Copied! <pre># Trazar las curvas de p\u00e9rdida\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)\n</pre> # Trazar las curvas de p\u00e9rdida from helper_functions import plot_loss_curves  plot_loss_curves(pretrained_vit_results) <p>\u00a1Guau!</p> <p>Estas son algunas curvas de p\u00e9rdida cercanas a las de un libro de texto (realmente buenas) (consulte [04. PyTorch Custom Datasets secci\u00f3n 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss -curve-look-like) para saber c\u00f3mo deber\u00eda ser una curva de p\u00e9rdidas ideal).</p> <p>\u00a1Ese es el poder del aprendizaje por transferencia!</p> <p>Logramos obtener resultados sobresalientes con la misma arquitectura del modelo, excepto que nuestra implementaci\u00f3n personalizada fue entrenada desde cero (peor rendimiento) y este modelo de extracci\u00f3n de caracter\u00edsticas tiene el poder de pesos previamente entrenados de ImageNet detr\u00e1s.</p> <p>\u00bfQu\u00e9 opinas?</p> <p>\u00bfNuestro modelo de extracci\u00f3n de caracter\u00edsticas mejorar\u00eda m\u00e1s si continuaras entren\u00e1ndolo?</p> In\u00a0[\u00a0]: Copied! <pre># guardar el modelo\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n</pre> # guardar el modelo from going_modular.going_modular import utils  utils.save_model(model=pretrained_vit,                  target_dir=\"models\",                  model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\") <p>Y como estamos pensando en implementar este modelo, ser\u00eda bueno saber su tama\u00f1o (en megabytes o MB).</p> <p>Como queremos que nuestra aplicaci\u00f3n Food Vision Mini se ejecute r\u00e1pidamente, generalmente un modelo m\u00e1s peque\u00f1o con buen rendimiento ser\u00e1 mejor que un modelo m\u00e1s grande con gran rendimiento.</p> <p>Podemos verificar el tama\u00f1o de nuestro modelo en bytes usando el atributo <code>st_size</code> de Python [<code>pathlib.Path().stat()</code>](https://docs.python.org/3/library/pathlib.html# pathlib.Path.stat) mientras le pasa el nombre de ruta de archivo de nuestro modelo.</p> <p>Luego podemos escalar el tama\u00f1o en bytes a megabytes.</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <p>Hmm, parece que nuestro modelo de extractor de funciones ViT para Food Vision Mini result\u00f3 tener un tama\u00f1o de aproximadamente 327 MB.</p> <p>\u00bfC\u00f3mo se compara esto con el modelo de extracci\u00f3n de caracter\u00edsticas EffNetB2 en [07. Secci\u00f3n 9 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it)?</p> Modelo Tama\u00f1o del modelo (MB) P\u00e9rdida de prueba Precisi\u00f3n de la prueba Extractor de funciones EffNetB2 ^ 29 ~0,3906 ~0,9384 Extractor de funciones ViT 327 ~0,1084 ~0,9384 <p>Nota: ^ el modelo EffNetB2 de referencia se entren\u00f3 con un 20 % de datos de pizza, bistec y sushi (el doble de im\u00e1genes) en lugar del extractor de funciones ViT, que se entren\u00f3 con un 10 % de pizza, bistec y sushi. datos. Un ejercicio ser\u00eda entrenar el modelo de extracci\u00f3n de funciones de ViT con la misma cantidad de datos y ver cu\u00e1nto mejoran los resultados.</p> <p>El modelo EffNetB2 es aproximadamente 11 veces m\u00e1s peque\u00f1o que el modelo ViT con resultados similares en cuanto a p\u00e9rdida de prueba y precisi\u00f3n.</p> <p>Sin embargo, los resultados del modelo ViT pueden mejorar m\u00e1s cuando se entrena con los mismos datos (20% de datos de pizza, bistec y sushi).</p> <p>Pero en t\u00e9rminos de implementaci\u00f3n, si estuvi\u00e9ramos comparando estos dos modelos, algo que tendr\u00edamos que considerar es si la precisi\u00f3n adicional del modelo ViT vale el aumento de ~11 veces en el tama\u00f1o del modelo.</p> <p>Quiz\u00e1s un modelo tan grande tardar\u00eda m\u00e1s en cargarse/ejecutarse y no proporcionar\u00eda una experiencia tan buena como EffNetB2, que funciona de manera similar pero en un tama\u00f1o mucho m\u00e1s reducido.</p> In\u00a0[\u00a0]: Copied! <pre>import requests\n\n# Funci\u00f3n de importaci\u00f3n para hacer predicciones sobre im\u00e1genes y trazarlas.\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Configurar ruta de imagen personalizada\ncustom_image_path = image_path / \"04-pizza-dad.jpeg\"\n\n# Descarga la imagen si a\u00fan no existe\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predecir en imagen personalizada\npred_and_plot_image(model=pretrained_vit,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> import requests  # Funci\u00f3n de importaci\u00f3n para hacer predicciones sobre im\u00e1genes y trazarlas. from going_modular.going_modular.predictions import pred_and_plot_image  # Configurar ruta de imagen personalizada custom_image_path = image_path / \"04-pizza-dad.jpeg\"  # Descarga la imagen si a\u00fan no existe if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predecir en imagen personalizada pred_and_plot_image(model=pretrained_vit,                     image_path=custom_image_path,                     class_names=class_names) <p>\u00a1Dos pulgares arriba!</p> <p>\u00a1Felicidades!</p> <p>\u00a1Hemos recorrido todo el camino desde un trabajo de investigaci\u00f3n hasta un c\u00f3digo de modelo utilizable en nuestras propias im\u00e1genes personalizadas!</p>"},{"location":"12-08_pytorch_paper_replicating/#08-replicacion-de-papel-pytorch","title":"08. Replicaci\u00f3n de papel PyTorch\u00b6","text":"<p>Bienvenido al Proyecto Milestone 2: \u00a1Replicaci\u00f3n de papel PyTorch!</p> <p>En este proyecto, replicaremos un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico y crearemos un Vision Transformer (ViT) desde cero usando PyTorch.</p> <p>Luego veremos c\u00f3mo funciona ViT, una arquitectura de visi\u00f3n por computadora de \u00faltima generaci\u00f3n, en nuestro problema FoodVision Mini.</p> <p>Para Milestone Project 2 nos centraremos en recrear la arquitectura de visi\u00f3n por computadora Vision Transformer (ViT) y aplicarla a nuestro problema FoodVision Mini para clasificar diferentes im\u00e1genes de pizza, bistec y sushi.</p>"},{"location":"12-08_pytorch_paper_replicating/#que-se-replica-en-papel","title":"\u00bfQu\u00e9 se replica en papel?\u00b6","text":"<p>No es ning\u00fan secreto que el aprendizaje autom\u00e1tico avanza r\u00e1pidamente.</p> <p>Muchos de estos avances se publican en art\u00edculos de investigaci\u00f3n sobre aprendizaje autom\u00e1tico.</p> <p>Y el objetivo de la replicaci\u00f3n en papel es replicar estos avances con c\u00f3digo para que puedas utilizar las t\u00e9cnicas para tu propio problema.</p> <p>Por ejemplo, digamos que se lanza una nueva arquitectura de modelo que funciona mejor que cualquier otra arquitectura anterior en varios puntos de referencia, \u00bfno ser\u00eda bueno probar esa arquitectura en sus propios problemas?</p> <p>La replicaci\u00f3n de documentos de aprendizaje autom\u00e1tico implica convertir un documento de aprendizaje autom\u00e1tico compuesto de im\u00e1genes/diagramas, matem\u00e1ticas y texto en c\u00f3digo utilizable y, en nuestro caso, en c\u00f3digo PyTorch utilizable. Diagrama, ecuaciones matem\u00e1ticas y texto del art\u00edculo ViT.</p>"},{"location":"12-08_pytorch_paper_replicating/#que-es-un-trabajo-de-investigacion-sobre-aprendizaje-automatico","title":"\u00bfQu\u00e9 es un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico?\u00b6","text":"<p>Un art\u00edculo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico es un art\u00edculo cient\u00edfico que detalla los hallazgos de un grupo de investigaci\u00f3n en un \u00e1rea espec\u00edfica.</p> <p>El contenido de un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico puede variar de un art\u00edculo a otro, pero generalmente sigue la estructura:</p> Secci\u00f3n Contenido Resumen Una descripci\u00f3n general/resumen de los principales hallazgos/contribuciones del art\u00edculo. Introducci\u00f3n Cu\u00e1l es el problema principal del art\u00edculo y detalles de los m\u00e9todos anteriores utilizados para intentar resolverlo. M\u00e9todo \u00bfC\u00f3mo llevaron a cabo los investigadores su investigaci\u00f3n? Por ejemplo, \u00bfqu\u00e9 modelos, fuentes de datos y configuraciones de capacitaci\u00f3n se utilizaron? Resultados \u00bfCu\u00e1les son los resultados del art\u00edculo? Si se utiliz\u00f3 un nuevo tipo de modelo o configuraci\u00f3n de entrenamiento, \u00bfc\u00f3mo se compararon los resultados de los hallazgos con trabajos anteriores? (Aqu\u00ed es donde el seguimiento de experimentos resulta \u00fatil) Conclusi\u00f3n \u00bfCu\u00e1les son las limitaciones de los m\u00e9todos sugeridos? \u00bfCu\u00e1les son algunos de los pr\u00f3ximos pasos para la comunidad de investigaci\u00f3n? Referencias \u00bfQu\u00e9 recursos/otros art\u00edculos examinaron los investigadores para construir su propio cuerpo de trabajo? Ap\u00e9ndice \u00bfHay alg\u00fan recurso/hallazgo adicional para analizar que no se haya incluido en ninguna de las secciones anteriores?"},{"location":"12-08_pytorch_paper_replicating/#por-que-replicar-un-trabajo-de-investigacion-sobre-aprendizaje-automatico","title":"\u00bfPor qu\u00e9 replicar un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico?\u00b6","text":"<p>Un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico suele ser una presentaci\u00f3n de meses de trabajo y experimentos realizados por algunos de los mejores equipos de aprendizaje autom\u00e1tico del mundo condensados \u200b\u200ben unas pocas p\u00e1ginas de texto.</p> <p>Y si estos experimentos conducen a mejores resultados en un \u00e1rea relacionada con el problema en el que est\u00e1s trabajando, ser\u00eda bueno comprobarlos.</p> <p>Adem\u00e1s, replicar el trabajo de otros es una manera fant\u00e1stica de practicar tus habilidades.</p> <p></p> <p>George Hotz es fundador de comma.ai, una empresa de veh\u00edculos aut\u00f3nomos y transmite en vivo codificaci\u00f3n de aprendizaje autom\u00e1tico en Twitch y esos videos se publican completos en YouTube. Saqu\u00e9 esta cita de una de sus transmisiones en vivo. La \"\u066d\" es para se\u00f1alar que la ingenier\u00eda de aprendizaje autom\u00e1tico a menudo implica pasos adicionales de preprocesamiento de datos y hacer que sus modelos est\u00e9n disponibles para que otros los utilicen (implementaci\u00f3n).</p> <p>Cuando empiece a intentar replicar trabajos de investigaci\u00f3n, probablemente se sentir\u00e1 abrumado.</p> <p>Eso es normal.</p> <p>Los equipos de investigaci\u00f3n pasan semanas, meses y, a veces, a\u00f1os creando estos trabajos, por lo que tiene sentido si te lleva alg\u00fan tiempo incluso leerlos y mucho menos reproducirlos.</p> <p>Replicar la investigaci\u00f3n es un problema muy dif\u00edcil, bibliotecas y herramientas de aprendizaje autom\u00e1tico fenomenales como HuggingFace, [Modelos de im\u00e1genes PyTorch](https://github.com/rwightman/pytorch-image -models) (biblioteca <code>timm</code>) y fast.ai nacieron para hacer que la investigaci\u00f3n sobre aprendizaje autom\u00e1tico sea m\u00e1s accesible.</p>"},{"location":"12-08_pytorch_paper_replicating/#donde-puede-encontrar-ejemplos-de-codigo-para-articulos-de-investigacion-sobre-aprendizaje-automatico","title":"\u00bfD\u00f3nde puede encontrar ejemplos de c\u00f3digo para art\u00edculos de investigaci\u00f3n sobre aprendizaje autom\u00e1tico?\u00b6","text":"<p>Una de las primeras cosas que notar\u00e1 cuando se trata de investigaci\u00f3n sobre aprendizaje autom\u00e1tico es que hay mucha.</p> <p>As\u00ed que cuidado, tratar de estar al tanto es como intentar dejar atr\u00e1s una rueda de h\u00e1mster.</p> <p>Siga sus intereses, elija algunas cosas que le llamen la atenci\u00f3n.</p> <p>Dicho esto, hay varios lugares para encontrar y leer art\u00edculos (y c\u00f3digos) de investigaci\u00f3n sobre aprendizaje autom\u00e1tico:</p> Recurso \u00bfQu\u00e9 es? arXiv Se pronuncia \"archivo\", arXiv es un recurso abierto y gratuito para leer art\u00edculos t\u00e9cnicos sobre todo, desde f\u00edsica hasta ciencias de la computaci\u00f3n (incluido el aprendizaje autom\u00e1tico). AK Twitter La cuenta de Twitter de AK publica aspectos destacados de las investigaciones sobre aprendizaje autom\u00e1tico, a menudo con demostraciones en vivo casi todos los d\u00edas. No entiendo 9/10 publicaciones, pero me resulta divertido explorarlas de vez en cuando. Documentos con c\u00f3digo Una colecci\u00f3n seleccionada de art\u00edculos de aprendizaje autom\u00e1tico destacados, activos y de tendencia, muchos de los cuales incluyen recursos de c\u00f3digo adjuntos. Tambi\u00e9n incluye una colecci\u00f3n de conjuntos de datos comunes de aprendizaje autom\u00e1tico, puntos de referencia y modelos actuales de \u00faltima generaci\u00f3n. repositorio GitHub <code>vit-pytorch</code> de lucidrains Menos un lugar para encontrar art\u00edculos de investigaci\u00f3n y m\u00e1s un ejemplo de c\u00f3mo se ve la replicaci\u00f3n de art\u00edculos con c\u00f3digo a mayor escala y con un enfoque espec\u00edfico. El repositorio <code>vit-pytorch</code> es una colecci\u00f3n de arquitecturas modelo Vision Transformer de varios art\u00edculos de investigaci\u00f3n replicadas con c\u00f3digo PyTorch (gran parte de la inspiraci\u00f3n para este cuaderno se obtuvo de este repositorio). <p>Nota: Esta lista est\u00e1 lejos de ser exhaustiva. S\u00f3lo enumero algunos lugares, los que uso con m\u00e1s frecuencia personalmente. As\u00ed que cuidado con el sesgo. Sin embargo, he notado que incluso esta breve lista a menudo satisface parcialmente mis necesidades de saber lo que sucede en el campo. Un poco m\u00e1s y podr\u00eda volverme loco.</p>"},{"location":"12-08_pytorch_paper_replicating/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>En lugar de hablar de replicar un art\u00edculo, vamos a ponernos manos a la obra y realmente replicar un art\u00edculo.</p> <p>El proceso para replicar todos los art\u00edculos ser\u00e1 ligeramente diferente, pero al ver c\u00f3mo es hacer uno, tendremos el impulso para hacer m\u00e1s.</p> <p>M\u00e1s espec\u00edficamente, vamos a replicar el art\u00edculo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico Una imagen vale 16x16 palabras: transformadores para el reconocimiento de im\u00e1genes a escala (art\u00edculo ViT) con PyTorch.</p> <p>La arquitectura de la red neuronal Transformer se introdujo originalmente en el art\u00edculo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico [La atenci\u00f3n es todo lo que necesita] (https://arxiv.org/abs/1706.03762).</p> <p>Y la arquitectura Transformer original fue dise\u00f1ada para funcionar en secuencias de texto unidimensionales (1D).</p> <p>Generalmente se considera que una arquitectura transformadora es cualquier red neuronal que utiliza el mecanismo de atenci\u00f3n) como su capa de aprendizaje principal. Similar a c\u00f3mo una red neuronal convolucional (CNN) utiliza convoluciones como su capa de aprendizaje principal.</p> <p>Como sugiere el nombre, la arquitectura Vision Transformer (ViT) fue dise\u00f1ada para adaptar la arquitectura Transformer original a los problemas de visi\u00f3n (la clasificaci\u00f3n es la primera y muchas otras le han seguido).</p> <p>El Vision Transformer original ha pasado por varias iteraciones en los \u00faltimos a\u00f1os; sin embargo, nos centraremos en replicar el original, tambi\u00e9n conocido como el \"Vision Transformer vainilla\". Porque si puedes recrear el original, puedes adaptarte a los dem\u00e1s.</p> <p>Nos centraremos en construir la arquitectura ViT seg\u00fan el art\u00edculo original de ViT y aplicarla a FoodVision Mini.</p> Tema Contenido 0. Obteniendo configuraci\u00f3n Hemos escrito bastante c\u00f3digo \u00fatil en las \u00faltimas secciones, descargu\u00e9moslo y asegur\u00e9monos de poder usarlo nuevamente. 1. Obtener datos Obtengamos el conjunto de datos de clasificaci\u00f3n de im\u00e1genes de pizza, bistec y sushi que hemos estado usando y construyamos un Vision Transformer para intentar mejorar los resultados del modelo FoodVision Mini. 2. Crear conjuntos de datos y cargadores de datos Usaremos el script <code>data_setup.py</code> que escribimos en el cap\u00edtulo 05. PyTorch se vuelve modular para configurar nuestros DataLoaders. 3. Replicaci\u00f3n del art\u00edculo de ViT: descripci\u00f3n general Replicar un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico puede ser un desaf\u00edo justo, as\u00ed que antes de comenzar, dividamos el trabajo de ViT en partes m\u00e1s peque\u00f1as, para que podamos replicar el papel pedazo por pedazo. 4. Ecuaci\u00f3n 1: La incrustaci\u00f3n de parches La arquitectura ViT se compone de cuatro ecuaciones principales, siendo la primera la incorporaci\u00f3n de parches y posiciones. O convertir una imagen en una secuencia de parches que se pueden aprender. 5. Ecuaci\u00f3n 2: Atenci\u00f3n de m\u00faltiples cabezales (MSA) El mecanismo de autoatenci\u00f3n/autoatenci\u00f3n de m\u00faltiples cabezales (MSA) est\u00e1 en el coraz\u00f3n de cada arquitectura Transformer, incluida la arquitectura ViT. Creemos un bloque MSA utilizando las capas integradas de PyTorch. 6. Ecuaci\u00f3n 3: Perceptr\u00f3n multicapa (MLP) La arquitectura ViT utiliza un perceptr\u00f3n multicapa como parte de su codificador Transformer y para su capa de salida. Comencemos creando un MLP para Transformer Encoder. 7. Creaci\u00f3n del codificador transformador Un codificador de transformador generalmente se compone de capas alternas de MSA (ecuaci\u00f3n 2) y MLP (ecuaci\u00f3n 3) unidas entre s\u00ed mediante conexiones residuales. Creemos uno apilando las capas que creamos en las secciones 5 y 6 una encima de la otra. 8. Junt\u00e1ndolo todo para crear ViT Tenemos todas las piezas del rompecabezas para crear la arquitectura ViT. Junt\u00e9moslas en una sola clase que podamos llamar modelo. 9. Configurando el c\u00f3digo de entrenamiento para nuestro modelo ViT El entrenamiento de nuestra implementaci\u00f3n ViT personalizada es similar a todos los dem\u00e1s modelos que hemos entrenado anteriormente. Y gracias a nuestra funci\u00f3n <code>train()</code> en <code>engine.py</code> podemos empezar a entrenar con unas pocas l\u00edneas de c\u00f3digo. 10. Usando un ViT previamente entrenado de <code>torchvision.models</code> Entrenar un modelo grande como ViT suele requerir una buena cantidad de datos. Dado que solo estamos trabajando con una peque\u00f1a cantidad de im\u00e1genes de pizza, bistec y sushi, veamos si podemos aprovechar el poder del aprendizaje por transferencia para mejorar nuestro rendimiento. 11. Haga predicciones en una imagen personalizada La magia del aprendizaje autom\u00e1tico es verlo funcionar con sus propios datos, as\u00ed que tomemos nuestro modelo de mejor rendimiento y pongamos a prueba FoodVision Mini en la infame imagen pizza-dad (una foto de mi padre comiendo pizza). <p>Nota: A pesar de que nos centraremos en replicar el art\u00edculo de ViT, evite atascarse demasiado en un art\u00edculo en particular, ya que a menudo aparecer\u00e1n m\u00e9todos nuevos y mejores r\u00e1pidamente, por lo que la habilidad debe ser permanecer curioso mientras desarrolla las habilidades fundamentales para convertir las matem\u00e1ticas y las palabras de una p\u00e1gina en c\u00f3digo funcional.</p>"},{"location":"12-08_pytorch_paper_replicating/#terminologia","title":"Terminolog\u00eda\u00b6","text":"<p>Habr\u00e1 unas cuantas siglas a lo largo de este cuaderno.</p> <p>A la luz de esto, aqu\u00ed hay algunas definiciones:</p> <ul> <li>ViT: significa Vision Transformer (la principal arquitectura de red neuronal en la que nos centraremos en replicar).</li> <li>Art\u00edculo de ViT: abreviatura del art\u00edculo de investigaci\u00f3n original sobre aprendizaje autom\u00e1tico que present\u00f3 la arquitectura ViT, [Una imagen vale 16x16 palabras: transformadores para el reconocimiento de im\u00e1genes a escala](https://arxiv.org/abs /2010.11929), cada vez que se menciona art\u00edculo ViT, puede estar seguro de que hace referencia a este art\u00edculo.</li> </ul>"},{"location":"12-08_pytorch_paper_replicating/#donde-puedes-obtener-ayuda","title":"\u00bfD\u00f3nde puedes obtener ayuda?\u00b6","text":"<p>Todos los materiales de este curso est\u00e1n disponibles en GitHub.</p> <p>Si tiene problemas, puede hacer una pregunta en el curso [p\u00e1gina de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).</p> <p>Y, por supuesto, est\u00e1 la documentaci\u00f3n de PyTorch y los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"12-08_pytorch_paper_replicating/#0-configuracion","title":"0. Configuraci\u00f3n\u00b6","text":"<p>Como lo hicimos anteriormente, asegur\u00e9monos de tener todos los m\u00f3dulos que necesitaremos para esta secci\u00f3n.</p> <p>Importaremos los scripts de Python (como <code>data_setup.py</code> y <code>engine.py</code>) que creamos en 05. PyTorch se vuelve modular.</p> <p>Para hacerlo, descargaremos el directorio <code>going_modular</code> del repositorio <code>pytorch-deep-learning</code> (si a\u00fan no lo tienes).</p> <p>Tambi\u00e9n obtendremos el paquete <code>torchinfo</code> si no est\u00e1 disponible.</p> <p><code>torchinfo</code> nos ayudar\u00e1 m\u00e1s adelante a darnos una representaci\u00f3n visual de nuestro modelo.</p> <p>Y dado que m\u00e1s adelante usaremos el paquete <code>torchvision</code> v0.13 (disponible a partir de julio de 2022), nos aseguraremos de tener las \u00faltimas versiones.</p>"},{"location":"12-08_pytorch_paper_replicating/#1-obtener-datos","title":"1. Obtener datos\u00b6","text":"<p>Como continuamos con FoodVision Mini, descarguemos el conjunto de datos de im\u00e1genes de pizza, bistec y sushi que hemos estado usando.</p> <p>Para hacerlo podemos usar la funci\u00f3n <code>download_data()</code> de <code>helper_functions.py</code> que creamos en [07. Secci\u00f3n 1 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data).</p> <p>Usaremos el enlace sin formato de GitHub de los datos <code>pizza_steak_sushi.zip</code> y el <code>destino</code> a <code>pizza_steak_sushi</code>.</p>"},{"location":"12-08_pytorch_paper_replicating/#2-crear-conjuntos-de-datos-y-cargadores-de-datos","title":"2. Crear conjuntos de datos y cargadores de datos\u00b6","text":"<p>Ahora que tenemos algunos datos, convirt\u00e1moslos en <code>DataLoader</code>.</p> <p>Para hacerlo podemos usar la funci\u00f3n <code>create_dataloaders()</code> en <code>data_setup.py</code> .</p> <p>Primero, crearemos una transformaci\u00f3n para preparar nuestras im\u00e1genes.</p> <p>Aqu\u00ed entrar\u00e1 una de las primeras referencias al art\u00edculo de ViT.</p> <p>En la Tabla 3, la resoluci\u00f3n de entrenamiento se menciona como 224 (alto = 224, ancho = 224).</p> <p></p> <p>A menudo puede encontrar varias configuraciones de hiperpar\u00e1metros enumeradas en una tabla. En este caso todav\u00eda estamos preparando nuestros datos, por lo que nos preocupan principalmente aspectos como el tama\u00f1o de la imagen y el tama\u00f1o del lote. Fuente: Tabla 3 en art\u00edculo de ViT.</p> <p>As\u00ed que nos aseguraremos de que nuestra transformaci\u00f3n cambie el tama\u00f1o de nuestras im\u00e1genes de manera adecuada.</p> <p>Y dado que entrenaremos nuestro modelo desde cero (para empezar, no habr\u00e1 aprendizaje por transferencia), no proporcionaremos una transformaci\u00f3n \"normalizada\" como lo hicimos en [06. Secci\u00f3n 2.1 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#21-creating-a-transform-for-torchvisionmodels-manual-creation).</p>"},{"location":"12-08_pytorch_paper_replicating/#21-preparar-transformaciones-para-imagenes","title":"2.1 Preparar transformaciones para im\u00e1genes\u00b6","text":""},{"location":"12-08_pytorch_paper_replicating/#22-convertir-imagenes-en-dataloaders","title":"2.2 Convertir im\u00e1genes en <code>DataLoader</code>'s\u00b6","text":"<p>\u00a1Transformaciones creadas!</p> <p>Ahora creemos nuestro <code>DataLoader</code>.</p> <p>El documento de ViT establece el uso de un tama\u00f1o de lote de 4096, que es 128 veces el tama\u00f1o del lote que hemos estado usando (32).</p> <p>Sin embargo, nos quedaremos con un tama\u00f1o de lote de 32.</p> <p>\u00bfPor qu\u00e9?</p> <p>Porque es posible que parte del hardware (incluido el nivel gratuito de Google Colab) no pueda manejar un tama\u00f1o de lote de 4096.</p> <p>Tener un tama\u00f1o de lote de 4096 significa que deben caber 4096 im\u00e1genes a la vez en la memoria de la GPU.</p> <p>Esto funciona cuando tienes el hardware para manejarlo, como lo hace a menudo un equipo de investigaci\u00f3n de Google, pero cuando est\u00e1s ejecutando en una sola GPU (como usando Google Colab), es una buena idea asegurarse de que las cosas funcionen primero con un tama\u00f1o de lote m\u00e1s peque\u00f1o. idea.</p> <p>Una extensi\u00f3n de este proyecto podr\u00eda ser probar un valor de tama\u00f1o de lote mayor y ver qu\u00e9 sucede.</p> <p>Nota: Estamos usando el par\u00e1metro <code>pin_memory=True</code> en la funci\u00f3n <code>create_dataloaders()</code> para acelerar el c\u00e1lculo. <code>pin_memory=True</code> evita la copia innecesaria de memoria entre la memoria de la CPU y la GPU al \"fijar\" ejemplos que se han visto antes. Aunque los beneficios de esto probablemente se ver\u00e1n con conjuntos de datos de mayor tama\u00f1o (nuestro conjunto de datos FoodVision Mini es bastante peque\u00f1o). Sin embargo, establecer <code>pin_memory=True</code> no siempre mejora el rendimiento (este es otro de esos escenarios en el aprendizaje autom\u00e1tico donde algunas cosas funcionan a veces y no otras veces), as\u00ed que es mejor experimentar, experimentar , experimento. Consulte la documentaci\u00f3n de PyTorch <code>torch.utils.data.DataLoader</code> o [C\u00f3mo hacer que el aprendizaje profundo sea mejor desde los primeros principios] ](https://horace.io/brrr_intro.html) de Horace He para obtener m\u00e1s informaci\u00f3n.</p>"},{"location":"12-08_pytorch_paper_replicating/#23-visualizar-una-sola-imagen","title":"2.3 Visualizar una sola imagen\u00b6","text":"<p>Ahora que hemos cargado nuestros datos, \u00a1visualicemos, visualicemos, visualicemos!</p> <p>Un paso importante en el art\u00edculo de ViT es preparar las im\u00e1genes en parches.</p> <p>Llegaremos a lo que esto significa en la [secci\u00f3n 4](https://www.learnpytorch.io/08_pytorch_paper_replicating/#4-equation-1-split-data-into-patches-and-creating-the-class-position -y-incrustaci\u00f3n de parches) pero por ahora, veamos una sola imagen y su etiqueta.</p> <p>Para hacerlo, obtengamos una sola imagen y etiqueta de un lote de datos e inspeccionemos sus formas.</p>"},{"location":"12-08_pytorch_paper_replicating/#3-replicacion-del-articulo-de-vit-descripcion-general","title":"3. Replicaci\u00f3n del art\u00edculo de ViT: descripci\u00f3n general\u00b6","text":"<p>Antes de escribir m\u00e1s c\u00f3digo, analicemos lo que estamos haciendo.</p> <p>Nos gustar\u00eda replicar el documento ViT para nuestro propio problema, FoodVision Mini.</p> <p>Entonces nuestras entradas modelo son: im\u00e1genes de pizza, bistec y sushi.</p> <p>Y nuestros resultados del modelo ideales son: etiquetas previstas de pizza, bistec o sushi.</p> <p>No es diferente a lo que hemos estado haciendo en las secciones anteriores.</p> <p>La pregunta es: \u00bfc\u00f3mo pasamos de nuestros insumos a los resultados deseados?</p>"},{"location":"12-08_pytorch_paper_replicating/#31-entradas-y-salidas-capas-y-bloques","title":"3.1 Entradas y salidas, capas y bloques\u00b6","text":"<p>ViT es una arquitectura de red neuronal de aprendizaje profundo.</p> <p>Y cualquier arquitectura de red neuronal generalmente se compone de capas.</p> <p>Y una colecci\u00f3n de capas a menudo se denomina bloque.</p> <p>Y apilar muchos bloques es lo que nos da la arquitectura completa.</p> <p>Una capa toma una entrada (por ejemplo, un tensor de imagen), realiza alg\u00fan tipo de funci\u00f3n en ella (por ejemplo, lo que hay en el m\u00e9todo <code>forward()</code> de la capa) y luego devuelve una salida.</p> <p>Entonces, si una capa \u00fanica toma una entrada y proporciona una salida, entonces una colecci\u00f3n de capas o un bloque tambi\u00e9n toma una entrada y proporciona una salida.</p> <p>Hagamos esto concreto:</p> <ul> <li>Capa: toma una entrada, realiza una funci\u00f3n en ella y devuelve una salida.</li> <li>Bloque: una colecci\u00f3n de capas, toma una entrada, realiza una serie de funciones en ella y devuelve una salida.</li> <li>Arquitectura (o modelo): una colecci\u00f3n de bloques, toma una entrada, realiza una serie de funciones en ella y devuelve una salida.</li> </ul> <p>Esta ideolog\u00eda es la que vamos a utilizar para replicar el art\u00edculo de ViT.</p> <p>Vamos a tomarlo capa por capa, bloque por bloque, funci\u00f3n por funci\u00f3n, juntando las piezas del rompecabezas como Lego para obtener la arquitectura general deseada.</p> <p>La raz\u00f3n por la que hacemos esto es porque mirar un trabajo de investigaci\u00f3n completo puede resultar intimidante.</p> <p>Entonces, para una mejor comprensi\u00f3n, lo desglosaremos, comenzando con las entradas y salidas de una sola capa y avanzando hasta las entradas y salidas de todo el modelo.</p> <p>Una arquitectura moderna de aprendizaje profundo suele ser una colecci\u00f3n de capas y bloques. Donde las capas toman una entrada (datos como representaci\u00f3n num\u00e9rica) y la manipulan usando alg\u00fan tipo de funci\u00f3n (por ejemplo, la f\u00f3rmula de autoatenci\u00f3n que se muestra arriba, sin embargo, esta funci\u00f3n podr\u00eda ser casi cualquier cosa) y luego la generan. Los bloques generalmente son pilas de capas una encima de otra que hacen algo similar a una sola capa pero varias veces.</p>"},{"location":"12-08_pytorch_paper_replicating/#32-especifico-de-que-esta-hecho-vit","title":"3.2 Espec\u00edfico: \u00bfDe qu\u00e9 est\u00e1 hecho ViT?\u00b6","text":"<p>Hay muchos peque\u00f1os detalles sobre el modelo ViT esparcidos por todo el art\u00edculo.</p> <p>\u00a1Encontrarlos todos es como una gran b\u00fasqueda del tesoro!</p> <p>Recuerde, un trabajo de investigaci\u00f3n suele incluir meses de trabajo comprimidos en unas pocas p\u00e1ginas, por lo que es comprensible que requiera pr\u00e1ctica para replicarlo.</p> <p>Sin embargo, los tres recursos principales que veremos para el dise\u00f1o arquitect\u00f3nico son:</p> <ol> <li>Figura 1: proporciona una descripci\u00f3n general del modelo en un sentido gr\u00e1fico; casi podr\u00edas recrear la arquitectura solo con esta figura.</li> <li>Cuatro ecuaciones en la secci\u00f3n 3.1: estas ecuaciones brindan una base un poco m\u00e1s matem\u00e1tica a los bloques de colores en la Figura 1.</li> <li>Tabla 1: esta tabla muestra las diversas configuraciones de hiperpar\u00e1metros (como la cantidad de capas y la cantidad de unidades ocultas) para diferentes variantes del modelo ViT. Nos centraremos en la versi\u00f3n m\u00e1s peque\u00f1a, ViT-Base.</li> </ol>"},{"location":"12-08_pytorch_paper_replicating/#321-explorando-la-figura-1","title":"3.2.1 Explorando la Figura 1\u00b6","text":"<p>Comencemos repasando la Figura 1 del documento ViT.</p> <p>Las principales cosas a las que prestaremos atenci\u00f3n son:</p> <ol> <li>Capas: toma una entrada, realiza una operaci\u00f3n o funci\u00f3n en la entrada y produce una salida.</li> <li>Bloques: una colecci\u00f3n de capas, que a su vez tambi\u00e9n toma una entrada y produce una salida.</li> </ol> <p></p> <p>Figura 1 del ViT Paper que muestra las diferentes entradas, salidas, capas y bloques que crean la arquitectura. Nuestro objetivo ser\u00e1 replicar cada uno de estos usando el c\u00f3digo PyTorch.</p> <p>La arquitectura ViT se compone de varias etapas:</p> <ul> <li>Incrustaci\u00f3n de parche + posici\u00f3n (entradas): convierte la imagen de entrada en una secuencia de parches de imagen y agrega un n\u00famero de posici\u00f3n para especificar en qu\u00e9 orden aparece el parche.</li> <li>Proyecci\u00f3n lineal de parches aplanados (parches incrustados) - Los parches de imagen se convierten en una incrustaci\u00f3n, la ventaja de utilizar una incrustaci\u00f3n en lugar de solo los valores de la imagen es que una incrustaci\u00f3n es una representaci\u00f3n aprendible (normalmente en forma de vector) de la imagen que puede mejorar con entrenamiento.</li> <li>Norma: es la abreviatura de \"Normalizaci\u00f3n de capas\" o \"LayerNorm\", una t\u00e9cnica para regularizar (reducir el sobreajuste) una red neuronal. Puede usar LayerNorm a trav\u00e9s de la capa PyTorch <code>torch.nn.LayerNorm()</code>.</li> <li>Atenci\u00f3n de m\u00faltiples cabezas: esta es una [capa de autoatenci\u00f3n de m\u00faltiples cabezas] (https://paperswithcode.com/method/multi-head-attention) o \"MSA\" para abreviar. Puede crear una capa MSA a trav\u00e9s de la capa PyTorch <code>torch.nn.MultiheadAttention()</code>.</li> <li>MLP (o Perceptr\u00f3n multicapa) - Un MLP a menudo puede referirse a cualquier colecci\u00f3n de capas de avance (o en el caso de PyTorch, una colecci\u00f3n de capas con un m\u00e9todo <code>forward()</code>). En el art\u00edculo de ViT, los autores se refieren al MLP como \"bloque MLP\" y contiene dos [<code>torch.nn.Linear()</code>](https://pytorch.org/docs/stable/generated/torch.nn. Linear.html) capas con una activaci\u00f3n de no linealidad <code>torch.nn.GELU()</code> entre ellas (secci\u00f3n 3.1) y una capa <code>torch.nn.Dropout()</code> despu\u00e9s de cada una (Ap\u00e9ndice B.1).</li> <li>Transformer Encoder: Transformer Encoder es una colecci\u00f3n de las capas enumeradas anteriormente. Hay dos conexiones de salto dentro del codificador Transformer (los s\u00edmbolos \"+\"), lo que significa que las entradas de la capa se env\u00edan directamente a las capas inmediatas, as\u00ed como a las capas posteriores. La arquitectura general de ViT se compone de varios codificadores Transformer apilados uno encima del otro.</li> <li>MLP Head: esta es la capa de salida de la arquitectura, convierte las caracter\u00edsticas aprendidas de una entrada en una salida de clase. Dado que estamos trabajando en la clasificaci\u00f3n de im\u00e1genes, tambi\u00e9n podr\u00eda llamarlo \"cabeza clasificadora\". La estructura del MLP Head es similar al bloque MLP.</li> </ul> <p>Es posible que observe que muchas de las piezas de la arquitectura ViT se pueden crear con capas de PyTorch existentes.</p> <p>Esto se debe a c\u00f3mo est\u00e1 dise\u00f1ado PyTorch, uno de los prop\u00f3sitos principales de PyTorch es crear capas de redes neuronales reutilizables tanto para investigadores como para profesionales del aprendizaje autom\u00e1tico.</p> <p>Pregunta: \u00bfPor qu\u00e9 no codificar todo desde cero?</p> <p>Definitivamente podr\u00eda hacerlo reproduciendo todas las ecuaciones matem\u00e1ticas del documento con capas de PyTorch personalizadas y eso sin duda ser\u00eda un ejercicio educativo; sin embargo, generalmente se prefiere el uso de capas de PyTorch preexistentes, ya que las capas preexistentes a menudo se han probado exhaustivamente. y se verifica el rendimiento para garantizar que se ejecuten correcta y r\u00e1pidamente.</p> <p>Nota: Nos centraremos en escribir c\u00f3digo PyTorch para crear estas capas. Para conocer los antecedentes de lo que hace cada una de estas capas, sugerir\u00eda leer el documento ViT en su totalidad o leer los recursos vinculados para cada capa.</p> <p>Tomemos la Figura 1 y adapt\u00e9mosla a nuestro problema FoodVision Mini de clasificar im\u00e1genes de comida en pizza, bistec o sushi.</p> <p></p> <p>Figura 1 del ViT Paper adaptado para su uso con FoodVision Mini. Entra una imagen de comida (pizza), la imagen se convierte en parches y luego se proyecta en una incrustaci\u00f3n. Luego, la incrustaci\u00f3n viaja a trav\u00e9s de las distintas capas y bloques y (con suerte) se devuelve la clase \"pizza\".</p>"},{"location":"12-08_pytorch_paper_replicating/#322-explorando-las-cuatro-ecuaciones","title":"3.2.2 Explorando las cuatro ecuaciones\u00b6","text":"<p>La siguiente(s) parte(s) principal(es) del art\u00edculo de ViT que veremos son las cuatro ecuaciones de la secci\u00f3n 3.1.</p> <p></p> <p>Estas cuatro ecuaciones representan las matem\u00e1ticas detr\u00e1s de las cuatro partes principales de la arquitectura ViT.</p> <p>La Secci\u00f3n 3.1 describe cada uno de estos (parte del texto se ha omitido por motivos de brevedad, el texto en negrita es m\u00edo):</p> N\u00famero de ecuaci\u00f3n Descripci\u00f3n de la secci\u00f3n 3.1 del art\u00edculo de ViT 1 ...El Transformador utiliza un tama\u00f1o de vector latente constante $D$ en todas sus capas, por lo que aplanamos los parches y los asignamos a dimensiones $D$ con una proyecci\u00f3n lineal entrenable (Ec. 1). Nos referimos al resultado de esta proyecci\u00f3n como incrustaciones de parches... Las incrustaciones de posici\u00f3n se agregan a las incrustaciones de parches para retener informaci\u00f3n posicional. Usamos incrustaciones de posiciones 1D que se pueden aprender... 2 El codificador Transformer (Vaswani et al., 2017) consta de capas alternas de autoatenci\u00f3n multicabezal (MSA, ver Ap\u00e9ndice A) y bloques MLP (Ec. 2, 3). Layernorm (LN) se aplica antes de cada bloque y conexiones residuales despu\u00e9s de cada bloque (Wang et al., 2019; Baevski &amp; Auli, 2019). 3 Igual que la ecuaci\u00f3n 2. 4 De manera similar al token [clase] de BERT, anteponemos una incrustaci\u00f3n que se puede aprender a la secuencia de parches incrustados $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, cuyo estado en la salida del codificador Transformer $\\left(\\mathbf{z}_{L}^{0}\\right)$ sirve como representaci\u00f3n de la imagen $\\mathbf{y} $ (Ecuaci\u00f3n 4)... <p>Asignemos estas descripciones a la arquitectura ViT en la Figura 1.</p> <p></p> <p>Conectando la Figura 1 del art\u00edculo de ViT con las cuatro ecuaciones de la secci\u00f3n 3.1 que describen las matem\u00e1ticas detr\u00e1s de cada una de las capas/bloques.</p> <p>Est\u00e1n sucediendo muchas cosas en la imagen de arriba, pero al seguir las l\u00edneas y flechas de colores se revelan los conceptos principales de la arquitectura ViT.</p> <p>\u00bfQu\u00e9 tal si desglosamos m\u00e1s cada ecuaci\u00f3n (nuestro objetivo ser\u00e1 recrearlas con c\u00f3digo)?</p> <p>En todas las ecuaciones (excepto la ecuaci\u00f3n 4), \"$\\mathbf{z}$\" es la salida sin procesar de una capa particular:</p> <ol> <li>$\\mathbf{z}_{0}$ es \"z cero\" (esta es la salida de la capa de incrustaci\u00f3n del parche inicial).</li> <li>$\\mathbf{z}_{\\ell}^{\\prime}$ es \"z de una capa particular prime\" (o un valor intermedio de z).</li> <li>$\\mathbf{z}_{\\ell}$ es \"z de una capa particular\".</li> </ol> <p>Y $\\mathbf{y}$ es el resultado general de la arquitectura.</p>"},{"location":"12-08_pytorch_paper_replicating/#323-descripcion-general-de-la-ecuacion-1","title":"3.2.3 Descripci\u00f3n general de la ecuaci\u00f3n 1\u00b6","text":"<p>$$ \\begin{alineado} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {clase }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R} ^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{alineado} $$</p> <p>Esta ecuaci\u00f3n trata con el token de clase, la incrustaci\u00f3n de parches y la incrustaci\u00f3n de posici\u00f3n ($\\mathbf{E}$ es para incrustar) de la imagen de entrada.</p> <p>En forma vectorial, la incrustaci\u00f3n podr\u00eda verse as\u00ed:</p> <pre><code>pit\u00f3n\nx_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n</code></pre> <p>Donde cada uno de los elementos del vector se puede aprender (su <code>requires_grad=True</code>).</p>"},{"location":"12-08_pytorch_paper_replicating/#324-descripcion-general-de-la-ecuacion-2","title":"3.2.4 Descripci\u00f3n general de la ecuaci\u00f3n 2\u00b6","text":"<p>$$ \\begin{alineado} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right )+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{alineado} $$</p> <p>Esto dice que para cada capa desde $1$ hasta $L$ (el n\u00famero total de capas), hay una capa de atenci\u00f3n de m\u00faltiples cabezales (MSA) que envuelve una capa LayerNorm (LN).</p> <p>La adici\u00f3n al final equivale a sumar la entrada a la salida y formar una [conexi\u00f3n omitida/residual] (https://paperswithcode.com/method/residual-connection).</p> <p>Llamaremos a esta capa \"bloque MSA\".</p> <p>En pseudoc\u00f3digo, esto podr\u00eda verse as\u00ed:</p> <pre><code>pit\u00f3n\nx_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n</code></pre> <p>Observe la conexi\u00f3n de salto al final (agregando la entrada de las capas a la salida de las capas).</p>"},{"location":"12-08_pytorch_paper_replicating/#325-descripcion-general-de-la-ecuacion-3","title":"3.2.5 Descripci\u00f3n general de la ecuaci\u00f3n 3\u00b6","text":"<p>$$ \\begin{alineado} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+ \\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\\\ \\end{alineado} $$</p> <p>Esto dice que para cada capa desde $1$ hasta $L$ (el n\u00famero total de capas), tambi\u00e9n hay una capa de Perceptr\u00f3n multicapa (MLP) que envuelve una capa LayerNorm (LN).</p> <p>La adici\u00f3n al final muestra la presencia de una conexi\u00f3n de salto/residual.</p> <p>Llamaremos a esta capa \"bloque MLP\".</p> <p>En pseudoc\u00f3digo, esto podr\u00eda verse as\u00ed:</p> <pre><code>pit\u00f3n\nx_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n</code></pre> <p>Observe la conexi\u00f3n de salto al final (agregando la entrada de las capas a la salida de las capas).</p>"},{"location":"12-08_pytorch_paper_replicating/#326-descripcion-general-de-la-ecuacion-4","title":"3.2.6 Descripci\u00f3n general de la ecuaci\u00f3n 4\u00b6","text":"<p>$$ \\begin{alineado} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{alineado} $$</p> <p>Esto dice que para la \u00faltima capa $L$, la salida $y$ es el token de \u00edndice 0 de $z$ envuelto en una capa LayerNorm (LN).</p> <p>O en nuestro caso, el \u00edndice 0 de <code>x_output_MLP_block</code>:</p> <pre><code>pit\u00f3n\ny = capa_lineal(capa_LN(x_output_MLP_block[0]))\n</code></pre> <p>Por supuesto, hay algunas simplificaciones anteriores, pero nos ocuparemos de ellas cuando comencemos a escribir c\u00f3digo PyTorch para cada secci\u00f3n.</p> <p>Nota: La secci\u00f3n anterior cubre mucha informaci\u00f3n. Pero no olvides que si algo no tiene sentido, siempre puedes investigarlo m\u00e1s a fondo. Haciendo preguntas como \"\u00bfqu\u00e9 es una conexi\u00f3n residual?\".</p>"},{"location":"12-08_pytorch_paper_replicating/#327-explorando-la-tabla-1","title":"3.2.7 Explorando la Tabla 1\u00b6","text":"<p>La \u00faltima pieza del rompecabezas de la arquitectura ViT en la que nos centraremos (por ahora) es la Tabla 1.</p> Modelo Capas Tama\u00f1o oculto $D$ Tama\u00f1o MLP Cabezas Par\u00e1metros ViT-Base 12 768 3072 12 86 millones de d\u00f3lares ViT-Grande 24 1024 4096 16 $307M$ ViT-enorme 32 1280 5120 16 $632M$ Tabla 1: Detalles de las variantes del modelo Vision Transformer. Fuente: documento ViT. <p>Esta tabla muestra los distintos hiperpar\u00e1metros de cada una de las arquitecturas ViT.</p> <p>Puede ver que los n\u00fameros aumentan gradualmente de ViT-Base a ViT-Huge.</p> <p>Nos centraremos en replicar ViT-Base (comenzaremos poco a poco y ampliaremos cuando sea necesario), pero escribiremos c\u00f3digo que pueda ampliarse f\u00e1cilmente a variantes m\u00e1s grandes.</p> <p>Desglosando los hiperpar\u00e1metros:</p> <ul> <li>Capas - \u00bfCu\u00e1ntos bloques de Transformer Encoder hay? (cada uno de estos contendr\u00e1 un bloque MSA y un bloque MLP)</li> <li>Tama\u00f1o oculto $D$: esta es la dimensi\u00f3n de incrustaci\u00f3n en toda la arquitectura, este ser\u00e1 el tama\u00f1o del vector en el que se convierte nuestra imagen cuando se parchea e incrusta. Generalmente, cuanto mayor sea la dimensi\u00f3n de incrustaci\u00f3n, m\u00e1s informaci\u00f3n se podr\u00e1 capturar y mejores resultados. Sin embargo, una mayor integraci\u00f3n tiene el costo de m\u00e1s computaci\u00f3n.</li> <li>Tama\u00f1o de MLP - \u00bfCu\u00e1l es la cantidad de unidades ocultas en las capas de MLP?</li> <li>Cabezas - \u00bfCu\u00e1ntas cabezas hay en las capas de Atenci\u00f3n de m\u00faltiples cabezas?</li> <li>Params - \u00bfCu\u00e1l es el n\u00famero total de par\u00e1metros del modelo? Generalmente, m\u00e1s par\u00e1metros conducen a un mejor rendimiento, pero a costa de m\u00e1s procesamiento. Notar\u00e1s que incluso ViT-Base tiene muchos m\u00e1s par\u00e1metros que cualquier otro modelo que hayamos usado hasta ahora.</li> </ul> <p>Usaremos estos valores como configuraci\u00f3n de hiperpar\u00e1metros para nuestra arquitectura ViT.</p>"},{"location":"12-08_pytorch_paper_replicating/#33-mi-flujo-de-trabajo-para-replicar-articulos","title":"3.3 Mi flujo de trabajo para replicar art\u00edculos\u00b6","text":"<p>Cuando empiezo a trabajar en la replicaci\u00f3n de un art\u00edculo, sigo los siguientes pasos:</p> <ol> <li>Lea el documento completo de principio a fin una vez (para tener una idea de los conceptos principales).</li> <li>Vuelva a revisar cada secci\u00f3n y vea c\u00f3mo se alinean entre s\u00ed y comience a pensar en c\u00f3mo podr\u00edan convertirse en c\u00f3digo (como se muestra arriba).</li> <li>Repita el paso 2 hasta que tenga un esquema bastante bueno.</li> <li>Utilice mathpix.com (una herramienta muy \u00fatil) para convertir cualquier secci\u00f3n del papel en Markdown/LaTeX para guardarlo en cuadernos.</li> <li>Replica la versi\u00f3n m\u00e1s simple posible del modelo.</li> <li>Si me quedo atascado, busca otros ejemplos.</li> </ol> <p></p> <p>Convertir las cuatro ecuaciones del documento ViT en LaTeX/markdown editable usando mathpix.com.</p> <p>Ya hemos realizado los primeros pasos anteriores (y si a\u00fan no ha le\u00eddo el documento completo, le animo a que lo pruebe) pero en lo que nos centraremos a continuaci\u00f3n es en el paso 5: replicar el versi\u00f3n m\u00e1s simple posible del modelo.</p> <p>Por eso comenzamos con ViT-Base.</p> <p>Replicar la versi\u00f3n m\u00e1s peque\u00f1a posible de la arquitectura, hacerla funcionar y luego podremos ampliarla si quisi\u00e9ramos.</p> <p>Nota: Si nunca antes ha le\u00eddo un trabajo de investigaci\u00f3n, muchos de los pasos anteriores pueden resultar intimidantes. Pero no te preocupes, como todo, tus habilidades para leer y replicar art\u00edculos mejorar\u00e1n con la pr\u00e1ctica. No olvide que un trabajo de investigaci\u00f3n suele ser meses de trabajo de muchas personas comprimidos en unas pocas p\u00e1ginas. As\u00ed que intentar replicarlo por tu cuenta no es tarea f\u00e1cil.</p>"},{"location":"12-08_pytorch_paper_replicating/#4-ecuacion-1-dividir-los-datos-en-parches-y-crear-la-clase-la-posicion-y-la-incrustacion-del-parche","title":"4. Ecuaci\u00f3n 1: dividir los datos en parches y crear la clase, la posici\u00f3n y la incrustaci\u00f3n del parche\u00b6","text":"<p>Recuerdo que uno de mis amigos ingenieros de aprendizaje autom\u00e1tico sol\u00eda decir \"se trata de la integraci\u00f3n\".</p> <p>Es decir, si puede representar sus datos de una manera buena y f\u00e1cil de aprender (ya que las incrustaciones son representaciones que se pueden aprender), es probable que un algoritmo de aprendizaje pueda funcionar bien con ellos.</p> <p>Dicho esto, comencemos por crear las incrustaciones de clases, posiciones y parches para la arquitectura ViT.</p> <p>Comenzaremos con la incrustaci\u00f3n del parche.</p> <p>Esto significa que convertiremos nuestras im\u00e1genes de entrada en una secuencia de parches y luego los incrustaremos.</p> <p>Recuerde que una incrustaci\u00f3n es una representaci\u00f3n que se puede aprender de alguna forma y, a menudo, es un vector.</p> <p>El t\u00e9rmino aprendeble es importante porque significa que la representaci\u00f3n num\u00e9rica de una imagen de entrada (que ve el modelo) se puede mejorar con el tiempo.</p> <p>Comenzaremos siguiendo el p\u00e1rrafo inicial de la secci\u00f3n 3.1 del documento de ViT (negrita m\u00eda):</p> <p>El transformador est\u00e1ndar recibe como entrada una secuencia 1D de incrustaciones de tokens. Para manejar im\u00e1genes 2D, remodelamos la imagen $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ en una secuencia de parches 2D aplanados $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$, donde $(H, W)$ es la resoluci\u00f3n de la imagen original, $C$ es la n\u00famero de canales, $(P, P)$ es la resoluci\u00f3n de cada parche de imagen y $N=H W / P^{2}$ es el n\u00famero resultante de parches, que tambi\u00e9n sirve como longitud efectiva de la secuencia de entrada para el transformador. . El Transformador utiliza un tama\u00f1o de vector latente constante $D$ en todas sus capas, por lo que aplanamos los parches y los asignamos a dimensiones $D$ con una proyecci\u00f3n lineal entrenable (Ec. 1). Nos referimos al resultado de esta proyecci\u00f3n como incrustaciones de parches.</p> <p>Y el tama\u00f1o, estamos tratando con formas de im\u00e1genes, tengamos en cuenta la l\u00ednea de la Tabla 3 del art\u00edculo de ViT:</p> <p>La resoluci\u00f3n del entrenamiento es 224.</p> <p>Analicemos el texto anterior.</p> <ul> <li>$D$ es el tama\u00f1o de las incrustaciones de parches; en la Tabla 1 se pueden encontrar diferentes valores de $D$ para modelos ViT de varios tama\u00f1os.</li> <li>La imagen comienza como 2D con tama\u00f1o ${H \\times W \\times C}$.<ul> <li>$(H, W)$ es la resoluci\u00f3n de la imagen original (alto, ancho).</li> <li>$C$ es el n\u00famero de canales.</li> </ul> </li> <li>La imagen se convierte en una secuencia de parches 2D aplanados con tama\u00f1o ${N \\times\\left(P^{2} \\cdot C\\right)}$.<ul> <li>$(P, P)$ es la resoluci\u00f3n de cada parche de imagen (tama\u00f1o del parche).</li> <li>$N=H W / P^{2}$ es el n\u00famero resultante de parches, que tambi\u00e9n sirve como longitud de la secuencia de entrada para el Transformer.</li> </ul> </li> </ul> <p></p> <p>Mapeo del parche y la porci\u00f3n de incrustaci\u00f3n de posici\u00f3n de la arquitectura ViT de la Figura 1 a la Ecuaci\u00f3n 1. El p\u00e1rrafo inicial de la secci\u00f3n 3.1 describe las diferentes formas de entrada y salida de la capa de incrustaci\u00f3n del parche.</p>"},{"location":"12-08_pytorch_paper_replicating/#41-calculo-manual-de-formas-de-entrada-y-salida-de-incrustacion-de-parches","title":"4.1 C\u00e1lculo manual de formas de entrada y salida de incrustaci\u00f3n de parches\u00b6","text":"<p>\u00bfQu\u00e9 tal si comenzamos calculando estos valores de forma de entrada y salida a mano?</p> <p>Para hacerlo, creemos algunas variables para imitar cada uno de los t\u00e9rminos (como $H$, $W$, etc.) anteriores.</p> <p>Usaremos un tama\u00f1o de parche ($P$) de 16 ya que es la versi\u00f3n de mejor rendimiento que utiliza ViT-Base (consulte la columna \"ViT-B/16\" de la Tabla 5 en el documento de ViT para obtener m\u00e1s informaci\u00f3n).</p>"},{"location":"12-08_pytorch_paper_replicating/#42-convertir-una-sola-imagen-en-parches","title":"4.2 Convertir una sola imagen en parches\u00b6","text":"<p>Ahora que conocemos las formas ideales de entrada y salida para nuestra capa de incrustaci\u00f3n de parche, avancemos hacia su creaci\u00f3n.</p> <p>Lo que estamos haciendo es dividir la arquitectura general en partes m\u00e1s peque\u00f1as, centr\u00e1ndonos en las entradas y salidas de capas individuales.</p> <p>Entonces, \u00bfc\u00f3mo creamos la capa de incrustaci\u00f3n del parche?</p> <p>Llegaremos a eso en breve, primero, \u00a1visualicemos, visualicemos, visualicemos! c\u00f3mo se ve convertir una imagen en parches.</p> <p>Comencemos con nuestra imagen \u00fanica.</p>"},{"location":"12-08_pytorch_paper_replicating/#43-creando-parches-de-imagen-con-torchnnconv2d","title":"4.3 Creando parches de imagen con <code>torch.nn.Conv2d()</code>\u00b6","text":"<p>Hemos visto c\u00f3mo se ve una imagen cuando se convierte en parches, ahora comencemos a replicar las capas de incrustaci\u00f3n de parches con PyTorch.</p> <p>Para visualizar nuestra imagen \u00fanica, escribimos c\u00f3digo para recorrer las diferentes dimensiones de alto y ancho de una sola imagen y trazar parches individuales.</p> <p>Esta operaci\u00f3n es muy similar a la operaci\u00f3n convolucional que vimos en 03. Secci\u00f3n 7.1 de PyTorch Computer Vision: paso a paso por <code>nn.Conv2d()</code>.</p> <p>De hecho, los autores del art\u00edculo de ViT mencionan en la secci\u00f3n 3.1 que la integraci\u00f3n del parche se puede lograr con una red neuronal convolucional (CNN):</p> <p>Arquitectura h\u00edbrida. Como alternativa a los parches de im\u00e1genes sin procesar, la secuencia de entrada se puede formar a partir de mapas de caracter\u00edsticas de una CNN (LeCun et al., 1989). En este modelo h\u00edbrido, la proyecci\u00f3n de incorporaci\u00f3n de parches $\\mathbf{E}$ (Ec. 1) se aplica a parches extra\u00eddos de un mapa de caracter\u00edsticas de CNN. Como caso especial, los parches pueden tener un tama\u00f1o espacial $1 \\times 1$, lo que significa que la secuencia de entrada se obtiene simplemente aplanando las dimensiones espaciales del mapa de caracter\u00edsticas y proyect\u00e1ndolas a la dimensi\u00f3n del Transformador. La incrustaci\u00f3n de entrada de clasificaci\u00f3n y la incrustaci\u00f3n de posici\u00f3n se agregan como se describe anteriormente.</p> <p>El \"mapa de caracter\u00edsticas\" al que se refieren son los pesos/activaciones producidos por una capa convolucional que pasa sobre una imagen determinada.</p> <p></p> <p>Al establecer los par\u00e1metros <code>kernel_size</code> y <code>stride</code> de una capa <code>torch.nn.Conv2d()</code> iguales al <code>patch_size</code>, podemos obtener efectivamente una capa que divide nuestra imagen en parches y crea una incrustaci\u00f3n que se puede aprender (denominada \"Proyecci\u00f3n lineal\" en el art\u00edculo de ViT) de cada parche.</p> <p>\u00bfRecuerda nuestras formas de entrada y salida ideales para la capa de incrustaci\u00f3n de parches?</p> <ul> <li>Entrada: La imagen comienza como 2D con tama\u00f1o ${H \\times W \\times C}$.</li> <li>Salida: La imagen se convierte en una secuencia 1D de parches 2D aplanados con tama\u00f1o ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>O para un tama\u00f1o de imagen de 224 y un tama\u00f1o de parche de 16:</p> <ul> <li>Entrada (imagen 2D): (224, 224, 3) -&gt; (alto, ancho, canales de color)</li> <li>Salida (parches 2D aplanados): (196, 768) -&gt; (n\u00famero de parches, dimensi\u00f3n de incrustaci\u00f3n)</li> </ul> <p>Podemos recrearlos con:</p> <ul> <li><code>torch.nn.Conv2d()</code> para convertir nuestra imagen en parches de mapas de caracter\u00edsticas de CNN.</li> <li><code>torch.nn.Flatten()</code> para aplanar las dimensiones espaciales del mapa de caracter\u00edsticas.</li> </ul> <p>Comencemos con la capa <code>torch.nn.Conv2d()</code>.</p> <p>Podemos replicar la creaci\u00f3n de parches estableciendo <code>kernel_size</code> y <code>stride</code> iguales a <code>patch_size</code>.</p> <p>Esto significa que cada n\u00facleo convolucional tendr\u00e1 el tama\u00f1o <code>(patch_size x patch_size)</code> o si <code>patch_size=16</code>, <code>(16 x 16)</code> (el equivalente a un parche completo).</p> <p>Y cada paso o \"zancada\" del n\u00facleo convolucional tendr\u00e1 una longitud de \"patch_size\" p\u00edxeles o \"16\" p\u00edxeles (equivalente a pasar al siguiente parche).</p> <p>Estableceremos <code>in_channels=3</code> para el n\u00famero de canales de color en nuestra imagen y estableceremos <code>out_channels=768</code>, lo mismo que el valor $D$ en la Tabla 1 para ViT-Base (esta es la dimensi\u00f3n de incrustaci\u00f3n , cada imagen se incrustar\u00e1 en un vector que se puede aprender de tama\u00f1o 768).</p>"},{"location":"12-08_pytorch_paper_replicating/#44-aplanando-la-incrustacion-del-parche-con-torchnnflatten","title":"4.4 Aplanando la incrustaci\u00f3n del parche con <code>torch.nn.Flatten()</code>\u00b6","text":"<p>Hemos convertido nuestra imagen en incrustaciones de parches, pero todav\u00eda est\u00e1n en formato 2D.</p> <p>\u00bfC\u00f3mo logramos que adquieran la forma de salida deseada de la capa de incrustaci\u00f3n de parches del modelo ViT?</p> <ul> <li>Salida deseada (secuencia 1D de parches 2D aplanados): (196, 768) -&gt; (n\u00famero de parches, dimensi\u00f3n de incrustaci\u00f3n) -&gt; ${N \\times\\left(P^{2} \\cdot C\\ derecha)}$</li> </ul> <p>Comprobemos la forma actual.</p>"},{"location":"12-08_pytorch_paper_replicating/#45-convertir-la-capa-de-incrustacion-del-parche-vit-en-un-modulo-pytorch","title":"4.5 Convertir la capa de incrustaci\u00f3n del parche ViT en un m\u00f3dulo PyTorch\u00b6","text":"<p>Es hora de poner todo lo que hemos hecho para crear el parche incrustado en una sola capa de PyTorch.</p> <p>Podemos hacerlo subclasificando <code>nn.Module</code> y creando un peque\u00f1o \"modelo\" de PyTorch para realizar todos los pasos anteriores.</p> <p>Espec\u00edficamente:</p> <ol> <li>Cree una clase llamada <code>PatchEmbedding</code> que subclase <code>nn.Module</code> (para que pueda usarse como una capa de PyTorch).</li> <li>Inicialice la clase con los par\u00e1metros <code>in_channels=3</code>, <code>patch_size=16</code> (para ViT-Base) y <code>embedding_dim=768</code> (esto es $D$ para ViT-Base de la Tabla 1).</li> <li>Cree una capa para convertir una imagen en parches usando <code>nn.Conv2d()</code> (como en 4.3 anterior).</li> <li>Cree una capa para aplanar los mapas de caracter\u00edsticas del parche en una sola dimensi\u00f3n (como en 4.4 arriba).</li> <li>Defina un m\u00e9todo <code>forward()</code> para tomar una entrada y pasarla a trav\u00e9s de las capas creadas en 3 y 4.</li> <li>Aseg\u00farese de que la forma de salida refleje la forma de salida requerida de la arquitectura ViT (${N \\times\\left(P^{2} \\cdot C\\right)}$).</li> </ol> <p>\u00a1Vamos a hacerlo!</p>"},{"location":"12-08_pytorch_paper_replicating/#46-creando-la-incrustacion-del-token-de-clase","title":"4.6 Creando la incrustaci\u00f3n del token de clase\u00b6","text":"<p>Bien, ya hemos incorporado el parche de imagen. Es hora de empezar a trabajar en la incorporaci\u00f3n del token de clase.</p> <p>O $\\mathbf{x}_\\text {class }$ de la ecuaci\u00f3n 1.</p> <p></p> <p>Izquierda: Figura 1 del art\u00edculo de ViT con el \"token de clasificaci\u00f3n\" o token de incrustaci\u00f3n <code>[clase]</code> que vamos a recrear resaltado. Derecha: Ecuaci\u00f3n 1 y secci\u00f3n 3.1 del documento de ViT que se relacionan con el token de incorporaci\u00f3n de clase que se puede aprender.</p> <p>Al leer el segundo p\u00e1rrafo de la secci\u00f3n 3.1 del art\u00edculo de ViT, vemos la siguiente descripci\u00f3n:</p> <p>De manera similar al token <code>[ class ]</code> de BERT, anteponemos una incrustaci\u00f3n que se puede aprender a la secuencia de parches incrustados $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text { clase }}\\right)$, cuyo estado en la salida del codificador Transformer $\\left(\\mathbf{z}_{L}^{0}\\right)$ sirve como representaci\u00f3n de la imagen $\\mathbf{y}$ (Ecuaci\u00f3n 4).</p> <p>Nota: BERT (Representaciones de codificador bidireccional de Transformers) es uno de los art\u00edculos de investigaci\u00f3n originales sobre aprendizaje autom\u00e1tico que utiliza la arquitectura Transformer para lograr resultados sobresalientes en entornos naturales. tareas de procesamiento del lenguaje (PNL) y es donde se origin\u00f3 la idea de tener un token <code>[clase]</code> al comienzo de una secuencia, siendo clase una descripci\u00f3n de la clase de \"clasificaci\u00f3n\" a la que pertenec\u00eda la secuencia.</p> <p>Por lo tanto, necesitamos \"preparar una incrustaci\u00f3n que se pueda aprender en la secuencia de parches incrustados\".</p> <p>Comencemos viendo nuestra secuencia de tensor de parches incrustados (creado en la secci\u00f3n 4.5) y su forma.</p>"},{"location":"12-08_pytorch_paper_replicating/#47-creando-la-incrustacion-de-posicion","title":"4.7 Creando la incrustaci\u00f3n de posici\u00f3n\u00b6","text":"<p>Bueno, tenemos la incrustaci\u00f3n del token de clase y la incrustaci\u00f3n del parche. Ahora, \u00bfc\u00f3mo podr\u00edamos crear la incrustaci\u00f3n de posici\u00f3n?</p> <p>O $\\mathbf{E}_{\\text {pos }}$ de la ecuaci\u00f3n 1 donde $E$ significa \"incrustaci\u00f3n\".</p> <p></p> <p>Izquierda: Figura 1 del art\u00edculo de ViT con la posici\u00f3n incrustada que vamos a recrear resaltada. Derecha: Ecuaci\u00f3n 1 y secci\u00f3n 3.1 del art\u00edculo de ViT que se relacionan con la incrustaci\u00f3n de posiciones.</p> <p>Averig\u00fcemos m\u00e1s leyendo la secci\u00f3n 3.1 del art\u00edculo de ViT (negrita m\u00eda):</p> <p>Las incrustaciones de posici\u00f3n se agregan a las incrustaciones de parches para retener la informaci\u00f3n posicional. Usamos incrustaciones de posici\u00f3n 1D est\u00e1ndar que se pueden aprender, ya que no hemos observado mejoras significativas en el rendimiento al utilizar incrustaciones de posici\u00f3n 2D m\u00e1s avanzadas (Ap\u00e9ndice D.4). La secuencia resultante de vectores de incrustaci\u00f3n sirve como entrada para el codificador.</p> <p>Con \"retener informaci\u00f3n posicional\" los autores quieren decir que quieren que la arquitectura sepa en qu\u00e9 \"orden\" vienen los parches. Es decir, el parche dos viene despu\u00e9s del parche uno y el parche tres viene despu\u00e9s del parche dos y as\u00ed sucesivamente.</p> <p>Esta informaci\u00f3n posicional puede ser importante al considerar lo que hay en una imagen (sin informaci\u00f3n posicional, se podr\u00eda considerar que una secuencia aplanada no tiene orden y, por lo tanto, ning\u00fan parche se relaciona con ning\u00fan otro parche).</p> <p>Para comenzar a crear las incrustaciones de posiciones, veamos nuestras incrustaciones actuales.</p>"},{"location":"12-08_pytorch_paper_replicating/#48-poniendolo-todo-junto-de-la-imagen-a-la-incrustacion","title":"4.8 Poni\u00e9ndolo todo junto: de la imagen a la incrustaci\u00f3n\u00b6","text":"<p>Muy bien, hemos recorrido un largo camino en t\u00e9rminos de convertir nuestras im\u00e1genes de entrada en una ecuaci\u00f3n 1 de incrustaci\u00f3n y replicaci\u00f3n de la secci\u00f3n 3.1 del art\u00edculo de ViT:</p> <p>$$ \\begin{alineado} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {clase }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R} ^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{alineado} $$</p> <p>Ahora juntemos todo en una sola celda de c\u00f3digo y pasemos de la imagen de entrada ($\\mathbf{x}$) a la incrustaci\u00f3n de salida ($\\mathbf{z}_0$).</p> <p>Podemos hacerlo mediante:</p> <ol> <li>Establecer el tama\u00f1o del parche (usaremos <code>16</code> ya que se usa ampliamente en todo el documento y para ViT-Base).</li> <li>Obtener una sola imagen, imprimir su forma y almacenar su alto y ancho.</li> <li>Agregar una dimensi\u00f3n por lotes a la imagen \u00fanica para que sea compatible con nuestra capa <code>PatchEmbedding</code>.</li> <li>Crear una capa <code>PatchEmbedding</code> (la que hicimos en la secci\u00f3n 4.5) con <code>patch_size=16</code> y <code>embedding_dim=768</code> (de la Tabla 1 para ViT-Base).</li> <li>Pasar la imagen \u00fanica a trav\u00e9s de la capa <code>PatchEmbedding</code> en 4 para crear una secuencia de incrustaciones de parches.</li> <li>Crear una incrustaci\u00f3n de token de clase como en la secci\u00f3n 4.6.</li> <li>Anteponer la incorporaci\u00f3n del token de clase a las incorporaciones de parches creadas en el paso 5.</li> <li>Creando una posici\u00f3n incrustada como en la secci\u00f3n 4.7.</li> <li>Agregar la incrustaci\u00f3n de posici\u00f3n al token de clase y las incrustaciones de parches creadas en el paso 7.</li> </ol> <p>Tambi\u00e9n nos aseguraremos de configurar las semillas aleatorias con <code>set_seeds()</code> e imprimiremos las formas de diferentes tensores a lo largo del camino.</p>"},{"location":"12-08_pytorch_paper_replicating/#5-ecuacion-2-atencion-de-multiples-cabezas-msa","title":"5. Ecuaci\u00f3n 2: Atenci\u00f3n de m\u00faltiples cabezas (MSA)\u00b6","text":"<p>Tenemos nuestros datos de entrada parcheados e integrados, ahora pasemos a la siguiente parte de la arquitectura ViT.</p> <p>Para comenzar, dividiremos la secci\u00f3n Transformer Encoder en dos partes (comenzaremos poco a poco y aumentaremos cuando sea necesario).</p> <p>La primera es la ecuaci\u00f3n 2 y la segunda la ecuaci\u00f3n 3.</p> <p>Recuerde que la ecuaci\u00f3n 2 dice:</p> <p>$$ \\begin{alineado} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right )+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{alineado} $$</p> <p>Esto indica una capa de Atenci\u00f3n de m\u00faltiples cabezales (MSA) envuelta en una capa LayerNorm (LN) con una conexi\u00f3n residual (la entrada a la capa se agrega a la salida de la capa).</p> <p>Nos referiremos a la ecuaci\u00f3n 2 como \"bloque MSA\".</p> <p>***Izquierda:** Figura 1 del art\u00edculo de ViT con las capas Multi-Head Attention y Norm, as\u00ed como la conexi\u00f3n residual (+) resaltada dentro del bloque Transformer Encoder. Derecha: Mapeo de la capa Multi-Head Self Attention (MSA), la capa Norm y la conexi\u00f3n residual a sus respectivas partes de la ecuaci\u00f3n 2 en el documento ViT.*</p> <p>Muchas capas que se encuentran en los art\u00edculos de investigaci\u00f3n ya est\u00e1n implementadas en marcos modernos de aprendizaje profundo como PyTorch.</p> <p>Dicho esto, para replicar estas capas y conexi\u00f3n residual con el c\u00f3digo PyTorch podemos usar:</p> <ul> <li>Autoatenci\u00f3n de m\u00faltiples cabezales (MSA) - <code>torch.nn.MultiheadAttention()</code>.</li> <li>Norma (LN o LayerNorm) - <code>torch.nn.LayerNorm()</code>.</li> <li>Conexi\u00f3n residual: agregue la entrada a la salida (veremos esto m\u00e1s adelante cuando creemos el bloque Transformer Encoder completo en la secci\u00f3n 7.1).</li> </ul>"},{"location":"12-08_pytorch_paper_replicating/#51-la-capa-layernorm-ln","title":"5.1 La capa LayerNorm (LN)\u00b6","text":"<p>Normalizaci\u00f3n de capas (<code>torch.nn.LayerNorm()</code> o Norm o LayerNorm o LN) normaliza una entrada en la \u00faltima dimensi\u00f3n.</p> <p>Puede encontrar la definici\u00f3n formal de <code>torch.nn.LayerNorm()</code> en la documentaci\u00f3n de PyTorch.</p> <p>El par\u00e1metro principal de <code>torch.nn.LayerNorm()</code> de PyTorch es <code>normalized_shape</code>, que podemos configurar para que sea igual al tama\u00f1o de dimensi\u00f3n sobre el que nos gustar\u00eda normalizar (en nuestro caso ser\u00e1 $D$ o <code>768 </code> para ViT-Base).</p> <p>\u00bfQu\u00e9 hace?</p> <p>La normalizaci\u00f3n de capas ayuda a mejorar el tiempo de entrenamiento y la generalizaci\u00f3n del modelo (capacidad de adaptarse a datos invisibles).</p> <p>Me gusta pensar en cualquier tipo de normalizaci\u00f3n como \"obtener los datos en un formato similar\" o \"obtener muestras de datos en una distribuci\u00f3n similar\".</p> <p>Imag\u00ednese intentar subir (o bajar) un conjunto de escaleras, todas con diferentes alturas y longitudes.</p> <p>Se necesitar\u00edan algunos ajustes en cada paso, \u00bfverdad?</p> <p>Y lo que aprendes en cada paso no necesariamente ayudar\u00e1 con el siguiente, ya que todos son diferentes, lo que aumenta el tiempo que te lleva subir las escaleras.</p> <p>La normalizaci\u00f3n (incluida la normalizaci\u00f3n de capas) es el equivalente a hacer que todas las escaleras tengan la misma altura y longitud, excepto que las escaleras son sus muestras de datos.</p> <p>Entonces, as\u00ed como puedes subir (o bajar) escaleras con alturas y longitudes similares mucho m\u00e1s f\u00e1cilmente que aquellas con alturas y anchos diferentes, las redes neuronales pueden optimizar muestras de datos con distribuciones similares (medias y desviaciones est\u00e1ndar similares) m\u00e1s f\u00e1cilmente que aquellas con diferentes alturas y longitudes. distribuciones.</p>"},{"location":"12-08_pytorch_paper_replicating/#52-la-capa-de-atencion-automatica-de-multiples-cabezales-msa","title":"5.2 La capa de atenci\u00f3n autom\u00e1tica de m\u00faltiples cabezales (MSA)\u00b6","text":"<p>El poder de la autoatenci\u00f3n y la atenci\u00f3n de m\u00faltiples cabezas (autoatenci\u00f3n aplicada varias veces) se revel\u00f3 en la forma de la arquitectura Transformer original introducida en [La atenci\u00f3n es todo lo que necesitas](https://arxiv.org /abs/1706.03762) art\u00edculo de investigaci\u00f3n.</p> <p>Dise\u00f1ado originalmente para la entrada de texto, el mecanismo de autoatenci\u00f3n original toma una secuencia de palabras y luego calcula qu\u00e9 palabra debe prestar m\u00e1s \"atenci\u00f3n\" a otra palabra.</p> <p>En otras palabras, en la frase \"el perro salt\u00f3 la valla\", quiz\u00e1s la palabra \"perro\" se relacione fuertemente con \"salt\u00f3\" y \"valla\".</p> <p>Esto se simplifica pero la premisa sigue siendo las im\u00e1genes.</p> <p>Dado que nuestra entrada es una secuencia de parches de im\u00e1genes en lugar de palabras, la autoatenci\u00f3n y, a su vez, la atenci\u00f3n de m\u00faltiples cabezas calcular\u00e1n qu\u00e9 parche de una imagen est\u00e1 m\u00e1s relacionado con otro parche, formando eventualmente una representaci\u00f3n aprendida de una imagen.</p> <p>Pero lo m\u00e1s importante es que la capa hace esto por s\u00ed sola teniendo en cuenta los datos (no le decimos qu\u00e9 patrones aprender).</p> <p>Y si la representaci\u00f3n aprendida que forman las capas usando MSA es buena, veremos los resultados en el rendimiento de nuestro modelo.</p> <p>Hay muchos recursos en l\u00ednea para aprender m\u00e1s sobre la arquitectura de Transformer y el mecanismo de atenci\u00f3n en l\u00ednea, como la maravillosa [publicaci\u00f3n ilustrada de Transformer] de Jay Alammar (https://jalammar.github.io/illustrated-transformer/) y la [publicaci\u00f3n ilustrada de atenci\u00f3n] (https ://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/).</p> <p>Nos centraremos m\u00e1s en codificar una implementaci\u00f3n de PyTorch MSA existente que en crear la nuestra propia.</p> <p>Sin embargo, puede encontrar la definici\u00f3n formal de la implementaci\u00f3n de MSA del documento ViT en el Ap\u00e9ndice A:</p> <p>***Izquierda:** Descripci\u00f3n general de la arquitectura de Vision Transformer de la Figura 1 del documento de ViT. Derecha: Las definiciones de la ecuaci\u00f3n 2, la secci\u00f3n 3.1 y el Ap\u00e9ndice A del documento de ViT resaltadas para reflejar sus respectivas partes en la Figura 1.*</p> <p>La imagen de arriba resalta la triple entrada de incrustaci\u00f3n en la capa MSA.</p> <p>Esto se conoce como entrada de consulta, clave, valor o qkv para abreviar, que es fundamental para el mecanismo de autoatenci\u00f3n.</p> <p>En nuestro caso, la entrada de triple incrustaci\u00f3n ser\u00e1n tres versiones de la salida de la capa Norma, una para consulta, clave y valor.</p> <p>O tres versiones de nuestro parche de imagen normalizado por capas y de incrustaciones de posici\u00f3n creadas en la secci\u00f3n 4.8.</p> <p>Podemos implementar la capa MSA en PyTorch con <code>torch.nn.MultiheadAttention()</code> con los par\u00e1metros:</p> <ul> <li><code>embed_dim</code>: la dimensi\u00f3n de incrustaci\u00f3n de la Tabla 1 (tama\u00f1o oculto $D$).</li> <li><code>num_heads</code> - cu\u00e1ntas cabezas de atenci\u00f3n usar (de aqu\u00ed proviene el t\u00e9rmino \"multiheads\"), este valor tambi\u00e9n se encuentra en la Tabla 1 (Cabezas).</li> <li><code>dropout</code>: si se aplica o no el abandono a la capa de atenci\u00f3n (seg\u00fan el Ap\u00e9ndice B.1, el abandono no se utiliza despu\u00e9s de las proyecciones qkv).</li> <li><code>batch_first</code>: \u00bfnuestra dimensi\u00f3n del lote es lo primero? (s\u00ed lo hace)</li> </ul>"},{"location":"12-08_pytorch_paper_replicating/#53-replicando-la-ecuacion-2-con-capas-de-pytorch","title":"5.3 Replicando la Ecuaci\u00f3n 2 con capas de PyTorch\u00b6","text":"<p>Pongamos en pr\u00e1ctica todo lo que hemos discutido sobre las capas LayerNorm (LN) y Multi-Head Attention (MSA) en la ecuaci\u00f3n 2.</p> <p>Para hacerlo, haremos:</p> <ol> <li>Cree una clase llamada <code>MultiheadSelfAttentionBlock</code> que herede de <code>torch.nn.Module</code>.</li> <li>Inicialice la clase con hiperpar\u00e1metros de la Tabla 1 del art\u00edculo de ViT para el modelo ViT-Base.</li> <li>Cree una capa de normalizaci\u00f3n de capa (LN) con <code>torch.nn.LayerNorm()</code> con el par\u00e1metro <code>normalized_shape</code> igual que nuestra dimensi\u00f3n de incrustaci\u00f3n ($D$ de la Tabla 1).</li> <li>Cree una capa de atenci\u00f3n de m\u00faltiples cabezas (MSA) con los par\u00e1metros apropiados <code>embed_dim</code>, <code>num_heads</code>, <code>dropout</code> y <code>batch_first</code>.</li> <li>Cree un m\u00e9todo <code>forward()</code> para nuestra clase pasando las entradas a trav\u00e9s de la capa LN y la capa MSA.</li> </ol>"},{"location":"12-08_pytorch_paper_replicating/#6-ecuacion-3-perceptron-multicapa-mlp","title":"6. Ecuaci\u00f3n 3: Perceptr\u00f3n multicapa (MLP)\u00b6","text":"<p>\u00a1Estamos en racha aqu\u00ed!</p> <p>Sigamos as\u00ed y repliquemos la ecuaci\u00f3n 3:</p> <p>$$ \\begin{alineado} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+ \\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\end{alineado} $$</p> <p>Aqu\u00ed MLP significa \"perceptr\u00f3n multicapa\" y LN significa \"normalizaci\u00f3n de capa\" (como se discuti\u00f3 anteriormente).</p> <p>Y la adici\u00f3n al final es la conexi\u00f3n de salto/residual.</p> <p>Nos referiremos a la ecuaci\u00f3n 3 como el \"bloque MLP\" del codificador Transformer (observe c\u00f3mo continuamos con la tendencia de dividir la arquitectura en partes m\u00e1s peque\u00f1as).</p> <p>***Izquierda:** Figura 1 del art\u00edculo de ViT con las capas MLP y Norm, as\u00ed como la conexi\u00f3n residual (+) resaltada dentro del bloque Transformer Encoder. Derecha: Mapeo de la capa de perceptr\u00f3n multicapa (MLP), la capa de norma (LN) y la conexi\u00f3n residual con sus respectivas partes de la ecuaci\u00f3n 3 en el art\u00edculo de ViT.*</p>"},{"location":"12-08_pytorch_paper_replicating/#61-las-capas-mlp","title":"6.1 Las capas MLP\u00b6","text":"<p>El t\u00e9rmino MLP es bastante amplio ya que puede referirse a casi cualquier combinaci\u00f3n de m\u00faltiples capas (de ah\u00ed el \"multi\" en perceptr\u00f3n multicapa).</p> <p>Pero generalmente sigue el patr\u00f3n de:</p> <p><code>capa lineal -&gt; capa no lineal -&gt; capa lineal -&gt; capa no lineal</code></p> <p>En el caso del art\u00edculo de ViT, la estructura de MLP se define en la secci\u00f3n 3.1:</p> <p>El MLP contiene dos capas con una no linealidad GELU.</p> <p>Donde \"dos capas\" se refiere a capas lineales (<code>torch.nn.Linear()</code> en PyTorch) y \"GELU no linealidad\" es la funci\u00f3n de activaci\u00f3n no lineal GELU (Unidades lineales de error gaussiano) ([<code>torch.nn.GELU()</code>](https://pytorch.org/docs/stable/generated/torch.nn.GELU .html) en PyTorch).</p> <p>Nota: Una capa lineal (<code>torch.nn.Linear()</code>) a veces tambi\u00e9n puede denominarse \"capa densa\" o \"capa de avance\". Algunos art\u00edculos incluso utilizan los tres t\u00e9rminos para describir lo mismo (como en el art\u00edculo de ViT).</p> <p>Otro detalle furtivo sobre el bloque MLP no aparece hasta el Ap\u00e9ndice B.1 (Capacitaci\u00f3n):</p> <p>La Tabla 3 resume nuestras configuraciones de entrenamiento para nuestros diferentes modelos. ...La eliminaci\u00f3n, cuando se usa, se aplica despu\u00e9s de cada capa densa excepto las proyecciones qkv y directamente despu\u00e9s de agregar incrustaciones posicionales a parches.</p> <p>Esto significa que cada capa lineal (o capa densa) en el bloque MLP tiene una capa de eliminaci\u00f3n ([<code>torch.nn.Dropout()</code>](https://pytorch.org/docs/stable/generated/torch.nn. Dropout.html) en PyTorch).</p> <p>cuyo valor se puede encontrar en la Tabla 3 del art\u00edculo de ViT (para ViT-Base, <code>dropout=0.1</code>).</p> <p>Sabiendo esto, la estructura de nuestro bloque MLP ser\u00e1:</p> <p><code>norma de capa -&gt; capa lineal -&gt; capa no lineal -&gt; abandono -&gt; capa lineal -&gt; abandono</code></p> <p>Con valores de hiperpar\u00e1metros para las capas lineales disponibles en la Tabla 1 (el tama\u00f1o de MLP es el n\u00famero de unidades ocultas entre las capas lineales y el tama\u00f1o oculto $D$ es el tama\u00f1o de salida del bloque MLP).</p>"},{"location":"12-08_pytorch_paper_replicating/#62-replicando-la-ecuacion-3-con-capas-de-pytorch","title":"6.2 Replicando la Ecuaci\u00f3n 3 con capas de PyTorch\u00b6","text":"<p>Pongamos en pr\u00e1ctica todo lo que hemos discutido sobre las capas LayerNorm (LN) y MLP (MSA) en la ecuaci\u00f3n 3.</p> <p>Para hacerlo, haremos:</p> <ol> <li>Cree una clase llamada <code>MLPBlock</code> que herede de <code>torch.nn.Module</code>.</li> <li>Inicialice la clase con hiperpar\u00e1metros de la Tabla 1 y la Tabla 3 del documento de ViT para el modelo ViT-Base.</li> <li>Cree una capa de normalizaci\u00f3n de capa (LN) con <code>torch.nn.LayerNorm()</code> con el par\u00e1metro <code>normalized_shape</code> igual que nuestra dimensi\u00f3n de incrustaci\u00f3n ($D$ de la Tabla 1).</li> <li>Cree una serie secuencial de capas MLP usando <code>torch.nn.Linear()</code>, <code>torch.nn.Dropout()</code> y <code>torch.nn.GELU()</code> con valores de hiperpar\u00e1metro apropiados de la Tabla 1 y Tabla 3.</li> <li>Cree un m\u00e9todo <code>forward()</code> para nuestra clase pasando las entradas a trav\u00e9s de la capa LN y las capas MLP.</li> </ol>"},{"location":"12-08_pytorch_paper_replicating/#7-cree-el-codificador-del-transformador","title":"7. Cree el codificador del transformador\u00b6","text":"<p>Es hora de apilar nuestro <code>MultiheadSelfAttentionBlock</code> (ecuaci\u00f3n 2) y <code>MLPBlock</code> (ecuaci\u00f3n 3) y crear el codificador Transformer de la arquitectura ViT.</p> <p>En el aprendizaje profundo, un \"codificador\" o \"codificador autom\u00e1tico\" generalmente se refiere a una pila de capas que \"codifica\" una entrada (la convierte en alguna forma de representaci\u00f3n num\u00e9rica). ).</p> <p>En nuestro caso, Transformer Encoder codificar\u00e1 nuestra imagen parcheada incrust\u00e1ndola en una representaci\u00f3n aprendida utilizando una serie de capas alternas de bloques MSA y bloques MLP, seg\u00fan la secci\u00f3n 3.1 del documento ViT:</p> <p>El codificador Transformer (Vaswani et al., 2017) consta de capas alternas de autoatenci\u00f3n multicabezal (MSA, ver Ap\u00e9ndice A) y bloques MLP (Ec. 2, 3). Layernorm (LN) se aplica antes de cada bloque y conexiones residuales despu\u00e9s de cada bloque (Wang et al., 2019; Baevski &amp; Auli, 2019).</p> <p>Hemos creado bloques MSA y MLP pero \u00bfqu\u00e9 pasa con las conexiones residuales?</p> <p>Las conexiones residuales (tambi\u00e9n llamadas conexiones de omisi\u00f3n), se introdujeron por primera vez en el art\u00edculo [Aprendizaje residual profundo para el reconocimiento de im\u00e1genes](https://arxiv.org /abs/1512.03385v1) y se logran agregando una(s) capa(s) de entrada a su salida posterior.</p> <p>Donde la salida de la subsecuencia podr\u00eda ser una o m\u00e1s capas posteriores.</p> <p>En el caso de la arquitectura ViT, la conexi\u00f3n residual significa que la entrada del bloque MSA se vuelve a agregar a la salida del bloque MSA antes de pasar al bloque MLP.</p> <p>Y lo mismo sucede con el bloque MLP antes de pasar al siguiente bloque Transformer Encoder.</p> <p>O en pseudoc\u00f3digo:</p> <p><code>x_input -&gt; MSA_block -&gt; [MSA_block_output + x_input] -&gt; MLP_block -&gt; [MLP_block_output + MSA_block_output + x_input] -&gt; ...</code></p> <p>\u00bfQu\u00e9 hace esto?</p> <p>Una de las ideas principales detr\u00e1s de las conexiones residuales es que evitan que los valores de peso y las actualizaciones de gradiente sean demasiado peque\u00f1os y, por lo tanto, permiten redes m\u00e1s profundas y, a su vez, permiten aprender representaciones m\u00e1s profundas.</p> <p>Nota: La ic\u00f3nica arquitectura de visi\u00f3n por computadora \"ResNet\" recibe su nombre debido a la introducci\u00f3n de conexiones residuales. Puede encontrar muchas versiones previamente entrenadas de arquitecturas ResNet en <code>torchvision.models</code>.</p>"},{"location":"12-08_pytorch_paper_replicating/#71-creacion-de-un-codificador-transformer-combinando-nuestras-capas-personalizadas","title":"7.1 Creaci\u00f3n de un codificador Transformer combinando nuestras capas personalizadas\u00b6","text":"<p>Basta de hablar, veamos esto en acci\u00f3n y hagamos un codificador ViT Transformer con PyTorch combinando nuestras capas creadas previamente.</p> <p>Para hacerlo, haremos:</p> <ol> <li>Cree una clase llamada <code>TransformerEncoderBlock</code> que herede de <code>torch.nn.Module</code>.</li> <li>Inicialice la clase con hiperpar\u00e1metros de la Tabla 1 y la Tabla 3 del documento de ViT para el modelo ViT-Base.</li> <li>Cree una instancia de un bloque MSA para la ecuaci\u00f3n 2 usando nuestro <code>MultiheadSelfAttentionBlock</code> de la secci\u00f3n 5.2 con los par\u00e1metros apropiados.</li> <li>Cree una instancia de un bloque MLP para la ecuaci\u00f3n 3 usando nuestro <code>MLPBlock</code> de la secci\u00f3n 6.2 con los par\u00e1metros apropiados.</li> <li>Cree un m\u00e9todo <code>forward()</code> para nuestra clase <code>TransformerEncoderBlock</code>.</li> <li>Cree una conexi\u00f3n residual para el bloque MSA (para la ecuaci\u00f3n 2).</li> <li>Cree una conexi\u00f3n residual para el bloque MLP (para la ecuaci\u00f3n 3).</li> </ol>"},{"location":"12-08_pytorch_paper_replicating/#72-creacion-de-un-codificador-transformer-con-las-capas-transformer-de-pytorch","title":"7.2 Creaci\u00f3n de un codificador Transformer con las capas Transformer de PyTorch\u00b6","text":"<p>Hasta ahora, hemos creado nosotros mismos los componentes y la capa Transformer Encoder.</p> <p>Pero debido a su aumento en popularidad y efectividad, PyTorch ahora tiene capas de transformadores incorporadas como parte de <code>torch.nn</code>.</p> <p>Por ejemplo, podemos recrear el <code>TransformerEncoderBlock</code> que acabamos de crear usando [<code>torch.nn.TransformerEncoderLayer()</code>](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch. nn.TransformerEncoderLayer) y configurando los mismos hiperpar\u00e1metros que arriba.</p>"},{"location":"12-08_pytorch_paper_replicating/#8-juntandolo-todo-para-crear-vit","title":"8. Junt\u00e1ndolo todo para crear ViT\u00b6","text":"<p>Est\u00e1 bien, est\u00e1 bien, \u00a1hemos recorrido un largo camino!</p> <p>Pero ahora es el momento de hacer lo emocionante de juntar todas las piezas del rompecabezas.</p> <p>Combinaremos todos los bloques que hemos creado para replicar la arquitectura ViT completa.</p> <p>Desde el parche y la incrustaci\u00f3n posicional hasta los codificadores Transformer y el cabezal MLP.</p> <p>Pero espera, a\u00fan no hemos creado la ecuaci\u00f3n 4...</p> <p>$$ \\begin{alineado} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{alineado} $$</p> <p>No se preocupe, podemos incluir la ecuaci\u00f3n 4 en nuestra clase de arquitectura ViT general.</p> <p>Todo lo que necesitamos es una capa <code>torch.nn.LayerNorm()</code> y una capa <code>torch.nn.Linear()</code> para convertir el \u00edndice 0 ($\\mathbf{z}_{L}^{0}$) de el logit del codificador del transformador genera el n\u00famero objetivo de clases que tenemos.</p> <p>Para crear la arquitectura completa, tambi\u00e9n necesitaremos apilar varios de nuestros <code>TransformerEncoderBlock</code> uno encima del otro, podemos hacer esto pasando una lista de ellos a <code>torch.nn.Sequential()</code> (esto hacer un rango secuencial de <code>TransformerEncoderBlock</code>s).</p> <p>Nos centraremos en los hiperpar\u00e1metros de ViT-Base de la Tabla 1, pero nuestro c\u00f3digo deber\u00eda poder adaptarse a otras variantes de ViT.</p> <p>Crear ViT ser\u00e1 nuestro mayor bloque de c\u00f3digo hasta el momento, \u00a1pero podemos hacerlo!</p> <p>Finalmente, para darle vida a nuestra propia implementaci\u00f3n de ViT, hagamos lo siguiente:</p> <ol> <li>Cree una clase llamada <code>ViT</code> que herede de <code>torch.nn.Module</code>.</li> <li>Inicialice la clase con hiperpar\u00e1metros de la Tabla 1 y la Tabla 3 del documento de ViT para el modelo ViT-Base.</li> <li>Aseg\u00farese de que el tama\u00f1o de la imagen sea divisible por el tama\u00f1o del parche (la imagen debe dividirse en parches pares).</li> <li>Calcule el n\u00famero de parches usando la f\u00f3rmula $N=H W / P^{2}$, donde $H$ es la altura de la imagen, $W$ es el ancho de la imagen y $P$ es el tama\u00f1o del parche.</li> <li>Cree un token de incrustaci\u00f3n de clase que se pueda aprender (ecuaci\u00f3n 1) como se hizo anteriormente en la secci\u00f3n 4.6.</li> <li>Cree un vector de incrustaci\u00f3n de posici\u00f3n que se pueda aprender (ecuaci\u00f3n 1) como se hizo anteriormente en la secci\u00f3n 4.7.</li> <li>Configure la capa de eliminaci\u00f3n de incrustaci\u00f3n como se explica en el Ap\u00e9ndice B.1 del documento de ViT.</li> <li>Cree la capa de incrustaci\u00f3n de parches utilizando la clase <code>PatchEmbedding</code> como se indica arriba en la secci\u00f3n 4.5.</li> <li>Cree una serie de bloques Transformer Encoder pasando una lista de <code>TransformerEncoderBlock</code> creados en la secci\u00f3n 7.1 a <code>torch.nn.Sequential()</code> (ecuaciones 2 y 3).</li> <li>Cree el encabezado MLP (tambi\u00e9n llamado encabezado clasificador o ecuaci\u00f3n 4) pasando una capa <code>torch.nn.LayerNorm()</code> (LN) y una capa <code>torch.nn.Linear(out_features=num_classes)</code> (donde <code>num_classes </code> es el n\u00famero objetivo de clases) capa lineal a <code>torch.nn.Sequential()</code>.</li> <li>Cree un m\u00e9todo <code>forward()</code> que acepte una entrada.</li> <li>Obtenga el tama\u00f1o del lote de la entrada (la primera dimensi\u00f3n de la forma).</li> <li>Cree la incrustaci\u00f3n de parche utilizando la capa creada en el paso 8 (ecuaci\u00f3n 1).</li> <li>Cree la incrustaci\u00f3n del token de clase usando la capa creada en el paso 5 y exp\u00e1ndala a trav\u00e9s del n\u00famero de lotes encontrados en el paso 11 usando [<code>torch.Tensor.expand()</code>](https://pytorch.org/docs/stable /generated/torch.Tensor.expand.html) (ecuaci\u00f3n 1).</li> <li>Concatene la incrustaci\u00f3n del token de clase creada en el paso 13 con la primera dimensi\u00f3n de la incrustaci\u00f3n del parche creada en el paso 12 usando [<code>torch.cat()</code>](https://pytorch.org/docs/stable/generated/torch. cat.html) (ecuaci\u00f3n 1).</li> <li>Agregue la incrustaci\u00f3n de posici\u00f3n creada en el paso 6 a la incrustaci\u00f3n de parche y token de clase creada en el paso 14 (ecuaci\u00f3n 1).</li> <li>Pase el parche y coloque la incrustaci\u00f3n a trav\u00e9s de la capa de eliminaci\u00f3n creada en el paso 7.</li> <li>Pase el parche y la posici\u00f3n incrustada del paso 16 a trav\u00e9s de la pila de capas de Transformer Encoder creada en el paso 9 (ecuaciones 2 y 3).</li> <li>Pase el \u00edndice 0 de la salida de la pila de capas de Transformer Encoder del paso 17 a trav\u00e9s del cabezal clasificador creado en el paso 10 (ecuaci\u00f3n 4).</li> <li>Baila y grita \u00a1\u00a1\u00a1woohoo!!! \u00a1Acabamos de construir un transformador de visi\u00f3n!</li> </ol> <p>\u00bfEst\u00e1s listo?</p> <p>Vamos.</p>"},{"location":"12-08_pytorch_paper_replicating/#81-obteniendo-un-resumen-visual-de-nuestro-modelo-vit","title":"8.1 Obteniendo un resumen visual de nuestro modelo ViT\u00b6","text":"<p>Creamos a mano nuestra propia versi\u00f3n de la arquitectura ViT y vimos que un tensor de imagen aleatorio puede fluir a trav\u00e9s de ella.</p> <p>\u00bfQu\u00e9 tal si usamos <code>torchinfo.summary()</code> para obtener una descripci\u00f3n visual de las formas de entrada y salida de todas las capas de nuestro modelo?</p> <p>Nota: El documento de ViT establece el uso de un tama\u00f1o de lote de 4096 para el entrenamiento; sin embargo, esto requiere una gran cantidad de memoria de procesamiento de CPU/GPU para manejarlo (cuanto mayor sea el tama\u00f1o del lote, m\u00e1s memoria se requerir\u00e1). Entonces, para asegurarnos de que no tengamos errores de memoria, nos quedaremos con un tama\u00f1o de lote de 32. Siempre puedes aumentar esto m\u00e1s adelante si tienes acceso a hardware con m\u00e1s memoria.</p>"},{"location":"12-08_pytorch_paper_replicating/#9-configurando-el-codigo-de-entrenamiento-para-nuestro-modelo-vit","title":"9. Configurando el c\u00f3digo de entrenamiento para nuestro modelo ViT\u00b6","text":"<p>Bueno, es hora de la parte f\u00e1cil.</p> <p>\u00a1Capacitaci\u00f3n!</p> <p>\u00bfPor qu\u00e9 f\u00e1cil?</p> <p>Porque tenemos la mayor parte de lo que necesitamos listo, desde nuestro modelo (<code>vit</code>) hasta nuestros DataLoaders (<code>train_dataloader</code>, <code>test_dataloader</code>) y las funciones de entrenamiento que creamos en 05. PyTorch Going Modular secci\u00f3n 4.</p> <p>Para entrenar nuestro modelo podemos importar la funci\u00f3n <code>train()</code> desde [<code>going_modular.going_modular.engine</code>](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/train .py).</p> <p>Todo lo que necesitamos es una funci\u00f3n de p\u00e9rdida y un optimizador.</p>"},{"location":"12-08_pytorch_paper_replicating/#91-creando-un-optimizador","title":"9.1 Creando un optimizador\u00b6","text":"<p>Buscando en el art\u00edculo de ViT \"optimizador\", secci\u00f3n 4.1 sobre estados de entrenamiento y ajuste:</p> <p>Entrenamiento y ajuste. Entrenamos todos los modelos, incluidos ResNets, usando Adam (Kingma &amp; Ba, 2015) con $\\beta_{1}=0.9, \\beta_{2}=0.999$, un tama\u00f1o de lote de 4096 y aplicar una ca\u00edda de peso alta de $0.1$, que encontramos \u00fatil para la transferencia de todos los modelos (el Ap\u00e9ndice D.1 muestra que, en contraste con las pr\u00e1cticas comunes, Adam funciona ligeramente mejor que SGD para ResNets en nuestro entorno).</p> <p>Entonces podemos ver que eligieron usar el optimizador \"Adam\" ([<code>torch.optim.Adam()</code>](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch. optim.Adam)) en lugar de SGD (descenso de gradiente estoc\u00e1stico, [<code>torch.optim.SGD()</code>](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim .SGD)).</p> <p>Los autores establecieron los valores $\\beta$ (beta) de Adam en $\\beta_{1}=0.9, \\beta_{2}=0.999$, estos son los valores predeterminados para el par\u00e1metro <code>betas</code> en <code>torch.optim.Adam( betas=(0.9, 0.999))</code>.</p> <p>Tambi\u00e9n indican el uso de [decaimiento de peso] (https://paperswithcode.com/method/weight-decay) (reduciendo lentamente los valores de los pesos durante la optimizaci\u00f3n para evitar el sobreajuste), podemos configurar esto con el par\u00e1metro <code>weight_decay</code> en <code>torch.optim.Adam(weight_decay=0.3)</code> (de acuerdo con la configuraci\u00f3n de ViT-* entrenado en ImageNet-1k).</p> <p>Estableceremos la tasa de aprendizaje del optimizador en 0,003 seg\u00fan la Tabla 3 (de acuerdo con la configuraci\u00f3n de ViT-* entrenado en ImageNet-1k).</p> <p>Y como se mencion\u00f3 anteriormente, usaremos un tama\u00f1o de lote menor que 4096 debido a limitaciones de hardware (si tiene una GPU grande, no dude en aumentarlo).</p>"},{"location":"12-08_pytorch_paper_replicating/#92-creando-una-funcion-de-perdida","title":"9.2 Creando una funci\u00f3n de p\u00e9rdida\u00b6","text":"<p>Curiosamente, buscar en el art\u00edculo de ViT \"p\u00e9rdida\" o \"funci\u00f3n de p\u00e9rdida\" o \"criterio\" no arroja resultados.</p> <p>Sin embargo, dado que el problema objetivo con el que estamos trabajando es la clasificaci\u00f3n de clases m\u00faltiples (lo mismo para el art\u00edculo de ViT), usaremos [<code>torch.nn.CrossEntropyLoss()</code>](https://pytorch.org/docs /stable/generated/torch.nn.CrossEntropyLoss.html).</p>"},{"location":"12-08_pytorch_paper_replicating/#93-entrenando-nuestro-modelo-vit","title":"9.3 Entrenando nuestro modelo ViT\u00b6","text":"<p>Bien, ahora que sabemos qu\u00e9 optimizador y funci\u00f3n de p\u00e9rdida vamos a usar, configuremos el c\u00f3digo de entrenamiento para entrenar nuestro ViT.</p> <p>Comenzaremos importando el script <code>engine.py</code> desde <code>going_modular.going_modular</code> luego configuraremos el optimizador y la funci\u00f3n de p\u00e9rdida y finalmente usaremos la funci\u00f3n <code>train()</code> de <code>engine.py</code> para entrenar nuestro modelo ViT para 10 \u00e9pocas (estamos usando una cantidad menor de \u00e9pocas que el documento ViT para asegurarnos de que todo funcione).</p>"},{"location":"12-08_pytorch_paper_replicating/#94-lo-que-le-falta-a-nuestra-configuracion-de-entrenamiento","title":"9.4 Lo que le falta a nuestra configuraci\u00f3n de entrenamiento\u00b6","text":"<p>La arquitectura ViT original logra buenos resultados en varios puntos de referencia de clasificaci\u00f3n de im\u00e1genes (a la par o mejores que muchos resultados de \u00faltima generaci\u00f3n cuando se lanz\u00f3).</p> <p>Sin embargo, nuestros resultados (hasta ahora) no son tan buenos.</p> <p>Hay varias razones por las que esto podr\u00eda deberse, pero la principal es la escala.</p> <p>El art\u00edculo original de ViT utiliza una cantidad mucho mayor de datos que el nuestro (en el aprendizaje profundo, generalmente siempre es bueno tener m\u00e1s datos) y un programa de entrenamiento m\u00e1s largo (consulte la Tabla 3).</p> Valor de hiperpar\u00e1metro Papel ViT Nuestra implementaci\u00f3n N\u00famero de im\u00e1genes de entrenamiento 1,3 millones (ImageNet-1k), 14 millones (ImageNet-21k), 303 millones (JFT) 225 \u00c9pocas 7 (para el conjunto de datos m\u00e1s grande), 90, 300 (para ImageNet) 10 Tama\u00f1o del lote 4096 32 Preparaci\u00f3n de la tasa de aprendizaje 10k pasos (Tabla 3) Ninguno [Decaimiento de la tasa de aprendizaje](https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a ,ayuda%20tanto%20optimizaci\u00f3n%20 como%20generalizaci\u00f3n.) Lineal/Coseno (Tabla 3) Ninguno Recorte de degradado Norma global 1 (Tabla 3) Ninguno <p>Aunque nuestra arquitectura ViT es la misma que la del art\u00edculo, los resultados del art\u00edculo ViT se lograron utilizando muchos m\u00e1s datos y un esquema de capacitaci\u00f3n m\u00e1s elaborado que el nuestro.</p> <p>Debido al tama\u00f1o de la arquitectura ViT y su gran n\u00famero de par\u00e1metros (mayores capacidades de aprendizaje) y la cantidad de datos que utiliza (mayores oportunidades de aprendizaje), muchas de las t\u00e9cnicas utilizadas en el esquema de capacitaci\u00f3n en papel de ViT, como el calentamiento de la tasa de aprendizaje, el aprendizaje La ca\u00edda de velocidad y el recorte de gradiente est\u00e1n dise\u00f1ados espec\u00edficamente para prevenir el sobreajuste (regularizaci\u00f3n).</p> <p>Nota: Para cualquier t\u00e9cnica de la que no est\u00e9 seguro, a menudo puede encontrar r\u00e1pidamente un ejemplo buscando \"NOMBRE DE LA T\u00c9CNICA de pytorch\", por ejemplo, digamos que desea aprender sobre el calentamiento de la velocidad de aprendizaje y lo que hace, podr\u00eda hacerlo. busque \"calentamiento de la tasa de aprendizaje de pytorch\".</p> <p>La buena noticia es que hay muchos modelos ViT previamente entrenados (que utilizan grandes cantidades de datos) disponibles en l\u00ednea; veremos uno en acci\u00f3n en la secci\u00f3n 10.</p>"},{"location":"12-08_pytorch_paper_replicating/#95-trazar-las-curvas-de-perdidas-de-nuestro-modelo-vit","title":"9.5 Trazar las curvas de p\u00e9rdidas de nuestro modelo ViT\u00b6","text":"<p>Hemos entrenado nuestro modelo ViT y hemos visto los resultados como n\u00fameros en una p\u00e1gina.</p> <p>Pero sigamos ahora el lema del explorador de datos de \u00a1visualizar, visualizar, visualizar!</p> <p>Y una de las mejores cosas que se pueden visualizar para un modelo son sus curvas de p\u00e9rdida.</p> <p>Para comprobar las curvas de p\u00e9rdida de nuestro modelo ViT, podemos usar la funci\u00f3n <code>plot_loss_curves</code> de <code>helper_functions.py</code> que creamos en [04. Secci\u00f3n 7.8 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0).</p>"},{"location":"12-08_pytorch_paper_replicating/#10-usando-un-vit-previamente-entrenado-de-torchvisionmodels-en-el-mismo-conjunto-de-datos","title":"10. Usando un ViT previamente entrenado de <code>torchvision.models</code> en el mismo conjunto de datos\u00b6","text":"<p>Hemos discutido los beneficios de usar modelos previamente entrenados en 06. Aprendizaje por transferencia de PyTorch.</p> <p>Pero dado que ahora entrenamos nuestro propio ViT desde cero y logramos resultados no \u00f3ptimos, los beneficios del aprendizaje por transferencia (usando un modelo previamente entrenado) realmente brillan.</p>"},{"location":"12-08_pytorch_paper_replicating/#101-por-que-utilizar-un-modelo-previamente-entrenado","title":"10.1 \u00bfPor qu\u00e9 utilizar un modelo previamente entrenado?\u00b6","text":"<p>Una nota importante en muchos art\u00edculos de investigaci\u00f3n sobre aprendizaje autom\u00e1tico moderno es que muchos de los resultados se obtienen con grandes conjuntos de datos y vastos recursos inform\u00e1ticos.</p> <p>Y en el aprendizaje autom\u00e1tico moderno, el ViT original totalmente entrenado probablemente no se considerar\u00eda una configuraci\u00f3n de entrenamiento \"s\u00faper grande\" (los modelos son cada vez m\u00e1s grandes).</p> <p>Leyendo la secci\u00f3n 4.2 del art\u00edculo de ViT:</p> <p>Finalmente, el modelo ViT-L/16 previamente entrenado en el conjunto de datos p\u00fablico ImageNet-21k tambi\u00e9n funciona bien en la mayor\u00eda de los conjuntos de datos, aunque requiere menos recursos para el entrenamiento previo: podr\u00eda entrenarse usando una nube est\u00e1ndar TPUv3 con 8 n\u00facleos en aproximadamente 30 dias.</p> <p>A partir de julio de 2022, el precio por alquilar un TPUv3 (Unidad de procesamiento Tensor versi\u00f3n 3) con 8 n\u00facleos en Google Cloud es de $8 USD por hora.</p> <p>Alquilar uno por 30 d\u00edas seguidos costar\u00eda $5,760 USD.</p> <p>Este costo (monetario y de tiempo) puede ser viable para algunos equipos de investigaci\u00f3n o empresas m\u00e1s grandes, pero para muchas personas no lo es.</p> <p>Por lo tanto, tener un modelo previamente entrenado disponible a trav\u00e9s de recursos como <code>torchvision.models</code>, la biblioteca [<code>timm</code> (Torch Image Models)](https:/ /github.com/rwightman/pytorch-image-models), el HuggingFace Hub o incluso de los propios autores de los art\u00edculos (hay una tendencia creciente entre los investigadores de aprendizaje autom\u00e1tico a publicar El c\u00f3digo y los modelos previamente entrenados de sus art\u00edculos de investigaci\u00f3n, soy un gran admirador de esta tendencia, muchos de estos recursos se pueden encontrar en Paperswithcode.com).</p> <p>Si est\u00e1 concentrado en aprovechar los beneficios de una arquitectura de modelo espec\u00edfica en lugar de crear su arquitectura personalizada, le recomiendo encarecidamente utilizar un modelo previamente entrenado.</p>"},{"location":"12-08_pytorch_paper_replicating/#102-obtener-un-modelo-vit-previamente-entrenado-y-crear-un-extractor-de-funciones","title":"10.2 Obtener un modelo ViT previamente entrenado y crear un extractor de funciones\u00b6","text":"<p>Podemos obtener un modelo ViT previamente entrenado desde <code>torchvision.models</code>.</p> <p>Empezaremos desde arriba asegur\u00e1ndonos primero de tener las versiones correctas de \"torch\" y \"torchvision\".</p> <p>Nota: El siguiente c\u00f3digo requiere <code>torch</code> v0.12+ y <code>torchvision</code> v0.13+ para utilizar la \u00faltima API de pesos de modelo <code>torchvision</code>.</p>"},{"location":"12-08_pytorch_paper_replicating/#103-preparacion-de-datos-para-el-modelo-vit-previamente-entrenado","title":"10.3 Preparaci\u00f3n de datos para el modelo ViT previamente entrenado\u00b6","text":"<p>Descargamos y creamos DataLoaders para nuestro propio modelo ViT en la secci\u00f3n 2.</p> <p>As\u00ed que no necesariamente necesitamos hacerlo de nuevo.</p> <p>Pero en nombre de la pr\u00e1ctica, descarguemos algunos datos de im\u00e1genes (im\u00e1genes de pizza, bistec y sushi para Food Vision Mini), configuremos directorios de tren y prueba y luego transformemos las im\u00e1genes en tensores y DataLoaders.</p> <p>Podemos descargar im\u00e1genes de pizza, bistec y sushi desde el curso GitHub y la funci\u00f3n <code>download_data()</code> que creamos en [07. Secci\u00f3n 1 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data).</p>"},{"location":"12-08_pytorch_paper_replicating/#104-modelo-vit-del-extractor-de-funciones-del-tren","title":"10.4 Modelo ViT del extractor de funciones del tren\u00b6","text":"<p>Modelo de extractor de funciones listo, DataLoaders listos, \u00a1es hora de entrenar!</p> <p>Como antes, usaremos el optimizador Adam (<code>torch.optim.Adam()</code>) con una tasa de aprendizaje de <code>1e-3</code> y <code>torch.nn.CrossEntropyLoss()</code> como funci\u00f3n de p\u00e9rdida.</p> <p>Nuestra funci\u00f3n <code>engine.train()</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 4 se encargar\u00e1 del resto.</p>"},{"location":"12-08_pytorch_paper_replicating/#105-trazar-curvas-de-perdida-del-modelo-vit-del-extractor-de-caracteristicas","title":"10.5 Trazar curvas de p\u00e9rdida del modelo ViT del extractor de caracter\u00edsticas\u00b6","text":"<p>Nuestros n\u00fameros de modelo de funciones ViT previamente entrenados se ven bien en los conjuntos de entrenamiento y prueba.</p> <p>\u00bfC\u00f3mo se ven las curvas de p\u00e9rdidas?</p>"},{"location":"12-08_pytorch_paper_replicating/#106-guardar-el-modelo-vit-del-extractor-de-funciones-y-verificar-el-tamano-del-archivo","title":"10.6 Guardar el modelo ViT del extractor de funciones y verificar el tama\u00f1o del archivo\u00b6","text":"<p>Parece que nuestro modelo de extractor de funciones ViT est\u00e1 funcionando bastante bien para nuestro problema Food Vision Mini.</p> <p>Quiz\u00e1s queramos intentar implementarlo y ver c\u00f3mo va en producci\u00f3n (en este caso, implementar significa poner nuestro modelo entrenado en una aplicaci\u00f3n que alguien podr\u00eda usar, por ejemplo, tomar fotograf\u00edas de comida con su tel\u00e9fono inteligente y ver si nuestro modelo piensa que es pizza). filete o sushi).</p> <p>Para hacerlo, primero podemos guardar nuestro modelo con la funci\u00f3n <code>utils.save_model()</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 5.</p>"},{"location":"12-08_pytorch_paper_replicating/#11-haz-predicciones-sobre-una-imagen-personalizada","title":"11. Haz predicciones sobre una imagen personalizada\u00b6","text":"<p>Y finalmente, terminaremos con la prueba definitiva, prediciendo sobre nuestros propios datos personalizados.</p> <p>Descarguemos la imagen del pap\u00e1 de la pizza (una foto de mi pap\u00e1 comiendo pizza) y usemos nuestro extractor de funciones ViT para predecirla.</p> <p>Para hacerlo, podemos usar la funci\u00f3n <code>pred_and_plot()</code> que creamos en [06. Secci\u00f3n 6 de PyTorch Transfer Learning] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set), por conveniencia, guard\u00e9 esta funci\u00f3n en <code>going_modular .going_modular.predictions.py</code> en el curso GitHub.</p>"},{"location":"12-08_pytorch_paper_replicating/#principales-conclusiones","title":"Principales conclusiones\u00b6","text":"<ul> <li>Con la explosi\u00f3n del aprendizaje autom\u00e1tico, todos los d\u00edas aparecen nuevos art\u00edculos de investigaci\u00f3n que detallan los avances. Y es imposible mantenerse al d\u00eda con todo, pero puede limitar las cosas a su propio caso de uso, como lo que hicimos aqu\u00ed, replicando un documento de visi\u00f3n por computadora para FoodVision Mini.</li> <li>Los art\u00edculos de investigaci\u00f3n sobre aprendizaje autom\u00e1tico a menudo contienen meses de investigaci\u00f3n realizados por equipos de personas inteligentes comprimidos en unas pocas p\u00e1ginas (por lo que desentra\u00f1ar todos los detalles y replicar el art\u00edculo en su totalidad puede ser un desaf\u00edo).</li> <li>El objetivo de la replicaci\u00f3n en papel es convertir art\u00edculos de investigaci\u00f3n sobre aprendizaje autom\u00e1tico (texto y matem\u00e1ticas) en c\u00f3digo utilizable.<ul> <li>Dicho esto, muchos equipos de investigaci\u00f3n de aprendizaje autom\u00e1tico est\u00e1n comenzando a publicar c\u00f3digo con sus art\u00edculos y uno de los mejores lugares para verlo es Paperswithcode.com.</li> </ul> </li> <li>Dividir un trabajo de investigaci\u00f3n sobre aprendizaje autom\u00e1tico en entradas y salidas (\u00bfqu\u00e9 entra y sale de cada capa/bloque/modelo?) y capas (\u00bfc\u00f3mo manipula cada capa la entrada?) y bloques (una colecci\u00f3n de capas) y replicar cada parte paso a paso (como lo hemos hecho en este cuaderno) puede ser muy \u00fatil para la comprensi\u00f3n.</li> <li>Hay modelos previamente entrenados disponibles para muchas arquitecturas de modelos de \u00faltima generaci\u00f3n y, con el poder del aprendizaje por transferencia, a menudo funcionan muy bien con pocos datos.</li> <li>Los modelos m\u00e1s grandes generalmente funcionan mejor, pero tambi\u00e9n ocupan m\u00e1s espacio (ocupan m\u00e1s espacio de almacenamiento y pueden tardar m\u00e1s en realizar inferencias).<ul> <li>Una gran pregunta es: en cuanto a la implementaci\u00f3n, \u00bfvale la pena o est\u00e1 alineado con el caso de uso el rendimiento adicional de un modelo m\u00e1s grande?</li> </ul> </li> </ul>"},{"location":"12-08_pytorch_paper_replicating/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Nota: Estos ejercicios requieren el uso de <code>torchvision</code> v0.13+ (lanzado en julio de 2022); las versiones anteriores pueden funcionar, pero probablemente tendr\u00e1n errores.</p> <p>Todos los ejercicios se centran en practicar el c\u00f3digo anterior.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Todos los ejercicios deben completarse utilizando c\u00f3digo independiente del dispositivo.</p> <p>Recursos:</p> <ul> <li>[Cuaderno de plantilla de ejercicios para 08] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/08_pytorch_paper_replicating_exercises.ipynb).</li> <li>Cuaderno de soluciones de ejemplo para 08 (pruebe los ejercicios antes de mirar esto).<ul> <li>Vea un [video tutorial de las soluciones en YouTube] en vivo (https://youtu.be/tjpW_BY8y3g) (errores y todo).</li> </ul> </li> </ul> <ol> <li>Replica la arquitectura ViT que creamos con [capas transformadoras de PyTorch] integradas (https://pytorch.org/docs/stable/nn.html#transformer-layers).<ul> <li>Querr\u00e1 considerar reemplazar nuestra clase <code>TransformerEncoderBlock()</code> con [<code>torch.nn.TransformerEncoderLayer()</code>](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html #torch.nn.TransformerEncoderLayer) (contienen las mismas capas que nuestros bloques personalizados).</li> <li>Puedes apilar <code>torch.nn.TransformerEncoderLayer()</code> uno encima del otro con [<code>torch.nn.TransformerEncoder()</code>](https://pytorch.org/docs/stable/generated/torch.nn .TransformerEncoder.html#torch.nn.TransformerEncoder).</li> </ul> </li> <li>Convierta la arquitectura ViT personalizada que creamos en un script de Python, por ejemplo, <code>vit.py</code>.<ul> <li>Deber\u00eda poder importar un modelo ViT completo usando algo como <code>from vit import ViT</code>.</li> </ul> </li> <li>Entrene un modelo de extracci\u00f3n de funciones de ViT previamente entrenado (como el que creamos en [08. PyTorch Paper Replicating secci\u00f3n 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-bring-in-pretrained-vit-from -torchvisionmodels-on-same-dataset)) en el 20% de los datos de pizza, bistec y sushi, como el conjunto de datos que utilizamos en [07. Secci\u00f3n 7.3 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#73-download- Different-datasets).<ul> <li>Vea c\u00f3mo funciona en comparaci\u00f3n con el modelo EffNetB2 con el que lo comparamos en [08. Secci\u00f3n 10.6 de replicaci\u00f3n de papel de PyTorch] (https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size).</li> </ul> </li> <li>Intente repetir los pasos del ejercicio 3, pero esta vez use los pesos previamente entrenados \"<code>ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</code>\" de [<code>torchvision.models.vit_b_16()</code>](https://pytorch.org/vision/stable/models/ generado/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16).<ul> <li>Nota: ViT previamente entrenado con pesos SWAG tiene un tama\u00f1o de imagen de entrada m\u00ednimo de <code>(384, 384)</code> (el ViT previamente entrenado en el ejercicio 3 tiene un tama\u00f1o de entrada m\u00ednimo de <code>(224, 224)</code>), aunque esto es accesible en el m\u00e9todo de pesos <code>.transforms()</code>.</li> </ul> </li> <li>Nuestra arquitectura de modelo ViT personalizada imita fielmente la del documento ViT; sin embargo, nuestra receta de capacitaci\u00f3n omite algunas cosas. Investigue algunos de los siguientes temas de la Tabla 3 en el art\u00edculo de ViT que nos perdimos y escriba una oraci\u00f3n sobre cada uno y c\u00f3mo podr\u00eda ayudar con la capacitaci\u00f3n: *Preentrenamiento de ImageNet-21k (m\u00e1s datos).<ul> <li>Calentamiento del ritmo de aprendizaje.</li> <li>Ca\u00edda de la tasa de aprendizaje.</li> <li>Recorte de degradado.</li> </ul> </li> </ol>"},{"location":"12-08_pytorch_paper_replicating/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>Ha habido varias iteraciones y ajustes en Vision Transformer desde su lanzamiento original y los m\u00e1s concisos y de mejor rendimiento (a julio de 2022) se pueden ver en [Mejores l\u00edneas base de ViT simples para ImageNet-1k](https:// arxiv.org/abs/2205.01580). A pesar de las actualizaciones, nos limitamos a replicar un \"Transformer Vanilla Vision\" en este port\u00e1til porque si comprendes la estructura del original, puedes conectar diferentes iteraciones.</li> <li>El repositorio<code>vit-pytorch</code> en GitHub de lucidrains es uno de los recursos m\u00e1s extensos de diferentes arquitecturas ViT implementadas en PyTorch. Es una referencia fenomenal y la utilic\u00e9 a menudo para crear los materiales que hemos analizado en este cap\u00edtulo.</li> <li>PyTorch tiene su [implementaci\u00f3n propia de la arquitectura ViT en GitHub] (https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py), se utiliza como base de los modelos ViT previamente entrenados. en <code>torchvision.modelos</code>.</li> <li>Jay Alammar tiene fant\u00e1sticas ilustraciones y explicaciones en su blog sobre el [mecanismo de atenci\u00f3n] (https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) ( la base de los modelos Transformer) y Modelos Transformer.</li> <li>Adrish Dey tiene un fant\u00e1stico resumen de Layer Normalization (un componente principal de la La arquitectura ViT) puede ayudar al entrenamiento de redes neuronales.</li> <li>El mecanismo de autoatenci\u00f3n (y autoatenci\u00f3n de m\u00faltiples cabezales) est\u00e1 en el coraz\u00f3n de la arquitectura ViT, as\u00ed como de muchas otras arquitecturas Transformer, y se introdujo originalmente en [La atenci\u00f3n es todo lo que necesita](https:/ /arxiv.org/abs/1706.03762) documento.</li> <li>El canal de YouTube de Yannic Kilcher es un recurso sensacional para recorridos visuales de art\u00edculos; puede ver sus videos de los siguientes art\u00edculos:<ul> <li>Atenci\u00f3n es todo lo que necesita (el documento que present\u00f3 la arquitectura Transformer).</li> <li>[Una imagen vale 16 x 16 palabras: transformadores para el reconocimiento de im\u00e1genes a escala] (https://youtu.be/TrdevFK_am4) (el art\u00edculo que present\u00f3 la arquitectura ViT).</li> </ul> </li> </ul>"},{"location":"13-09_pytorch_model_deployment/","title":"09. Implementaci\u00f3n del modelo PyTorch","text":"<p>Ver c\u00f3digo fuente | [Ver diapositivas] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf)</p> In\u00a0[\u00a0]: Copied! <pre># Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+.\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # Para que este port\u00e1til se ejecute con API actualizadas, necesitamos torch 1.12+ y torchvision 0.13+. try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <p>Nota: Si est\u00e1 utilizando Google Colab y la celda de arriba comienza a instalar varios paquetes de software, es posible que deba reiniciar su tiempo de ejecuci\u00f3n despu\u00e9s de ejecutar la celda de arriba. Despu\u00e9s de reiniciar, puede ejecutar la celda nuevamente y verificar que tenga las versiones correctas de <code>torch</code> y <code>torchvision</code>.</p> <p>Ahora continuaremos con las importaciones regulares, configurando el c\u00f3digo independiente del dispositivo y esta vez tambi\u00e9n obtendremos [<code>helper_functions.py</code>](https://github.com/mrdbourke/pytorch-deep-learning/blob/ main/helper_functions.py) script de GitHub.</p> <p>El script <code>helper_functions.py</code> contiene varias funciones que creamos en secciones anteriores:</p> <ul> <li><code>set_seeds()</code> para configurar las semillas aleatorias (creadas en 07. Secci\u00f3n 0 de seguimiento de experimentos de PyTorch).</li> <li><code>download_data()</code> para descargar una fuente de datos mediante un enlace (creado en [07. Secci\u00f3n 1 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).</li> <li><code>plot_loss_curves()</code> para inspeccionar los resultados del entrenamiento de nuestro modelo (creado en [04. PyTorch Custom Datasets secci\u00f3n 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of- modelo-0))</li> </ul> <p>Nota: Puede ser una mejor idea que muchas de las funciones en el script <code>helper_functions.py</code> se fusionen en <code>going_modular/going_modular/utils.py</code>, tal vez sea una extensi\u00f3n que le gustar\u00eda probar .</p> In\u00a0[\u00a0]: Copied! <pre># Continuar con las importaciones regulares\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Intente obtener torchinfo, inst\u00e1lelo si no funciona\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continuar con las importaciones regulares import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Intente obtener torchinfo, inst\u00e1lelo si no funciona try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Intente importar el directorio going_modular, desc\u00e1rguelo de GitHub si no funciona try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <p>Finalmente, configuraremos un c\u00f3digo independiente del dispositivo para asegurarnos de que nuestros modelos se ejecuten en la GPU.</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device In\u00a0[\u00a0]: Copied! <pre># Descargue im\u00e1genes de pizza, bistec y sushi desde GitHub\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\ndata_20_percent_path\n</pre> # Descargue im\u00e1genes de pizza, bistec y sushi desde GitHub data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\")  data_20_percent_path <p>\u00a1Maravilloso!</p> <p>Ahora que tenemos un conjunto de datos, creemos rutas de entrenamiento y prueba.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar rutas de directorio para entrenar y probar im\u00e1genes\ntrain_dir = data_20_percent_path / \"train\"\ntest_dir = data_20_percent_path / \"test\"\n</pre> # Configurar rutas de directorio para entrenar y probar im\u00e1genes train_dir = data_20_percent_path / \"train\" test_dir = data_20_percent_path / \"test\" In\u00a0[\u00a0]: Copied! <pre># 1. Configurar pesas EffNetB2 previamente entrenadas\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n\n# 2. Obtenga transformaciones EffNetB2\neffnetb2_transforms = effnetb2_weights.transforms()\n\n# 3. Configurar el modelo previamente entrenado\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n\n# 4. Congele las capas base en el modelo (esto congelar\u00e1 todas las capas para empezar)\nfor param in effnetb2.parameters():\n    param.requires_grad = False\n</pre> # 1. Configurar pesas EffNetB2 previamente entrenadas effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # 2. Obtenga transformaciones EffNetB2 effnetb2_transforms = effnetb2_weights.transforms()  # 3. Configurar el modelo previamente entrenado effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"  # 4. Congele las capas base en el modelo (esto congelar\u00e1 todas las capas para empezar) for param in effnetb2.parameters():     param.requires_grad = False <p>Ahora, para cambiar el encabezado del clasificador, primero inspeccion\u00e9moslo usando el atributo \"clasificador\" de nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre># Consulte el cabezal clasificador EffNetB2\neffnetb2.classifier\n</pre> # Consulte el cabezal clasificador EffNetB2 effnetb2.classifier <p>\u00a1Excelente! Para cambiar el encabezado del clasificador para adaptarlo a nuestro propio problema, reemplacemos la variable <code>out_features</code> con el mismo n\u00famero de clases que tenemos (en nuestro caso, <code>out_features=3</code>, una para pizza, bistec, sushi).</p> <p>Nota: Este proceso de cambiar las capas de salida/cabezal clasificador depender\u00e1 del problema en el que est\u00e9 trabajando. Por ejemplo, si quisiera un n\u00famero diferente de salidas o un tipo diferente de salida, tendr\u00eda que cambiar las capas de salida en consecuencia.</p> In\u00a0[\u00a0]: Copied! <pre># 5. Actualice el cabezal del clasificador.\neffnetb2.classifier = nn.Sequential(\n    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n    nn.Linear(in_features=1408, # keep in_features same \n              out_features=3)) # change out_features to suit our number of classes\n</pre> # 5. Actualice el cabezal del clasificador. effnetb2.classifier = nn.Sequential(     nn.Dropout(p=0.3, inplace=True), # keep dropout layer same     nn.Linear(in_features=1408, # keep in_features same                out_features=3)) # change out_features to suit our number of classes <p>\u00a1Hermoso!</p> In\u00a0[\u00a0]: Copied! <pre>def create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 4. Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 5. Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # 4. Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # 5. Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <p>\u00a1Guau! Es una funci\u00f3n muy bonita, prob\u00e9mosla.</p> In\u00a0[\u00a0]: Copied! <pre>effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n                                                      seed=42)\n</pre> effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,                                                       seed=42) <p>Sin errores, genial, ahora para probarlo realmente, obtengamos un resumen con <code>torchinfo.summary()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from torchinfo import summary\n\n# # Imprimir resumen del modelo EffNetB2 (descomentar para obtener un resultado completo)\n# resumen(effnetb2,\n# tama\u00f1o_entrada=(1, 3, 224, 224),\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Imprimir resumen del modelo EffNetB2 (descomentar para obtener un resultado completo) # resumen(effnetb2, # tama\u00f1o_entrada=(1, 3, 224, 224), # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"]) <p>\u00a1Capas base congeladas, capas superiores entrenables y personalizadas!</p> In\u00a0[\u00a0]: Copied! <pre># Configurar cargadores de datos\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                 test_dir=test_dir,\n                                                                                                 transform=effnetb2_transforms,\n                                                                                                 batch_size=32)\n</pre> # Configurar cargadores de datos from going_modular.going_modular import data_setup train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                  test_dir=test_dir,                                                                                                  transform=effnetb2_transforms,                                                                                                  batch_size=32) In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Optimizador de configuraci\u00f3n\noptimizer = torch.optim.Adam(params=effnetb2.parameters(),\n                             lr=1e-3)\n# Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Establezca semillas para la reproducibilidad y entrene el modelo.\nset_seeds()\neffnetb2_results = engine.train(model=effnetb2,\n                                train_dataloader=train_dataloader_effnetb2,\n                                test_dataloader=test_dataloader_effnetb2,\n                                epochs=10,\n                                optimizer=optimizer,\n                                loss_fn=loss_fn,\n                                device=device)\n</pre> from going_modular.going_modular import engine  # Optimizador de configuraci\u00f3n optimizer = torch.optim.Adam(params=effnetb2.parameters(),                              lr=1e-3) # Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n loss_fn = torch.nn.CrossEntropyLoss()  # Establezca semillas para la reproducibilidad y entrene el modelo. set_seeds() effnetb2_results = engine.train(model=effnetb2,                                 train_dataloader=train_dataloader_effnetb2,                                 test_dataloader=test_dataloader_effnetb2,                                 epochs=10,                                 optimizer=optimizer,                                 loss_fn=loss_fn,                                 device=device) In\u00a0[\u00a0]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(effnetb2_results) <p>\u00a1Guau!</p> <p>Esas son algunas curvas de p\u00e9rdidas bonitas.</p> <p>Parece que nuestro modelo est\u00e1 funcionando bastante bien y tal vez se beneficiar\u00eda de un entrenamiento un poco m\u00e1s largo y potencialmente de algo de [aumento de datos](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data -aumento) (para ayudar a prevenir un posible sobreajuste que se produzca debido a un entrenamiento m\u00e1s prolongado).</p> In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import utils\n\n# guardar el modelo\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n</pre> from going_modular.going_modular import utils  # guardar el modelo utils.save_model(model=effnetb2,                  target_dir=\"models\",                  model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\") In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes\npretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")\n</pre> from pathlib import Path  # Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\") In\u00a0[\u00a0]: Copied! <pre># Cuente el n\u00famero de par\u00e1metros en EffNetB2\neffnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\neffnetb2_total_params\n</pre> # Cuente el n\u00famero de par\u00e1metros en EffNetB2 effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters()) effnetb2_total_params <p>\u00a1Excelente!</p> <p>Ahora pongamos todo en un diccionario para poder hacer comparaciones m\u00e1s adelante.</p> In\u00a0[\u00a0]: Copied! <pre># Crear un diccionario con estad\u00edsticas de EffNetB2\neffnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n                  \"number_of_parameters\": effnetb2_total_params,\n                  \"model_size (MB)\": pretrained_effnetb2_model_size}\neffnetb2_stats\n</pre> # Crear un diccionario con estad\u00edsticas de EffNetB2 effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],                   \"test_acc\": effnetb2_results[\"test_acc\"][-1],                   \"number_of_parameters\": effnetb2_total_params,                   \"model_size (MB)\": pretrained_effnetb2_model_size} effnetb2_stats <p>\u00a1\u00c9pico!</p> <p>\u00a1Parece que nuestro modelo EffNetB2 funciona con m\u00e1s del 95% de precisi\u00f3n!</p> <p>Criterio n\u00famero 1: actuar con una precisi\u00f3n superior al 95%, \u00a1marca!</p> In\u00a0[\u00a0]: Copied! <pre># Consulte la capa de cabezales ViT\nvit = torchvision.models.vit_b_16()\nvit.heads\n</pre> # Consulte la capa de cabezales ViT vit = torchvision.models.vit_b_16() vit.heads <p>Sabiendo esto, tenemos todas las piezas del rompecabezas que necesitamos.</p> In\u00a0[\u00a0]: Copied! <pre>def create_vit_model(num_classes:int=3, \n                     seed:int=42):\n    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of target classes. Defaults to 3.\n        seed (int, optional): random seed value for output layer. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): ViT-B/16 feature extractor model. \n        transforms (torchvision.transforms): ViT-B/16 image transforms.\n    \"\"\"\n    # Create ViT_B_16 pretrained weights, transforms and model\n    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.vit_b_16(weights=weights)\n\n    # Freeze all layers in model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head to suit our needs (this will be trainable)\n    torch.manual_seed(seed)\n    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n                                          out_features=num_classes)) # update to reflect target number of classes\n    \n    return model, transforms\n</pre> def create_vit_model(num_classes:int=3,                       seed:int=42):     \"\"\"Creates a ViT-B/16 feature extractor model and transforms.      Args:         num_classes (int, optional): number of target classes. Defaults to 3.         seed (int, optional): random seed value for output layer. Defaults to 42.      Returns:         model (torch.nn.Module): ViT-B/16 feature extractor model.          transforms (torchvision.transforms): ViT-B/16 image transforms.     \"\"\"     # Create ViT_B_16 pretrained weights, transforms and model     weights = torchvision.models.ViT_B_16_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.vit_b_16(weights=weights)      # Freeze all layers in model     for param in model.parameters():         param.requires_grad = False      # Change classifier head to suit our needs (this will be trainable)     torch.manual_seed(seed)     model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model                                           out_features=num_classes)) # update to reflect target number of classes          return model, transforms <p>\u00a1La funci\u00f3n de creaci\u00f3n de modelos de extracci\u00f3n de caracter\u00edsticas ViT est\u00e1 lista!</p> <p>Prob\u00e9moslo.</p> In\u00a0[\u00a0]: Copied! <pre># Crear modelo ViT y transformaciones.\nvit, vit_transforms = create_vit_model(num_classes=3,\n                                       seed=42)\n</pre> # Crear modelo ViT y transformaciones. vit, vit_transforms = create_vit_model(num_classes=3,                                        seed=42) <p>Sin errores, \u00a1es encantador verlo!</p> <p>Ahora obtengamos un resumen atractivo de nuestro modelo ViT usando <code>torchinfo.summary()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from torchinfo import summary\n\n# # Imprimir resumen del modelo del extractor de funciones de ViT (descomentar para obtener un resultado completo)\n# resumen(vit,\n# tama\u00f1o_entrada=(1, 3, 224, 224),\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Imprimir resumen del modelo del extractor de funciones de ViT (descomentar para obtener un resultado completo) # resumen(vit, # tama\u00f1o_entrada=(1, 3, 224, 224), # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"]) <p></p> <p>Al igual que nuestro modelo de extracci\u00f3n de funciones EffNetB2, las capas base de nuestro modelo ViT est\u00e1n congeladas y la capa de salida se personaliza seg\u00fan nuestras necesidades.</p> <p>\u00bfNotas la gran diferencia?</p> <p>Nuestro modelo ViT tiene muchos m\u00e1s par\u00e1metros que nuestro modelo EffNetB2. Quiz\u00e1s esto entre en juego cuando comparemos nuestros modelos en cuanto a velocidad y rendimiento m\u00e1s adelante.</p> In\u00a0[\u00a0]: Copied! <pre># Configurar cargadores de datos ViT\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                       test_dir=test_dir,\n                                                                                       transform=vit_transforms,\n                                                                                       batch_size=32)\n</pre> # Configurar cargadores de datos ViT from going_modular.going_modular import data_setup train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                        test_dir=test_dir,                                                                                        transform=vit_transforms,                                                                                        batch_size=32) In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Optimizador de configuraci\u00f3n\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=1e-3)\n# Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Entrene el modelo ViT con semillas configuradas para lograr reproducibilidad\nset_seeds()\nvit_results = engine.train(model=vit,\n                           train_dataloader=train_dataloader_vit,\n                           test_dataloader=test_dataloader_vit,\n                           epochs=10,\n                           optimizer=optimizer,\n                           loss_fn=loss_fn,\n                           device=device)\n</pre> from going_modular.going_modular import engine  # Optimizador de configuraci\u00f3n optimizer = torch.optim.Adam(params=vit.parameters(),                              lr=1e-3) # Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n loss_fn = torch.nn.CrossEntropyLoss()  # Entrene el modelo ViT con semillas configuradas para lograr reproducibilidad set_seeds() vit_results = engine.train(model=vit,                            train_dataloader=train_dataloader_vit,                            test_dataloader=test_dataloader_vit,                            epochs=10,                            optimizer=optimizer,                            loss_fn=loss_fn,                            device=device) In\u00a0[\u00a0]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(vit_results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(vit_results) <p>\u00a1Ohh si!</p> <p>Esas son algunas curvas de p\u00e9rdidas bonitas. Al igual que nuestro modelo de extracci\u00f3n de funciones EffNetB2, parece que nuestro modelo ViT podr\u00eda beneficiarse de un tiempo de entrenamiento un poco m\u00e1s largo y tal vez algo de [aumento de datos](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of- transforma-datos-aumento) (para ayudar a prevenir el sobreajuste).</p> In\u00a0[\u00a0]: Copied! <pre># guardar el modelo\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=vit,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n</pre> # guardar el modelo from going_modular.going_modular import utils  utils.save_model(model=vit,                  target_dir=\"models\",                  model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\") In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes\npretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <p>Hmm, \u00bfc\u00f3mo se compara el tama\u00f1o del modelo del extractor de funciones ViT con el tama\u00f1o de nuestro modelo EffNetB2?</p> <p>Lo descubriremos en breve cuando comparemos todas las caracter\u00edsticas de nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre># Contar el n\u00famero de par\u00e1metros en ViT\nvit_total_params = sum(torch.numel(param) for param in vit.parameters())\nvit_total_params\n</pre> # Contar el n\u00famero de par\u00e1metros en ViT vit_total_params = sum(torch.numel(param) for param in vit.parameters()) vit_total_params <p>\u00a1Vaya, eso parece bastante m\u00e1s que nuestro EffNetB2!</p> <p>Nota: Una mayor cantidad de par\u00e1metros (o pesos/patrones) generalmente significa que un modelo tiene una mayor capacidad de aprender; si realmente utiliza esta capacidad adicional es otra historia. A la luz de esto, nuestro modelo EffNetB2 tiene 7.705.221 par\u00e1metros, mientras que nuestro modelo ViT tiene 85.800.963 (11,1 veces m\u00e1s), por lo que podr\u00edamos suponer que nuestro modelo ViT tiene m\u00e1s capacidad de aprender, si se le dan m\u00e1s datos (m\u00e1s oportunidades de aprender). Sin embargo, esta mayor capacidad de aprender a menudo viene acompa\u00f1ada de un mayor tama\u00f1o del modelo y un mayor tiempo para realizar la inferencia.</p> <p>Ahora creemos un diccionario con algunas caracter\u00edsticas importantes de nuestro modelo ViT.</p> In\u00a0[\u00a0]: Copied! <pre># Crear diccionario de estad\u00edsticas de ViT\nvit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n             \"test_acc\": vit_results[\"test_acc\"][-1],\n             \"number_of_parameters\": vit_total_params,\n             \"model_size (MB)\": pretrained_vit_model_size}\n\nvit_stats\n</pre> # Crear diccionario de estad\u00edsticas de ViT vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],              \"test_acc\": vit_results[\"test_acc\"][-1],              \"number_of_parameters\": vit_total_params,              \"model_size (MB)\": pretrained_vit_model_size}  vit_stats <p>\u00a1Lindo! Parece que nuestro modelo ViT tambi\u00e9n logra una precisi\u00f3n superior al 95%.</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Obtenga todas las rutas de datos de prueba\nprint(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\ntest_data_paths[:5]\n</pre> from pathlib import Path  # Obtenga todas las rutas de datos de prueba print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\") test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\")) test_data_paths[:5] In\u00a0[\u00a0]: Copied! <pre>import pathlib\nimport torch\n\nfrom PIL import Image\nfrom timeit import default_timer as timer \nfrom tqdm.auto import tqdm\nfrom typing import List, Dict\n\n# 1. Cree una funci\u00f3n para devolver una lista de diccionarios con muestra, etiqueta de verdad, predicci\u00f3n, probabilidad de predicci\u00f3n y tiempo de predicci\u00f3n.\ndef pred_and_store(paths: List[pathlib.Path], \n                   model: torch.nn.Module,\n                   transform: torchvision.transforms, \n                   class_names: List[str], \n                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:\n    \n    # 2. Create an empty list to store prediction dictionaires\n    pred_list = []\n    \n    # 3. Loop through target paths\n    for path in tqdm(paths):\n        \n        # 4. Create empty dictionary to store prediction information for each sample\n        pred_dict = {}\n\n        # 5. Get the sample path and ground truth class name\n        pred_dict[\"image_path\"] = path\n        class_name = path.parent.stem\n        pred_dict[\"class_name\"] = class_name\n        \n        # 6. Start the prediction timer\n        start_time = timer()\n        \n        # 7. Open image path\n        img = Image.open(path)\n        \n        # 8. Transform the image, add batch dimension and put image on target device\n        transformed_image = transform(img).unsqueeze(0).to(device) \n        \n        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n        model.to(device)\n        model.eval()\n        \n        # 10. Get prediction probability, predicition label and prediction class\n        with torch.inference_mode():\n            pred_logit = model(transformed_image) # perform inference on target sample \n            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n\n            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n            pred_dict[\"pred_class\"] = pred_class\n            \n            # 12. End the timer and calculate time per pred\n            end_time = timer()\n            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n\n        # 13. Does the pred match the true label?\n        pred_dict[\"correct\"] = class_name == pred_class\n\n        # 14. Add the dictionary to the list of preds\n        pred_list.append(pred_dict)\n    \n    # 15. Return list of prediction dictionaries\n    return pred_list\n</pre> import pathlib import torch  from PIL import Image from timeit import default_timer as timer  from tqdm.auto import tqdm from typing import List, Dict  # 1. Cree una funci\u00f3n para devolver una lista de diccionarios con muestra, etiqueta de verdad, predicci\u00f3n, probabilidad de predicci\u00f3n y tiempo de predicci\u00f3n. def pred_and_store(paths: List[pathlib.Path],                     model: torch.nn.Module,                    transform: torchvision.transforms,                     class_names: List[str],                     device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:          # 2. Create an empty list to store prediction dictionaires     pred_list = []          # 3. Loop through target paths     for path in tqdm(paths):                  # 4. Create empty dictionary to store prediction information for each sample         pred_dict = {}          # 5. Get the sample path and ground truth class name         pred_dict[\"image_path\"] = path         class_name = path.parent.stem         pred_dict[\"class_name\"] = class_name                  # 6. Start the prediction timer         start_time = timer()                  # 7. Open image path         img = Image.open(path)                  # 8. Transform the image, add batch dimension and put image on target device         transformed_image = transform(img).unsqueeze(0).to(device)                   # 9. Prepare model for inference by sending it to target device and turning on eval() mode         model.to(device)         model.eval()                  # 10. Get prediction probability, predicition label and prediction class         with torch.inference_mode():             pred_logit = model(transformed_image) # perform inference on target sample              pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities             pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label             pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU              # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)              pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)             pred_dict[\"pred_class\"] = pred_class                          # 12. End the timer and calculate time per pred             end_time = timer()             pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)          # 13. Does the pred match the true label?         pred_dict[\"correct\"] = class_name == pred_class          # 14. Add the dictionary to the list of preds         pred_list.append(pred_dict)          # 15. Return list of prediction dictionaries     return pred_list <p>\u00a1Ho, ho!</p> <p>\u00a1Qu\u00e9 funci\u00f3n tan atractiva!</p> <p>Y sabes qu\u00e9, dado que nuestro <code>pred_and_store()</code> es una funci\u00f3n de utilidad bastante buena para hacer y almacenar predicciones, podr\u00eda almacenarse en [<code>going_modular.going_modular.predictions.py</code>](https://github.com/mrdbourke /pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) para su uso posterior. Podr\u00eda ser una extensi\u00f3n que le gustar\u00eda probar; consulte 05. PyTorch Going Modular para obtener ideas.</p> In\u00a0[\u00a0]: Copied! <pre># Haga predicciones en todo el conjunto de datos de prueba con EffNetB2\neffnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                          model=effnetb2,\n                                          transform=effnetb2_transforms,\n                                          class_names=class_names,\n                                          device=\"cpu\") # make predictions on CPU\n</pre> # Haga predicciones en todo el conjunto de datos de prueba con EffNetB2 effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,                                           model=effnetb2,                                           transform=effnetb2_transforms,                                           class_names=class_names,                                           device=\"cpu\") # make predictions on CPU  <p>\u00a1Lindo! \u00a1Mira esas predicciones volar!</p> <p>Inspeccionemos la primera pareja y veamos c\u00f3mo se ven.</p> In\u00a0[\u00a0]: Copied! <pre># Inspeccione los primeros 2 diccionarios de predicci\u00f3n.\neffnetb2_test_pred_dicts[:2]\n</pre> # Inspeccione los primeros 2 diccionarios de predicci\u00f3n. effnetb2_test_pred_dicts[:2] <p>\u00a1Guau!</p> <p>Parece que nuestra funci\u00f3n <code>pred_and_store()</code> funcion\u00f3 bien.</p> <p>Gracias a nuestra lista de estructura de datos de diccionarios, tenemos mucha informaci\u00f3n \u00fatil que podemos inspeccionar m\u00e1s a fondo.</p> <p>Para hacerlo, convierta nuestra lista de diccionarios en un DataFrame de pandas.</p> In\u00a0[\u00a0]: Copied! <pre># Convierta test_pred_dicts en un DataFrame\nimport pandas as pd\neffnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\neffnetb2_test_pred_df.head()\n</pre> # Convierta test_pred_dicts en un DataFrame import pandas as pd effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts) effnetb2_test_pred_df.head() <p>\u00a1Hermoso!</p> <p>Mire con qu\u00e9 facilidad esos diccionarios de predicci\u00f3n se convierten en un formato estructurado sobre el que podemos realizar an\u00e1lisis.</p> <p>Como encontrar cu\u00e1ntas predicciones nuestro modelo EffNetB2 se equivoc\u00f3...</p> In\u00a0[\u00a0]: Copied! <pre># Comprobar n\u00famero de predicciones correctas\neffnetb2_test_pred_df.correct.value_counts()\n</pre> # Comprobar n\u00famero de predicciones correctas effnetb2_test_pred_df.correct.value_counts() <p>Cinco predicciones err\u00f3neas de un total de 150, \u00a1nada mal!</p> <p>\u00bfY qu\u00e9 tal el tiempo medio de predicci\u00f3n?</p> In\u00a0[\u00a0]: Copied! <pre># Encuentre el tiempo promedio por predicci\u00f3n\neffnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")\n</pre> # Encuentre el tiempo promedio por predicci\u00f3n effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4) print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\") <p>Mmmm, \u00bfc\u00f3mo cumple ese tiempo promedio de predicci\u00f3n con nuestros criterios de rendimiento de nuestro modelo en tiempo real (~30 FPS o 0,03 segundos por predicci\u00f3n)?</p> <p>Nota: Los tiempos de predicci\u00f3n ser\u00e1n diferentes seg\u00fan los distintos tipos de hardware (por ejemplo, una CPU Intel i9 local frente a una CPU Google Colab). Cuanto mejor y m\u00e1s r\u00e1pido sea el hardware, generalmente, m\u00e1s r\u00e1pida ser\u00e1 la predicci\u00f3n. Por ejemplo, en mi PC local de aprendizaje profundo con un chip Intel i9, mi tiempo promedio de predicci\u00f3n con EffNetB2 es de alrededor de 0,031 segundos (un poco menos que el tiempo real). Sin embargo, en Google Colab (no estoy seguro de qu\u00e9 hardware de CPU utiliza Colab, pero parece que podr\u00eda ser un [Intel(R) Xeon(R)](https://stackoverflow.com/questions/47805170/whats-the -hardware-spec-for-google-colaboratory)), mi tiempo promedio de predicci\u00f3n con EffNetB2 es de aproximadamente 0,1396 segundos (3-4 veces m\u00e1s lento).</p> <p>Agreguemos nuestro tiempo promedio por predicci\u00f3n de EffNetB2 a nuestro diccionario <code>effnetb2_stats</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Agregue el tiempo de predicci\u00f3n promedio de EffNetB2 al diccionario de estad\u00edsticas\neffnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\neffnetb2_stats\n</pre> # Agregue el tiempo de predicci\u00f3n promedio de EffNetB2 al diccionario de estad\u00edsticas effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred effnetb2_stats In\u00a0[\u00a0]: Copied! <pre># Haga una lista de diccionarios de predicci\u00f3n con el modelo de extracci\u00f3n de funciones de ViT en im\u00e1genes de prueba\nvit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                     model=vit,\n                                     transform=vit_transforms,\n                                     class_names=class_names,\n                                     device=\"cpu\")\n</pre> # Haga una lista de diccionarios de predicci\u00f3n con el modelo de extracci\u00f3n de funciones de ViT en im\u00e1genes de prueba vit_test_pred_dicts = pred_and_store(paths=test_data_paths,                                      model=vit,                                      transform=vit_transforms,                                      class_names=class_names,                                      device=\"cpu\") <p>\u00a1Predicciones hechas!</p> <p>Ahora echemos un vistazo a la primera pareja.</p> In\u00a0[\u00a0]: Copied! <pre># Verifique las primeras predicciones de ViT en el conjunto de datos de prueba\nvit_test_pred_dicts[:2]\n</pre> # Verifique las primeras predicciones de ViT en el conjunto de datos de prueba vit_test_pred_dicts[:2] <p>\u00a1Maravilloso!</p> <p>Y al igual que antes, dado que las predicciones de nuestro modelo ViT tienen la forma de una lista de diccionarios, podemos convertirlas f\u00e1cilmente en un DataFrame de pandas para una inspecci\u00f3n m\u00e1s detallada.</p> In\u00a0[\u00a0]: Copied! <pre># Convierta vit_test_pred_dicts en un DataFrame\nimport pandas as pd\nvit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\nvit_test_pred_df.head()\n</pre> # Convierta vit_test_pred_dicts en un DataFrame import pandas as pd vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts) vit_test_pred_df.head() <p>\u00bfCu\u00e1ntas predicciones acert\u00f3 nuestro modelo ViT?</p> In\u00a0[\u00a0]: Copied! <pre># Cuente el n\u00famero de predicciones correctas.\nvit_test_pred_df.correct.value_counts()\n</pre> # Cuente el n\u00famero de predicciones correctas. vit_test_pred_df.correct.value_counts() <p>\u00a1Guau!</p> <p>Nuestro modelo ViT funcion\u00f3 un poco mejor que nuestro modelo EffNetB2 en t\u00e9rminos de predicciones correctas, solo dos muestras incorrectas en todo el conjunto de datos de prueba.</p> <p>Como extensi\u00f3n, es posible que desee visualizar las predicciones incorrectas del modelo ViT y ver si hay alguna raz\u00f3n por la cual podr\u00edan haberse equivocado.</p> <p>\u00bfQu\u00e9 tal si calculamos cu\u00e1nto tiempo tard\u00f3 el modelo ViT en realizar cada predicci\u00f3n?</p> In\u00a0[\u00a0]: Copied! <pre># Calcular el tiempo promedio por predicci\u00f3n para el modelo ViT\nvit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")\n</pre> # Calcular el tiempo promedio por predicci\u00f3n para el modelo ViT vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4) print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\") <p>Bueno, eso parece un poco m\u00e1s lento que el tiempo promedio por predicci\u00f3n de nuestro modelo EffNetB2, pero \u00bfc\u00f3mo se ve en t\u00e9rminos de nuestro segundo criterio: velocidad?</p> <p>Por ahora, agreguemos el valor a nuestro diccionario <code>vit_stats</code> para que podamos compararlo con las estad\u00edsticas de nuestro modelo EffNetB2.</p> <p>Nota: El tiempo promedio por valor de predicci\u00f3n depender\u00e1 en gran medida del hardware en el que los realice. Por ejemplo, para el modelo ViT, mi tiempo promedio por predicci\u00f3n (en la CPU) fue de 0,0693 a 0,0777 segundos en mi PC local de aprendizaje profundo con una CPU Intel i9. Mientras que en Google Colab, mi tiempo promedio por predicci\u00f3n con el modelo ViT fue de 0,6766 a 0,7113 segundos.</p> In\u00a0[\u00a0]: Copied! <pre># Agregue el tiempo de predicci\u00f3n promedio para el modelo ViT en la CPU\nvit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\nvit_stats\n</pre> # Agregue el tiempo de predicci\u00f3n promedio para el modelo ViT en la CPU vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred vit_stats In\u00a0[\u00a0]: Copied! <pre># Convierta los diccionarios de estad\u00edsticas en DataFrame\ndf = pd.DataFrame([effnetb2_stats, vit_stats])\n\n# Agregar columna para nombres de modelos\ndf[\"model\"] = [\"EffNetB2\", \"ViT\"]\n\n# Convertir precisi\u00f3n a porcentajes\ndf[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n\ndf\n</pre> # Convierta los diccionarios de estad\u00edsticas en DataFrame df = pd.DataFrame([effnetb2_stats, vit_stats])  # Agregar columna para nombres de modelos df[\"model\"] = [\"EffNetB2\", \"ViT\"]  # Convertir precisi\u00f3n a porcentajes df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)  df <p>\u00a1Maravilloso!</p> <p>Parece que nuestros modelos son bastante parecidos en t\u00e9rminos de precisi\u00f3n general de las pruebas, pero \u00bfc\u00f3mo se ven en otros campos?</p> <p>Una forma de averiguarlo ser\u00eda dividir las estad\u00edsticas del modelo ViT por las estad\u00edsticas del modelo EffNetB2 para descubrir las diferentes proporciones entre los modelos.</p> <p>Creemos otro DataFrame para hacerlo.</p> In\u00a0[\u00a0]: Copied! <pre># Compare ViT con EffNetB2 seg\u00fan diferentes caracter\u00edsticas\npd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n             columns=[\"ViT to EffNetB2 ratios\"]).T\n</pre> # Compare ViT con EffNetB2 seg\u00fan diferentes caracter\u00edsticas pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics              columns=[\"ViT to EffNetB2 ratios\"]).T <p>Parece que nuestro modelo ViT supera al modelo EffNetB2 en todas las m\u00e9tricas de rendimiento (p\u00e9rdida de prueba, donde menor es mejor y precisi\u00f3n de la prueba, donde mayor es mejor), pero a expensas de tener:</p> <ul> <li>11x+ el n\u00famero de par\u00e1metros.</li> <li>11x+ el tama\u00f1o del modelo.</li> <li>2,5 veces m\u00e1s el tiempo de predicci\u00f3n por imagen.</li> </ul> <p>\u00bfValen la pena estas compensaciones?</p> <p>Quiz\u00e1s si tuvi\u00e9ramos una potencia inform\u00e1tica ilimitada, pero para nuestro caso de uso de implementar el modelo FoodVision Mini en un dispositivo m\u00e1s peque\u00f1o (por ejemplo, un tel\u00e9fono m\u00f3vil), probablemente comenzar\u00edamos con el modelo EffNetB2 para realizar predicciones m\u00e1s r\u00e1pidas con un rendimiento ligeramente reducido pero dram\u00e1ticamente m\u00e1s peque\u00f1o. tama\u00f1o.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Cree un gr\u00e1fico a partir del marco de datos de comparaci\u00f3n de modelos.\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(data=df, \n                     x=\"time_per_pred_cpu\", \n                     y=\"test_acc\", \n                     c=[\"blue\", \"orange\"], # what colours to use?\n                     s=\"model_size (MB)\") # size the dots by the model sizes\n\n# 2. Agregue t\u00edtulos, etiquetas y personalice el tama\u00f1o de fuente por motivos est\u00e9ticos.\nax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\nax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\nax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\nax.tick_params(axis='both', labelsize=12)\nax.grid(True)\n\n# 3. Anotar con nombres de modelos\nfor index, row in df.iterrows():\n    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n                size=12)\n\n# 4. Crea una leyenda basada en los tama\u00f1os del modelo.\nhandles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\nmodel_size_legend = ax.legend(handles, \n                              labels, \n                              loc=\"lower right\", \n                              title=\"Model size (MB)\",\n                              fontsize=12)\n\n# guarda la figura\n!mdkir images/\nplt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n\n# mostrar la figura\nplt.show()\n</pre> # 1. Cree un gr\u00e1fico a partir del marco de datos de comparaci\u00f3n de modelos. fig, ax = plt.subplots(figsize=(12, 8)) scatter = ax.scatter(data=df,                       x=\"time_per_pred_cpu\",                       y=\"test_acc\",                       c=[\"blue\", \"orange\"], # what colours to use?                      s=\"model_size (MB)\") # size the dots by the model sizes  # 2. Agregue t\u00edtulos, etiquetas y personalice el tama\u00f1o de fuente por motivos est\u00e9ticos. ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18) ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14) ax.set_ylabel(\"Test accuracy (%)\", fontsize=14) ax.tick_params(axis='both', labelsize=12) ax.grid(True)  # 3. Anotar con nombres de modelos for index, row in df.iterrows():     ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270                  xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),                 size=12)  # 4. Crea una leyenda basada en los tama\u00f1os del modelo. handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5) model_size_legend = ax.legend(handles,                                labels,                                loc=\"lower right\",                                title=\"Model size (MB)\",                               fontsize=12)  # guarda la figura !mdkir images/ plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")  # mostrar la figura plt.show() <p>\u00a1Guau!</p> <p>La gr\u00e1fica realmente visualiza la velocidad versus rendimiento; en otras palabras, cuando tienes un modelo profundo m\u00e1s grande y de mejor rendimiento (como nuestro modelo ViT), generalmente lleva m\u00e1s tiempo realizar la inferencia (mayor latencia).</p> <p>Hay excepciones a la regla y constantemente se publican nuevas investigaciones para ayudar a que los modelos m\u00e1s grandes funcionen m\u00e1s r\u00e1pido.</p> <p>Y puede resultar tentador simplemente implementar el modelo de mejor rendimiento, pero tambi\u00e9n es bueno tener en cuenta d\u00f3nde funcionar\u00e1 el modelo.</p> <p>En nuestro caso, las diferencias entre los niveles de rendimiento de nuestro modelo (en la p\u00e9rdida de prueba y la precisi\u00f3n de la prueba) no son demasiado extremas.</p> <p>Pero como para empezar nos gustar\u00eda poner \u00e9nfasis en la velocidad, seguiremos implementando EffNetB2 ya que es m\u00e1s r\u00e1pido y ocupa mucho menos espacio.</p> <p>Nota: Los tiempos de predicci\u00f3n ser\u00e1n diferentes seg\u00fan los diferentes tipos de hardware (por ejemplo, Intel i9 frente a CPU de Google Colab frente a GPU), por lo que es importante pensar y probar d\u00f3nde terminar\u00e1 su modelo. Hacer preguntas como \"\u00bfd\u00f3nde se ejecutar\u00e1 el modelo?\" o \"\u00bfcu\u00e1l es el escenario ideal para ejecutar el modelo?\" y luego realizar experimentos para intentar proporcionar respuestas en el camino hacia la implementaci\u00f3n es muy \u00fatil.</p> In\u00a0[\u00a0]: Copied! <pre># Importar/instalar Gradio\ntry:\n    import gradio as gr\nexcept: \n    !pip -q install gradio\n    import gradio as gr\n    \nprint(f\"Gradio version: {gr.__version__}\")\n</pre> # Importar/instalar Gradio try:     import gradio as gr except:      !pip -q install gradio     import gradio as gr      print(f\"Gradio version: {gr.__version__}\") <p>\u00a1Gradio listo!</p> <p>Convirtamos FoodVision Mini en una aplicaci\u00f3n de demostraci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># Ponga EffNetB2 en la CPU\neffnetb2.to(\"cpu\") \n\n# Verifique el dispositivo\nnext(iter(effnetb2.parameters())).device\n</pre> # Ponga EffNetB2 en la CPU effnetb2.to(\"cpu\")   # Verifique el dispositivo next(iter(effnetb2.parameters())).device <p>Y ahora creemos una funci\u00f3n llamada <code>predict()</code> para replicar el flujo de trabajo anterior.</p> In\u00a0[\u00a0]: Copied! <pre>from typing import Tuple, Dict\n\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n</pre> from typing import Tuple, Dict  def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time <p>\u00a1Hermoso!</p> <p>Ahora veamos nuestra funci\u00f3n en acci\u00f3n realizando una predicci\u00f3n en una imagen aleatoria del conjunto de datos de prueba.</p> <p>Comenzaremos obteniendo una lista de todas las rutas de im\u00e1genes del directorio de prueba y luego seleccionaremos una al azar.</p> <p>Luego abriremos la imagen seleccionada al azar con <code>PIL.Image.open()</code>.</p> <p>Finalmente, pasaremos la imagen a nuestra funci\u00f3n <code>predict()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import random\nfrom PIL import Image\n\n# Obtenga una lista de todas las rutas de archivos de im\u00e1genes de prueba\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n\n# Seleccione aleatoriamente una ruta de imagen de prueba\nrandom_image_path = random.sample(test_data_paths, k=1)[0]\n\n# Abra la imagen de destino\nimage = Image.open(random_image_path)\nprint(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n\n# Predecir sobre la imagen de destino e imprimir los resultados.\npred_dict, pred_time = predict(img=image)\nprint(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\nprint(f\"Prediction time: {pred_time} seconds\")\n</pre> import random from PIL import Image  # Obtenga una lista de todas las rutas de archivos de im\u00e1genes de prueba test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))  # Seleccione aleatoriamente una ruta de imagen de prueba random_image_path = random.sample(test_data_paths, k=1)[0]  # Abra la imagen de destino image = Image.open(random_image_path) print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")  # Predecir sobre la imagen de destino e imprimir los resultados. pred_dict, pred_time = predict(img=image) print(f\"Prediction label and probability dictionary: \\n{pred_dict}\") print(f\"Prediction time: {pred_time} seconds\") <p>\u00a1Lindo!</p> <p>Al ejecutar la celda de arriba varias veces, podemos ver diferentes probabilidades de predicci\u00f3n para cada etiqueta de nuestro modelo EffNetB2, as\u00ed como el tiempo que tom\u00f3 cada predicci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre># Cree una lista de entradas de ejemplo para nuestra demostraci\u00f3n de Gradio\nexample_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\nexample_list\n</pre> # Cree una lista de entradas de ejemplo para nuestra demostraci\u00f3n de Gradio example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)] example_list <p>\u00a1Perfecto!</p> <p>Nuestra demostraci\u00f3n de Gradio los mostrar\u00e1 como entradas de ejemplo para nuestra demostraci\u00f3n para que las personas puedan probarlo y ver qu\u00e9 hace sin cargar ninguno de sus propios datos.</p> In\u00a0[\u00a0]: Copied! <pre>import gradio as gr\n\n# Crear cadenas de t\u00edtulo, descripci\u00f3n y art\u00edculo.\ntitle = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Crea la demostraci\u00f3n de Gradio\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# \u00a1Lanza la demostraci\u00f3n!\ndemo.launch(debug=False, # print errors locally?\n            share=True) # generate a publically shareable URL?\n</pre> import gradio as gr  # Crear cadenas de t\u00edtulo, descripci\u00f3n y art\u00edculo. title = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Crea la demostraci\u00f3n de Gradio demo = gr.Interface(fn=predict, # mapping function from input to output                     inputs=gr.Image(type=\"pil\"), # what are the inputs?                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?                              gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs                     examples=example_list,                      title=title,                     description=description,                     article=article)  # \u00a1Lanza la demostraci\u00f3n! demo.launch(debug=False, # print errors locally?             share=True) # generate a publically shareable URL? <p>Demo de FoodVision Mini Gradio ejecut\u00e1ndose en Google Colab y en el navegador (el enlace cuando se ejecuta desde Google Colab solo dura 72 horas). Puedes ver la demo permanente en vivo en Hugging Face Spaces.</p> <p>\u00a1\u00a1\u00a1Guau!!! \u00a1\u00a1\u00a1Qu\u00e9 demostraci\u00f3n tan \u00e9pica!!!</p> <p>FoodVision Mini ha cobrado vida oficialmente en una interfaz que alguien podr\u00eda usar y probar.</p> <p>Si configura el par\u00e1metro <code>share=True</code> en el m\u00e9todo <code>launch()</code>, Gradio tambi\u00e9n le proporciona un enlace para compartir como <code>https://123XYZ.gradio.app</code> (este enlace es solo un ejemplo y probablemente est\u00e9 vencido). ) que es v\u00e1lido por 72 horas.</p> <p>El enlace proporciona un proxy a la interfaz de Gradio que inici\u00f3.</p> <p>Para un alojamiento m\u00e1s permanente, puede cargar su aplicaci\u00f3n Gradio en Hugging Face Spaces o en cualquier lugar que ejecute c\u00f3digo Python.</p> In\u00a0[\u00a0]: Copied! <pre>import shutil\nfrom pathlib import Path\n\n# Crear una mini ruta de demostraci\u00f3n de FoodVision\nfoodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n\n# Elimine los archivos que ya puedan existir all\u00ed y cree un nuevo directorio\nif foodvision_mini_demo_path.exists():\n    shutil.rmtree(foodvision_mini_demo_path)\n    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n                                    exist_ok=True) # create it even if it already exists?\nelse:\n    # If the file doesn't exist, create it anyway\n    foodvision_mini_demo_path.mkdir(parents=True, \n                                    exist_ok=True)\n    \n# Comprueba lo que hay en la carpeta.\n!ls demos/foodvision_mini/\n</pre> import shutil from pathlib import Path  # Crear una mini ruta de demostraci\u00f3n de FoodVision foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")  # Elimine los archivos que ya puedan existir all\u00ed y cree un nuevo directorio if foodvision_mini_demo_path.exists():     shutil.rmtree(foodvision_mini_demo_path)     foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?                                     exist_ok=True) # create it even if it already exists? else:     # If the file doesn't exist, create it anyway     foodvision_mini_demo_path.mkdir(parents=True,                                      exist_ok=True)      # Comprueba lo que hay en la carpeta. !ls demos/foodvision_mini/ In\u00a0[\u00a0]: Copied! <pre>import shutil\nfrom pathlib import Path\n\n# 1. Cree un directorio de ejemplos\nfoodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\nfoodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n\n# 2. Recopile tres rutas de im\u00e1genes de conjuntos de datos de prueba aleatorias\nfoodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n\n# 3. Copie las tres im\u00e1genes aleatorias al directorio de ejemplos.\nfor example in foodvision_mini_examples:\n    destination = foodvision_mini_examples_path / example.name\n    print(f\"[INFO] Copying {example} to {destination}\")\n    shutil.copy2(src=example, dst=destination)\n</pre> import shutil from pathlib import Path  # 1. Cree un directorio de ejemplos foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\" foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)  # 2. Recopile tres rutas de im\u00e1genes de conjuntos de datos de prueba aleatorias foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),                             Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),                             Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]  # 3. Copie las tres im\u00e1genes aleatorias al directorio de ejemplos. for example in foodvision_mini_examples:     destination = foodvision_mini_examples_path / example.name     print(f\"[INFO] Copying {example} to {destination}\")     shutil.copy2(src=example, dst=destination) <p>Ahora, para verificar que nuestros ejemplos est\u00e9n presentes, enumeremos el contenido de nuestro directorio <code>demos/foodvision_mini/examples/</code> con [<code>os.listdir()</code>](https://docs.python.org/3/library/os. html#os.listdir) y luego formatee las rutas de archivo en una lista de listas (para que sea compatible con el par\u00e1metro <code>example</code> <code>gradio.Interface()</code> de Gradio) .</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Obtenga rutas de archivos de ejemplo en una lista de listas\nexample_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\nexample_list\n</pre> import os  # Obtenga rutas de archivos de ejemplo en una lista de listas example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)] example_list In\u00a0[\u00a0]: Copied! <pre>import shutil\n\n# Crear una ruta de origen para nuestro modelo de destino.\neffnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n\n# Crear una ruta de destino para nuestro modelo objetivo.\neffnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n\n# Intenta mover el archivo\ntry:\n    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n    \n    # Move the model\n    shutil.move(src=effnetb2_foodvision_mini_model_path, \n                dst=effnetb2_foodvision_mini_model_destination)\n    \n    print(f\"[INFO] Model move complete.\")\n\n# Si el modelo ya ha sido movido, verifique si existe.\nexcept:\n    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n</pre> import shutil  # Crear una ruta de origen para nuestro modelo de destino. effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"  # Crear una ruta de destino para nuestro modelo objetivo. effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]  # Intenta mover el archivo try:     print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")          # Move the model     shutil.move(src=effnetb2_foodvision_mini_model_path,                  dst=effnetb2_foodvision_mini_model_destination)          print(f\"[INFO] Model move complete.\")  # Si el modelo ya ha sido movido, verifique si existe. except:     print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")     print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\") In\u00a0[\u00a0]: Copied! <pre>%%writefile demos/foodvision_mini/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> %%writefile demos/foodvision_mini/model.py import torch import torchvision  from torch import nn   def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms In\u00a0[\u00a0]: Copied! <pre>%%writefile demos/foodvision_mini/app.py\n# ## 1. Configuraci\u00f3n de importaciones y nombres de clases ###\nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Configurar nombres de clases\nclass_names = [\"pizza\", \"steak\", \"sushi\"]\n\n# ## 2. Preparaci\u00f3n del modelo y transforma ###\n\n# Crear modelo EffNetB2\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=3, # len(class_names) would also work\n)\n\n# Cargar pesos guardados\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n# ## 3. Funci\u00f3n de predicci\u00f3n ###\n\n# Crear funci\u00f3n de predicci\u00f3n\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n# ## 4. Aplicaci\u00f3n Gradio ###\n\n# Crear cadenas de t\u00edtulo, descripci\u00f3n y art\u00edculo.\ntitle = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Crear una lista de ejemplos desde el directorio \"examples/\"\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Crea la demostraci\u00f3n de Gradio\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    # Create examples list from \"examples/\" directory\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# \u00a1Lanza la demostraci\u00f3n!\ndemo.launch()\n</pre> %%writefile demos/foodvision_mini/app.py # ## 1. Configuraci\u00f3n de importaciones y nombres de clases ### import gradio as gr import os import torch  from model import create_effnetb2_model from timeit import default_timer as timer from typing import Tuple, Dict  # Configurar nombres de clases class_names = [\"pizza\", \"steak\", \"sushi\"]  # ## 2. Preparaci\u00f3n del modelo y transforma ###  # Crear modelo EffNetB2 effnetb2, effnetb2_transforms = create_effnetb2_model(     num_classes=3, # len(class_names) would also work )  # Cargar pesos guardados effnetb2.load_state_dict(     torch.load(         f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",         map_location=torch.device(\"cpu\"),  # load to CPU     ) )  # ## 3. Funci\u00f3n de predicci\u00f3n ###  # Crear funci\u00f3n de predicci\u00f3n def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time  # ## 4. Aplicaci\u00f3n Gradio ###  # Crear cadenas de t\u00edtulo, descripci\u00f3n y art\u00edculo. title = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Crear una lista de ejemplos desde el directorio \"examples/\" example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]  # Crea la demostraci\u00f3n de Gradio demo = gr.Interface(fn=predict, # mapping function from input to output                     inputs=gr.Image(type=\"pil\"), # what are the inputs?                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?                              gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs                     # Create examples list from \"examples/\" directory                     examples=example_list,                      title=title,                     description=description,                     article=article)  # \u00a1Lanza la demostraci\u00f3n! demo.launch() In\u00a0[\u00a0]: Copied! <pre>%%writefile demos/foodvision_mini/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n</pre> %%writefile demos/foodvision_mini/requirements.txt torch==1.12.0 torchvision==0.13.0 gradio==3.1.4 <p>\u00a1Lindo!</p> <p>\u00a1Tenemos oficialmente todos los archivos que necesitamos para implementar nuestra demostraci\u00f3n de FoodVision Mini!</p> In\u00a0[\u00a0]: Copied! <pre>!ls demos/foodvision_mini\n</pre> !ls demos/foodvision_mini <p>\u00a1Estos son todos los archivos que hemos creado!</p> <p>Para comenzar a cargar nuestros archivos en Hugging Face, descargu\u00e9moslos ahora desde Google Colab (o dondequiera que est\u00e9 ejecutando este cuaderno).</p> <p>Para hacerlo, primero comprimiremos los archivos en una \u00fanica carpeta zip mediante el comando:</p> <pre><code>zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n</code></pre> <p>D\u00f3nde:</p> <ul> <li><code>zip</code> significa \"zip\", como en \"comprima los archivos en el siguiente directorio\".</li> <li><code>-r</code> significa \"recursivo\", como en \"revisar todos los archivos en el directorio de destino\".</li> <li><code>../foodvision_mini.zip</code> es el directorio de destino donde nos gustar\u00eda comprimir nuestros archivos.</li> <li><code>*</code> significa \"todos los archivos en el directorio actual\".</li> <li><code>-x</code> significa \"excluir estos archivos\".</li> </ul> <p>Podemos descargar nuestro archivo zip de Google Colab usando <code>google.colab.files.download(\"demos/foodvision_mini.zip\")</code> ( Pondremos esto dentro de un bloque <code>try</code> y <code>except</code> en caso de que no estemos ejecutando el c\u00f3digo dentro de Google Colab y, de ser as\u00ed, imprimiremos un mensaje que indicar\u00e1 que descarguemos los archivos manualmente).</p> <p>\u00a1Prob\u00e9moslo!</p> In\u00a0[\u00a0]: Copied! <pre># Cambie y luego comprima la carpeta foodvision_mini pero excluya ciertos archivos\n!cd demos/foodvision_mini &amp;&amp; zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Descargue la aplicaci\u00f3n FoodVision Mini comprimida (si se ejecuta en Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_mini.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")\n</pre> # Cambie y luego comprima la carpeta foodvision_mini pero excluya ciertos archivos !cd demos/foodvision_mini &amp;&amp; zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"  # Descargue la aplicaci\u00f3n FoodVision Mini comprimida (si se ejecuta en Google Colab) try:     from google.colab import files     files.download(\"demos/foodvision_mini.zip\") except:     print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\") <p>\u00a1Guau!</p> <p>Parece que nuestro comando <code>zip</code> fue exitoso.</p> <p>Si est\u00e1 ejecutando este cuaderno en Google Colab, deber\u00eda ver que un archivo comienza a descargarse en su navegador.</p> <p>De lo contrario, puede ver la carpeta <code>foodvision_mini.zip</code> (y m\u00e1s) en el [curso GitHub en el directorio <code>demos/</code>](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/ poblaci\u00f3n).</p> In\u00a0[\u00a0]: Copied! <pre># IPython es una biblioteca para ayudar a que Python sea interactivo\nfrom IPython.display import IFrame\n\n# Incrustar la demostraci\u00f3n de FoodVision Mini Gradio\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)\n</pre> # IPython es una biblioteca para ayudar a que Python sea interactivo from IPython.display import IFrame  # Incrustar la demostraci\u00f3n de FoodVision Mini Gradio IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750) In\u00a0[\u00a0]: Copied! <pre># Cree un modelo EffNetB2 capaz de adaptarse a 101 clases para Food101\neffnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n</pre> # Cree un modelo EffNetB2 capaz de adaptarse a 101 clases para Food101 effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101) <p>\u00a1Hermoso!</p> <p>Ahora obtengamos un resumen de nuestro modelo.</p> In\u00a0[\u00a0]: Copied! <pre>from torchinfo import summary\n\n# # Obtenga un resumen del extractor de funciones EffNetB2 para Food101 con 101 clases de salida (descomentar para obtener una salida completa)\n# resumen(effnetb2_food101,\n# tama\u00f1o_entrada=(1, 3, 224, 224),\n# col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"],\n# ancho_columna=20,\n# row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Obtenga un resumen del extractor de funciones EffNetB2 para Food101 con 101 clases de salida (descomentar para obtener una salida completa) # resumen(effnetb2_food101, # tama\u00f1o_entrada=(1, 3, 224, 224), # col_names=[\"input_size\", \"output_size\", \"num_params\", \"entrenable\"], # ancho_columna=20, # row_settings=[\"var_names\"]) <p> \u00a1Lindo!</p> <p>Vea c\u00f3mo, al igual que nuestro modelo EffNetB2 para FoodVision Mini, las capas base est\u00e1n congeladas (\u00e9stas est\u00e1n previamente entrenadas en ImageNet) y las capas externas (las capas <code>clasificadoras</code>) se pueden entrenar con una forma de salida de <code>[batch_size, 101]</code> (<code>101 </code> para 101 clases en Food101).</p> <p>Ahora que vamos a tratar con bastante m\u00e1s datos de lo habitual, \u00bfqu\u00e9 tal si agregamos un poco de aumento de datos a nuestras transformaciones (<code>effnetb2_transforms</code>) para aumentar los datos de entrenamiento?</p> <p>Nota: El aumento de datos es una t\u00e9cnica que se utiliza para alterar la apariencia de una muestra de entrenamiento de entrada (por ejemplo, rotar una imagen o sesgarla ligeramente) para aumentar artificialmente la diversidad de un conjunto de datos de entrenamiento y, con suerte, evitar el sobreajuste. Puede ver m\u00e1s sobre el aumento de datos en [04. Secci\u00f3n 6 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation).</p> <p>Compongamos una canalizaci\u00f3n <code>torchvision.transforms</code> para usar <code>torchvision.transforms.TrivialAugmentWide()</code> (el mismo aumento de datos utilizado por el equipo de PyTorch en sus [recetas de visi\u00f3n por computadora] (https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break- mejoras de precisi\u00f3n de clave)) as\u00ed como <code>effnetb2_transforms</code> para transformar nuestras im\u00e1genes de entrenamiento.</p> In\u00a0[\u00a0]: Copied! <pre># Cree transformaciones de datos de entrenamiento de Food101 (realice solo aumento de datos en las im\u00e1genes de entrenamiento)\nfood101_train_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.TrivialAugmentWide(),\n    effnetb2_transforms,\n])\n</pre> # Cree transformaciones de datos de entrenamiento de Food101 (realice solo aumento de datos en las im\u00e1genes de entrenamiento) food101_train_transforms = torchvision.transforms.Compose([     torchvision.transforms.TrivialAugmentWide(),     effnetb2_transforms, ]) <p>\u00a1\u00c9pico!</p> <p>Ahora comparemos <code>food101_train_transforms</code> (para los datos de entrenamiento) y <code>effnetb2_transforms</code> (para los datos de prueba/inferencia).</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Training transforms:\\n{food101_train_transforms}\\n\") \nprint(f\"Testing transforms:\\n{effnetb2_transforms}\")\n</pre> print(f\"Training transforms:\\n{food101_train_transforms}\\n\")  print(f\"Testing transforms:\\n{effnetb2_transforms}\") In\u00a0[\u00a0]: Copied! <pre>from torchvision import datasets\n\n# Directorio de datos de configuraci\u00f3n\nfrom pathlib import Path\ndata_dir = Path(\"data\")\n\n# Obtenga datos de entrenamiento (~750 im\u00e1genes x 101 clases de alimentos)\ntrain_data = datasets.Food101(root=data_dir, # path to download data to\n                              split=\"train\", # dataset split to get\n                              transform=food101_train_transforms, # perform data augmentation on training data\n                              download=True) # want to download?\n\n# Obtenga datos de prueba (~250 im\u00e1genes x 101 clases de alimentos)\ntest_data = datasets.Food101(root=data_dir,\n                             split=\"test\",\n                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n                             download=True)\n</pre> from torchvision import datasets  # Directorio de datos de configuraci\u00f3n from pathlib import Path data_dir = Path(\"data\")  # Obtenga datos de entrenamiento (~750 im\u00e1genes x 101 clases de alimentos) train_data = datasets.Food101(root=data_dir, # path to download data to                               split=\"train\", # dataset split to get                               transform=food101_train_transforms, # perform data augmentation on training data                               download=True) # want to download?  # Obtenga datos de prueba (~250 im\u00e1genes x 101 clases de alimentos) test_data = datasets.Food101(root=data_dir,                              split=\"test\",                              transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data                              download=True) <p>\u00a1Datos descargados!</p> <p>Ahora podemos obtener una lista de todos los nombres de clases usando <code>train_data.classes</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Obtener nombres de clases de Food101\nfood101_class_names = train_data.classes\n\n# Ver los primeros 10\nfood101_class_names[:10]\n</pre> # Obtener nombres de clases de Food101 food101_class_names = train_data.classes  # Ver los primeros 10 food101_class_names[:10] <p>\u00a1Ho, ho! Esas son algunas comidas que suenan deliciosas (aunque nunca he o\u00eddo hablar de los \"bu\u00f1uelos\"... actualizaci\u00f3n: despu\u00e9s de una b\u00fasqueda r\u00e1pida en Google, los bu\u00f1uelos tambi\u00e9n se ven deliciosos). Puede ver una lista completa de los nombres de las clases Food101 en el curso GitHub en [<code>extras/food101_class_names.txt</code>](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names. TXT).</p> In\u00a0[\u00a0]: Copied! <pre>def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n\n    Args:\n        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n        split_size (float, optional): How much of the dataset should be split? \n            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n        seed (int, optional): Seed for random generator. Defaults to 42.\n\n    Returns:\n        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n            random_split_2 is of size (1-split_size)*len(dataset).\n    \"\"\"\n    # Create split lengths based on original dataset length\n    length_1 = int(len(dataset) * split_size) # desired length\n    length_2 = len(dataset) - length_1 # remaining length\n        \n    # Print out info\n    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n    \n    # Create splits with given random seed\n    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n                                                                   lengths=[length_1, length_2],\n                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n    return random_split_1, random_split_2\n</pre> def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):     \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.      Args:         dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.         split_size (float, optional): How much of the dataset should be split?              E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.         seed (int, optional): Seed for random generator. Defaults to 42.      Returns:         tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and              random_split_2 is of size (1-split_size)*len(dataset).     \"\"\"     # Create split lengths based on original dataset length     length_1 = int(len(dataset) * split_size) # desired length     length_2 = len(dataset) - length_1 # remaining length              # Print out info     print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")          # Create splits with given random seed     random_split_1, random_split_2 = torch.utils.data.random_split(dataset,                                                                     lengths=[length_1, length_2],                                                                    generator=torch.manual_seed(seed)) # set the random seed for reproducible splits     return random_split_1, random_split_2 <p>\u00a1Se cre\u00f3 la funci\u00f3n de divisi\u00f3n del conjunto de datos!</p> <p>Ahora prob\u00e9moslo creando una divisi\u00f3n del 20% del conjunto de datos de prueba y capacitaci\u00f3n de Food101.</p> In\u00a0[\u00a0]: Copied! <pre># Crear capacitaci\u00f3n Divisi\u00f3n del 20% de Food101\ntrain_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n                                                 split_size=0.2)\n\n# Crear pruebas con una divisi\u00f3n del 20% de Food101\ntest_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n                                                split_size=0.2)\n\nlen(train_data_food101_20_percent), len(test_data_food101_20_percent)\n</pre> # Crear capacitaci\u00f3n Divisi\u00f3n del 20% de Food101 train_data_food101_20_percent, _ = split_dataset(dataset=train_data,                                                  split_size=0.2)  # Crear pruebas con una divisi\u00f3n del 20% de Food101 test_data_food101_20_percent, _ = split_dataset(dataset=test_data,                                                 split_size=0.2)  len(train_data_food101_20_percent), len(test_data_food101_20_percent) <p>\u00a1Excelente!</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\n\nBATCH_SIZE = 32\nNUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n\n# Crear Food101 20 por ciento de entrenamiento DataLoader\ntrain_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n                                                                  batch_size=BATCH_SIZE,\n                                                                  shuffle=True,\n                                                                  num_workers=NUM_WORKERS)\n# Crear Food101 20 por ciento de prueba DataLoader\ntest_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n                                                                 batch_size=BATCH_SIZE,\n                                                                 shuffle=False,\n                                                                 num_workers=NUM_WORKERS)\n</pre> import os import torch  BATCH_SIZE = 32 NUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs  # Crear Food101 20 por ciento de entrenamiento DataLoader train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,                                                                   batch_size=BATCH_SIZE,                                                                   shuffle=True,                                                                   num_workers=NUM_WORKERS) # Crear Food101 20 por ciento de prueba DataLoader test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,                                                                  batch_size=BATCH_SIZE,                                                                  shuffle=False,                                                                  num_workers=NUM_WORKERS) In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Optimizador de configuraci\u00f3n\noptimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n                             lr=1e-3)\n\n# Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n\n# Quiere superar el art\u00edculo original de Food101 con un 20 % de datos, necesita m\u00e1s del 56,4 % seg\u00fan el conjunto de datos de prueba\nset_seeds()    \neffnetb2_food101_results = engine.train(model=effnetb2_food101,\n                                        train_dataloader=train_dataloader_food101_20_percent,\n                                        test_dataloader=test_dataloader_food101_20_percent,\n                                        optimizer=optimizer,\n                                        loss_fn=loss_fn,\n                                        epochs=5,\n                                        device=device)\n</pre> from going_modular.going_modular import engine  # Optimizador de configuraci\u00f3n optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),                              lr=1e-3)  # Funci\u00f3n de p\u00e9rdida de configuraci\u00f3n loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes  # Quiere superar el art\u00edculo original de Food101 con un 20 % de datos, necesita m\u00e1s del 56,4 % seg\u00fan el conjunto de datos de prueba set_seeds()     effnetb2_food101_results = engine.train(model=effnetb2_food101,                                         train_dataloader=train_dataloader_food101_20_percent,                                         test_dataloader=test_dataloader_food101_20_percent,                                         optimizer=optimizer,                                         loss_fn=loss_fn,                                         epochs=5,                                         device=device) <p>Woooo!!!!</p> <p>Parece que superamos los resultados del art\u00edculo original de Food101 de 56,4% de precisi\u00f3n con solo el 20% de los datos de entrenamiento (aunque solo evaluamos el 20% de los datos de las pruebas tambi\u00e9n, para replicar completamente los resultados, podr\u00edamos evaluar el 100% de las pruebas). datos).</p> <p>\u00a1Ese es el poder del aprendizaje por transferencia!</p> In\u00a0[\u00a0]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Consulte las curvas de p\u00e9rdidas de FoodVision Big\nplot_loss_curves(effnetb2_food101_results)\n</pre> from helper_functions import plot_loss_curves  # Consulte las curvas de p\u00e9rdidas de FoodVision Big plot_loss_curves(effnetb2_food101_results) <p>\u00a1\u00a1\u00a1Lindo!!!</p> <p>Parece que nuestras t\u00e9cnicas de regularizaci\u00f3n (aumento de datos y suavizado de etiquetas) ayudaron a evitar que nuestro modelo se sobreajustara (la p\u00e9rdida de entrenamiento sigue siendo mayor que la p\u00e9rdida de prueba), lo que indica que nuestro modelo tiene un poco m\u00e1s de capacidad para aprender y podr\u00eda mejorar con m\u00e1s entrenamiento.</p> In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import utils\n\n# Crear una ruta modelo\neffnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n\n# Guardar modelo FoodVision Big\nutils.save_model(model=effnetb2_food101,\n                 target_dir=\"models\",\n                 model_name=effnetb2_food101_model_path)\n</pre> from going_modular.going_modular import utils  # Crear una ruta modelo effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"   # Guardar modelo FoodVision Big utils.save_model(model=effnetb2_food101,                  target_dir=\"models\",                  model_name=effnetb2_food101_model_path) <p>\u00a1Modelo guardado!</p> <p>Antes de continuar, asegur\u00e9monos de poder volver a cargarlo.</p> <p>Lo haremos creando primero una instancia de modelo con <code>create_effnetb2_model(num_classes=101)</code> (101 clases para todas las clases de Food101).</p> <p>Y luego cargar el <code>state_dict()</code> guardado con [<code>torch.nn.Module.load_state_dict()</code>](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict #torch.nn.Module.load_state_dict) y <code>torch.load()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Crear una instancia EffNetB2 compatible con Food101\nloaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\n# Cargue el state_dict() del modelo guardado\nloaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))\n</pre> # Crear una instancia EffNetB2 compatible con Food101 loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)  # Cargue el state_dict() del modelo guardado loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\")) In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes\npretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")\n</pre> from pathlib import Path  # Obtenga el tama\u00f1o del modelo en bytes y luego convi\u00e9rtalo a megabytes pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\") <p>Mmm, parece que el tama\u00f1o del modelo se mantuvo pr\u00e1cticamente igual (30 MB para FoodVision Big y 29 MB para FoodVision Mini) a pesar del gran aumento en el n\u00famero de clases.</p> <p>Esto se debe a que todos los par\u00e1metros adicionales para FoodVision Big est\u00e1n solo en la \u00faltima capa (el encabezado del clasificador).</p> <p>Todas las capas base son iguales entre FoodVision Big y FoodVision Mini.</p> <p>Volver arriba y comparar los res\u00famenes de los modelos dar\u00e1 m\u00e1s detalles.</p> Modelo Forma de salida (n\u00fam. de clases) Par\u00e1metros entrenables Par\u00e1metros totales Tama\u00f1o del modelo (MB) FoodVision Mini (extractor de funciones EffNetB2) 3 4.227 7.705.221 29 FoodVision Big (extractor de funciones EffNetB2) 101 142.309 7.843.303 30 In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Crear ruta de demostraci\u00f3n de FoodVision Big\nfoodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n\n# Haga el directorio de demostraci\u00f3n de FoodVision Big\nfoodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n\n# Directorio de ejemplos de demostraci\u00f3n de Make FoodVision Big\n(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)\n</pre> from pathlib import Path  # Crear ruta de demostraci\u00f3n de FoodVision Big foodvision_big_demo_path = Path(\"demos/foodvision_big/\")  # Haga el directorio de demostraci\u00f3n de FoodVision Big foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)  # Directorio de ejemplos de demostraci\u00f3n de Make FoodVision Big (foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># Descargar y mover una imagen de ejemplo\n!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n\n# Mueva el modelo entrenado a la carpeta de demostraci\u00f3n de FoodVision Big (se producir\u00e1 un error si el modelo ya se ha movido)\n!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big\n</pre> # Descargar y mover una imagen de ejemplo !wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg  !mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg  # Mueva el modelo entrenado a la carpeta de demostraci\u00f3n de FoodVision Big (se producir\u00e1 un error si el modelo ya se ha movido) !mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big In\u00a0[\u00a0]: Copied! <pre># Consulte los primeros 10 nombres de clases de Food101\nfood101_class_names[:10]\n</pre> # Consulte los primeros 10 nombres de clases de Food101 food101_class_names[:10] <p>Maravilloso, ahora podemos escribirlos en un archivo de texto creando primero una ruta a <code>demos/foodvision_big/class_names.txt</code> y luego abriendo un archivo con <code>open()</code> de Python y luego escribiendo en \u00e9l dejando una nueva l\u00ednea para cada clase. .</p> <p>Idealmente, queremos que los nombres de nuestras clases se guarden como:</p> <pre><code>tarta de manzana\nCostillitas\nbaklava\nCarpaccio de carne\ntartar_de_carne\n...\n</code></pre> In\u00a0[\u00a0]: Copied! <pre># Crear ruta a los nombres de clases de Food101\nfoodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n\n# Escriba la lista de nombres de clases de Food101 en el archivo\nwith open(foodvision_big_class_names_path, \"w\") as f:\n    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class\n</pre> # Crear ruta a los nombres de clases de Food101 foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"  # Escriba la lista de nombres de clases de Food101 en el archivo with open(foodvision_big_class_names_path, \"w\") as f:     print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")     f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class <p>Excelente, ahora asegur\u00e9monos de poder leerlos.</p> <p>Para hacerlo usaremos <code>open()</code> de Python en modo lectura (<code>\"r\"</code>) y luego usaremos <code>readlines( )</code> m\u00e9todo para leer cada l\u00ednea de nuestro archivo <code>class_names.txt</code>.</p> <p>Y podemos guardar los nombres de las clases en una lista eliminando el valor de nueva l\u00ednea de cada uno de ellos con una lista de comprensi\u00f3n y <code>strip()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Abra el archivo de nombres de clases Food101 y lea cada l\u00ednea en una lista\nwith open(foodvision_big_class_names_path, \"r\") as f:\n    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n    \n# Ver los primeros 5 nombres de clases cargados nuevamente en\nfood101_class_names_loaded[:5]\n</pre> # Abra el archivo de nombres de clases Food101 y lea cada l\u00ednea en una lista with open(foodvision_big_class_names_path, \"r\") as f:     food101_class_names_loaded = [food.strip() for food in  f.readlines()]      # Ver los primeros 5 nombres de clases cargados nuevamente en food101_class_names_loaded[:5] In\u00a0[\u00a0]: Copied! <pre>%%writefile demos/foodvision_big/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> %%writefile demos/foodvision_big/model.py import torch import torchvision  from torch import nn   def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms In\u00a0[\u00a0]: Copied! <pre>%%writefile demos/foodvision_big/app.py\n# ## 1. Configuraci\u00f3n de importaciones y nombres de clases ###\nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Configurar nombres de clases\nwith open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n    class_names = [food_name.strip() for food_name in  f.readlines()]\n    \n# ## 2. Preparaci\u00f3n del modelo y transforma ###\n\n# Crear modelo\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=101, # could also use len(class_names)\n)\n\n# Cargar pesos guardados\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n# ## 3. Funci\u00f3n de predicci\u00f3n ###\n\n# Crear funci\u00f3n de predicci\u00f3n\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n# ## 4. Aplicaci\u00f3n Gradio ###\n\n# Crear cadenas de t\u00edtulo, descripci\u00f3n y art\u00edculo.\ntitle = \"FoodVision Big \ud83c\udf54\ud83d\udc41\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Crear una lista de ejemplos desde el directorio \"examples/\"\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Crear interfaz Gradio\ndemo = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=[\n        gr.Label(num_top_classes=5, label=\"Predictions\"),\n        gr.Number(label=\"Prediction time (s)\"),\n    ],\n    examples=example_list,\n    title=title,\n    description=description,\n    article=article,\n)\n\n# \u00a1Inicia la aplicaci\u00f3n!\ndemo.launch()\n</pre> %%writefile demos/foodvision_big/app.py # ## 1. Configuraci\u00f3n de importaciones y nombres de clases ### import gradio as gr import os import torch  from model import create_effnetb2_model from timeit import default_timer as timer from typing import Tuple, Dict  # Configurar nombres de clases with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt     class_names = [food_name.strip() for food_name in  f.readlines()]      # ## 2. Preparaci\u00f3n del modelo y transforma ###  # Crear modelo effnetb2, effnetb2_transforms = create_effnetb2_model(     num_classes=101, # could also use len(class_names) )  # Cargar pesos guardados effnetb2.load_state_dict(     torch.load(         f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",         map_location=torch.device(\"cpu\"),  # load to CPU     ) )  # ## 3. Funci\u00f3n de predicci\u00f3n ###  # Crear funci\u00f3n de predicci\u00f3n def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time  # ## 4. Aplicaci\u00f3n Gradio ###  # Crear cadenas de t\u00edtulo, descripci\u00f3n y art\u00edculo. title = \"FoodVision Big \ud83c\udf54\ud83d\udc41\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Crear una lista de ejemplos desde el directorio \"examples/\" example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]  # Crear interfaz Gradio demo = gr.Interface(     fn=predict,     inputs=gr.Image(type=\"pil\"),     outputs=[         gr.Label(num_top_classes=5, label=\"Predictions\"),         gr.Number(label=\"Prediction time (s)\"),     ],     examples=example_list,     title=title,     description=description,     article=article, )  # \u00a1Inicia la aplicaci\u00f3n! demo.launch() In\u00a0[\u00a0]: Copied! <pre>%%writefile demos/foodvision_big/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n</pre> %%writefile demos/foodvision_big/requirements.txt torch==1.12.0 torchvision==0.13.0 gradio==3.1.4 In\u00a0[\u00a0]: Copied! <pre># Comprima la carpeta foodvision_big pero excluya ciertos archivos\n!cd demos/foodvision_big &amp;&amp; zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Descargue la aplicaci\u00f3n FoodVision Big comprimida (si se ejecuta en Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_big.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download()\")\n</pre> # Comprima la carpeta foodvision_big pero excluya ciertos archivos !cd demos/foodvision_big &amp;&amp; zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"  # Descargue la aplicaci\u00f3n FoodVision Big comprimida (si se ejecuta en Google Colab) try:     from google.colab import files     files.download(\"demos/foodvision_big.zip\") except:     print(\"Not running in Google Colab, can't use google.colab.files.download()\") In\u00a0[\u00a0]: Copied! <pre># IPython es una biblioteca para ayudar a trabajar con Python de forma interactiva\nfrom IPython.display import IFrame\n\n# Incruste la demostraci\u00f3n de FoodVision Big Gradio como un iFrame\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)\n</pre> # IPython es una biblioteca para ayudar a trabajar con Python de forma interactiva from IPython.display import IFrame  # Incruste la demostraci\u00f3n de FoodVision Big Gradio como un iFrame IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750) <p>\u00a1\u00bf\u00a1Cuan genial es eso!?!</p> <p>Hemos recorrido un largo camino desde la construcci\u00f3n de modelos PyTorch para predecir una l\u00ednea recta... \u00a1ahora estamos construyendo modelos de visi\u00f3n por computadora accesibles para personas de todo el mundo!</p>"},{"location":"13-09_pytorch_model_deployment/#09-implementacion-del-modelo-pytorch","title":"09. Implementaci\u00f3n del modelo PyTorch\u00b6","text":"<p>Bienvenido al Proyecto Milestone 3: \u00a1Implementaci\u00f3n del modelo PyTorch!</p> <p>Hemos recorrido un largo camino con nuestro proyecto FoodVision Mini.</p> <p>Pero hasta ahora solo nosotros hemos podido acceder a nuestros modelos PyTorch.</p> <p>\u00bfQu\u00e9 tal si le damos vida a FoodVision Mini y lo hacemos p\u00fablicamente accesible?</p> <p>En otras palabras, \u00a1vamos a implementar nuestro modelo FoodVision Mini en Internet como una aplicaci\u00f3n utilizable!</p> <p>Probando la versi\u00f3n implementada de FoodVision Mini (lo que vamos a crear) en mi almuerzo. \u00a1La modelo tambi\u00e9n acert\u00f3 \ud83c\udf63!</p>"},{"location":"13-09_pytorch_model_deployment/#que-es-la-implementacion-del-modelo-de-aprendizaje-automatico","title":"\u00bfQu\u00e9 es la implementaci\u00f3n del modelo de aprendizaje autom\u00e1tico?\u00b6","text":"<p>Implementaci\u00f3n del modelo de aprendizaje autom\u00e1tico es el proceso de hacer que su modelo de aprendizaje autom\u00e1tico sea accesible para alguien o algo m\u00e1s.</p> <p>Alguien m\u00e1s es una persona que puede interactuar con tu modelo de alguna manera.</p> <p>Por ejemplo, alguien que toma una fotograf\u00eda de una comida con su tel\u00e9fono inteligente y luego hace que nuestro modelo FoodVision Mini la clasifique en pizza, filete o sushi.</p> <p>Otra cosa podr\u00eda ser otro programa, aplicaci\u00f3n o incluso otro modelo que interact\u00fae con sus modelos de aprendizaje autom\u00e1tico.</p> <p>Por ejemplo, una base de datos bancaria podr\u00eda depender de un modelo de aprendizaje autom\u00e1tico que haga predicciones sobre si una transacci\u00f3n es fraudulenta o no antes de transferir fondos.</p> <p>O un sistema operativo puede reducir su consumo de recursos bas\u00e1ndose en un modelo de aprendizaje autom\u00e1tico que hace predicciones sobre cu\u00e1nta energ\u00eda usa generalmente alguien en momentos espec\u00edficos del d\u00eda.</p> <p>Estos casos de uso tambi\u00e9n se pueden mezclar y combinar.</p> <p>Por ejemplo, el sistema de visi\u00f3n por computadora de un autom\u00f3vil Tesla interactuar\u00e1 con el programa de planificaci\u00f3n de rutas del autom\u00f3vil (algo m\u00e1s) y luego el programa de planificaci\u00f3n de rutas recibir\u00e1 informaci\u00f3n y comentarios del conductor (otra persona).</p> <p></p> <p>La implementaci\u00f3n del modelo de aprendizaje autom\u00e1tico implica poner su modelo a disposici\u00f3n de alguien o de algo m\u00e1s. Por ejemplo, alguien podr\u00eda usar su modelo como parte de una aplicaci\u00f3n de reconocimiento de alimentos (como FoodVision Mini o Nutrify). Y algo m\u00e1s podr\u00eda ser otro modelo o programa que utilice su modelo, como un sistema bancario que utilice un modelo de aprendizaje autom\u00e1tico para detectar si una transacci\u00f3n es fraudulenta o no.</p>"},{"location":"13-09_pytorch_model_deployment/#por-que-implementar-un-modelo-de-aprendizaje-automatico","title":"\u00bfPor qu\u00e9 implementar un modelo de aprendizaje autom\u00e1tico?\u00b6","text":"<p>Una de las cuestiones filos\u00f3ficas m\u00e1s importantes en el aprendizaje autom\u00e1tico es:</p> <p>Implementar un modelo es tan importante como entrenarlo.</p> <p>Porque aunque puedes tener una idea bastante clara de c\u00f3mo funcionar\u00e1 tu modelo evalu\u00e1ndolo en un conjunto de pruebas bien dise\u00f1ado o visualizando sus resultados, nunca sabes realmente c\u00f3mo funcionar\u00e1 hasta que lo liberas.</p> <p>Hacer que personas que nunca han usado su modelo interact\u00faen con \u00e9l a menudo revelar\u00e1 casos extremos en los que nunca pens\u00f3 durante el entrenamiento.</p> <p>Por ejemplo, \u00bfqu\u00e9 pasar\u00eda si alguien subiera una foto que no fuera de comida a nuestro modelo FoodVision Mini?</p> <p>Una soluci\u00f3n ser\u00eda crear otro modelo que primero clasifique las im\u00e1genes como \"comida\" o \"no comida\" y pase primero la imagen de destino a trav\u00e9s de ese modelo (esto es lo que hace Nutrify).</p> <p>Luego, si la imagen es de \"comida\", pasa a nuestro modelo FoodVision Mini y se clasifica en pizza, bistec o sushi.</p> <p>Y si \"no es comida\", se muestra un mensaje.</p> <p>Pero \u00bfy si estas predicciones estuvieran equivocadas?</p> <p>\u00bfQu\u00e9 pasa entonces?</p> <p>Puedes ver c\u00f3mo estas preguntas podr\u00edan continuar.</p> <p>Por lo tanto, esto resalta la importancia de la implementaci\u00f3n del modelo: le ayuda a descubrir errores en su modelo que no son obvios durante el entrenamiento/prueba.</p> <p>Cubrimos un flujo de trabajo de PyTorch en 01. Flujo de trabajo de PyTorch. Pero una vez que se tiene un buen modelo, la implementaci\u00f3n es un buen siguiente paso. El monitoreo implica ver c\u00f3mo funciona su modelo en la divisi\u00f3n de datos m\u00e1s importante: los datos del mundo real. Para obtener m\u00e1s recursos sobre implementaci\u00f3n y monitoreo, consulte Recursos adicionales de PyTorch.</p>"},{"location":"13-09_pytorch_model_deployment/#diferentes-tipos-de-implementacion-de-modelos-de-aprendizaje-automatico","title":"Diferentes tipos de implementaci\u00f3n de modelos de aprendizaje autom\u00e1tico\u00b6","text":"<p>Se podr\u00edan escribir libros completos sobre los diferentes tipos de implementaci\u00f3n de modelos de aprendizaje autom\u00e1tico (y muchos buenos se enumeran en [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and -ingenier\u00eda-de-aprendizaje-profundo)).</p> <p>Y el campo a\u00fan se est\u00e1 desarrollando en t\u00e9rminos de mejores pr\u00e1cticas.</p> <p>Pero me gusta empezar con la pregunta:</p> <p>\"\u00bfCu\u00e1l es el escenario m\u00e1s ideal para utilizar mi modelo de aprendizaje autom\u00e1tico?\"</p> <p>Y luego trabaje hacia atr\u00e1s desde all\u00ed.</p> <p>Por supuesto, es posible que no lo sepas de antemano. Pero eres lo suficientemente inteligente como para imaginar esas cosas.</p> <p>En el caso de FoodVision Mini, nuestro escenario ideal podr\u00eda ser:</p> <ul> <li>Alguien toma una foto en un dispositivo m\u00f3vil (a trav\u00e9s de una aplicaci\u00f3n o navegador web).</li> <li>La predicci\u00f3n vuelve r\u00e1pidamente.</li> </ul> <p>F\u00e1cil.</p> <p>Entonces tenemos dos criterios principales:</p> <ol> <li>El modelo deber\u00eda funcionar en un dispositivo m\u00f3vil (esto significa que habr\u00e1 algunas restricciones inform\u00e1ticas).</li> <li>El modelo debe hacer predicciones r\u00e1pidas (porque una aplicaci\u00f3n lenta es una aplicaci\u00f3n aburrida).</li> </ol> <p>Y, por supuesto, seg\u00fan su caso de uso, sus requisitos pueden variar.</p> <p>Puede notar que los dos puntos anteriores se dividen en otras dos preguntas:</p> <ol> <li>\u00bfA d\u00f3nde ir\u00e1? - Es decir, \u00bfd\u00f3nde se almacenar\u00e1?</li> <li>\u00bfC\u00f3mo va a funcionar? - Es decir, \u00bfdevuelve predicciones inmediatamente? \u00bfO vienen m\u00e1s tarde?</li> </ol> <p>Al comenzar a implementar modelos de aprendizaje autom\u00e1tico, es \u00fatil comenzar preguntando cu\u00e1l es el caso de uso m\u00e1s ideal y luego trabajar hacia atr\u00e1s desde all\u00ed, preguntando hacia d\u00f3nde ir\u00e1 el modelo y luego c\u00f3mo funcionar\u00e1.</p>"},{"location":"13-09_pytorch_model_deployment/#a-donde-ira","title":"\u00bfA d\u00f3nde ir\u00e1?\u00b6","text":"<p>Cuando implementas tu modelo de aprendizaje autom\u00e1tico, \u00bfd\u00f3nde reside?</p> <p>El debate principal aqu\u00ed suele ser en el dispositivo (tambi\u00e9n llamado borde/en el navegador) o en la nube (una computadora/servidor que no es el dispositivo real desde donde alguien/algo llama al modelo).</p> <p>Ambos tienen pros y contras.</p> Ubicaci\u00f3n de implementaci\u00f3n Ventajas Desventajas En el dispositivo (borde/en el navegador) Puede ser muy r\u00e1pido (ya que no salen datos del dispositivo) Potencia inform\u00e1tica limitada (los modelos m\u00e1s grandes tardan m\u00e1s en ejecutarse) Preservaci\u00f3n de la privacidad (nuevamente, ning\u00fan dato tiene que salir del dispositivo) Espacio de almacenamiento limitado (se requiere un tama\u00f1o de modelo m\u00e1s peque\u00f1o) No se requiere conexi\u00f3n a Internet (a veces) A menudo se requieren habilidades espec\u00edficas del dispositivo En la nube Potencia inform\u00e1tica casi ilimitada (puede ampliarse cuando sea necesario) Los costos pueden salirse de control (si no se aplican l\u00edmites de escala adecuados) Puede implementar un modelo y usarlo en todas partes (a trav\u00e9s de API) Las predicciones pueden ser m\u00e1s lentas debido a que los datos tienen que salir del dispositivo y las predicciones tienen que regresar (latencia de red) V\u00ednculos con el ecosistema de nube existente Los datos deben salir del dispositivo (esto puede causar problemas de privacidad) <p>Hay m\u00e1s detalles sobre estos, pero dej\u00e9 recursos en el extracurriculum para obtener m\u00e1s informaci\u00f3n.</p> <p>Pongamos un ejemplo.</p> <p>Si implementamos FoodVision Mini como una aplicaci\u00f3n, queremos que funcione bien y r\u00e1pido.</p> <p>Entonces, \u00bfqu\u00e9 modelo preferir\u00edamos?</p> <ol> <li>Un modelo en el dispositivo que funciona con una precisi\u00f3n del 95 % con un tiempo de inferencia (latencia) de un segundo por predicci\u00f3n.</li> <li>Un modelo en la nube que funciona con una precisi\u00f3n del 98 % con un tiempo de inferencia de 10 segundos por predicci\u00f3n (un modelo mejor y m\u00e1s grande, pero lleva m\u00e1s tiempo calcularlo).</li> </ol> <p>He inventado estos n\u00fameros, pero muestran una diferencia potencial entre el dispositivo y la nube.</p> <p>La opci\u00f3n 1 podr\u00eda ser potencialmente un modelo m\u00e1s peque\u00f1o, de menor rendimiento y que funcione m\u00e1s r\u00e1pido porque puede caber en un dispositivo m\u00f3vil.</p> <p>La opci\u00f3n 2 podr\u00eda potencialmente ser un modelo m\u00e1s grande y con mayor rendimiento que requiere m\u00e1s computaci\u00f3n y almacenamiento, pero tarda un poco m\u00e1s en ejecutarse porque tenemos que enviar datos desde el dispositivo y recuperarlos (por lo que, aunque la predicci\u00f3n real pueda ser r\u00e1pida, el tiempo de red y la transferencia de datos debe tenerse en cuenta).</p> <p>Para FoodVision Mini, probablemente preferir\u00edamos la opci\u00f3n 1, porque el peque\u00f1o impacto en el rendimiento se ve superado con creces por la velocidad de inferencia m\u00e1s r\u00e1pida.</p> <p></p> <p>En el caso del sistema de visi\u00f3n por computadora de un autom\u00f3vil Tesla, \u00bfcu\u00e1l ser\u00eda mejor? \u00bfUn modelo m\u00e1s peque\u00f1o que funciona bien en el dispositivo (el modelo est\u00e1 en el autom\u00f3vil) o un modelo m\u00e1s grande que funciona mejor en la nube? En este caso, preferir\u00edas que el modelo estuviera en el auto. El tiempo de red adicional que tomar\u00eda para que los datos vayan del autom\u00f3vil a la nube y luego de regreso al autom\u00f3vil simplemente no valdr\u00eda la pena (o incluso ser\u00eda potencialmente imposible en \u00e1reas con mala se\u00f1al).</p> <p>Nota: Para ver un ejemplo completo de c\u00f3mo es implementar un modelo de PyTorch en un dispositivo perimetral, consulte el [tutorial de PyTorch sobre c\u00f3mo lograr inferencia en tiempo real (30 fps+)](https://pytorch.org/ tutorials/intermediate/realtime_rpi.html) con un modelo de visi\u00f3n por computadora en una Raspberry Pi.</p>"},{"location":"13-09_pytorch_model_deployment/#como-va-a-funcionar","title":"\u00bfC\u00f3mo va a funcionar?\u00b6","text":"<p>Volviendo al caso de uso ideal, cuando implementa su modelo de aprendizaje autom\u00e1tico, \u00bfc\u00f3mo deber\u00eda funcionar?</p> <p>Es decir, \u00bfle gustar\u00eda que se le devolvieran las predicciones de inmediato?</p> <p>\u00bfO est\u00e1 bien que sucedan m\u00e1s tarde?</p> <p>Estos dos escenarios generalmente se denominan:</p> <ul> <li>En l\u00ednea (en tiempo real): las predicciones/inferencias ocurren inmediatamente. Por ejemplo, alguien sube una imagen, la imagen se transforma y se devuelven predicciones o alguien realiza una compra y un modelo verifica que la transacci\u00f3n no es fraudulenta para que la compra pueda realizarse.</li> <li>Sin conexi\u00f3n (por lotes): las predicciones/inferencias ocurren peri\u00f3dicamente. Por ejemplo, una aplicaci\u00f3n de fotos clasifica sus im\u00e1genes en diferentes categor\u00edas (como playa, hora de comer, familia, amigos) mientras su dispositivo m\u00f3vil est\u00e1 enchufado a la carga.</li> </ul> <p>Nota: \"Lote\" se refiere a la inferencia que se realiza en varias muestras a la vez. Sin embargo, para agregar un poco de confusi\u00f3n, el procesamiento por lotes puede realizarse inmediatamente/en l\u00ednea (se clasifican varias im\u00e1genes a la vez) y/o fuera de l\u00ednea (se predicen/entrenan varias im\u00e1genes a la vez).</p> <p>La principal diferencia entre cada ser: las predicciones se realizan de forma inmediata o peri\u00f3dica.</p> <p>Peri\u00f3dicamente tambi\u00e9n puede tener una escala de tiempo variable, desde cada pocos segundos hasta cada pocas horas o d\u00edas.</p> <p>Y puedes mezclar y combinar los dos.</p> <p>En el caso de FoodVision Mini, queremos que nuestro proceso de inferencia se realice en l\u00ednea (en tiempo real), de modo que cuando alguien suba una imagen de pizza, bistec o sushi, los resultados de la predicci\u00f3n se devuelvan inmediatamente (cualquier cosa m\u00e1s lenta de lo que lo har\u00eda el tiempo real). hacer una experiencia aburrida).</p> <p>Pero para nuestro proceso de capacitaci\u00f3n, est\u00e1 bien que suceda por lotes (fuera de l\u00ednea), que es lo que hemos estado haciendo a lo largo de los cap\u00edtulos anteriores.</p>"},{"location":"13-09_pytorch_model_deployment/#formas-de-implementar-un-modelo-de-aprendizaje-automatico","title":"Formas de implementar un modelo de aprendizaje autom\u00e1tico\u00b6","text":"<p>Hemos analizado un par de opciones para implementar modelos de aprendizaje autom\u00e1tico (en el dispositivo y en la nube).</p> <p>Y cada uno de estos tendr\u00e1 sus requisitos espec\u00edficos:</p> <p>| Herramienta/recurso | Tipo de implementaci\u00f3n | | ----- | ----- | | Kit de aprendizaje autom\u00e1tico de Google | En el dispositivo (Android e iOS) | | Core ML de Apple y paquete Python <code>coremltools</code> | En el dispositivo (todos los dispositivos Apple) | | Sagemaker de Amazon Web Service (AWS) | Nube | | Vertex AI de Google Cloud | Nube | | Aprendizaje autom\u00e1tico de Azure de Microsoft | Nube | | Abrazando espacios faciales | Nube | | API con FastAPI | Servidor en la nube/autohospedado | | API con TorchServe | Servidor en la nube/autohospedado | | ONNX (Intercambio de redes neuronales abiertas) | Muchos/general | | Muchos m\u00e1s... |</p> <p>Nota: Una interfaz de programaci\u00f3n de aplicaciones (API) es una forma en que dos (o m\u00e1s) programas inform\u00e1ticos interact\u00faan entre s\u00ed. Por ejemplo, si su modelo se implement\u00f3 como API, podr\u00eda escribir un programa que pudiera enviarle datos y luego recibir predicciones.</p> <p>La opci\u00f3n que elija depender\u00e1 en gran medida de lo que est\u00e9 creando y con qui\u00e9n est\u00e9 trabajando.</p> <p>Pero con tantas opciones, puede resultar muy intimidante.</p> <p>As\u00ed que lo mejor es empezar poco a poco y hacerlo sencillo.</p> <p>Y una de las mejores formas de hacerlo es convertir su modelo de aprendizaje autom\u00e1tico en una aplicaci\u00f3n de demostraci\u00f3n con Gradio y luego implementarlo en Hugging Face Spaces.</p> <p>M\u00e1s adelante haremos precisamente eso con FoodVision Mini.</p> <p>Un pu\u00f1ado de lugares y herramientas para alojar e implementar modelos de aprendizaje autom\u00e1tico. Hay muchas cosas que me he perdido, as\u00ed que si desea agregar m\u00e1s, deje una discusi\u00f3n en GitHub.</p>"},{"location":"13-09_pytorch_model_deployment/#que-vamos-a-cubrir","title":"Qu\u00e9 vamos a cubrir\u00b6","text":"<p>Ya basta de hablar de implementar un modelo de aprendizaje autom\u00e1tico.</p> <p>Convirt\u00e1monos en ingenieros de aprendizaje autom\u00e1tico e implementemos uno.</p> <p>Nuestro objetivo es implementar nuestro modelo FoodVision a trav\u00e9s de una aplicaci\u00f3n de demostraci\u00f3n de Gradio con las siguientes m\u00e9tricas:</p> <ol> <li>Rendimiento: 95%+ precisi\u00f3n.</li> <li>Velocidad: inferencia en tiempo real de 30 FPS+ (cada predicci\u00f3n tiene una latencia inferior a ~0,03 s).</li> </ol> <p>Comenzaremos ejecutando un experimento para comparar nuestros dos mejores modelos hasta el momento: extractores de funciones EffNetB2 y ViT.</p> <p>Luego implementaremos el que se acerque m\u00e1s a nuestras m\u00e9tricas objetivo.</p> <p>Finalmente, terminaremos con un (GRANDE) bono sorpresa.</p> Tema Contenido 0. Obteniendo configuraci\u00f3n Hemos escrito bastante c\u00f3digo \u00fatil en las \u00faltimas secciones, descargu\u00e9moslo y asegur\u00e9monos de poder usarlo nuevamente. 1. Obtener datos Descarguemos el conjunto de datos <code>pizza_steak_sushi_20_percent.zip</code> para que podamos entrenar nuestros modelos que anteriormente ten\u00edan mejor rendimiento en el mismo conjunto de datos. 2. Esquema del experimento de implementaci\u00f3n del modelo FoodVision Mini Incluso en el proyecto del tercer hito, todav\u00eda realizaremos m\u00faltiples experimentos para ver qu\u00e9 modelo (EffNetB2 o ViT) se acerca m\u00e1s a nuestras m\u00e9tricas objetivo. 3. Creando un extractor de funciones EffNetB2 Un extractor de funciones EfficientNetB2 tuvo el mejor rendimiento en nuestro conjunto de datos de pizza, bistec y sushi en [07. Seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/), vamos a recrearlo como candidato para su implementaci\u00f3n. 4. Creando un extractor de funciones ViT Un extractor de funciones ViT ha sido el modelo con mejor rendimiento hasta ahora en nuestro conjunto de datos de pizza, bistec y sushi en 08. PyTorch Paper Replicating, vamos a recrearlo como candidato para su implementaci\u00f3n junto con EffNetB2. 5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas Hemos creado dos de los modelos con mejor rendimiento hasta el momento. Hagamos predicciones con ellos y realicemos un seguimiento de sus resultados. 6. Comparaci\u00f3n de resultados de modelos, tiempos de predicci\u00f3n y tama\u00f1o Comparemos nuestros modelos para ver cu\u00e1l funciona mejor con nuestros objetivos. 7. Dar vida a FoodVision Mini creando una demostraci\u00f3n de Gradio Uno de nuestros modelos funciona mejor que el otro (en t\u00e9rminos de nuestros objetivos), as\u00ed que \u00a1convirt\u00e1moslo en una demostraci\u00f3n de aplicaci\u00f3n funcional! 8. Convirtiendo nuestra demostraci\u00f3n de FoodVision Mini Gradio en una aplicaci\u00f3n implementable Nuestra demostraci\u00f3n de la aplicaci\u00f3n Gradio funciona localmente, \u00a1prepar\u00e9mosla para su implementaci\u00f3n! 9. Implementando nuestra demostraci\u00f3n de Gradio en HuggingFace Spaces \u00a1Llevemos FoodVision Mini a la web y hag\u00e1moslo accesible p\u00fablicamente para todos! 10. Creando una GRAN sorpresa Hemos creado FoodVision Mini, es hora de dar un paso m\u00e1s. 11. Desplegando nuestra GRAN sorpresa Implementar una aplicaci\u00f3n fue divertido, \u00bfqu\u00e9 tal si hacemos dos?"},{"location":"13-09_pytorch_model_deployment/#donde-puedes-obtener-ayuda","title":"\u00bfD\u00f3nde puedes obtener ayuda?\u00b6","text":"<p>Todos los materiales de este curso est\u00e1n disponibles en GitHub.</p> <p>Si tiene problemas, puede hacer una pregunta en el curso [p\u00e1gina de debates de GitHub] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).</p> <p>Y, por supuesto, est\u00e1 la documentaci\u00f3n de PyTorch y los foros de desarrolladores de PyTorch, un lugar muy \u00fatil para todo lo relacionado con PyTorch.</p>"},{"location":"13-09_pytorch_model_deployment/#0-configuracion","title":"0. Configuraci\u00f3n\u00b6","text":"<p>Como lo hicimos anteriormente, asegur\u00e9monos de tener todos los m\u00f3dulos que necesitaremos para esta secci\u00f3n.</p> <p>Importaremos los scripts de Python (como <code>data_setup.py</code> y <code>engine.py</code>) que creamos en 05. PyTorch se vuelve modular.</p> <p>Para hacerlo, descargaremos el directorio <code>going_modular</code> del repositorio <code>pytorch-deep-learning</code> (si a\u00fan no lo tenemos).</p> <p>Tambi\u00e9n obtendremos el paquete <code>torchinfo</code> si no est\u00e1 disponible.</p> <p><code>torchinfo</code> nos ayudar\u00e1 m\u00e1s adelante a darnos una representaci\u00f3n visual de nuestro modelo.</p> <p>Y dado que m\u00e1s adelante usaremos el paquete <code>torchvision</code> v0.13 (disponible a partir de julio de 2022), nos aseguraremos de tener las \u00faltimas versiones.</p> <p>Nota: Si est\u00e1s usando Google Colab y a\u00fan no tienes una GPU activada, ahora es el momento de activar una a trav\u00e9s de <code>Runtime -&gt; Cambiar tipo de tiempo de ejecuci\u00f3n -&gt; Acelerador de hardware -&gt; GPU</code> .</p>"},{"location":"13-09_pytorch_model_deployment/#1-obtener-datos","title":"1. Obtener datos\u00b6","text":"<p>Lo dejamos en 08. PyTorch Paper Replicating comparando nuestro propio modelo de extractor de funciones Vision Transformer (ViT) con El modelo de extracci\u00f3n de caracter\u00edsticas EfficientNetB2 (EffNetB2) que creamos en 07. Seguimiento de experimentos de PyTorch.</p> <p>Y descubrimos que hab\u00eda una ligera diferencia en la comparaci\u00f3n.</p> <p>El modelo EffNetB2 se entren\u00f3 en el 20 % de los datos de pizza, bistec y sushi de Food101, mientras que el modelo ViT se entren\u00f3 en el 10 %.</p> <p>Dado que nuestro objetivo es implementar el mejor modelo para nuestro problema FoodVision Mini, comencemos descargando el [conjunto de datos del 20 % de pizza, bistec y sushi] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main /data/pizza_steak_sushi_20_percent.zip) y entrene un extractor de funciones EffNetB2 y un extractor de funciones ViT en \u00e9l y luego compare los dos modelos.</p> <p>De esta manera, compararemos manzanas con manzanas (un modelo entrenado en un conjunto de datos con otro modelo entrenado en el mismo conjunto de datos).</p> <p>Nota: El conjunto de datos que estamos descargando es una muestra de todo el conjunto de datos de Food101 (101 clases de comida con 1.000 im\u00e1genes cada una). M\u00e1s espec\u00edficamente, 20% se refiere al 20% de im\u00e1genes de las clases de pizza, bistec y sushi seleccionadas al azar. Puede ver c\u00f3mo se cre\u00f3 este conjunto de datos en <code>extras/04_custom_data_creation.ipynb</code> y m\u00e1s detalles en [ 04. Secci\u00f3n 1 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).</p> <p>Podemos descargar los datos usando la funci\u00f3n <code>download_data()</code> que creamos en 07. Secci\u00f3n 1 de seguimiento de experimentos de PyTorch de [<code>helper_functions.py</code>](https://github.com/mrdbourke/pytorch-deep-learning /blob/main/helper_functions.py).</p>"},{"location":"13-09_pytorch_model_deployment/#2-esquema-del-experimento-de-implementacion-del-modelo-foodvision-mini","title":"2. Esquema del experimento de implementaci\u00f3n del modelo FoodVision Mini\u00b6","text":"<p>El modelo implementado ideal, FoodVision Mini, funciona bien y r\u00e1pido.</p> <p>Nos gustar\u00eda que nuestro modelo funcione lo m\u00e1s cerca posible del tiempo real.</p> <p>En este caso, el tiempo real es ~30 FPS (cuadros por segundo) porque eso es [aproximadamente qu\u00e9 tan r\u00e1pido puede ver el ojo humano] (https://www.healthline.com/health/human-eye-fps) (hay debate sobre esto, pero usemos ~30FPS como nuestro punto de referencia).</p> <p>Y para clasificar tres clases diferentes (pizza, bistec y sushi), nos gustar\u00eda un modelo que funcione con una precisi\u00f3n superior al 95 %.</p> <p>Por supuesto, una mayor precisi\u00f3n ser\u00eda buena, pero esto podr\u00eda sacrificar la velocidad.</p> <p>Entonces nuestros objetivos son:</p> <ol> <li>Rendimiento: un modelo que funciona con una precisi\u00f3n superior al 95 %.</li> <li>Velocidad: un modelo que puede clasificar una imagen a ~30 FPS (tiempo de inferencia de 0,03 segundos por imagen, tambi\u00e9n conocido como latencia).</li> </ol> <p>Objetivos de implementaci\u00f3n de FoodVision Mini. Nos gustar\u00eda un modelo de predicci\u00f3n r\u00e1pida y con buen rendimiento (porque una aplicaci\u00f3n lenta es aburrida).</p> <p>Pondremos \u00e9nfasis en la velocidad, es decir, preferir\u00edamos un modelo con un rendimiento superior al 90 % a ~30 FPS que un modelo con un rendimiento superior al 95 % a 10 FPS.</p> <p>Para intentar lograr estos resultados, incluyamos nuestros modelos con mejor rendimiento de las secciones anteriores:</p> <ol> <li>Extractor de funciones EffNetB2 (EffNetB2 para abreviar): creado originalmente en 07. Secci\u00f3n 7.5 de seguimiento de experimentos de PyTorch usando [<code>torchvision.models.ficientnet_b2()</code>](https://pytorch.org /vision/stable/models/generated/torchvision.models.ficientnet_b2.html#ficientnet-b2) con capas de <code>clasificador</code> ajustadas.</li> <li>Extractor de funciones ViT-B/16 (ViT para abreviar): creado originalmente en [08. Secci\u00f3n 10 de replicaci\u00f3n de papel de PyTorch] (https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) usando <code>torchvision.models.vit_b_16 ()</code> con capas de <code>cabeza</code> ajustadas.<ul> <li>Nota ViT-B/16 significa \"Vision Transformer Base, tama\u00f1o de parche 16\".</li> </ul> </li> </ol> <p></p> <p>Nota: Un \"modelo de extracci\u00f3n de caracter\u00edsticas\" a menudo comienza con un modelo que ha sido previamente entrenado en un conjunto de datos similar a su propio problema. Las capas base del modelo previamente entrenado a menudo se dejan congeladas (los patrones/pesos previamente entrenados permanecen iguales) mientras que algunas de las capas superiores (o clasificador/cabeza de clasificaci\u00f3n) se personalizan seg\u00fan su propio problema entrenando con sus propios datos. Cubrimos el concepto de un modelo de extracci\u00f3n de caracter\u00edsticas en [06. Secci\u00f3n 3.4 de aprendizaje por transferencia de PyTorch] (https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs).</p>"},{"location":"13-09_pytorch_model_deployment/#3-creando-un-extractor-de-funciones-effnetb2","title":"3. Creando un extractor de funciones EffNetB2\u00b6","text":"<p>Primero creamos un modelo de extracci\u00f3n de caracter\u00edsticas EffNetB2 en [07. Secci\u00f3n 7.5 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).</p> <p>Y al final de esa secci\u00f3n vimos que funcion\u00f3 muy bien.</p> <p>As\u00ed que ahora vamos a recrearlo aqu\u00ed para que podamos comparar sus resultados con un extractor de funciones de ViT entrenado con los mismos datos.</p> <p>Para hacerlo podemos:</p> <ol> <li>Configure los pesos previamente entrenados como <code>weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT</code>, donde \"<code>DEFAULT</code>\" significa \"mejor disponible actualmente\" (o podr\u00eda usar <code>weights=\"DEFAULT\"</code>).</li> <li>Obtenga las transformaciones de la imagen del modelo previamente entrenado a partir de los pesos con el m\u00e9todo <code>transforms()</code> (los necesitamos para poder convertir nuestras im\u00e1genes al mismo formato en el que se entren\u00f3 el EffNetB2 previamente entrenado).</li> <li>Cree una instancia de modelo previamente entrenada pasando los pesos a una instancia de [<code>torchvision.models.ficientnet_b2</code>](https://pytorch.org/vision/stable/models/generated/torchvision.models.ficientnet_b2.html#ficientnet -b2).</li> <li>Congele las capas base en el modelo.</li> <li>Actualizar el cabezal del clasificador para adaptarlo a nuestros propios datos.</li> </ol>"},{"location":"13-09_pytorch_model_deployment/#31-creando-una-funcion-para-hacer-un-extractor-de-caracteristicas-effnetb2","title":"3.1 Creando una funci\u00f3n para hacer un extractor de caracter\u00edsticas EffNetB2\u00b6","text":"<p>Parece que nuestro extractor de funciones EffNetB2 est\u00e1 listo para funcionar; sin embargo, dado que aqu\u00ed hay bastantes pasos involucrados, \u00bfqu\u00e9 tal si convertimos el c\u00f3digo anterior en una funci\u00f3n que podamos reutilizar m\u00e1s adelante?</p> <p>Lo llamaremos <code>create_effnetb2_model()</code> y necesitar\u00e1 un n\u00famero personalizable de clases y un par\u00e1metro inicial aleatorio para su reproducibilidad.</p> <p>Idealmente, devolver\u00e1 un extractor de funciones EffNetB2 junto con sus transformaciones asociadas.</p>"},{"location":"13-09_pytorch_model_deployment/#32-creando-cargadores-de-datos-para-effnetb2","title":"3.2 Creando cargadores de datos para EffNetB2\u00b6","text":"<p>Nuestro extractor de funciones EffNetB2 est\u00e1 listo, es hora de crear algunos <code>DataLoader</code>.</p> <p>Podemos hacer esto usando la funci\u00f3n <code>data_setup.create_dataloaders()</code> que creamos en  05. PyTorch Going Modular secci\u00f3n 2.</p> <p>Usaremos un <code>batch_size</code> de 32 y transformaremos nuestras im\u00e1genes usando <code>effnetb2_transforms</code> para que est\u00e9n en el mismo formato en el que se entren\u00f3 nuestro modelo <code>effnetb2</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#33-entrenamiento-del-extractor-de-funciones-de-effnetb2","title":"3.3 Entrenamiento del extractor de funciones de EffNetB2\u00b6","text":"<p>Modelo listo, <code>DataLoader</code>s listo, \u00a1entrenemos!</p> <p>Al igual que en [07. Secci\u00f3n 7.6 de seguimiento de experimentos de PyTorch] (https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code), diez \u00e9pocas deber\u00edan ser suficientes para obtener buenos resultados.</p> <p>Podemos hacerlo creando un optimizador (usaremos [<code>torch.optim.Adam()</code>](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim .Adam) con una tasa de aprendizaje de <code>1e-3</code>), una funci\u00f3n de p\u00e9rdida (usaremos [<code>torch.nn.CrossEntropyLoss()</code>](https://pytorch.org/docs/stable/generated/torch .nn.CrossEntropyLoss.html) para clasificaci\u00f3n de clases m\u00faltiples) y luego pasarlos junto con nuestro <code>DataLoader</code>s al [<code>engine.train()</code>](https://github.com/mrdbourke/pytorch-deep -learning/blob/main/going_modular/going_modular/engine.py) funci\u00f3n que creamos en 05. PyTorch Going Modular secci\u00f3n 4.</p>"},{"location":"13-09_pytorch_model_deployment/#34-inspeccionando-las-curvas-de-perdida-de-effnetb2","title":"3.4 Inspeccionando las curvas de p\u00e9rdida de EffNetB2\u00b6","text":"<p>\u00a1Lindo!</p> <p>Como vimos en 07. Seguimiento de experimentos de PyTorch, el modelo de extracci\u00f3n de caracter\u00edsticas EffNetB2 funciona bastante bien con nuestros datos.</p> <p>Convirtamos sus resultados en curvas de p\u00e9rdidas para inspeccionarlos m\u00e1s a fondo.</p> <p>Nota: Las curvas de p\u00e9rdida son una de las mejores formas de visualizar el rendimiento de su modelo. Para obtener m\u00e1s informaci\u00f3n sobre las curvas de p\u00e9rdidas, consulte 04. Secci\u00f3n 8 de conjuntos de datos personalizados de PyTorch: \u00bfC\u00f3mo deber\u00eda ser una curva de p\u00e9rdida ideal?</p>"},{"location":"13-09_pytorch_model_deployment/#35-guardar-el-extractor-de-funciones-de-effnetb2","title":"3.5 Guardar el extractor de funciones de EffNetB2\u00b6","text":"<p>Ahora que tenemos un modelo entrenado con buen rendimiento, guard\u00e9moslo en un archivo para poder importarlo y usarlo m\u00e1s tarde.</p> <p>Para guardar nuestro modelo podemos usar la funci\u00f3n <code>utils.save_model()</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 5.</p> <p>Estableceremos <code>target_dir</code> en <code>\"models\"</code> y <code>model_name</code> en <code>\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"</code> (un poco completo, pero al menos sabemos lo que est\u00e1 pasando).</p>"},{"location":"13-09_pytorch_model_deployment/#36-comprobar-el-tamano-del-extractor-de-funciones-effnetb2","title":"3.6 Comprobar el tama\u00f1o del extractor de funciones EffNetB2\u00b6","text":"<p>Dado que uno de nuestros criterios para implementar un modelo que impulse FoodVision Mini es la velocidad (~30 FPS o mejor), verifiquemos el tama\u00f1o de nuestro modelo.</p> <p>\u00bfPor qu\u00e9 comprobar el tama\u00f1o?</p> <p>Bueno, aunque no siempre es as\u00ed, el tama\u00f1o de un modelo puede influir en su velocidad de inferencia.</p> <p>Es decir, si un modelo tiene m\u00e1s par\u00e1metros, generalmente realiza m\u00e1s operaciones y cada una de estas operaciones requiere cierta potencia inform\u00e1tica.</p> <p>Y como nos gustar\u00eda que nuestro modelo funcione en dispositivos con potencia inform\u00e1tica limitada (por ejemplo, en un dispositivo m\u00f3vil o en un navegador web), generalmente, cuanto m\u00e1s peque\u00f1o sea el tama\u00f1o, mejor (siempre que siga funcionando bien en t\u00e9rminos de precisi\u00f3n). .</p> <p>Para verificar el tama\u00f1o de nuestro modelo en bytes, podemos usar <code>pathlib.Path.stat(\"path_to_model\").st_size</code> de Python .stat) y luego podemos convertirlo (aproximadamente) a megabytes dividi\u00e9ndolo por <code>(1024*1024)</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#37-recopilacion-de-estadisticas-del-extractor-de-funciones-de-effnetb2","title":"3.7 Recopilaci\u00f3n de estad\u00edsticas del extractor de funciones de EffNetB2\u00b6","text":"<p>Tenemos algunas estad\u00edsticas sobre nuestro modelo de extractor de funciones EffNetB2, como p\u00e9rdida de prueba, precisi\u00f3n de la prueba y tama\u00f1o del modelo. \u00bfQu\u00e9 tal si las recopilamos todas en un diccionario para poder compararlas con el pr\u00f3ximo extractor de funciones ViT?</p> <p>Y calcularemos uno extra por diversi\u00f3n, el n\u00famero total de par\u00e1metros.</p> <p>Podemos hacerlo contando el n\u00famero de elementos (o patrones/pesos) en <code>effnetb2.parameters()</code>. Accederemos al n\u00famero de elementos en cada par\u00e1metro usando <code>torch.numel()</code> (abreviatura de \"n\u00famero de elementos \") m\u00e9todo.</p>"},{"location":"13-09_pytorch_model_deployment/#4-creacion-de-un-extractor-de-funciones-vit","title":"4. Creaci\u00f3n de un extractor de funciones ViT\u00b6","text":"<p>Es hora de continuar con nuestros experimentos de modelado de FoodVision Mini.</p> <p>Esta vez vamos a crear un extractor de funciones de ViT.</p> <p>Y lo haremos de la misma manera que el extractor de funciones EffNetB2, excepto que esta vez con [<code>torchvision.models.vit_b_16()</code>](https://pytorch.org/vision/stable/models/generated/torchvision. models.vit_b_16.html#torchvision.models.vit_b_16) en lugar de <code>torchvision.models.ficientnet_b2()</code>.</p> <p>Comenzaremos creando una funci\u00f3n llamada <code>create_vit_model()</code> que ser\u00e1 muy similar a <code>create_effnetb2_model()</code> excepto, por supuesto, que devolver\u00e1 un modelo extractor de caracter\u00edsticas ViT y transformaciones en lugar de EffNetB2.</p> <p>Otra peque\u00f1a diferencia es que la capa de salida de <code>torchvision.models.vit_b_16()</code> se llama <code>cabezas</code> en lugar de <code>clasificador</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#41-crear-cargadores-de-datos-para-vit","title":"4.1 Crear cargadores de datos para ViT\u00b6","text":"<p>Tenemos nuestro modelo ViT listo, ahora creemos algunos <code>DataLoader</code>s para \u00e9l.</p> <p>Haremos esto de la misma manera que hicimos para EffNetB2 excepto que usaremos <code>vit_transforms</code> para transformar nuestras im\u00e1genes al mismo formato en el que se entren\u00f3 el modelo ViT.</p>"},{"location":"13-09_pytorch_model_deployment/#42-extractor-de-funciones-de-vit-de-entrenamiento","title":"4.2 Extractor de funciones de ViT de entrenamiento\u00b6","text":"<p>Sabes que hora es...</p> <p>...es hora de entrenargggggg (cantado con la misma melod\u00eda que la canci\u00f3n Closing Time).</p> <p>Entrenemos nuestro modelo de extracci\u00f3n de caracter\u00edsticas ViT durante 10 \u00e9pocas usando nuestra funci\u00f3n <code>engine.train()</code> con <code>torch.optim.Adam()</code> y una tasa de aprendizaje de <code>1e-3</code> como nuestro optimizador y <code>torch.nn.CrossEntropyLoss ()</code> como nuestra funci\u00f3n de p\u00e9rdida.</p> <p>Usaremos nuestra funci\u00f3n <code>set_seeds()</code> antes del entrenamiento para intentar que nuestros resultados sean lo m\u00e1s reproducibles posible.</p>"},{"location":"13-09_pytorch_model_deployment/#43-inspeccionando-las-curvas-de-perdidas-de-vit","title":"4.3 Inspeccionando las curvas de p\u00e9rdidas de ViT\u00b6","text":"<p>Muy bien, est\u00e1 bien, est\u00e1 bien, modelo ViT entrenado, seamos visuales y veamos algunas curvas de p\u00e9rdida.</p> <p>Nota: No olvide que puede ver c\u00f3mo deber\u00eda verse un conjunto ideal de curvas de p\u00e9rdida en [04. Secci\u00f3n 8 de conjuntos de datos personalizados de PyTorch] (https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like).</p>"},{"location":"13-09_pytorch_model_deployment/#44-guardar-el-extractor-de-funciones-de-vit","title":"4.4 Guardar el extractor de funciones de ViT\u00b6","text":"<p>\u00a1Nuestro modelo ViT est\u00e1 funcionando de manera excelente!</p> <p>As\u00ed que guard\u00e9moslo en un archivo para poder importarlo y usarlo m\u00e1s tarde si lo deseamos.</p> <p>Podemos hacerlo usando la funci\u00f3n <code>utils.save_model()</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 5.</p>"},{"location":"13-09_pytorch_model_deployment/#45-comprobar-el-tamano-del-extractor-de-funciones-vit","title":"4.5 Comprobar el tama\u00f1o del extractor de funciones ViT\u00b6","text":"<p>Y como queremos comparar nuestro modelo EffNetB2 con nuestro modelo ViT en funci\u00f3n de una serie de caracter\u00edsticas, averig\u00fcemos su tama\u00f1o.</p> <p>Para verificar el tama\u00f1o de nuestro modelo en bytes, podemos usar <code>pathlib.Path.stat(\"path_to_model\").st_size</code> de Python y luego podemos convertirlo (aproximadamente) a megabytes dividi\u00e9ndolo por <code>(1024*1024)</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#46-recopilacion-de-estadisticas-del-extractor-de-funciones-de-vit","title":"4.6 Recopilaci\u00f3n de estad\u00edsticas del extractor de funciones de ViT\u00b6","text":"<p>Reunamos todas las estad\u00edsticas del modelo de extracci\u00f3n de funciones de ViT.</p> <p>Lo vimos en el resumen anterior, pero calcularemos su n\u00famero total de par\u00e1metros.</p>"},{"location":"13-09_pytorch_model_deployment/#5-hacer-predicciones-con-nuestros-modelos-entrenados-y-cronometrarlas","title":"5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas\u00b6","text":"<p>Tenemos un par de modelos entrenados y ambos funcionan bastante bien.</p> <p>Ahora, \u00bfqu\u00e9 tal si los probamos haciendo lo que nos gustar\u00eda que hicieran?</p> <p>Es decir, veamos c\u00f3mo hacen predicciones (realizando inferencias).</p> <p>Sabemos que nuestros dos modelos funcionan con una precisi\u00f3n superior al 95 % en el conjunto de datos de prueba, pero \u00bfqu\u00e9 tan r\u00e1pidos son?</p> <p>Idealmente, si implementamos nuestro modelo FoodVision Mini en un dispositivo m\u00f3vil para que las personas puedan tomar fotograf\u00edas de sus alimentos e identificarlos, nos gustar\u00eda que las predicciones se realicen en tiempo real (~30 fotogramas por segundo).</p> <p>Por eso nuestro segundo criterio es: un modelo r\u00e1pido.</p> <p>Para saber cu\u00e1nto tiempo tarda cada uno de nuestros modelos en inferir el rendimiento, creemos una funci\u00f3n llamada <code>pred_and_store()</code> para iterar sobre cada una de las im\u00e1genes del conjunto de datos de prueba una por una y realizar una predicci\u00f3n.</p> <p>Calcularemos el tiempo de cada una de las predicciones y almacenaremos los resultados en un formato de predicci\u00f3n com\u00fan: una lista de diccionarios (donde cada elemento de la lista es una predicci\u00f3n \u00fanica y cada predicci\u00f3n \u00fanica es un diccionario).</p> <p>Nota: Calculamos las predicciones una por una en lugar de por lotes porque cuando se implementa nuestro modelo, probablemente solo har\u00e1 una predicci\u00f3n en una imagen a la vez. Es decir, alguien toma una foto y nuestro modelo predice sobre esa \u00fanica imagen.</p> <p>Como nos gustar\u00eda hacer predicciones en todas las im\u00e1genes del conjunto de prueba, primero obtengamos una lista de todas las rutas de las im\u00e1genes de prueba para que podamos iterar sobre ellas.</p> <p>Para hacerlo, usaremos <code>pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))</code> de Python. html#basic-use) para encontrar todas las rutas de archivos en un directorio de destino con la extensi\u00f3n <code>.jpg</code> (todas nuestras im\u00e1genes de prueba).</p>"},{"location":"13-09_pytorch_model_deployment/#51-crear-una-funcion-para-hacer-predicciones-en-todo-el-conjunto-de-datos-de-prueba","title":"5.1 Crear una funci\u00f3n para hacer predicciones en todo el conjunto de datos de prueba\u00b6","text":"<p>Ahora que tenemos una lista de nuestras rutas de im\u00e1genes de prueba, comencemos a trabajar en nuestra funci\u00f3n <code>pred_and_store()</code>:</p> <ol> <li>Cree una funci\u00f3n que tome una lista de rutas, un modelo PyTorch entrenado, una serie de transformaciones (para preparar im\u00e1genes), una lista de nombres de clases de destino y un dispositivo de destino.</li> <li>Cree una lista vac\u00eda para almacenar diccionarios de predicci\u00f3n (queremos que la funci\u00f3n devuelva una lista de diccionarios, uno para cada predicci\u00f3n).</li> <li>Recorra las rutas de entrada de destino (los pasos 4 a 14 se realizar\u00e1n dentro del bucle).</li> <li>Cree un diccionario vac\u00edo para cada iteraci\u00f3n del bucle para almacenar los valores de predicci\u00f3n por muestra.</li> <li>Obtenga la ruta de muestra y el nombre de la clase de verdad fundamental (podemos hacer esto infiriendo la clase a partir de la ruta).</li> <li>Inicie el temporizador de predicci\u00f3n usando <code>timeit.default_timer()</code> de Python.</li> <li>Abra la imagen usando <code>PIL.Image.open(path)</code>.</li> <li>Transforme la imagen para que pueda usarse con el modelo de destino, as\u00ed como agregue una dimensi\u00f3n por lotes y env\u00ede la imagen al dispositivo de destino.</li> <li>Prepare el modelo para la inferencia envi\u00e1ndolo al dispositivo de destino y activando el modo <code>eval()</code>.</li> <li>Active <code>torch.inference_mode()</code> y pase la imagen transformada de destino al modelo y calcule la probabilidad de predicci\u00f3n usando <code> torch.softmax()</code> y la etiqueta de destino usando <code>torch.argmax()</code>.</li> <li>Agregue la probabilidad de predicci\u00f3n y la clase de predicci\u00f3n al diccionario de predicci\u00f3n creado en el paso 4. Tambi\u00e9n aseg\u00farese de que la probabilidad de predicci\u00f3n est\u00e9 en la CPU para que pueda usarse con bibliotecas que no son de GPU, como NumPy y pandas, para una inspecci\u00f3n posterior.</li> <li>Finalice el temporizador de predicci\u00f3n iniciado en el paso 6 y agregue el tiempo al diccionario de predicci\u00f3n creado en el paso 4.</li> <li>Vea si la clase predicha coincide con la clase de verdad fundamental del paso 5 y agregue el resultado al diccionario de predicci\u00f3n creado en el paso 4.</li> <li>Agregue el diccionario de predicciones actualizado a la lista vac\u00eda de predicciones creada en el paso 2.</li> <li>Devuelve la lista de diccionarios de predicci\u00f3n.</li> </ol> <p>\u00a1Muchos pasos, pero nada que no podamos manejar!</p> <p>Vamos a hacerlo.</p>"},{"location":"13-09_pytorch_model_deployment/#52-realizacion-y-sincronizacion-de-predicciones-con-effnetb2","title":"5.2 Realizaci\u00f3n y sincronizaci\u00f3n de predicciones con EffNetB2\u00b6","text":"<p>\u00a1Es hora de probar nuestra funci\u00f3n <code>pred_and_store()</code>!</p> <p>Comencemos us\u00e1ndolo para hacer predicciones en todo el conjunto de datos de prueba con nuestro modelo EffNetB2, prestando atenci\u00f3n a dos detalles:</p> <ol> <li>Dispositivo: codificaremos el par\u00e1metro \"dispositivo\" para usar \"cpu\" porque cuando implementemos nuestro modelo, no siempre tendremos acceso a un dispositivo \"cuda\" (GPU). .<ul> <li>Hacer predicciones en la CPU tambi\u00e9n ser\u00e1 un buen indicador de la velocidad de inferencia porque generalmente las predicciones en dispositivos con CPU son m\u00e1s lentas que las de los dispositivos con GPU.</li> </ul> </li> <li>Transformaciones - Tambi\u00e9n nos aseguraremos de establecer el par\u00e1metro <code>transform</code> en <code>effnetb2_transforms</code> para asegurarnos de que las im\u00e1genes se abran y transformen de la misma manera en la que se entren\u00f3 nuestro modelo <code>effnetb2</code>.</li> </ol>"},{"location":"13-09_pytorch_model_deployment/#53-realizar-y-cronometrar-predicciones-con-vit","title":"5.3 Realizar y cronometrar predicciones con ViT\u00b6","text":"<p>Hemos hecho predicciones con nuestro modelo EffNetB2, ahora hagamos lo mismo con nuestro modelo ViT.</p> <p>Para hacerlo, podemos usar la funci\u00f3n <code>pred_and_store()</code> que creamos anteriormente, excepto que esta vez pasaremos nuestro modelo <code>vit</code> as\u00ed como <code>vit_transforms</code>.</p> <p>Y mantendremos las predicciones en la CPU a trav\u00e9s de <code>device=\"cpu\"</code> (una extensi\u00f3n natural aqu\u00ed ser\u00eda probar los tiempos de predicci\u00f3n en la CPU y en la GPU).</p>"},{"location":"13-09_pytorch_model_deployment/#6-comparacion-de-resultados-de-modelos-tiempos-de-prediccion-y-tamano","title":"6. Comparaci\u00f3n de resultados de modelos, tiempos de predicci\u00f3n y tama\u00f1o\u00b6","text":"<p>Nuestros dos mejores modelos contendientes han sido capacitados y evaluados.</p> <p>Ahora pong\u00e1moslos cara a cara y comparemos sus diferentes estad\u00edsticas.</p> <p>Para hacerlo, convierta nuestros diccionarios <code>effnetb2_stats</code> y <code>vit_stats</code> en un DataFrame de pandas.</p> <p>Agregaremos una columna para ver los nombres de los modelos y convertiremos la precisi\u00f3n de la prueba a un porcentaje completo en lugar de decimal.</p>"},{"location":"13-09_pytorch_model_deployment/#61-visualizacion-del-equilibrio-entre-velocidad-y-rendimiento","title":"6.1 Visualizaci\u00f3n del equilibrio entre velocidad y rendimiento\u00b6","text":"<p>Hemos visto que nuestro modelo ViT supera a nuestro modelo EffNetB2 en t\u00e9rminos de m\u00e9tricas de rendimiento, como p\u00e9rdida de prueba y precisi\u00f3n de prueba.</p> <p>Sin embargo, nuestro modelo EffNetB2 realiza predicciones m\u00e1s r\u00e1pido y tiene un tama\u00f1o de modelo mucho m\u00e1s peque\u00f1o.</p> <p>Nota: El tiempo de rendimiento o inferencia tambi\u00e9n suele denominarse \"latencia\".</p> <p>\u00bfQu\u00e9 tal si hacemos que este hecho sea visual?</p> <p>Podemos hacerlo creando un gr\u00e1fico con matplotlib:</p> <ol> <li>Cree un diagrama de dispersi\u00f3n a partir del marco de datos de comparaci\u00f3n para comparar los valores <code>time_per_pred_cpu</code> y <code>test_acc</code> de EffNetB2 y ViT.</li> <li>Agregue t\u00edtulos y etiquetas correspondientes a los datos y personalice el tama\u00f1o de fuente por motivos est\u00e9ticos.</li> <li>Anote las muestras en el diagrama de dispersi\u00f3n del paso 1 con sus etiquetas apropiadas (los nombres de los modelos).</li> <li>Cree una leyenda basada en los tama\u00f1os del modelo (<code>model_size (MB)</code>).</li> </ol>"},{"location":"13-09_pytorch_model_deployment/#7-dar-vida-a-foodvision-mini-creando-una-demostracion-de-gradio","title":"7. Dar vida a FoodVision Mini creando una demostraci\u00f3n de Gradio\u00b6","text":"<p>Hemos decidido que nos gustar\u00eda implementar el modelo EffNetB2 (para empezar, esto siempre se puede cambiar m\u00e1s adelante).</p> <p>Entonces, \u00bfc\u00f3mo podemos hacer eso?</p> <p>Hay varias formas de implementar un modelo de aprendizaje autom\u00e1tico, cada una con casos de uso espec\u00edficos (como se analiz\u00f3 anteriormente).</p> <p>Nos centraremos en la que quiz\u00e1s sea la forma m\u00e1s r\u00e1pida y ciertamente una de las m\u00e1s divertidas de implementar un modelo en Internet.</p> <p>Y eso es usando Gradio.</p> <p>\u00bfQu\u00e9 es Gradio?</p> <p>La p\u00e1gina de inicio lo describe maravillosamente:</p> <p>Gradio es la forma m\u00e1s r\u00e1pida de hacer una demostraci\u00f3n de su modelo de aprendizaje autom\u00e1tico con una interfaz web amigable para que cualquiera pueda usarlo, \u00a1en cualquier lugar!</p> <p>\u00bfPor qu\u00e9 crear una demostraci\u00f3n de tus modelos?</p> <p>Porque las m\u00e9tricas en el conjunto de prueba se ven bien, pero nunca se sabe realmente c\u00f3mo se desempe\u00f1a su modelo hasta que lo usa en la naturaleza.</p> <p>\u00a1As\u00ed que comencemos a implementar!</p> <p>Comenzaremos importando Gradio con el alias com\u00fan <code>gr</code> y, si no est\u00e1 presente, lo instalaremos.</p>"},{"location":"13-09_pytorch_model_deployment/#71-descripcion-general-de-gradio","title":"7.1 Descripci\u00f3n general de Gradio\u00b6","text":"<p>La premisa general de Gradio es muy similar a la que hemos ido repitiendo a lo largo del curso.</p> <p>\u00bfCu\u00e1les son nuestras entradas y salidas?</p> <p>\u00bfY c\u00f3mo deber\u00edamos llegar all\u00ed?</p> <p>Bueno, eso es lo que hace nuestro modelo de aprendizaje autom\u00e1tico.</p> <pre><code>entradas -&gt; modelo ML -&gt; salidas\n</code></pre> <p>En nuestro caso, para FoodVision Mini, nuestras entradas son im\u00e1genes de comida, nuestro modelo ML es EffNetB2 y nuestras salidas son clases de comida (pizza, bistec o sushi).</p> <pre><code>im\u00e1genes de alimentos -&gt; EffNetB2 -&gt; salidas\n</code></pre> <p>Aunque los conceptos de entradas y salidas pueden vincularse a casi cualquier otro tipo de problema de ML.</p> <p>Sus entradas y salidas pueden ser cualquier combinaci\u00f3n de lo siguiente:</p> <ul> <li>Im\u00e1genes</li> <li>Texto</li> <li>Video</li> <li>Datos tabulados *Audio</li> <li>N\u00fameros</li> <li>&amp; m\u00e1s</li> </ul> <p>Y el modelo de ML que cree depender\u00e1 de sus entradas y salidas.</p> <p>Gradio emula este paradigma creando una interfaz (<code>gradio.Interface()</code>) desde las entradas hasta las salidas.</p> <pre><code>gradio.Interface(fn, entradas, salidas)\n</code></pre> <p>Donde, \"fn\" es una funci\u00f3n de Python para asignar las \"entradas\" a las \"salidas\".</p> <p>Gradio proporciona una clase <code>Interfaz</code> muy \u00fatil para crear f\u00e1cilmente entradas -&gt; modelo/funci\u00f3n -&gt; flujo de trabajo de salidas donde las entradas y salidas pueden ser casi cualquier cosa que desee. Por ejemplo, puede ingresar Tweets (texto) para ver si tratan sobre aprendizaje autom\u00e1tico o no o ingrese un mensaje de texto para generar im\u00e1genes.</p> <p>Nota: Gradio tiene una gran cantidad de posibles opciones de \"entradas\" y \"salidas\" conocidas como \"Componentes\", desde im\u00e1genes hasta texto, n\u00fameros, audio, videos y m\u00e1s. Puede verlos todos en la [documentaci\u00f3n de componentes de Gradio] (https://gradio.app/docs/#components).</p>"},{"location":"13-09_pytorch_model_deployment/#72-creando-una-funcion-para-mapear-nuestras-entradas-y-salidas","title":"7.2 Creando una funci\u00f3n para mapear nuestras entradas y salidas\u00b6","text":"<p>Para crear nuestra demostraci\u00f3n de FoodVision Mini con Gradio, necesitaremos una funci\u00f3n para asignar nuestras entradas a nuestras salidas.</p> <p>Anteriormente creamos una funci\u00f3n llamada <code>pred_and_store()</code> para hacer predicciones con un modelo determinado en una lista de archivos de destino y almacenarlos en una lista de diccionarios.</p> <p>\u00bfQu\u00e9 tal si creamos una funci\u00f3n similar pero esta vez centr\u00e1ndonos en hacer una predicci\u00f3n en una sola imagen con nuestro modelo EffNetB2?</p> <p>M\u00e1s espec\u00edficamente, queremos una funci\u00f3n que tome una imagen como entrada, la preprocese (transforme), haga una predicci\u00f3n con EffNetB2 y luego devuelva la predicci\u00f3n (pred o etiqueta pred para abreviar), as\u00ed como la probabilidad de predicci\u00f3n (pred prob).</p> <p>Y ya que estamos aqu\u00ed, retrocedamos el tiempo que nos llev\u00f3 hacerlo tambi\u00e9n:</p> <pre><code>entrada: imagen -&gt; transformar -&gt; predecir con EffNetB2 -&gt; salida: pred, pred prob, tiempo necesario\n</code></pre> <p>Este ser\u00e1 nuestro par\u00e1metro <code>fn</code> para nuestra interfaz Gradio.</p> <p>Primero, asegur\u00e9monos de que nuestro modelo EffNetB2 est\u00e9 en la CPU (ya que nos atenemos a las predicciones solo de CPU, sin embargo, puedes cambiar esto si tienes acceso a una GPU).</p>"},{"location":"13-09_pytorch_model_deployment/#73-crear-una-lista-de-imagenes-de-ejemplo","title":"7.3 Crear una lista de im\u00e1genes de ejemplo\u00b6","text":"<p>Nuestra funci\u00f3n <code>predict()</code> nos permite ir desde entradas -&gt; transformar -&gt; modelo ML -&gt; salidas.</p> <p>Que es exactamente lo que necesitamos para nuestra demostraci\u00f3n de Graido.</p> <p>Pero antes de crear la demostraci\u00f3n, creemos una cosa m\u00e1s: una lista de ejemplos.</p> <p>La clase <code>Interface</code> de Gradio toma una lista de <code>ejemplos</code> como par\u00e1metro opcional (<code>gradio.Interface(examples=List[Any])</code>).</p> <p>Y el formato del par\u00e1metro \"ejemplos\" es una lista de listas.</p> <p>Entonces, creemos una lista de listas que contengan rutas de archivos aleatorias a nuestras im\u00e1genes de prueba.</p> <p>Tres ejemplos deber\u00edan ser suficientes.</p>"},{"location":"13-09_pytorch_model_deployment/#74-construyendo-una-interfaz-gradio","title":"7.4 Construyendo una interfaz Gradio\u00b6","text":"<p>\u00a1Es hora de juntar todo y darle vida a nuestra demostraci\u00f3n de FoodVision Mini!</p> <p>Creemos una interfaz Gradio para replicar el flujo de trabajo:</p> <pre><code>entrada: imagen -&gt; transformar -&gt; predecir con EffNetB2 -&gt; salida: pred, pred prob, tiempo necesario\n</code></pre> <p>Lo podemos hacer con la clase <code>gradio.Interface()</code> con los siguientes par\u00e1metros:</p> <ul> <li><code>fn</code> - una funci\u00f3n de Python para asignar <code>entradas</code> a <code>salidas</code>; en nuestro caso, usaremos nuestra funci\u00f3n <code>predict()</code>.</li> <li><code>inputs</code>: la entrada a nuestra interfaz, como una imagen usando <code>gradio.Image()</code> o <code>\"image\"</code>.</li> <li><code>outputs</code> - la salida de nuestra interfaz una vez que las <code>inputs</code> han pasado por <code>fn</code>, como una etiqueta usando <code>gradio.Label()</code> (para las etiquetas predichas de nuestro modelo) o n\u00famero usando <code>gradio.Number()</code> (para el tiempo de predicci\u00f3n de nuestro modelo).<ul> <li>Nota: Gradio viene con muchas opciones integradas de <code>entradas</code> y <code>salidas</code> conocidas como \"Componentes\".</li> </ul> </li> <li><code>ejemplos</code>: una lista de ejemplos para mostrar en la demostraci\u00f3n.</li> <li><code>title</code> - un t\u00edtulo de cadena de la demostraci\u00f3n.</li> <li><code>descripci\u00f3n</code>: una cadena de descripci\u00f3n de la demostraci\u00f3n.</li> <li><code>art\u00edculo</code>: una nota de referencia al final de la demostraci\u00f3n.</li> </ul> <p>Una vez que hayamos creado nuestra instancia de demostraci\u00f3n de <code>gr.Interface()</code>, podemos darle vida usando [<code>gradio.Interface().launch()</code>](https://gradio.app/docs/#launch -header) o el comando <code>demo.launch()</code>.</p> <p>\u00a1F\u00e1cil!</p>"},{"location":"13-09_pytorch_model_deployment/#8-convertir-nuestra-demostracion-foodvision-mini-gradio-en-una-aplicacion-implementable","title":"8. Convertir nuestra demostraci\u00f3n FoodVision Mini Gradio en una aplicaci\u00f3n implementable\u00b6","text":"<p>Hemos visto nuestro modelo FoodVision Mini cobrar vida a trav\u00e9s de una demostraci\u00f3n de Gradio.</p> <p>Pero \u00bfy si quisi\u00e9ramos compartirlo con nuestros amigos?</p> <p>Bueno, podr\u00edamos usar el enlace de Gradio proporcionado, sin embargo, el enlace compartido solo dura 72 horas.</p> <p>Para que nuestra demostraci\u00f3n de FoodVision Mini sea m\u00e1s permanente, podemos empaquetarla en una aplicaci\u00f3n y cargarla en Hugging Face Spaces.</p>"},{"location":"13-09_pytorch_model_deployment/#81-que-es-abrazar-los-espacios-faciales","title":"8.1 \u00bfQu\u00e9 es abrazar los espacios faciales?\u00b6","text":"<p>Hugging Face Spaces es un recurso que le permite alojar y compartir aplicaciones de aprendizaje autom\u00e1tico.</p> <p>Crear una demostraci\u00f3n es una de las mejores formas de mostrar y probar lo que ha hecho.</p> <p>Y Spaces te permite hacer precisamente eso.</p> <p>Puedes pensar en Hugging Face como el GitHub del aprendizaje autom\u00e1tico.</p> <p>Si tener un buen portafolio de GitHub muestra tus habilidades de codificaci\u00f3n, tener un buen portafolio de Hugging Face puede mostrar tus habilidades de aprendizaje autom\u00e1tico.</p> <p>Nota: Hay muchos otros lugares donde podr\u00edamos cargar y alojar nuestra aplicaci\u00f3n Gradio, como Google Cloud, AWS (Amazon Web Services) u otros proveedores de nube; sin embargo, usaremos Hugging Face Spaces debido a la facilidad de uso y la amplia adopci\u00f3n por parte de la comunidad de aprendizaje autom\u00e1tico.</p>"},{"location":"13-09_pytorch_model_deployment/#82-estructura-de-la-aplicacion-gradio-implementada","title":"8.2 Estructura de la aplicaci\u00f3n Gradio implementada\u00b6","text":"<p>Para cargar nuestra aplicaci\u00f3n de demostraci\u00f3n Gradio, queremos colocar todo lo relacionado con ella en un solo directorio.</p> <p>Por ejemplo, nuestra demostraci\u00f3n podr\u00eda estar en la ruta <code>demos/foodvision_mini/</code> con la estructura de archivos:</p> <pre><code>poblaci\u00f3n/\n\u2514\u2500\u2500 comidavision_mini/\n    \u251c\u2500\u2500 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n    \u251c\u2500\u2500 aplicaci\u00f3n.py\n    \u251c\u2500\u2500 ejemplos/\n    \u2502 \u251c\u2500\u2500 ejemplo_1.jpg\n    \u2502 \u251c\u2500\u2500 ejemplo_2.jpg\n    \u2502 \u2514\u2500\u2500 ejemplo_3.jpg\n    \u251c\u2500\u2500 modelo.py\n    \u2514\u2500\u2500 requisitos.txt\n</code></pre> <p>D\u00f3nde:</p> <ul> <li><code>09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code> es nuestro archivo modelo PyTorch entrenado.</li> <li><code>app.py</code> contiene nuestra aplicaci\u00f3n Gradio (similar al c\u00f3digo que inici\u00f3 la aplicaci\u00f3n).<ul> <li>Nota: <code>app.py</code> es el nombre de archivo predeterminado utilizado para Hugging Face Spaces. Si implementas tu aplicaci\u00f3n all\u00ed, Spaces buscar\u00e1 de manera predeterminada un archivo llamado <code>app.py</code> para ejecutar. Esto se puede cambiar en la configuraci\u00f3n.</li> </ul> </li> <li><code>examples/</code> contiene im\u00e1genes de ejemplo para usar con nuestra aplicaci\u00f3n Gradio.</li> <li><code>model.py</code> contiene la definici\u00f3n del modelo, as\u00ed como cualquier transformaci\u00f3n asociada con el modelo.</li> <li><code>requirements.txt</code> contiene las dependencias para ejecutar nuestra aplicaci\u00f3n, como <code>torch</code>, <code>torchvision</code> y <code>gradio</code>.</li> </ul> <p>\u00bfPor qu\u00e9 de esta manera?</p> <p>Porque es uno de los dise\u00f1os m\u00e1s simples con los que podr\u00edamos empezar.</p> <p>Nuestro enfoque es: \u00a1experimentar, experimentar, experimentar!</p> <p>Cuanto m\u00e1s r\u00e1pido podamos realizar experimentos m\u00e1s peque\u00f1os, mejores ser\u00e1n los m\u00e1s grandes.</p> <p>Vamos a trabajar para recrear la estructura anterior, pero puedes ver una aplicaci\u00f3n de demostraci\u00f3n en vivo ejecut\u00e1ndose en Hugging Face Spaces, as\u00ed como la estructura de archivos:</p> <ul> <li>Demostraci\u00f3n en vivo de Gradio de FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63.</li> <li>Estructura de archivos FoodVision Mini en Hugging Face Spaces.</li> </ul>"},{"location":"13-09_pytorch_model_deployment/#83-creando-una-carpeta-demos-para-almacenar-los-archivos-de-nuestra-aplicacion-foodvision-mini","title":"8.3 Creando una carpeta <code>demos</code> para almacenar los archivos de nuestra aplicaci\u00f3n FoodVision Mini\u00b6","text":"<p>Para comenzar, primero creemos un directorio <code>demos/</code> para almacenar todos los archivos de nuestra aplicaci\u00f3n FoodVision Mini.</p> <p>Podemos hacerlo con <code>pathlib.Path(\"path_to_dir\")</code> de Python para establecer la ruta del directorio y <code>pathlib. Path(\"path_to_dir\").mkdir()</code> para crearlo.</p>"},{"location":"13-09_pytorch_model_deployment/#84-crear-una-carpeta-de-imagenes-de-ejemplo-para-usar-con-nuestra-demostracion-de-foodvision-mini","title":"8.4 Crear una carpeta de im\u00e1genes de ejemplo para usar con nuestra demostraci\u00f3n de FoodVision Mini\u00b6","text":"<p>Ahora que tenemos un directorio para almacenar nuestros archivos de demostraci\u00f3n de FoodVision Mini, agreguemos algunos ejemplos.</p> <p>Tres im\u00e1genes de ejemplo del conjunto de datos de prueba deber\u00edan ser suficientes.</p> <p>Para hacerlo haremos:</p> <ol> <li>Cree un directorio <code>examples/</code> dentro del directorio <code>demos/foodvision_mini</code>.</li> <li>Elija tres im\u00e1genes aleatorias del conjunto de datos de prueba y recopile sus rutas de archivo en una lista.</li> <li>Copie las tres im\u00e1genes aleatorias del conjunto de datos de prueba al directorio <code>demos/foodvision_mini/examples/</code>.</li> </ol>"},{"location":"13-09_pytorch_model_deployment/#85-mover-nuestro-modelo-effnetb2-entrenado-a-nuestro-directorio-de-demostracion-foodvision-mini","title":"8.5 Mover nuestro modelo EffNetB2 entrenado a nuestro directorio de demostraci\u00f3n FoodVision Mini\u00b6","text":"<p>Anteriormente guardamos nuestro modelo de extractor de funciones FoodVision Mini EffNetB2 en <code>models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code>.</p> <p>Y en lugar de duplicar los archivos de modelo guardados, muevamos nuestro modelo a nuestro directorio <code>demos/foodvision_mini</code>.</p> <p>Podemos hacerlo usando el m\u00e9todo <code>shutil.move()</code> de Python y pasando <code>src</code> (la ruta fuente de el archivo de destino) y <code>dst</code> (la ruta de destino del archivo de destino al que se va a mover).</p>"},{"location":"13-09_pytorch_model_deployment/#86-convirtiendo-nuestro-modelo-effnetb2-en-un-script-python-modelpy","title":"8.6 Convirtiendo nuestro modelo EffNetB2 en un script Python (<code>model.py</code>)\u00b6","text":"<p>El <code>state_dict</code> de nuestro modelo actual se guarda en <code>demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code>.</p> <p>Para cargarlo podemos usar <code>model.load_state_dict()</code> junto con <code>torch.load()</code>.</p> <p>Nota: Para obtener una actualizaci\u00f3n sobre c\u00f3mo guardar y cargar un modelo (o el <code>state_dict</code> de un modelo en PyTorch, consulte [01. Fundamentos del flujo de trabajo de PyTorch, secci\u00f3n 5: Guardar y cargar un modelo de PyTorch](https://www. learnpytorch.io/01_pytorch_workflow/#5-served-and-loading-a-pytorch-model) o consulte la receta de PyTorch para [\u00bfQu\u00e9 es un <code>state_dict</code> en PyTorch?](https://pytorch.org/tutorials/recipes /recetas/what_is_state_dict.html)</p> <p>Pero antes de que podamos hacer esto, primero necesitamos una forma de crear una instancia de un \"modelo\".</p> <p>Para hacer esto de forma modular, crearemos un script llamado <code>model.py</code> que contiene nuestra funci\u00f3n <code>create_effnetb2_model()</code> que creamos en [secci\u00f3n 3.1: Creaci\u00f3n de una funci\u00f3n para crear un extractor de funciones EffNetB2](https: //www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor).</p> <p>De esa manera podemos importar la funci\u00f3n en otro script (ver <code>app.py</code> a continuaci\u00f3n) y luego usarla para crear nuestra instancia de <code>modelo</code> EffNetB2, as\u00ed como obtener sus transformaciones apropiadas.</p> <p>Al igual que en 05. PyTorch Going Modular, usaremos el comando m\u00e1gico <code>%%writefile path/to/file</code> para convertir una celda de c\u00f3digo en un archivo.</p>"},{"location":"13-09_pytorch_model_deployment/#87-convirtiendo-nuestra-aplicacion-foodvision-mini-gradio-en-un-script-python-apppy","title":"8.7 Convirtiendo nuestra aplicaci\u00f3n FoodVision Mini Gradio en un script Python (<code>app.py</code>)\u00b6","text":"<p>Ahora tenemos un script <code>model.py</code> as\u00ed como una ruta a un modelo guardado <code>state_dict</code> que podemos cargar.</p> <p>Es hora de construir <code>app.py</code>.</p> <p>Lo llamamos <code>app.py</code> porque, de forma predeterminada, cuando creas un espacio HuggingFace, busca un archivo llamado <code>app.py</code> para ejecutarlo y alojarlo (aunque puedes cambiar esto en la configuraci\u00f3n).</p> <p>Nuestro script <code>app.py</code> juntar\u00e1 todas las piezas del rompecabezas para crear nuestra demostraci\u00f3n de Gradio y tendr\u00e1 cuatro partes principales:</p> <ol> <li>Configuraci\u00f3n de importaciones y nombres de clases - Aqu\u00ed importaremos las diversas dependencias para nuestra demostraci\u00f3n, incluida la funci\u00f3n <code>create_effnetb2_model()</code> de <code>model.py</code>, as\u00ed como tambi\u00e9n configuraremos los diferentes nombres de clases para nuestra aplicaci\u00f3n FoodVision Mini. .</li> <li>Preparaci\u00f3n de modelos y transformaciones: aqu\u00ed crearemos una instancia de modelo EffNetB2 junto con las transformaciones que la acompa\u00f1an y luego cargaremos los pesos/<code>state_dict</code> del modelo guardado. Cuando cargamos el modelo, tambi\u00e9n configuraremos <code>map_location=torch.device(\"cpu\")</code> en [<code>torch.load()</code>](https://pytorch.org/docs/stable/generated/torch.load .html) para que nuestro modelo se cargue en la CPU independientemente del dispositivo en el que se entren\u00f3 (hacemos esto porque no necesariamente tendremos una GPU cuando implementemos y obtendremos un error si nuestro modelo est\u00e1 entrenado en GPU pero intente implementarlo en la CPU sin decirlo expl\u00edcitamente).</li> <li>Funci\u00f3n de predicci\u00f3n - <code>gradio.Interface()</code> de Gradio toma un par\u00e1metro <code>fn</code> para asignar entradas a salidas, nuestra funci\u00f3n <code>predict()</code> ser\u00e1 la misma que definimos anteriormente en la secci\u00f3n 7.2 : Creando una funci\u00f3n para mapear nuestras entradas y salidas, lo har\u00e1 tome una imagen y luego use las transformaciones cargadas para preprocesarla antes de usar el modelo cargado para hacer una predicci\u00f3n sobre ella.<ul> <li>Nota: Tendremos que crear la lista de ejemplo sobre la marcha mediante el par\u00e1metro <code>examples</code>. Podemos hacerlo creando una lista de archivos dentro del directorio <code>examples/</code> con: <code>[[\"examples/\" + example] por ejemplo en os.listdir(\"examples\")]</code>.</li> </ul> </li> <li>Aplicaci\u00f3n Gradio - Aqu\u00ed es donde vivir\u00e1 la l\u00f3gica principal de nuestra demostraci\u00f3n, crearemos una instancia <code>gradio.Interface()</code> llamada <code>demo</code> para juntar nuestras entradas, funci\u00f3n <code>predict()</code> y salidas. \u00a1Y terminaremos el script llamando a <code>demo.launch()</code> para iniciar nuestra demostraci\u00f3n de FoodVision Mini!</li> </ol>"},{"location":"13-09_pytorch_model_deployment/#88-creacion-de-un-archivo-de-requisitos-para-foodvision-mini-requirementstxt","title":"8.8 Creaci\u00f3n de un archivo de requisitos para FoodVision Mini (<code>requirements.txt</code>)\u00b6","text":"<p>El \u00faltimo archivo que debemos crear para nuestra aplicaci\u00f3n FoodVision Mini es un [archivo <code>requirements.txt</code>] (https://learnpython.com/blog/python-requirements-file/).</p> <p>Este ser\u00e1 un archivo de texto que contendr\u00e1 todas las dependencias necesarias para nuestra demostraci\u00f3n.</p> <p>Cuando implementemos nuestra aplicaci\u00f3n de demostraci\u00f3n en Hugging Face Spaces, buscar\u00e1 en este archivo e instalar\u00e1 las dependencias que definimos para que nuestra aplicaci\u00f3n pueda ejecutarse.</p> <p>\u00a1La buena noticia es que s\u00f3lo hay tres!</p> <ol> <li><code>antorcha==1.12.0</code></li> <li><code>torchvision==0.13.0</code></li> <li><code>gradio==3.1.4</code></li> </ol> <p>\"<code>==1.12.0</code>\" indica el n\u00famero de versi\u00f3n a instalar.</p> <p>Definir el n\u00famero de versi\u00f3n no es 100% obligatorio, pero lo haremos por ahora, de modo que si se producen actualizaciones importantes en futuras versiones, nuestra aplicaci\u00f3n a\u00fan se ejecuta (PD: si encuentra alg\u00fan error, no dude en publicarlo en el curso [Problemas de GitHub](https: //github.com/mrdbourke/pytorch-deep-learning/issues)).</p>"},{"location":"13-09_pytorch_model_deployment/#9-implementacion-de-nuestra-aplicacion-foodvision-mini-en-huggingface-spaces","title":"9. Implementaci\u00f3n de nuestra aplicaci\u00f3n FoodVision Mini en HuggingFace Spaces\u00b6","text":"<p>Tenemos un archivo que contiene nuestra demostraci\u00f3n de FoodVision Mini. Ahora, \u00bfc\u00f3mo hacemos para que se ejecute en Hugging Face Spaces?</p> <p>Hay dos opciones principales para cargar en un Hugging Face Space (tambi\u00e9n llamado Repositorio de Hugging Face, similares a un repositorio de git):</p> <ol> <li>Carga a trav\u00e9s de la interfaz web de Hugging Face (m\u00e1s f\u00e1cil).</li> <li>[Carga a trav\u00e9s de la l\u00ednea de comando o terminal] (https://huggingface.co/docs/hub/repositories-getting-started#terminal).<ul> <li>Bonificaci\u00f3n: Tambi\u00e9n puedes usar la biblioteca <code>huggingface_hub</code> para interactuar con Hugging Face, esta ser\u00eda una buena extensi\u00f3n de las dos opciones anteriores. .</li> </ul> </li> </ol> <p>No dude en leer la documentaci\u00f3n sobre ambas opciones, pero optaremos por la opci\u00f3n dos.</p> <p>Nota: Para alojar cualquier cosa en Hugging Face, deber\u00e1s registrarte para obtener una cuenta gratuita de Hugging Face.</p>"},{"location":"13-09_pytorch_model_deployment/#91-descarga-de-los-archivos-de-nuestra-aplicacion-foodvision-mini","title":"9.1 Descarga de los archivos de nuestra aplicaci\u00f3n FoodVision Mini\u00b6","text":"<p>Veamos los archivos de demostraci\u00f3n que tenemos dentro de <code>demos/foodvision_mini</code>.</p> <p>Para hacerlo, podemos usar el comando <code>!ls</code> seguido de la ruta del archivo de destino.</p> <p><code>ls</code> significa \"lista\" y <code>!</code> significa que queremos ejecutar el comando en el nivel de shell.</p>"},{"location":"13-09_pytorch_model_deployment/#92-ejecucion-de-nuestra-demostracion-foodvision-mini-localmente","title":"9.2 Ejecuci\u00f3n de nuestra demostraci\u00f3n FoodVision Mini localmente\u00b6","text":"<p>Si descarga el archivo <code>foodvision_mini.zip</code>, puede probarlo localmente de la siguiente manera:</p> <ol> <li>Descomprimiendo el archivo.</li> <li>Abrir la terminal o una l\u00ednea de comando.</li> <li>Cambiar al directorio <code>foodvision_mini</code> (<code>cd foodvision_mini</code>).</li> <li>Crear un entorno (<code>python3 -m venv env</code>).</li> <li>Activar el entorno (<code>source env/bin/activate</code>).</li> <li>Instalar los requisitos (<code>pip install -r requisitos.txt</code>, \"<code>-r</code>\" es para recursivo).<ul> <li>Nota: Este paso puede tardar entre 5 y 10 minutos dependiendo de tu conexi\u00f3n a Internet. Y si enfrenta errores, es posible que primero necesite actualizar <code>pip</code>: <code>pip install --upgrade pip</code>.</li> </ul> </li> <li>Ejecute la aplicaci\u00f3n (<code>python3 app.py</code>).</li> </ol> <p>Esto deber\u00eda dar como resultado una demostraci\u00f3n de Gradio como la que creamos anteriormente ejecut\u00e1ndose localmente en su m\u00e1quina en una URL como <code>http://127.0.0.1:7860/</code>.</p> <p>Nota: Si ejecuta la aplicaci\u00f3n localmente y observa que aparece un directorio <code>marcado/</code>, contiene muestras que han sido \"marcadas\".</p> <p>Por ejemplo, si alguien prueba la demostraci\u00f3n y el modelo produce un resultado incorrecto, la muestra se puede \"marcar\" y revisar para m\u00e1s adelante.</p> <p>Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo marcar en Gradio, consulte la documentaci\u00f3n sobre c\u00f3mo marcar.</p>"},{"location":"13-09_pytorch_model_deployment/#93-subir-a-hugging-face","title":"9.3 Subir a Hugging Face\u00b6","text":"<p>Hemos verificado que nuestra aplicaci\u00f3n FoodVision Mini funciona localmente; sin embargo, lo divertido de crear una demostraci\u00f3n de aprendizaje autom\u00e1tico es mostr\u00e1rsela a otras personas y permitirles usarla.</p> <p>Para hacerlo, cargaremos nuestra demostraci\u00f3n de FoodVision Mini en Hugging Face.</p> <p>Nota: La siguiente serie de pasos utiliza un flujo de trabajo Git (un sistema de seguimiento de archivos). Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo funciona Git, recomiendo consultar el tutorial de Git y GitHub para principiantes en freeCodeCamp.</p> <ol> <li>Reg\u00edstrese para obtener una cuenta de Hugging Face.</li> <li>Inicie un nuevo Hugging Face Space yendo a su perfil y luego haciendo clic en \"Nuevo espacio\".<ul> <li>Nota: Un espacio en Hugging Face tambi\u00e9n se conoce como \"repositorio de c\u00f3digos\" (un lugar para almacenar su c\u00f3digo/archivos) o \"repositorio\" para abreviar.</li> </ul> </li> <li>Dale un nombre al Espacio, por ejemplo, el m\u00edo se llama <code>mrdbourke/foodvision_mini</code>, puedes verlo aqu\u00ed: https://huggingface.co/spaces/mrdbourke/foodvision_mini</li> <li>Seleccione una licencia (yo us\u00e9 MIT).</li> <li>Seleccione Gradio como Space SDK (kit de desarrollo de software).<ul> <li>Nota: Puedes usar otras opciones como Streamlit, pero como nuestra aplicaci\u00f3n est\u00e1 construida con Gradio, nos quedaremos con eso.</li> </ul> </li> <li>Elija si su Espacio es p\u00fablico o privado (seleccion\u00e9 p\u00fablico porque me gustar\u00eda que mi Espacio est\u00e9 disponible para otros).</li> <li>Haga clic en \"Crear espacio\".</li> <li>Clone el repositorio localmente ejecutando algo como: <code>git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]</code> en la terminal o en el s\u00edmbolo del sistema.<ul> <li>Nota: Tambi\u00e9n puedes agregar archivos carg\u00e1ndolos en la pesta\u00f1a \"Archivos y versiones\".</li> </ul> </li> <li>Copie/mueva el contenido de la carpeta <code>foodvision_mini</code> descargada a la carpeta del repositorio clonado.</li> <li>Para cargar y rastrear archivos m\u00e1s grandes (por ejemplo, archivos de m\u00e1s de 10 MB o, en nuestro caso, nuestro archivo modelo PyTorch), necesitar\u00e1 instalar Git LFS (que significa para \"almacenamiento de archivos grandes de git\").</li> <li>Despu\u00e9s de haber instalado Git LFS, puedes activarlo ejecutando <code>git lfs install</code>.</li> <li>En el directorio <code>foodvision_mini</code>, realice un seguimiento de los archivos de m\u00e1s de 10 MB con Git LFS con <code>git lfs track \"*.file_extension\"</code>.<ul> <li>Realice un seguimiento del archivo de modelo EffNetB2 PyTorch con <code>git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"</code>.</li> </ul> </li> <li>Seguimiento de <code>.gitattributes</code> (creado autom\u00e1ticamente al clonar desde HuggingFace; este archivo ayudar\u00e1 a garantizar que nuestros archivos m\u00e1s grandes sean rastreados con Git LFS). Puede ver un archivo <code>.gitattributes</code> de ejemplo en [FoodVision Mini Hugging Face Space] (https://huggingface.co/spaces/mrdbourke/foodvision_mini/blob/main/.gitattributes).<ul> <li><code>git agregar .gitattributes</code></li> </ul> </li> <li>Agregue el resto de los archivos de la aplicaci\u00f3n <code>foodvision_mini</code> y conf\u00edrmelos con:<ul> <li><code>git agregar *</code></li> <li><code>git commit -m \"primer compromiso\"</code></li> </ul> </li> <li>Env\u00ede (cargue) los archivos a Hugging Face: *<code>git push</code></li> <li>Espere de 3 a 5 minutos hasta que se complete la compilaci\u00f3n (las compilaciones futuras ser\u00e1n m\u00e1s r\u00e1pidas) y su aplicaci\u00f3n estar\u00e1 activa.</li> </ol> <p>Si todo funcion\u00f3, deber\u00eda ver un ejemplo en vivo de nuestra demostraci\u00f3n de FoodVision Mini Gradio como este: https://huggingface.co/spaces/mrdbourke/foodvision_mini</p> <p>E incluso podemos insertar nuestra demostraci\u00f3n de FoodVision Mini Gradio en nuestra computadora port\u00e1til como un iframe con [<code>IPython.display.IFrame</code>](https:/ /ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) y un enlace a nuestro espacio en el formato <code>https://hf.space/embed/[YOUR_USERNAME] /[TU_NOMBRE_ESPACIO]/+</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#10-creando-foodvision-a-lo-grande","title":"10. Creando FoodVision a lo grande\u00b6","text":"<p>Hemos pasado las \u00faltimas secciones y cap\u00edtulos trabajando para darle vida a FoodVision Mini.</p> <p>Y ahora que lo hemos visto funcionar en una demostraci\u00f3n en vivo, \u00bfqu\u00e9 tal si damos un paso m\u00e1s?</p> <p>\u00bfC\u00f3mo?</p> <p>\u00a1Visi\u00f3n alimentaria a lo grande!</p> <p>Dado que FoodVision Mini est\u00e1 entrenado con im\u00e1genes de pizza, bistec y sushi del [conjunto de datos Food101] (https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) (101 clases de alimentos x 1000 im\u00e1genes cada una ), \u00bfqu\u00e9 tal si hacemos FoodVision Big entrenando un modelo en las 101 clases?</p> <p>\u00a1Pasaremos de tres clases a 101!</p> <p>\u00a1Desde pizza, bistec, sushi hasta pizza, bistec, sushi, hot dog, tarta de manzana, pastel de zanahoria, pastel de chocolate, fuegos franceses, pan de ajo, ramen, nachos, tacos y m\u00e1s!</p> <p>\u00bfC\u00f3mo?</p> <p>Bueno, tenemos todos los pasos implementados, todo lo que tenemos que hacer es modificar ligeramente nuestro modelo EffNetB2 y preparar un conjunto de datos diferente.</p> <p>Para finalizar Milestone Project 3, recreemos una demostraci\u00f3n de Gradio similar a FoodVision Mini (tres clases) pero para FoodVision Big (101 clases).</p> <p>FoodVision Mini funciona con tres clases de alimentos: pizza, bistec y sushi. Y FoodVision Big va un paso m\u00e1s all\u00e1 para trabajar en 101 clases de alimentos: todas las [clases en el conjunto de datos Food101](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names. TXT).</p>"},{"location":"13-09_pytorch_model_deployment/#101-creando-un-modelo-y-transformaciones-para-foodvision-big","title":"10.1 Creando un modelo y transformaciones para FoodVision Big\u00b6","text":"<p>Al crear FoodVision Mini vimos que el modelo EffNetB2 era un buen equilibrio entre velocidad y rendimiento (funcion\u00f3 bien a alta velocidad).</p> <p>As\u00ed que continuaremos usando el mismo modelo para FoodVision Big.</p> <p>Podemos crear un extractor de funciones EffNetB2 para Food101 usando nuestra funci\u00f3n <code>create_effnetb2_model()</code> que creamos anteriormente, en la [secci\u00f3n 3.1](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to -make-an-effnetb2-feature-extractor), y pas\u00e1ndole el par\u00e1metro <code>num_classes=101</code> (ya que Food101 tiene 101 clases).</p>"},{"location":"13-09_pytorch_model_deployment/#102-obtencion-de-datos-para-foodvision-big","title":"10.2 Obtenci\u00f3n de datos para FoodVision Big\u00b6","text":"<p>Para FoodVision Mini, creamos nuestras propias divisiones de datos personalizadas de todo el conjunto de datos de Food101.</p> <p>Para obtener el conjunto de datos completo de Food101, podemos usar <code>torchvision.datasets.Food101()</code>.</p> <p>Primero configuraremos una ruta al directorio <code>data/</code> para almacenar las im\u00e1genes.</p> <p>Luego descargaremos y transformaremos las divisiones del conjunto de datos de entrenamiento y prueba usando <code>food101_train_transforms</code> y <code>effnetb2_transforms</code> para transformar cada conjunto de datos respectivamente.</p> <p>Nota: Si est\u00e1 utilizando Google Colab, la siguiente celda tardar\u00e1 entre 3 y 5 minutos en ejecutarse por completo y descargar las im\u00e1genes de Food101 desde PyTorch.</p> <p>Esto se debe a que se est\u00e1n descargando m\u00e1s de 100.000 im\u00e1genes (101 clases x 1000 im\u00e1genes por clase). Si reinicia el tiempo de ejecuci\u00f3n de Google Colab y regresa a esta celda, las im\u00e1genes deber\u00e1n volver a descargarse. Alternativamente, si est\u00e1 ejecutando este cuaderno localmente, las im\u00e1genes se almacenar\u00e1n en cach\u00e9 y se almacenar\u00e1n en el directorio especificado por el par\u00e1metro <code>root</code> de <code>torchvision.datasets.Food101()</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#103-creacion-de-un-subconjunto-del-conjunto-de-datos-food101-para-experimentar-mas-rapido","title":"10.3 Creaci\u00f3n de un subconjunto del conjunto de datos Food101 para experimentar m\u00e1s r\u00e1pido\u00b6","text":"<p>Esto es opcional.</p> <p>No necesitamos crear otro subconjunto del conjunto de datos de Food101, podr\u00edamos entrenar y evaluar un modelo en las 101.000 im\u00e1genes completas.</p> <p>Pero para seguir entrenando r\u00e1pido, creemos una divisi\u00f3n del 20 % de los conjuntos de datos de entrenamiento y prueba.</p> <p>Nuestro objetivo ser\u00e1 ver si podemos superar los mejores resultados del [documento Food101] original (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) con solo el 20 % de los datos.</p> <p>Para desglosar los conjuntos de datos que hemos utilizado/utilizaremos:</p> Cuadernos(s) Nombre del proyecto Conjunto de datos N\u00famero de clases Im\u00e1genes de entrenamiento Im\u00e1genes de prueba 04, 05, 06, 07, 08 FoodVision Mini (10% de datos) Food101 divisi\u00f3n personalizada 3 (pizza, bistec, sushi) 225 75 07, 08, 09 FoodVision Mini (20% de datos) Food101 divisi\u00f3n personalizada 3 (pizza, bistec, sushi) 450 150 09 (este) FoodVision Big (20% de datos) Food101 divisi\u00f3n personalizada 101 (todas las clases de Food101) 15150 5050 Ampliaci\u00f3n FoodVision grande Food101 todos los datos 101 75750 25250 <p>\u00bfPuedes ver la tendencia?</p> <p>As\u00ed como el tama\u00f1o de nuestro modelo aument\u00f3 lentamente con el tiempo, tambi\u00e9n lo hizo el tama\u00f1o del conjunto de datos que hemos estado usando para los experimentos.</p> <p>Nota: Para superar realmente los resultados del art\u00edculo original de Food101 con el 20 % de los datos, tendr\u00edamos que entrenar un modelo con el 20 % de los datos de entrenamiento y luego evaluar nuestro modelo en el conjunto de pruebas completo en lugar de que la divisi\u00f3n que creamos. Dejar\u00e9 esto como un ejercicio de extensi\u00f3n para que lo pruebes. Tambi\u00e9n le recomiendo que intente entrenar un modelo en todo el conjunto de datos de entrenamiento de Food101.</p> <p>Para dividir nuestro FoodVision Big (20% de datos), creemos una funci\u00f3n llamada <code>split_dataset()</code> para dividir un conjunto de datos determinado en ciertas proporciones.</p> <p>Podemos usar <code>torch.utils.data.random_split()</code> para crear divisiones de tama\u00f1os determinados usando ` par\u00e1metro de longitudes.</p> <p>El par\u00e1metro <code>longitudes</code> acepta una lista de longitudes divididas deseadas donde el total de la lista debe ser igual a la longitud total del conjunto de datos.</p> <p>Por ejemplo, con un conjunto de datos de tama\u00f1o 100, podr\u00eda pasar <code>longitudes=[20, 80]</code> para recibir una divisi\u00f3n del 20 % y del 80 %.</p> <p>Querremos que nuestra funci\u00f3n devuelva dos divisiones, una con la longitud objetivo (por ejemplo, el 20 % de los datos de entrenamiento) y la otra con la longitud restante (por ejemplo, el 80 % restante de los datos de entrenamiento).</p> <p>Finalmente, estableceremos el par\u00e1metro <code>generador</code> en un valor <code>torch.manual_seed()</code> para mayor reproducibilidad.</p>"},{"location":"13-09_pytorch_model_deployment/#104-convirtiendo-nuestros-conjuntos-de-datos-de-food101-en-dataloaders","title":"10.4 Convirtiendo nuestros conjuntos de datos de Food101 en <code>DataLoader</code>s\u00b6","text":"<p>Ahora conviertamos nuestras divisiones del conjunto de datos Food101 20% en <code>DataLoader</code> usando <code>torch.utils.data.DataLoader()</code>.</p> <p>Estableceremos <code>shuffle=True</code> solo para los datos de entrenamiento y el tama\u00f1o del lote en <code>32</code> para ambos conjuntos de datos.</p> <p>Y estableceremos <code>num_workers</code> en <code>4</code> si el recuento de CPU est\u00e1 disponible o <code>2</code> si no lo est\u00e1 (aunque el valor de <code>num_workers</code> es muy experimental y depender\u00e1 del hardware que est\u00e9s usando, hay un  hilo de discusi\u00f3n activo sobre esto en los foros de PyTorch).</p>"},{"location":"13-09_pytorch_model_deployment/#105-entrenamiento-foodvision-modelo-grande","title":"10.5 Entrenamiento FoodVision Modelo grande\u00b6","text":"<p>\u00a1El modelo FoodVision Big y <code>DataLoader</code>s est\u00e1n listos!</p> <p>Hora de entrenar.</p> <p>Crearemos un optimizador usando <code>torch.optim.Adam()</code> y una tasa de aprendizaje de <code>1e-3</code>.</p> <p>Y debido a que tenemos tantas clases, tambi\u00e9n configuraremos una funci\u00f3n de p\u00e9rdida usando <code>torch.nn.CrossEntropyLoss()</code> con <code>label_smoothing=0.1</code>, en l\u00ednea con la tecnolog\u00eda de punta de <code>torchvision</code> receta de entrenamiento.</p> <p>\u00bfQu\u00e9 es suavizado de etiquetas?</p> <p>El suavizado de etiquetas es una t\u00e9cnica de regularizaci\u00f3n (regularizaci\u00f3n es otra palabra para describir el proceso de prevenci\u00f3n del sobreajuste) que reduce la valor que un modelo le da a cualquier etiqueta y lo distribuye entre las dem\u00e1s etiquetas.</p> <p>En esencia, en lugar de que un modelo se demasiado confiado en una sola etiqueta, el suavizado de etiquetas otorga un valor distinto de cero a otras etiquetas para ayudar en la generalizaci\u00f3n.</p> <p>Por ejemplo, si un modelo sin suavizado de etiquetas tuviera los siguientes resultados para 5 clases:</p> <pre><code>[0, 0, 0,99, 0,01, 0]\n</code></pre> <p>Un modelo con suavizado de etiquetas puede tener los siguientes resultados:</p> <pre><code>[0,01, 0,01, 0,96, 0,01, 0,01]\n</code></pre> <p>El modelo todav\u00eda conf\u00eda en su predicci\u00f3n de la clase 3, pero dar valores peque\u00f1os a las otras etiquetas obliga al modelo a considerar al menos otras opciones.</p> <p>Finalmente, para agilizar las cosas, entrenaremos nuestro modelo durante cinco \u00e9pocas usando la funci\u00f3n <code>engine.train()</code> que creamos en 05. PyTorch Going Modular secci\u00f3n 4 con el objetivo de superar al Food101 original resultado del art\u00edculo de 56,4% de precisi\u00f3n en el conjunto de prueba.</p> <p>\u00a1Entrenemos a nuestro modelo m\u00e1s grande hasta el momento!</p> <p>Nota: La ejecuci\u00f3n de la siguiente celda tardar\u00e1 entre 15 y 20 minutos en Google Colab. Esto se debe a que est\u00e1 entrenando el modelo m\u00e1s grande con la mayor cantidad de datos que hemos usado hasta ahora (15,150 im\u00e1genes de entrenamiento, 5050 im\u00e1genes de prueba). Y es una de las razones por las que antes decidimos dividir el 20% del conjunto de datos completo de Food101 (para que el entrenamiento no tomara m\u00e1s de una hora).</p>"},{"location":"13-09_pytorch_model_deployment/#106-inspeccionando-las-curvas-de-perdidas-del-modelo-foodvision-big","title":"10.6 Inspeccionando las curvas de p\u00e9rdidas del modelo FoodVision Big\u00b6","text":"<p>Hagamos visuales nuestras curvas de p\u00e9rdidas de FoodVision Big.</p> <p>Podemos hacerlo con la funci\u00f3n <code>plot_loss_curves()</code> de <code>helper_functions.py</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#107-guardar-y-cargar-foodvision-big","title":"10.7 Guardar y cargar FoodVision Big\u00b6","text":"<p>Ahora que hemos entrenado nuestro modelo m\u00e1s grande hasta el momento, guard\u00e9moslo para poder volver a cargarlo m\u00e1s tarde.</p>"},{"location":"13-09_pytorch_model_deployment/#108-comprobacion-del-tamano-del-modelo-grande-de-foodvision","title":"10.8 Comprobaci\u00f3n del tama\u00f1o del modelo grande de FoodVision\u00b6","text":"<p>Nuestro modelo FoodVision Big es capaz de clasificar 101 clases frente a las 3 clases de FoodVision Mini, \u00a1un aumento de 33,6 veces!</p> <p>\u00bfC\u00f3mo afecta esto al tama\u00f1o del modelo?</p> <p>Vamos a averiguar.</p>"},{"location":"13-09_pytorch_model_deployment/#11-convertir-nuestro-modelo-foodvision-big-en-una-aplicacion-implementable","title":"11. Convertir nuestro modelo FoodVision Big en una aplicaci\u00f3n implementable\u00b6","text":"<p>Tenemos un modelo EffNetB2 entrenado y guardado en el 20% del conjunto de datos de Food101.</p> <p>Y en lugar de dejar que nuestro modelo viva en una carpeta toda su vida, \u00a1implement\u00e9moslo!</p> <p>Implementaremos nuestro modelo FoodVision Big de la misma manera que implementamos nuestro modelo FoodVision Mini, como una demostraci\u00f3n de Gradio en Hugging Face Spaces.</p> <p>Para comenzar, creemos un directorio <code>demos/foodvision_big/</code> para almacenar nuestros archivos de demostraci\u00f3n de FoodVision Big, as\u00ed como un directorio <code>demos/foodvision_big/examples</code> para guardar una imagen de ejemplo con la que probar la demostraci\u00f3n.</p> <p>Cuando hayamos terminado tendremos la siguiente estructura de archivos:</p> <pre><code>poblaci\u00f3n/\n  comidavision_big/\n    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n    aplicaci\u00f3n.py\n    nombres_clase.txt\n    ejemplos/\n      ejemplo_1.jpg\n    modelo.py\n    requisitos.txt\n</code></pre> <p>D\u00f3nde:</p> <ul> <li><code>09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth</code> es nuestro archivo de modelo PyTorch entrenado.</li> <li><code>app.py</code> contiene nuestra aplicaci\u00f3n FoodVision Big Gradio.</li> <li><code>class_names.txt</code> contiene todos los nombres de clases de FoodVision Big.</li> <li><code>examples/</code> contiene im\u00e1genes de ejemplo para usar con nuestra aplicaci\u00f3n Gradio.</li> <li><code>model.py</code> contiene la definici\u00f3n del modelo, as\u00ed como cualquier transformaci\u00f3n asociada con el modelo.</li> <li><code>requirements.txt</code> contiene las dependencias para ejecutar nuestra aplicaci\u00f3n, como <code>torch</code>, <code>torchvision</code> y <code>gradio</code>.</li> </ul>"},{"location":"13-09_pytorch_model_deployment/#111-descargar-una-imagen-de-ejemplo-y-moverla-al-directorio-ejemplos","title":"11.1 Descargar una imagen de ejemplo y moverla al directorio <code>ejemplos</code>\u00b6","text":"<p>Para nuestra imagen de ejemplo, usaremos la fiel [imagen <code>pizza-dad</code>] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad. jpeg) (una foto de mi pap\u00e1 comiendo pizza).</p> <p>As\u00ed que descargu\u00e9moslo del curso GitHub mediante el comando <code>!wget</code> y luego podemos moverlo a <code>demos/foodvision_big/examples</code> con el comando <code>!mv</code> (abreviatura de \"mover\").</p> <p>Mientras estamos aqu\u00ed, trasladaremos nuestro modelo Food101 EffNetB2 entrenado de <code>models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth</code> a <code>demos/foodvision_big</code> tambi\u00e9n.</p>"},{"location":"13-09_pytorch_model_deployment/#112-guardar-nombres-de-clases-de-food101-en-un-archivo-class_namestxt","title":"11.2 Guardar nombres de clases de Food101 en un archivo (<code>class_names.txt</code>)\u00b6","text":"<p>Debido a que hay tantas clases en el conjunto de datos Food101, en lugar de almacenarlas como una lista en nuestro archivo <code>app.py</code>, guard\u00e9moslas en un archivo <code>.txt</code> y le\u00e1moslas cuando sea necesario.</p> <p>Primero recordaremos c\u00f3mo se ven revisando <code>food101_class_names</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#113-convirtiendo-nuestro-modelo-foodvision-big-en-un-script-de-python-modelpy","title":"11.3 Convirtiendo nuestro modelo FoodVision Big en un script de Python (<code>model.py</code>)\u00b6","text":"<p>Al igual que en la demostraci\u00f3n de FoodVision Mini, creemos un script que sea capaz de crear una instancia de un modelo de extracci\u00f3n de caracter\u00edsticas EffNetB2 junto con sus transformaciones necesarias.</p>"},{"location":"13-09_pytorch_model_deployment/#114-convirtiendo-nuestra-aplicacion-foodvision-big-gradio-en-un-script-python-apppy","title":"11.4 Convirtiendo nuestra aplicaci\u00f3n FoodVision Big Gradio en un script Python (<code>app.py</code>)\u00b6","text":"<p>Tenemos un script <code>model.py</code> de FoodVision Big, ahora creemos un script <code>app.py</code> de FoodVision Big.</p> <p>De nuevo, esto ser\u00e1 pr\u00e1cticamente igual que el script <code>app.py</code> de FoodVision Mini, excepto que cambiaremos:</p> <ol> <li>Configuraci\u00f3n de importaciones y nombres de clases - La variable <code>class_names</code> ser\u00e1 una lista para todas las clases de Food101 en lugar de pizza, bistec o sushi. Podemos acceder a ellos a trav\u00e9s de <code>demos/foodvision_big/class_names.txt</code>.</li> <li>Preparaci\u00f3n de modelos y transformaciones - El <code>modelo</code> tendr\u00e1 <code>num_classes=101</code> en lugar de <code>num_classes=3</code>. Tambi\u00e9n nos aseguraremos de cargar los pesos de <code>\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"</code> (nuestra ruta del modelo FoodVision Big).</li> <li>Funci\u00f3n de predicci\u00f3n: seguir\u00e1 siendo la misma que <code>app.py</code> de FoodVision Mini.</li> <li>Aplicaci\u00f3n Gradio: la interfaz de Gradio tendr\u00e1 diferentes par\u00e1metros de \"t\u00edtulo\", \"descripci\u00f3n\" y \"art\u00edculo\" para reflejar los detalles de FoodVision Big.</li> </ol> <p>Tambi\u00e9n nos aseguraremos de guardarlo en <code>demos/foodvision_big/app.py</code> usando el comando m\u00e1gico <code>%%writefile</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#115-creacion-de-un-archivo-de-requisitos-para-foodvision-big-requirementstxt","title":"11.5 Creaci\u00f3n de un archivo de requisitos para FoodVision Big (<code>requirements.txt</code>)\u00b6","text":"<p>Ahora todo lo que necesitamos es un archivo <code>requirements.txt</code> para indicarle a nuestro Hugging Face Space qu\u00e9 dependencias requiere nuestra aplicaci\u00f3n FoodVision Big.</p>"},{"location":"13-09_pytorch_model_deployment/#116-descarga-de-nuestros-archivos-de-la-aplicacion-foodvision-big","title":"11.6 Descarga de nuestros archivos de la aplicaci\u00f3n FoodVision Big\u00b6","text":"<p>Tenemos todos los archivos que necesitamos para implementar nuestra aplicaci\u00f3n FoodVision Big en Hugging Face, ahora comprim\u00e1moslos y descarg\u00e1moslos.</p> <p>Usaremos el mismo proceso que usamos para la aplicaci\u00f3n FoodVision Mini anterior en la [secci\u00f3n 9.1: Descarga de los archivos de nuestra aplicaci\u00f3n Foodvision Mini](https://www.learnpytorch.io/09_pytorch_model_deployment/#91-downloading-our-foodvision -archivos-mini-aplicaci\u00f3n).</p>"},{"location":"13-09_pytorch_model_deployment/#117-implementacion-de-nuestra-aplicacion-foodvision-big-en-huggingface-spaces","title":"11.7 Implementaci\u00f3n de nuestra aplicaci\u00f3n FoodVision Big en HuggingFace Spaces\u00b6","text":"<p>\u00a1Hermoso!</p> <p>\u00a1Es hora de darle vida a nuestro modelo m\u00e1s grande de todo el curso!</p> <p>Implementemos nuestra demostraci\u00f3n de FoodVision Big Gradio en Hugging Face Spaces para que podamos probarla de forma interactiva y permitir que otros experimenten la magia de nuestros esfuerzos de aprendizaje autom\u00e1tico.</p> <p>Nota: Hay varias formas de cargar archivos en Hugging Face Spaces. Los siguientes pasos tratan a Hugging Face como un repositorio git para rastrear archivos. Sin embargo, tambi\u00e9n puede cargar directamente en Hugging Face Spaces a trav\u00e9s de la interfaz web o por la biblioteca<code>huggingface_hub</code>.</p> <p>La buena noticia es que ya hemos realizado los pasos para hacerlo con FoodVision Mini, as\u00ed que ahora todo lo que tenemos que hacer es personalizarlos para que se adapten a FoodVision Big:</p> <ol> <li>Reg\u00edstrese para obtener una cuenta de Hugging Face.</li> <li>Inicie un nuevo Hugging Face Space yendo a su perfil y luego haciendo clic en \"Nuevo espacio\".<ul> <li>Nota: Un espacio en Hugging Face tambi\u00e9n se conoce como \"repositorio de c\u00f3digos\" (un lugar para almacenar su c\u00f3digo/archivos) o \"repositorio\" para abreviar.</li> </ul> </li> <li>Dale un nombre al Espacio, por ejemplo, el m\u00edo se llama <code>mrdbourke/foodvision_big</code>, puedes verlo aqu\u00ed: https://huggingface.co/spaces/mrdbourke/foodvision_big</li> <li>Seleccione una licencia (yo us\u00e9 MIT).</li> <li>Seleccione Gradio como Space SDK (kit de desarrollo de software).<ul> <li>Nota: Puedes usar otras opciones como Streamlit, pero como nuestra aplicaci\u00f3n est\u00e1 construida con Gradio, nos quedaremos con eso.</li> </ul> </li> <li>Elija si su Espacio es p\u00fablico o privado (seleccion\u00e9 p\u00fablico porque me gustar\u00eda que mi Espacio est\u00e9 disponible para otros).</li> <li>Haga clic en \"Crear espacio\".</li> <li>Clone el repositorio localmente ejecutando: <code>git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]</code> en la terminal o en el s\u00edmbolo del sistema.<ul> <li>Nota: Tambi\u00e9n puedes agregar archivos carg\u00e1ndolos en la pesta\u00f1a \"Archivos y versiones\".</li> </ul> </li> <li>Copie/mueva el contenido de la carpeta <code>foodvision_big</code> descargada a la carpeta del repositorio clonado.</li> <li>Para cargar y rastrear archivos m\u00e1s grandes (por ejemplo, archivos de m\u00e1s de 10 MB o, en nuestro caso, nuestro archivo modelo PyTorch), necesitar\u00e1 instalar Git LFS (que significa para \"almacenamiento de archivos grandes de git\").</li> <li>Despu\u00e9s de haber instalado Git LFS, puedes activarlo ejecutando <code>git lfs install</code>.</li> <li>En el directorio <code>foodvision_big</code>, realice un seguimiento de los archivos de m\u00e1s de 10 MB con Git LFS con <code>git lfs track \"*.file_extension\"</code>.<ul> <li>Realice un seguimiento del archivo de modelo EffNetB2 PyTorch con <code>git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"</code>.</li> <li>Nota: Si recibe alg\u00fan error al cargar im\u00e1genes, es posible que tambi\u00e9n deba rastrearlas con <code>git lfs</code>, por ejemplo, <code>git lfs track \"examples/04-pizza-dad.jpg\"</code></li> </ul> </li> <li>Seguimiento de <code>.gitattributes</code> (creado autom\u00e1ticamente al clonar desde HuggingFace; este archivo ayudar\u00e1 a garantizar que nuestros archivos m\u00e1s grandes sean rastreados con Git LFS). Puede ver un archivo <code>.gitattributes</code> de ejemplo en [FoodVision Big Hugging Face Space] (https://huggingface.co/spaces/mrdbourke/foodvision_big/blob/main/.gitattributes).<ul> <li><code>git agregar .gitattributes</code></li> </ul> </li> <li>Agregue el resto de los archivos de la aplicaci\u00f3n <code>foodvision_big</code> y conf\u00edrmelos con:<ul> <li><code>git agregar *</code></li> <li><code>git commit -m \"primer compromiso\"</code></li> </ul> </li> <li>Env\u00ede (cargue) los archivos a Hugging Face: *<code>git push</code></li> <li>Espere de 3 a 5 minutos hasta que se complete la compilaci\u00f3n (las compilaciones futuras ser\u00e1n m\u00e1s r\u00e1pidas) y su aplicaci\u00f3n estar\u00e1 activa.</li> </ol> <p>Si todo funcion\u00f3 correctamente, \u00a1nuestra demostraci\u00f3n de FoodVision Big Gradio deber\u00eda estar lista para clasificar!</p> <p>Puedes ver mi versi\u00f3n aqu\u00ed: https://huggingface.co/spaces/mrdbourke/foodvision_big/</p> <p>O incluso podemos insertar nuestra demostraci\u00f3n de FoodVision Big Gradio directamente en nuestro cuaderno como un iframe con [<code>IPython.display.IFrame</code>](https: //ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) y un enlace a nuestro espacio en el formato <code>https://hf.space/embed/[YOUR_USERNAME ]/[TU_NOMBRE_ESPACIO]/+</code>.</p>"},{"location":"13-09_pytorch_model_deployment/#principales-conclusiones","title":"Principales conclusiones\u00b6","text":"<ul> <li>La implementaci\u00f3n es tan importante como la capacitaci\u00f3n. Una vez que tenga un buen modelo de trabajo, su primera pregunta deber\u00eda ser: \u00bfc\u00f3mo puedo implementarlo y hacerlo accesible para otros? La implementaci\u00f3n le permite probar su modelo en el mundo real en lugar de en conjuntos de prueba y capacitaci\u00f3n privados.</li> <li>Tres preguntas para la implementaci\u00f3n del modelo de aprendizaje autom\u00e1tico:<ol> <li>\u00bfCu\u00e1l es el caso de uso m\u00e1s ideal para el modelo (qu\u00e9 tan bien y qu\u00e9 tan r\u00e1pido funciona)?</li> <li>\u00bfA d\u00f3nde ir\u00e1 el modelo (en el dispositivo o en la nube)?</li> <li>\u00bfC\u00f3mo funcionar\u00e1 el modelo (las predicciones est\u00e1n en l\u00ednea o fuera de l\u00ednea)?</li> </ol> </li> <li>Las opciones de implementaci\u00f3n son abundantes. Pero es mejor empezar de forma sencilla. Una de las mejores formas actuales (digo actual porque estas cosas siempre est\u00e1n cambiando) es usar Gradio para crear una demostraci\u00f3n y alojarla en Hugging Face Spaces. Comience de manera simple y ampl\u00edelo cuando sea necesario.</li> <li>Nunca dejes de experimentar. Las necesidades de tu modelo de aprendizaje autom\u00e1tico probablemente cambiar\u00e1n con el tiempo, por lo que implementar un \u00fanico modelo no es el \u00faltimo paso. Es posible que encuentre cambios en el conjunto de datos, por lo que tendr\u00e1 que actualizar su modelo. O se publica una nueva investigaci\u00f3n y hay una mejor arquitectura para usar.<ul> <li>Por lo tanto, implementar un modelo es un paso excelente, pero probablemente querr\u00e1s actualizarlo con el tiempo.</li> </ul> </li> <li>La implementaci\u00f3n del modelo de aprendizaje autom\u00e1tico es parte de la pr\u00e1ctica de ingenier\u00eda de MLOps (operaciones de aprendizaje autom\u00e1tico). MLOps es una extensi\u00f3n de DevOps (operaciones de desarrollo) e involucra todas las partes de ingenier\u00eda relacionadas con el entrenamiento de un modelo: recopilaci\u00f3n y almacenamiento de datos, datos preprocesamiento, implementaci\u00f3n de modelos, monitoreo de modelos, control de versiones y m\u00e1s. Es un campo que evoluciona r\u00e1pidamente, pero existen algunos recursos s\u00f3lidos para aprender m\u00e1s, muchos de los cuales se encuentran en [Recursos adicionales de PyTorch](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and -ingenier\u00eda-de-aprendizaje-profundo).</li> </ul>"},{"location":"13-09_pytorch_model_deployment/#ejercicios","title":"Ejercicios\u00b6","text":"<p>Todos los ejercicios se centran en practicar el c\u00f3digo anterior.</p> <p>Deber\u00eda poder completarlos haciendo referencia a cada secci\u00f3n o siguiendo los recursos vinculados.</p> <p>Recursos:</p> <ul> <li>[Cuaderno de plantilla de ejercicios para 09] (https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/09_pytorch_model_deployment_exercises.ipynb).</li> <li>Cuaderno de soluciones de ejemplo para 09 pruebe los ejercicios antes de mirar esto.<ul> <li>Vea un [video tutorial de las soluciones en YouTube] en vivo (https://youtu.be/jOX5ZCkWO-0) (errores y todo).</li> </ul> </li> </ul> <ol> <li>Realice y programe predicciones con ambos modelos de extracci\u00f3n de caracter\u00edsticas en el conjunto de datos de prueba utilizando la GPU (<code>device=\"cuda\"</code>). Compare los tiempos de predicci\u00f3n del modelo en GPU y CPU: \u00bfesto cierra la brecha entre ellos? Por ejemplo, \u00bfhacer predicciones en la GPU hace que los tiempos de predicci\u00f3n del extractor de funciones de ViT se acerquen m\u00e1s a los tiempos de predicci\u00f3n del extractor de funciones de EffNetB2?<ul> <li>Encontrar\u00e1s el c\u00f3digo para realizar estos pasos en la [secci\u00f3n 5. Hacer predicciones con nuestros modelos entrenados y cronometrarlas](https://www.learnpytorch.io/09_pytorch_model_deployment/#5-making-predictions-with-our-trained -models-and-timing-them) y [secci\u00f3n 6. Comparaci\u00f3n de resultados de modelos, tiempos de predicci\u00f3n y tama\u00f1o](https://www.learnpytorch.io/09_pytorch_model_deployment/#6-comparing-model-results-prediction-times-and -tama\u00f1o).</li> </ul> </li> <li>El extractor de funciones de ViT parece tener m\u00e1s capacidad de aprendizaje (debido a m\u00e1s par\u00e1metros) que EffNetB2. \u00bfC\u00f3mo le va en la divisi\u00f3n m\u00e1s grande del 20 % de todo el conjunto de datos de Food101?<ul> <li>Entrene un extractor de funciones ViT en el conjunto de datos 20% Food101 durante 5 \u00e9pocas, tal como lo hicimos con EffNetB2 en la secci\u00f3n 10. Creando FoodVision Big.</li> </ul> </li> <li>Haga predicciones en el conjunto de datos de prueba 20 % de Food101 con el extractor de funciones ViT del ejercicio 2 y encuentre las predicciones \"m\u00e1s incorrectas\".<ul> <li>Las predicciones ser\u00e1n las que tengan mayor probabilidad de predicci\u00f3n pero con la etiqueta de predicci\u00f3n incorrecta.</li> <li>Escribe una oraci\u00f3n o dos sobre por qu\u00e9 crees que el modelo se equivoc\u00f3 en estas predicciones.</li> </ul> </li> <li>Eval\u00fae el extractor de funciones de ViT en todo el conjunto de datos de prueba de Food101 en lugar de solo la versi\u00f3n del 20 %, \u00bfc\u00f3mo funciona?<ul> <li>\u00bfSupera el mejor resultado del art\u00edculo original de Food101 con una precisi\u00f3n del 56,4%?</li> </ul> </li> <li>Dir\u00edjase a Paperswithcode.com y busque el modelo actual con mejor rendimiento en el conjunto de datos de Food101.<ul> <li>\u00bfQu\u00e9 modelo de arquitectura utiliza?</li> </ul> </li> <li>Escriba de 1 a 3 posibles puntos de falla de nuestros modelos FoodVision implementados y cu\u00e1les podr\u00edan ser algunas posibles soluciones.<ul> <li>Por ejemplo, \u00bfqu\u00e9 pasar\u00eda si alguien subiera una foto que no fuera de comida a nuestro modelo FoodVision Mini?</li> </ul> </li> <li>Elija cualquier conjunto de datos de <code>torchvision.datasets</code> y entrene un modelo de extracci\u00f3n de caracter\u00edsticas usando un modelo de <code>torchvision.models</code> (puede usar uno de los modelos que ya hemos creado, por ejemplo, EffNetB2 o ViT) durante 5 \u00e9pocas y luego implementar su modelo como una aplicaci\u00f3n Gradio en Hugging Face Espacios.<ul> <li>Es posible que desee elegir un conjunto de datos m\u00e1s peque\u00f1o/hacer una divisi\u00f3n m\u00e1s peque\u00f1a para que el entrenamiento no demore demasiado.</li> <li>\u00a1Me encantar\u00eda ver tus modelos implementados! As\u00ed que aseg\u00farese de compartirlos en Discord o en la [p\u00e1gina de debates de GitHub del curso] (https://github.com/mrdbourke/pytorch-deep-learning/discussions).</li> </ul> </li> </ol>"},{"location":"13-09_pytorch_model_deployment/#extracurricular","title":"Extracurricular\u00b6","text":"<ul> <li>La implementaci\u00f3n del modelo de aprendizaje autom\u00e1tico es generalmente un desaf\u00edo de ingenier\u00eda en lugar de un desaf\u00edo de aprendizaje autom\u00e1tico puro; consulte la [secci\u00f3n de ingenier\u00eda de aprendizaje autom\u00e1tico de recursos adicionales de PyTorch] (https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning -y-deep-learning-engineering) para obtener recursos sobre c\u00f3mo aprender m\u00e1s.<ul> <li>En el interior encontrar\u00e1 recomendaciones de recursos como el libro de Chip Huyen Designing Machine Learning Systems ( especialmente el cap\u00edtulo 7 sobre implementaci\u00f3n de modelos) y el [curso Made with ML MLOps] de Goku Mohandas (https://madewithml.com/#mlops).</li> </ul> </li> <li>A medida que empieces a construir m\u00e1s y m\u00e1s proyectos propios, es probable que empieces a usar Git (y potencialmente GitHub) con bastante frecuencia. Para obtener m\u00e1s informaci\u00f3n sobre ambos, recomiendo el video Git y GitHub para principiantes: curso intensivo en el canal de YouTube freeCodeCamp.</li> <li>S\u00f3lo hemos ara\u00f1ado la superficie de lo que es posible con Gradio. Para obtener m\u00e1s informaci\u00f3n, recomiendo consultar la documentaci\u00f3n completa, especialmente:<ul> <li>Todos los diferentes tipos de [componentes de entrada y salida] (https://gradio.app/docs/#components).</li> <li>La API de Gradio Blocks para flujos de trabajo m\u00e1s avanzados.</li> <li>El cap\u00edtulo del curso Hugging Face sobre [c\u00f3mo usar Gradio con Hugging Face] (https://huggingface.co/course/chapter9/1).</li> </ul> </li> <li>Los dispositivos Edge no se limitan a tel\u00e9fonos m\u00f3viles, incluyen computadoras peque\u00f1as como Raspberry Pi y el equipo de PyTorch tiene un fant\u00e1stico tutorial de publicaci\u00f3n de blog sobre la implementaci\u00f3n un modelo de PyTorch a uno.</li> <li>Para obtener una gu\u00eda fant\u00e1stica sobre el desarrollo de aplicaciones basadas en inteligencia artificial y aprendizaje autom\u00e1tico, consulte la [Gu\u00eda de personas + inteligencia artificial de Google] (https://pair.withgoogle.com/guidebook). Una de mis favoritas es la secci\u00f3n sobre [establecer las expectativas correctas] (https://pair.withgoogle.com/guidebook/patterns#set-the-right-expectations).<ul> <li>Cubr\u00ed m\u00e1s de este tipo de recursos, incluidas gu\u00edas de Apple, Microsoft y m\u00e1s en la edici\u00f3n de abril de 2021 de Machine Learning Monthly (un bolet\u00edn mensual que env\u00edo con lo \u00faltimo y lo mejor del campo de ML).</li> </ul> </li> <li>Si desea acelerar el tiempo de ejecuci\u00f3n de su modelo en la CPU, debe conocer TorchScript, [ONNX](https://pytorch .org/docs/stable/onnx.html) (Open Neural Network Exchange) y [OpenVINO](https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output. HTML). Al pasar de PyTorch puro a modelos ONNX/OpenVINO, he visto un aumento de ~2x+ en el rendimiento.</li> <li>Para convertir modelos en una API implementable y escalable, consulte la biblioteca TorchServe.</li> <li>Para ver un excelente ejemplo y una justificaci\u00f3n de por qu\u00e9 implementar un modelo de aprendizaje autom\u00e1tico en el navegador (una forma de implementaci\u00f3n perimetral) ofrece varios beneficios (sin retraso en la latencia de transferencia de red), consulte el art\u00edculo de Jo Kristian Bergum sobre Moving ML Inference from the Cloud hasta el borde.</li> </ul>"},{"location":"14-tutorial_spark_delta/","title":"Tutorial Delta Lake Spark","text":"In\u00a0[11]: Copied! <pre># %%\n# Import necessary libraries\nimport os\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\n# %%\n# Configure Spark session with Delta Lake dependencies\nspark = (\n    SparkSession.builder.appName(\"Delta Lake\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n    )\n</pre> # %% # Import necessary libraries import os from pyspark.sql import SparkSession import pyspark.sql.functions as F  # %% # Configure Spark session with Delta Lake dependencies spark = (     SparkSession.builder.appName(\"Delta Lake\")     .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")     .getOrCreate()     )  In\u00a0[12]: Copied! <pre># %%\n# Create a simple dataset\ndata = [(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 4), (\"e\", 5)]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\n</pre> # %% # Create a simple dataset data = [(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 4), (\"e\", 5)]  # Create a DataFrame df = spark.createDataFrame(data, [\"letter\", \"number\"])  In\u00a0[13]: Copied! <pre># %%\n# Write the data to a Delta Lake table\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"data/letters.delta\")\n</pre> # %% # Write the data to a Delta Lake table df.write.format(\"delta\").mode(\"overwrite\").save(\"data/letters.delta\")  <pre>                                                                                \r</pre> In\u00a0[14]: Copied! <pre>history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\nhistory_df.show(truncate=False)\n</pre> history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\") history_df.show(truncate=False) <pre>+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -&gt; Overwrite, partitionBy -&gt; []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -&gt; 6, numOutputRows -&gt; 5, numOutputBytes -&gt; 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n\n</pre> In\u00a0[15]: Copied! <pre># %%\n# Read the data from the Delta Lake table\ndf = spark.read.format(\"delta\").load(\"data/letters.delta\")\ndf.show()\n</pre> # %% # Read the data from the Delta Lake table df = spark.read.format(\"delta\").load(\"data/letters.delta\") df.show()  <pre>                                                                                \r</pre> <pre>+------+------+\n|letter|number|\n+------+------+\n|     d|     4|\n|     c|     3|\n|     e|     5|\n|     b|     2|\n|     a|     1|\n+------+------+\n\n</pre> In\u00a0[16]: Copied! <pre># %%\n# Append data to the Delta Lake table\nnew_data = [(\"f\", 6), (\"g\", 7)]\n\ndf_new = spark.createDataFrame(new_data, [\"letter\", \"number\"])\ndf_new.write.format(\"delta\").mode(\"append\").save(\"data/letters.delta\")\n\nhistory_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\nhistory_df.show(truncate=False)\n</pre> # %% # Append data to the Delta Lake table new_data = [(\"f\", 6), (\"g\", 7)]  df_new = spark.createDataFrame(new_data, [\"letter\", \"number\"]) df_new.write.format(\"delta\").mode(\"append\").save(\"data/letters.delta\")  history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\") history_df.show(truncate=False)  <pre>                                                                                \r</pre> <pre>+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|1      |2024-07-03 14:01:52.167|NULL  |NULL    |WRITE    |{mode -&gt; Append, partitionBy -&gt; []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 1809}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -&gt; Overwrite, partitionBy -&gt; []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -&gt; 6, numOutputRows -&gt; 5, numOutputBytes -&gt; 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n\n</pre> In\u00a0[17]: Copied! <pre># %%\n# Read the Delta Lake table with appended data\ndf = spark.read.format(\"delta\").load(\"data/letters.delta\")\ndf.show()\n</pre> # %% # Read the Delta Lake table with appended data df = spark.read.format(\"delta\").load(\"data/letters.delta\") df.show()  <pre>                                                                                \r</pre> <pre>+------+------+\n|letter|number|\n+------+------+\n|     d|     4|\n|     c|     3|\n|     g|     7|\n|     f|     6|\n|     e|     5|\n|     b|     2|\n|     a|     1|\n+------+------+\n\n</pre> In\u00a0[18]: Copied! <pre># %%\n# Update data in the Delta Lake table\ndf = spark.read.format(\"delta\").load(\"data/letters.delta\")\n\n# Update the data\ndf = df.withColumn(\n    \"number\", F.when(F.col(\"letter\") == \"a\", 100).otherwise(F.col(\"number\"))\n)\n\n# Overwrite the Delta Lake table with updated data\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"data/letters.delta\")\n</pre> # %% # Update data in the Delta Lake table df = spark.read.format(\"delta\").load(\"data/letters.delta\")  # Update the data df = df.withColumn(     \"number\", F.when(F.col(\"letter\") == \"a\", 100).otherwise(F.col(\"number\")) )  # Overwrite the Delta Lake table with updated data df.write.format(\"delta\").mode(\"overwrite\").save(\"data/letters.delta\")  <pre>                                                                                \r</pre> In\u00a0[19]: Copied! <pre>history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\nhistory_df.show(truncate=False)\n</pre> history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\") history_df.show(truncate=False) <pre>+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|2      |2024-07-03 14:02:04.829|NULL  |NULL    |WRITE    |{mode -&gt; Overwrite, partitionBy -&gt; []}|NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -&gt; 7, numOutputRows -&gt; 7, numOutputBytes -&gt; 4963}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n|1      |2024-07-03 14:01:52.167|NULL  |NULL    |WRITE    |{mode -&gt; Append, partitionBy -&gt; []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 1809}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -&gt; Overwrite, partitionBy -&gt; []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -&gt; 6, numOutputRows -&gt; 5, numOutputBytes -&gt; 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n\n</pre> In\u00a0[20]: Copied! <pre># %%\n# Read data from a specific version of the Delta Lake table\ndf_version1 = (\n    spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"data/letters.delta\")\n)\ndf_version1.show()\n</pre> # %% # Read data from a specific version of the Delta Lake table df_version1 = (     spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"data/letters.delta\") ) df_version1.show()  <pre>                                                                                \r</pre> <pre>+------+------+\n|letter|number|\n+------+------+\n|     d|     4|\n|     c|     3|\n|     g|     7|\n|     f|     6|\n|     e|     5|\n|     b|     2|\n|     a|     1|\n+------+------+\n\n</pre> In\u00a0[21]: Copied! <pre># %%\n# Get the history of changes made to the Delta Lake table\nhistory_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\")\nhistory_df.show(truncate=False)\n</pre> # %% # Get the history of changes made to the Delta Lake table history_df = spark.sql(\"DESCRIBE HISTORY 'data/letters.delta'\") history_df.show(truncate=False)  <pre>+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n|2      |2024-07-03 14:02:04.829|NULL  |NULL    |WRITE    |{mode -&gt; Overwrite, partitionBy -&gt; []}|NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -&gt; 7, numOutputRows -&gt; 7, numOutputBytes -&gt; 4963}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n|1      |2024-07-03 14:01:52.167|NULL  |NULL    |WRITE    |{mode -&gt; Append, partitionBy -&gt; []}   |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 1809}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n|0      |2024-07-03 14:01:44.696|NULL  |NULL    |WRITE    |{mode -&gt; Overwrite, partitionBy -&gt; []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -&gt; 6, numOutputRows -&gt; 5, numOutputBytes -&gt; 3936}|NULL        |Apache-Spark/3.5.1 Delta-Lake/3.2.0|\n+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n\n</pre>"},{"location":"14-tutorial_spark_delta/#tutorial-delta-lake","title":"Tutorial Delta Lake\u00b6","text":""},{"location":"14-tutorial_spark_delta/#1-create-a-simple-dataset","title":"1. Create a Simple Dataset\u00b6","text":"<p>To start with, let's create a simple dataset consisting of letters and numbers.</p>"},{"location":"14-tutorial_spark_delta/#2-write-data-to-a-delta-lake-table","title":"2. Write Data to a Delta Lake Table\u00b6","text":"<p>Now, we'll write the data to a Delta Lake table named <code>letters.delta</code>. Delta Lake provides ACID transactions and time travel capabilities, making it suitable for reliable data storage and versioning.</p>"},{"location":"14-tutorial_spark_delta/#3-read-data-from-the-delta-lake-table","title":"3. Read Data from the Delta Lake Table\u00b6","text":"<p>Let's read the data back from the Delta Lake table and display it.</p>"},{"location":"14-tutorial_spark_delta/#4-append-data-to-the-delta-lake-table","title":"4. Append Data to the Delta Lake Table\u00b6","text":"<p>We can append new data to the existing Delta Lake table without affecting the existing data.</p>"},{"location":"14-tutorial_spark_delta/#5-update-data-in-the-delta-lake-table","title":"5. Update Data in the Delta Lake Table\u00b6","text":"<p>Delta Lake supports updating data in place. Let's update the data to set the number to 100 where the letter is 'a'.</p>"},{"location":"14-tutorial_spark_delta/#6-read-data-from-a-specific-version-of-the-delta-lake-table","title":"6. Read Data from a Specific Version of the Delta Lake Table\u00b6","text":"<p>Delta Lake maintains a version history of changes. Let's retrieve data from a specific version of the Delta Lake table.</p>"},{"location":"14-tutorial_spark_delta/#7-get-history-of-changes-made-to-the-delta-lake-table","title":"7. Get History of Changes Made to the Delta Lake Table\u00b6","text":"<p>Delta Lake allows us to view the history of changes made to the table, which is useful for auditing and understanding data evolution over time.</p>"},{"location":"embeddings/","title":"Predicci\u00f3n de Caracter\u00edsticas de Nodos Utilizando Node2Vec en An\u00e1lisis de Grafos y Redes con Python","text":"<p>En el mundo de la Ciencia de Datos, el Aprendizaje Profundo y el An\u00e1lisis de Grafos convergen en t\u00e9cnicas poderosas para extraer conocimiento de estructuras complejas. Un enfoque particularmente eficaz es la utilizaci\u00f3n de Node2Vec, una t\u00e9cnica de incorporaci\u00f3n de nodos que captura relaciones y similitudes en un grafo. En este post, exploraremos c\u00f3mo aplicar Node2Vec para predecir caracter\u00edsticas de nodos en base a la posici\u00f3n que ocupan en el grafo.</p> In\u00a0[7]: Copied! <pre># Instalaci\u00f3n de bibliotecas\n!pip install networkx node2vec matplotlib scikit-learn netwulf\n</pre> # Instalaci\u00f3n de bibliotecas !pip install networkx node2vec matplotlib scikit-learn netwulf  <pre>Requirement already satisfied: networkx in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (2.8.8)\nRequirement already satisfied: node2vec in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (0.4.6)\nRequirement already satisfied: matplotlib in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (3.7.2)\nRequirement already satisfied: scikit-learn in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (1.3.0)\nCollecting netwulf\n  Using cached netwulf-0.1.5.tar.gz (236 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: gensim&lt;5.0.0,&gt;=4.1.2 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from node2vec) (4.3.1)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from node2vec) (1.3.2)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from node2vec) (1.25.2)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.55.1 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from node2vec) (4.66.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (4.42.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (23.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (10.0.0)\nRequirement already satisfied: pyparsing&lt;3.1,&gt;=2.3.1 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: scipy&gt;=1.5.0 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nCollecting simplejson&gt;=3.0 (from netwulf)\n  Downloading simplejson-3.19.1-cp310-cp310-macosx_10_9_x86_64.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.4/76.4 kB 1.5 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: smart-open&gt;=1.8.1 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (6.3.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nBuilding wheels for collected packages: netwulf\n  Building wheel for netwulf (setup.py) ... done\n  Created wheel for netwulf: filename=netwulf-0.1.5-py3-none-any.whl size=237993 sha256=2b3e4b24abaf99677902e46cf3c7b3c83686628a4925f8d3b46895485b0601ec\n  Stored in directory: /Users/fernandocarazo/Library/Caches/pip/wheels/04/f3/75/3ee8148fe5296ab40fc164cc09572b0e31255242ccee47b354\nSuccessfully built netwulf\nInstalling collected packages: simplejson, netwulf\nSuccessfully installed netwulf-0.1.5 simplejson-3.19.1\n</pre> In\u00a0[1]: Copied! <pre>import os\nimport networkx as nx\nfrom node2vec import Node2Vec\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n</pre> import os import networkx as nx from node2vec import Node2Vec import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt from sklearn.manifold import TSNE  <pre>/Users/fernandocarazo/opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Descargar la base de datos Cora\nos.system(\"curl -O https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\")\nos.system(\"tar -xvzf cora.tgz\")\n</pre>  # Descargar la base de datos Cora os.system(\"curl -O https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\") os.system(\"tar -xvzf cora.tgz\")  <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  164k  100  164k    0     0  76129      0  0:00:02  0:00:02 --:--:-- 76422\nx cora/\nx cora/README\nx cora/cora.cites\nx cora/cora.content\n</pre> Out[2]: <pre>0</pre> In\u00a0[3]: Copied! <pre>import os\nimport networkx as nx\nfrom node2vec import Node2Vec\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom urllib.request import urlretrieve\nimport tarfile\nimport seaborn as sns\nimport numpy as np\n\n# Descargar y extraer la base de datos Cora\nurl = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\nfile_path, _ = urlretrieve(url)\nwith tarfile.open(file_path, 'r:gz') as tar:\n    tar.extractall()\n    \n\n# idx_ran = np.random.choice(G.nodes(), 1000, replace=False)\n\n# Cargar el grafo desde la base de datos Cora\nG = nx.read_edgelist('cora/cora.cites')\nG.number_of_edges(), G.number_of_nodes()\n</pre>  import os import networkx as nx from node2vec import Node2Vec import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt from sklearn.manifold import TSNE from urllib.request import urlretrieve import tarfile import seaborn as sns import numpy as np  # Descargar y extraer la base de datos Cora url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz' file_path, _ = urlretrieve(url) with tarfile.open(file_path, 'r:gz') as tar:     tar.extractall()       # idx_ran = np.random.choice(G.nodes(), 1000, replace=False)  # Cargar el grafo desde la base de datos Cora G = nx.read_edgelist('cora/cora.cites') G.number_of_edges(), G.number_of_nodes()  Out[3]: <pre>(5278, 2708)</pre> In\u00a0[4]: Copied! <pre>#\u00a0Set of random nodes\n# G = G.subgraph(idx_ran)\n\n# Cargar las caracter\u00edsticas y etiquetas desde la base de datos Cora\nfeatures_df = pd.read_csv('cora/cora.content', sep='\\t', header=None)\nlabels_df = pd.read_csv('cora/cora.cites', sep='\\t', header=None,)\n\n# Dividir el DataFrame en caracter\u00edsticas (X) y etiquetas (y)\nX = features_df.iloc[:, 1:-1]\ny = features_df.iloc[:, -1]\ny\n</pre>  #\u00a0Set of random nodes # G = G.subgraph(idx_ran)  # Cargar las caracter\u00edsticas y etiquetas desde la base de datos Cora features_df = pd.read_csv('cora/cora.content', sep='\\t', header=None) labels_df = pd.read_csv('cora/cora.cites', sep='\\t', header=None,)  # Dividir el DataFrame en caracter\u00edsticas (X) y etiquetas (y) X = features_df.iloc[:, 1:-1] y = features_df.iloc[:, -1] y  Out[4]: <pre>0              Neural_Networks\n1                Rule_Learning\n2       Reinforcement_Learning\n3       Reinforcement_Learning\n4        Probabilistic_Methods\n                 ...          \n2703        Genetic_Algorithms\n2704        Genetic_Algorithms\n2705        Genetic_Algorithms\n2706                Case_Based\n2707           Neural_Networks\nName: 1434, Length: 2708, dtype: object</pre> In\u00a0[5]: Copied! <pre># Calculamos las posiciones de los nodos para el grafo\npos = nx.spring_layout(G, seed=42)\n</pre> # Calculamos las posiciones de los nodos para el grafo pos = nx.spring_layout(G, seed=42) In\u00a0[\u00a0]: Copied! <pre># Tambi\u00e9n se puede visualizar con netwulf\nimport netwulf as nf\nnf.visualize(G)\n</pre> # Tambi\u00e9n se puede visualizar con netwulf import netwulf as nf nf.visualize(G) In\u00a0[119]: Copied! <pre># Graficado del grafo\ncolors = y.astype('category').cat.codes\nplt.figure(figsize=(20,20))\nnx.draw(G, node_size=5, pos=pos, node_color=colors, cmap='Set1')\n</pre> # Graficado del grafo colors = y.astype('category').cat.codes plt.figure(figsize=(20,20)) nx.draw(G, node_size=5, pos=pos, node_color=colors, cmap='Set1') <ul> <li>dimensions: tama\u00f1o del vector de embeddings</li> <li>walk_length: longitud de los caminos aleatorios</li> <li>num_walks: n\u00famero de caminos aleatorios</li> <li>workers: n\u00famero de procesadores usados para el entrenamiento</li> <li>window: tama\u00f1o de la ventana para el algoritmo Word2Vec</li> <li>min_count: n\u00famero m\u00ednimo de veces que debe aparecer una palabra en el corpus</li> <li>batch_words: tama\u00f1o del lote de palabras para el algoritmo Word2Vec</li> </ul> In\u00a0[141]: Copied! <pre># Inicializar y entrenar el modelo Node2Vec\nnode2vec = Node2Vec(G, dimensions=30, walk_length=10, num_walks=10, workers=4)\n# Calcular los embbedings de los nodos\nembeddings = node2vec.fit(window=10, min_count=1, batch_words=4)\n</pre> # Inicializar y entrenar el modelo Node2Vec node2vec = Node2Vec(G, dimensions=30, walk_length=10, num_walks=10, workers=4) # Calcular los embbedings de los nodos embeddings = node2vec.fit(window=10, min_count=1, batch_words=4)  <pre>Computing transition probabilities:   3%|\u258e         | 77/2708 [00:00&lt;00:03, 681.68it/s]</pre> <pre>Computing transition probabilities: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2708/2708 [00:00&lt;00:00, 3366.45it/s]\nGenerating walks (CPU: 3): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  6.37it/s]\nGenerating walks (CPU: 1): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  6.51it/s]\nGenerating walks (CPU: 2): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  6.43it/s]\nGenerating walks (CPU: 4): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  8.49it/s]\n</pre> In\u00a0[143]: Copied! <pre>nodes_embs = embeddings.wv.vectors\n# Combinar las caracter\u00edsticas y embeddings en un solo DataFrame\ncombined_df = pd.concat([X, pd.DataFrame(nodes_embs)], axis=1)\ncombined_df\n</pre> nodes_embs = embeddings.wv.vectors # Combinar las caracter\u00edsticas y embeddings en un solo DataFrame combined_df = pd.concat([X, pd.DataFrame(nodes_embs)], axis=1) combined_df  Out[143]: 1 2 3 4 5 6 7 8 9 10 ... 20 21 22 23 24 25 26 27 28 29 0 0 0 0 0 0 0 0 0 0 0 ... 0.795933 0.415507 0.386307 -0.002911 0.348737 -0.176543 -0.572042 0.311663 0.001379 -0.537796 1 0 0 0 0 0 0 0 0 0 0 ... -0.708579 -0.030970 0.032896 0.395219 0.258655 0.610313 0.104422 0.323234 0.029849 -0.391190 2 0 0 0 0 0 0 0 0 0 0 ... -0.314460 -0.329790 0.277298 -0.220116 -0.535589 0.677442 0.126622 0.001399 -0.307883 -1.106844 3 0 0 0 0 0 0 0 0 0 0 ... -0.371517 -0.356573 -0.512346 0.324187 -0.277084 0.295629 -0.101541 0.213723 0.791657 -0.732938 4 0 0 0 0 0 0 0 0 0 0 ... -0.434078 -0.329435 -0.676453 0.075276 -0.032991 0.129072 -0.591269 0.333925 -0.214008 -0.650508 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2703 0 0 0 0 0 0 0 0 0 0 ... -0.040460 0.586297 0.837810 0.549309 -0.047041 0.253388 -0.153505 0.024145 -0.864390 -0.555191 2704 0 0 0 0 0 0 0 0 0 0 ... 0.010375 -0.255602 1.187112 0.567876 -1.027066 0.093529 -1.075994 -0.029149 0.516387 -0.182631 2705 0 0 0 0 0 0 0 0 0 0 ... -0.799883 -0.932348 -0.027764 0.010531 0.526442 0.202738 -0.725372 0.014941 0.030126 -1.108884 2706 0 0 0 0 1 0 0 0 0 0 ... -0.500194 -0.201946 -0.158183 0.342811 0.210704 0.389000 -0.003850 0.426401 0.113739 -0.203314 2707 0 0 0 0 0 0 0 0 0 0 ... 0.852381 -0.081238 0.889547 0.125725 -0.578516 -0.028220 -1.426841 0.500267 0.780594 -0.824779 <p>2708 rows \u00d7 1463 columns</p> In\u00a0[144]: Copied! <pre># Visualizaci\u00f3n de los embeddings con t-SNE\ntsne = TSNE(n_components=3, perplexity=30, n_iter=300)\nX_tsne = tsne.fit_transform(nodes_embs)\n\nplt.figure(figsize=(10, 8))\n# change categorical to numerical\nfrom  sklearn.preprocessing import LabelEncoder\n# numerical_y = LabelEncoder().fit_transform(y)\nnumerical_y = y.astype('category').cat.codes\n# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\nsns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='viridis')\nplt.title('Visualizaci\u00f3n de los Embeddings con t-SNE')\nplt.show()\n</pre>  # Visualizaci\u00f3n de los embeddings con t-SNE tsne = TSNE(n_components=3, perplexity=30, n_iter=300) X_tsne = tsne.fit_transform(nodes_embs)  plt.figure(figsize=(10, 8)) # change categorical to numerical from  sklearn.preprocessing import LabelEncoder # numerical_y = LabelEncoder().fit_transform(y) numerical_y = y.astype('category').cat.codes # plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis') sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='viridis') plt.title('Visualizaci\u00f3n de los Embeddings con t-SNE') plt.show() In\u00a0[145]: Copied! <pre># Dividir los datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inicializar y entrenar el modelo RandomForest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = rf_model.predict(X_test)\n\n# Calcular la precisi\u00f3n de las predicciones\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Precisi\u00f3n del modelo RandomForest: {accuracy}')\n</pre> # Dividir los datos en conjuntos de entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Inicializar y entrenar el modelo RandomForest rf_model = RandomForestClassifier(n_estimators=100, random_state=42) rf_model.fit(X_train, y_train)  # Realizar predicciones en el conjunto de prueba y_pred = rf_model.predict(X_test)  # Calcular la precisi\u00f3n de las predicciones accuracy = accuracy_score(y_test, y_pred) print(f'Precisi\u00f3n del modelo RandomForest: {accuracy}')  <pre>Precisi\u00f3n del modelo RandomForest: 0.7601476014760148\n</pre> In\u00a0[146]: Copied! <pre>combined_df.shape\n</pre> combined_df.shape Out[146]: <pre>(2708, 1463)</pre> In\u00a0[147]: Copied! <pre># Dividir los datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(combined_df, y, test_size=0.2, random_state=42)\n\n# Inicializar y entrenar el modelo RandomForest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Realizar predicciones en el conjunto de prueba\ny_pred = rf_model.predict(X_test)\n\n# Calcular la precisi\u00f3n de las predicciones\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Precisi\u00f3n del modelo RandomForest: {accuracy}')\n</pre> # Dividir los datos en conjuntos de entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split(combined_df, y, test_size=0.2, random_state=42)  # Inicializar y entrenar el modelo RandomForest rf_model = RandomForestClassifier(n_estimators=100, random_state=42) rf_model.fit(X_train, y_train)  # Realizar predicciones en el conjunto de prueba y_pred = rf_model.predict(X_test)  # Calcular la precisi\u00f3n de las predicciones accuracy = accuracy_score(y_test, y_pred) print(f'Precisi\u00f3n del modelo RandomForest: {accuracy}')  <pre>Precisi\u00f3n del modelo RandomForest: 0.7195571955719557\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"embeddings/#prediccion-de-caracteristicas-de-nodos-utilizando-node2vec-en-analisis-de-grafos-y-redes-con-python","title":"Predicci\u00f3n de Caracter\u00edsticas de Nodos Utilizando Node2Vec en An\u00e1lisis de Grafos y Redes con Python\u00b6","text":""},{"location":"embeddings/#preparacion-del-entorno","title":"Preparaci\u00f3n del Entorno\u00b6","text":"<p>Primero, es necesario instalar las bibliotecas requeridas:</p> <p><code>networkx</code>, <code>node2vec</code>, <code>matplotlib</code> y <code>gensim</code></p> <pre>!pip install networkx node2vec matplotlib gensim\n</pre>"},{"location":"embeddings/#descarga-de-datos","title":"Descarga de Datos\u00b6","text":"<p>En este ejemplo, se utilizar\u00e1 la base de datos de redes sociales \"KARATE Club\" para demostrar la predicci\u00f3n de caracter\u00edsticas de nodos:</p> <pre>import networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.karate_club_graph()\n</pre>"},{"location":"embeddings/#aplicacion-de-node2vec","title":"Aplicaci\u00f3n de Node2Vec\u00b6","text":"<p>Ahora, vamos a aplicar Node2Vec para obtener incorporaciones de nodos significativas:</p> <pre>node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\nmodel = node2vec.fit(window=10, min_count=1, batch_words=4)\n</pre>"},{"location":"embeddings/#prediccion-de-caracteristicas-de-nodos","title":"Predicci\u00f3n de Caracter\u00edsticas de Nodos\u00b6","text":"<p>Supongamos que queremos predecir la participaci\u00f3n en clubes de los nodos. Primero, necesitamos crear un conjunto de datos etiquetado:</p> <pre>import random\n\nfor node in G.nodes():\n    G.nodes[node]['label'] = random.choice([0, 1])\n</pre> <p>Luego, definimos una funci\u00f3n para predecir la participaci\u00f3n utilizando las incorporaciones de nodos:</p> <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef predict_participation(graph, model):\n    X = [model.wv[str(node)] for node in graph.nodes()]\n    y = [graph.nodes[node]['label'] for node in graph.nodes()]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    return accuracy\n</pre> <p>5. Visualizaci\u00f3n de Resultados</p> <p>Finalmente, podemos visualizar los resultados mediante un gr\u00e1fico de dispersi\u00f3n de las incorporaciones de nodos:</p> <pre>import matplotlib.pyplot as plt\n\nembeddings = [model.wv[str(node)] for node in G.nodes()]\nX = [emb[0] for emb in embeddings]\nY = [emb[1] for emb in embeddings]\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X, Y)\n\nfor i, txt in enumerate(G.nodes()):\n    plt.annotate(txt, (X[i], Y[i]), fontsize=8, alpha=0.5)\n\nplt.title(\"Incorporaciones de Nodos utilizando Node2Vec\")\nplt.xlabel(\"Dimensi\u00f3n 1\")\nplt.ylabel(\"Dimensi\u00f3n 2\")\nplt.show()\n</pre> <p>Conclusiones</p> <p>En este post, se explor\u00f3 c\u00f3mo utilizar Node2Vec para predecir caracter\u00edsticas de nodos en un grafo. Mediante el uso de incorporaciones de nodos y t\u00e9cnicas de aprendizaje autom\u00e1tico, se logr\u00f3 una precisi\u00f3n de predicci\u00f3n decente en la participaci\u00f3n de clubes. La visualizaci\u00f3n de las incorporaciones de nodos tambi\u00e9n proporcion\u00f3 una idea de c\u00f3mo las caracter\u00edsticas de los nodos se distribuyen en un espacio de baja dimensi\u00f3n.</p> <p>A trav\u00e9s de este ejemplo, se demuestra c\u00f3mo las t\u00e9cnicas de incorporaci\u00f3n de nodos, como Node2Vec, pueden ser aplicadas de manera efectiva en el an\u00e1lisis de grafos para realizar predicciones y comprender mejor las relaciones en conjuntos de datos complejos.</p>"}]}